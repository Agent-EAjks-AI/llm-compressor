{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>New Feature: Axolotl Sparse Finetuning Integration</p> <p>Easily finetune sparse LLMs through our seamless integration with Axolotl. Learn more.</p> <p>New Feature: AutoAWQ Integration</p> <p>Perform low-bit weight-only quantization efficiently using AutoAWQ, now part of LLM Compressor. Learn more.</p>"},{"location":"#llm-compressor","title":"LLM Compressor","text":"<p>LLM Compressor is an easy-to-use library for optimizing large language models for deployment with vLLM, enabling up to 5X faster, cheaper inference. It provides a comprehensive toolkit for:</p> <ul> <li>Applying a wide variety of compression algorithms, including weight and activation quantization, pruning, and more</li> <li>Seamlessly integrating with Hugging Face Transformers, Models, and Datasets </li> <li>Using a <code>safetensors</code>-based file format for compressed model storage that is compatible with <code>vLLM</code></li> <li>Supporting performant compression of large models via <code>accelerate</code></li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Weight and Activation Quantization: Reduce model size and improve inference performance for general and server-based applications with the latest research.<ul> <li>Supported Algorithms: GPTQ, AWQ, SmoothQuant, RTN</li> <li>Supported Formats: INT W8A8, FP W8A8</li> </ul> </li> <li>Weight-Only Quantization: Reduce model size and improve inference performance for latency sensitive applications with the latest research<ul> <li>Supported Algorithms: GPTQ, AWQ, RTN</li> <li>Supported Formats: INT W4A16, INT W8A16</li> </ul> </li> <li>Weight Pruning: Reduce model size and improve inference performance for all use cases with the latest research<ul> <li>Supported Algorithms: SparseGPT, Magnitude, Sparse Finetuning</li> <li>Supported Formats: 2:4 (semi-structured), unstructured</li> </ul> </li> </ul>"},{"location":"#key-sections","title":"Key Sections","text":"<ul> <li> <p> Getting Started</p> <p>Install LLM Compressor and learn how to apply your first optimization recipe.</p> <p> Getting started</p> </li> <li> <p> Guides</p> <p>Detailed guides covering compression schemes, algorithms, and advanced usage patterns.</p> <p> Guides</p> </li> <li> <p> Examples</p> <p>Step-by-step examples for different compression techniques and model types.</p> <p> Examples</p> </li> <li> <p> Developer Resources</p> <p>Information for contributors and developers extending LLM Compressor.</p> <p> Developer Resources</p> </li> </ul>"},{"location":"developer/","title":"Developer","text":"<p>Welcome to the Developer section of LLM Compressor! This area provides essential resources for developers who want to contribute to or extend LLM Compressor. Whether you're interested in fixing bugs, adding new features, improving documentation, or understanding the project's governance, you'll find comprehensive guides to help you get started.</p> <p>LLM Compressor is an open-source project that values community contributions. We maintain high standards for code quality, documentation, and community interactions to ensure that LLM Compressor remains a robust, reliable, and user-friendly tool for compressing large language models.</p>"},{"location":"developer/#developer-resources","title":"Developer Resources","text":"<ul> <li> <p> Code of Conduct</p> <p>Our community guidelines ensure that participation in the LLM Compressor project is a positive, inclusive, and respectful experience for everyone.</p> <p> Code of Conduct</p> </li> <li> <p> Contributing Guide</p> <p>Learn how to effectively contribute to LLM Compressor, including reporting bugs, suggesting features, improving documentation, and submitting code.</p> <p> Contributing Guide</p> </li> <li> <p> Development Guide</p> <p>Detailed instructions for setting up your development environment, implementing changes, and adhering to the project's coding standards and best practices.</p> <p> Development Guide</p> </li> </ul>"},{"location":"developer/code-of-conduct/","title":"LLM Compressor Code of Conduct","text":""},{"location":"developer/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"developer/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or email address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"developer/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"developer/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"developer/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement through GitHub, Slack, or Email. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"developer/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"developer/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"developer/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"developer/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"developer/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"developer/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla\u2019s code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"developer/contributing/","title":"Contributing to LLM Compressor","text":"<p>Thank you for your interest in contributing to LLM Compressor! Our community is open to everyone and welcomes all kinds of contributions, no matter how small or large. There are several ways you can contribute to the project:</p> <ul> <li>Identify and report any issues or bugs.</li> <li>Request or add new compression methods or research.</li> <li>Suggest or implement new features.</li> </ul> <p>However, remember that contributions aren't just about code. We believe in the power of community support; thus, answering queries, assisting others, and enhancing the documentation are highly regarded and beneficial contributions.</p> <p>Finally, one of the most impactful ways to support us is by raising awareness about LLM Compressor and the vLLM community. Talk about it in your blog posts, highlighting how it's driving your incredible projects. Express your support on Twitter if vLLM aids you, or simply offer your appreciation by starring our repository.</p>"},{"location":"developer/contributing/#setup-for-development","title":"Setup for development","text":""},{"location":"developer/contributing/#install-from-source","title":"Install from source","text":"<pre><code>pip install -e ./[dev]\n</code></pre>"},{"location":"developer/contributing/#code-styling-and-formatting-checks","title":"Code Styling and Formatting checks","text":"<pre><code>make style\nmake quality\n</code></pre>"},{"location":"developer/contributing/#testing","title":"Testing","text":"<pre><code>make test\n</code></pre>"},{"location":"developer/contributing/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"developer/contributing/#issue-reporting","title":"Issue Reporting","text":"<p>If you encounter a bug or have a feature request, please check our issues page first to see if someone else has already reported it. If not, please file a new issue, providing as much relevant information as possible.</p>"},{"location":"developer/contributing/#pull-requests-code-reviews","title":"Pull Requests &amp; Code Reviews","text":"<p>Please check the PR checklist in the PR template for detailed guide for contribution.</p>"},{"location":"developer/contributing/#thank-you","title":"Thank You","text":"<p>Finally, thank you for taking the time to read these guidelines and for your interest in contributing to LLM Compressor. Your contributions make LLM Compressor a great tool for everyone!</p>"},{"location":"developer/developing/","title":"Developing LLM Compressor","text":"<p>LLM Compressor is developed and tested using Python 3.8-3.11. To develop LLM Compressor, you will also need the development dependencies and to follow the styling guidelines.</p> <p>Here are some details to get started.</p>"},{"location":"developer/developing/#basic-commands","title":"Basic Commands","text":"<p>Development Installation</p> <pre><code>git clone https://github.com/vllm-project/llm-compressor\ncd llm-compressor\npython3 -m pip install -e \"./[dev]\"\n</code></pre> <p>This will clone the LLM Compressor repo, install it, and install the development dependencies.</p> <p>Code Styling and Formatting checks</p> <pre><code>make style\nmake quality\n</code></pre> <p>This will run automatic code styling using <code>ruff</code>, <code>flake8</code>, <code>black</code>, and <code>isort</code> to test that the repository's code matches its standards.</p> <p>EXAMPLE: test changes locally</p> <pre><code>make test\n</code></pre> <p>This will run the targeted LLM Compressor unit tests for the frameworks specified.</p> <p>File any error found before changes as an Issue and fix any errors found after making changes before submitting a Pull Request.</p>"},{"location":"developer/developing/#github-workflow","title":"GitHub Workflow","text":"<ol> <li> <p>Fork the <code>llm-compressor</code> repository into your GitHub account: https://github.com/vllm-project/llm-compressor.</p> </li> <li> <p>Clone your fork of the GitHub repository, replacing <code>&lt;username&gt;</code> with your GitHub username.</p> </li> </ol> <p>Use ssh (recommended):</p> <pre><code>git clone git@github.com:&lt;username&gt;/llm-compressor.git\n</code></pre> <p>Or https:</p> <pre><code>git clone https://github.com/&lt;username&gt;/llm-compressor.git\n</code></pre> <ol> <li>Add a remote to keep up with upstream changes.</li> </ol> <pre><code>git remote add upstream https://github.com/vllm-project/llm-compressor.git\n</code></pre> <p>If you already have a copy, fetch upstream changes.</p> <pre><code>git fetch upstream\n</code></pre> <ol> <li>Create a feature branch to work in.</li> </ol> <pre><code>git checkout -b feature-xxx upstream/main\n</code></pre> <ol> <li>Work in your feature branch.</li> </ol> <pre><code>git commit -a\n</code></pre> <ol> <li>Periodically rebase your changes</li> </ol> <pre><code>git pull --rebase\n</code></pre> <ol> <li>When done, combine (\"squash\") related commits into a single one</li> </ol> <pre><code>git rebase -i upstream/main\n</code></pre> <p>This will open your editor and allow you to re-order commits and merge them:    - Re-order the lines to change commit order (to the extent possible without creating conflicts)    - Prefix commits using <code>s</code> (squash) or <code>f</code> (fixup) to merge extraneous commits.</p> <ol> <li>Submit a pull-request</li> </ol> <pre><code>git push origin feature-xxx\n</code></pre> <p>Go to your fork main page</p> <pre><code>https://github.com/&lt;username&gt;/llm-compressor\n</code></pre> <p>If you recently pushed your changes GitHub will automatically pop up a <code>Compare &amp; pull request</code> button for any branches you recently pushed to. If you click that button it will automatically offer you to submit your pull-request to the <code>vllm-project/llm-compressor</code> repository.</p> <ul> <li>Give your pull-request a meaningful title.      You'll know your title is properly formatted once the <code>Semantic Pull Request</code> GitHub check      transitions from a status of \"pending\" to \"passed\".</li> <li> <p>In the description, explain your changes and the problem they are solving.</p> </li> <li> <p>Addressing code review comments</p> </li> </ul> <p>Repeat steps 5. through 7. to address any code review comments and rebase your changes if necessary.</p> <p>Push your updated changes to update the pull request</p> <pre><code>git push origin [--force] feature-xxx\n</code></pre> <p><code>--force</code> may be necessary to overwrite your existing pull request in case your   commit history was changed when performing the rebase.</p> <p>Note: Be careful when using <code>--force</code> since you may lose data if you are not careful.</p> <pre><code>git push origin --force feature-xxx\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Welcome to the LLM Compressor examples section! Here, you'll find practical demonstrations showing how to use LLM Compressor to optimize large language models for faster and more efficient deployment with vLLM. These examples will help you understand the various compression techniques and functionalities available in LLM Compressor, making it easier to apply them to your own models.</p> <p>To explore the examples, you can either navigate through the list provided in the sidebar or click next to see the next example in the series. Each example is designed to be self-contained, with clear instructions and code snippets that you can run directly.</p>"},{"location":"examples/awq/","title":"Quantizing Models with Activation-Aware Quantization (AWQ)","text":"<p>Activation Aware Quantization (AWQ) is a state-of-the-art technique to quantize the weights of large language models which involves using a small calibration dataset to calibrate the model. The AWQ algorithm utilizes calibration data to derive scaling factors which reduce the dynamic range of weights while minimizing accuracy loss to the most salient weight values.</p> <p>The AWQ implementation found in LLM Compressor is derived from the pioneering work of AutoAWQ and with assistance from its original maintainer, @casper-hansen.</p>"},{"location":"examples/awq/#awq-recipe","title":"AWQ Recipe","text":"<p>The AWQ recipe has been inferfaced as follows, where the <code>AWQModifier</code> adjusts model scales ahead of efficient weight quantization by the <code>QuantizationModifier</code></p> <pre><code>recipe = [\n    AWQModifier(bits=4, symmetric=False),\n    QuantizationModifier(\n        ignore=[\"lm_head\"],\n        config_groups={\n            \"group_0\": QuantizationScheme(\n                targets=[\"Linear\"],\n                weights=QuantizationArgs(\n                    num_bits=4,\n                    type=QuantizationType.INT,\n                    dynamic=False,\n                    symmetric=False,\n                    strategy=QuantizationStrategy.GROUP,\n                    group_size=128,\n                ),\n            )\n        },\n    ),\n]\n</code></pre>"},{"location":"examples/awq/#compressing-your-own-model","title":"Compressing Your Own Model","text":"<p>To use your own model, start with an existing example change the <code>model_id</code> to match your own model stub. <pre><code>model_id = \"path/to/your/model\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n</code></pre></p>"},{"location":"examples/awq/#adding-mappings","title":"Adding Mappings","text":"<p>In order to target weight and activation scaling locations within the model, the <code>AWQModifier</code> must be provided an AWQ mapping. For example, the AWQ mapping for the Llama family of models looks like this:</p> <pre><code>[\n    AWQMapping(\n        \"re:.*input_layernorm\",\n        [\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"],\n    ),\n    AWQMapping(\"re:.*v_proj\", [\"re:.*o_proj\"]),\n    AWQMapping(\n        \"re:.*post_attention_layernorm\",\n        [\"re:.*gate_proj\", \"re:.*up_proj\"],\n    ),\n    AWQMapping(\n        \"re:.*up_proj\",\n        [\"re:.*down_proj\"],\n    ),\n]\n</code></pre> <p>To support other model families, you can add supply your own mappings via the <code>mappings</code> argument with instantiating the <code>AWQModifier</code>, or you can add them to the registry here (contributions are welcome!)</p>"},{"location":"examples/big_models_with_accelerate/","title":"Quantizing Big Models with HF Accelerate","text":"<p><code>llmcompressor</code> integrates with <code>accelerate</code> to support quantizing large models such as Llama 70B and 405B, or quantizing any model with limited GPU resources.</p>"},{"location":"examples/big_models_with_accelerate/#overview","title":"Overview","text":"<p><code>accelerate</code> is a highly useful library in the Hugging Face ecosystem that supports for working with large models, including: - Offloading parameters to CPU - Sharding models across multiple GPUs with pipeline-parallelism</p>"},{"location":"examples/big_models_with_accelerate/#using-device_map","title":"Using <code>device_map</code>","text":"<p>To enable <code>accelerate</code> features with <code>llmcompressor</code>, simple insert <code>device_map</code> in <code>from_pretrained</code> during model load.</p> <pre><code>from transformers import AutoModelForCausalLM\nMODEL_ID = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\n# device_map=\"auto\" triggers usage of accelerate\n# if &gt; 1 GPU, the model will be sharded across the GPUs\n# if not enough GPU memory to fit the model, parameters are offloaded to the CPU\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\")\n</code></pre> <p><code>llmcompressor</code> is designed to respect the <code>device_map</code>, so calls to <code>oneshot</code>  will work properly out of the box for basic quantization with <code>QuantizationModifier</code>, even for CPU offloaded models. </p> <p>To enable CPU offloading for second-order quantization methods such as GPTQ, we need to  allocate additional memory upfront when computing the device map. Not doing so risks potentially going out-of-memory.</p> <pre><code>from llmcompressor.transformers.compression.helpers import calculate_offload_device_map\nfrom transformers import AutoModelForCausalLM\nMODEL_ID = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\n# Load model, reserving memory in the device map for sequential GPTQ (adjust num_gpus as needed)\ndevice_map = calculate_offload_device_map(MODEL_ID, reserve_for_hessians=True, num_gpus=1)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map=device_map,\n    torch_dtype=\"auto\",\n)\n</code></pre>"},{"location":"examples/big_models_with_accelerate/#practical-advice","title":"Practical Advice","text":"<p>When working with <code>accelerate</code>, it is important to keep in mind that CPU offloading and naive pipeline-parallelism will slow down forward passes through the model. As a result, we need to take care to ensure that the quantization methods used fit well with the offloading scheme as methods that require many forward passes though the model will be slowed down. If more gpu memory is not available, consider reducing the precision of the loaded model to a lower-width dtype such as <code>torch.bfloat16</code>.</p>"},{"location":"examples/big_models_with_accelerate/#examples","title":"Examples","text":"<p>We will show working examples for each use case: - CPU Offloading: Quantize <code>Llama-70B</code> to <code>FP8</code> using <code>PTQ</code> with a single GPU - Multi-GPU: Quantize <code>Llama-70B</code> to <code>INT8</code> using <code>GPTQ</code> and <code>SmoothQuant</code> with 2 GPUs</p>"},{"location":"examples/big_models_with_accelerate/#installation","title":"Installation","text":"<p>Install <code>llmcompressor</code>:</p> <pre><code>pip install llmcompressor\n</code></pre>"},{"location":"examples/big_models_with_accelerate/#cpu-offloading-fp8-quantization-with-ptq","title":"CPU Offloading: <code>FP8</code> Quantization with <code>PTQ</code>","text":"<p>CPU offloading is slow. As a result, we recommend using this feature only with data-free quantization methods. For example, when quantizing a model to <code>fp8</code>, we typically use simple <code>PTQ</code> to statically quantize the weights and use dynamic quantization for the activations. These methods do not require calibration data.</p> <ul> <li><code>cpu_offloading_fp8.py</code> demonstrates quantizing the weights and activations of <code>Llama-70B</code> to <code>fp8</code> on a single GPU:</li> </ul> <pre><code>export CUDA_VISIBLE_DEVICES=0\npython cpu_offloading_fp8.py\n</code></pre> <p>The resulting model <code>./Meta-Llama-3-70B-Instruct-FP8-Dynamic</code> is ready to run with <code>vllm</code>!</p>"},{"location":"examples/big_models_with_accelerate/#multi-gpu-int8-quantization-with-gptq","title":"Multi-GPU: <code>INT8</code> Quantization with <code>GPTQ</code>","text":"<p>For quantization methods that require calibration data (e.g. <code>GPTQ</code>), CPU offloading is too slow. For these methods, <code>llmcompressor</code> can use <code>accelerate</code> multi-GPU to quantize models that are larger than a single GPU. For example, when quantizing a model to <code>int8</code>, we typically use <code>GPTQ</code> to statically quantize the weights, which requires calibration data.</p> <ul> <li><code>multi_gpu_int8.py</code> demonstrates quantizing the weights and activations of <code>Llama-70B</code> to <code>int8</code> on 2 A100s:</li> </ul> <pre><code>export CUDA_VISIBLE_DEVICES=0,1\npython multi_gpu_int8.py\n</code></pre> <p>The resulting model <code>./Meta-Llama-3-70B-Instruct-INT8-Dynamic</code> is quantized and ready to run with <code>vllm</code>!</p>"},{"location":"examples/big_models_with_accelerate/#questions-or-feature-request","title":"Questions or Feature Request?","text":"<p>Please open up an issue on <code>vllm-project/llm-compressor</code></p>"},{"location":"examples/multimodal_audio/","title":"Quantizing Multimodal Audio Models","text":"<p>https://github.com/user-attachments/assets/6732c60b-1ebe-4bed-b409-c16c4415dff5</p> <p>Audio provided by Daniel Galvez et al. under creative commons license</p> <p><pre><code>&lt;|startoftranscript|&gt; &lt;|en|&gt;\n...\n\n&lt;|transcribe|&gt; &lt;|notimestamps|&gt;\nthat's where you have a lot of windows in the south no actually that's passive solar\nand passive solar is something that was developed and designed in the 1960s and 70s\nand it was a great thing for what it was at the time but it's not a passive house\n</code></pre> </p> <p>This directory contains example scripts for quantizing a variety of audio language models using the GPTQ quantization.</p>"},{"location":"examples/multimodal_audio/#compressing-your-own-model","title":"Compressing Your Own Model","text":"<p>To use your own multimodal modal, start with an existing example change the <code>model_id</code> to match your own model stub. <pre><code>model_id = \"path/to/your/model\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n</code></pre></p>"},{"location":"examples/multimodal_audio/#customizing-gptqmodifier-parameters","title":"Customizing GPTQModifier Parameters","text":"<p>The GPTQModifier is the modifier responsible for performing quantization of the model weights. For more information on quantizing with different weight schemes, see the <code>quantization_</code> examples in the examples folder.</p> <pre><code>recipe = [\n    GPTQModifier(\n        targets=\"Linear\",\n        scheme=\"W4A16\",\n        sequential_targets=[\"WhisperEncoderLayer\", \"WhisperDecoderLayer\"],\n        ignore=[\"lm_head\"],\n    )\n]\n</code></pre>"},{"location":"examples/multimodal_audio/#sequential-targets","title":"Sequential Targets","text":"<p>Sequential targets are the modules which determine the granularity of error propagation and activation offloading when performing forward passes of the model. These are typically the \"transformer blocks\" of the model, also referred to as \"layers\" with llm-compressor.</p> <p>Choosing sequential targets with higher granularity (for example \"Linear\" instead of \"LlamaDecoderLayer\") will result in fewer hessians being allocated at the same time, decreasing the memory requirements for compression. This may also increase the recovered accuracy of the model, as compression error is propagated at a higher granularity. However, using higher granularity sequential targets may also increase compression time, as more time is spent offloading and onloading activations.</p>"},{"location":"examples/multimodal_audio/#ignore","title":"Ignore","text":"<p>If your model is not traceable for your desired dataset, first consider adding any problematic modules to the ignore list. Doing this prevents the model tracer from tracing the internals of those modules, thereby avoid the untraceable operations.</p>"},{"location":"examples/multimodal_audio/#tracing-errors","title":"Tracing Errors","text":"<p>Because the architectures of audio-language models is often times more complex than those of typical decoder-only text models, you may encounter <code>torch.fx.TraceError</code>s when attempting to quantize your model. For more information on <code>torch.fx.TraceError</code>s, why they occur, and how to resolve them, please see the Model Tracing Guide.</p>"},{"location":"examples/multimodal_audio/#adding-your-own-smoothquant-mappings","title":"Adding Your Own Smoothquant Mappings","text":"<p>For a guide on adding smoothquant mappings for your dataset, see the SmoothQuant Guide.</p>"},{"location":"examples/multimodal_audio/#adding-your-own-data-collator","title":"Adding Your Own Data Collator","text":"<p>Most examples utilize a generic <code>data_collator</code> which correctly correlates data for most multimodal datasets. If you find that your model needs custom data collation (as is the case with pixtral), you can modify this function to reflect these model-specific requirements.</p>"},{"location":"examples/multimodal_audio/#sample-audio-provided-under-a-creative-commons-attribution-license","title":"Sample Audio Provided Under a Creative Commons Attribution License","text":"<p>https://creativecommons.org/licenses/by/4.0/legalcode <pre><code>@article{DBLP:journals/corr/abs-2111-09344,\n  author    = {Daniel Galvez and\n               Greg Diamos and\n               Juan Ciro and\n               Juan Felipe Cer{\\'{o}}n and\n               Keith Achorn and\n               Anjali Gopi and\n               David Kanter and\n               Maximilian Lam and\n               Mark Mazumder and\n               Vijay Janapa Reddi},\n  title     = {The People's Speech: {A} Large-Scale Diverse English Speech Recognition\n               Dataset for Commercial Usage},\n  journal   = {CoRR},\n  volume    = {abs/2111.09344},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2111.09344},\n  eprinttype = {arXiv},\n  eprint    = {2111.09344},\n  timestamp = {Mon, 22 Nov 2021 16:44:07 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09344.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre></p>"},{"location":"examples/multimodal_vision/","title":"Quantizing Multimodal Vision-Language Models","text":"<p> <p><pre><code>&lt;|system|&gt;\nYou are a helpful assistant.\n\n&lt;|user|&gt;\nPlease describe the animal in this image\n\n&lt;|assistant|&gt;\nThe animal in the image is a white kitten.\nIt has a fluffy coat and is resting on a white keyboard.\nThe kitten appears to be comfortable and relaxed, possibly enjoying the warmth of the keyboard.\n</code></pre> </p> <p>This directory contains example scripts for quantizing a variety of vision-language models using the GPTQ quantization. Most examples do not demonstrate quantizing separate vision encoder parameters if they exist, as compressing these parameters offers little benefit with respect to performance-accuracy tradeoff.</p>"},{"location":"examples/multimodal_vision/#compressing-your-own-model","title":"Compressing Your Own Model","text":"<p>To use your own multimodal modal, start with an existing example change the <code>model_id</code> to match your own model stub. <pre><code>model_id = \"path/to/your/model\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n</code></pre></p>"},{"location":"examples/multimodal_vision/#customizing-gptqmodifier-parameters","title":"Customizing GPTQModifier Parameters","text":"<p>The GPTQModifier is the modifier responsible for performing quantization of the model weights. For more information on quantizing with different weight schemes, see the <code>quantization_</code> examples in the examples folder.</p> <pre><code>recipe = [\n    GPTQModifier(\n        targets=\"Linear\",\n        scheme=\"W4A16\",\n        sequential_targets=[\"MistralDecoderLayer\"],\n        ignore=[\"re:.*lm_head\", \"re:vision_tower.*\", \"re:multi_modal_projector.*\"],\n    ),\n]\n</code></pre>"},{"location":"examples/multimodal_vision/#sequential-targets","title":"Sequential Targets","text":"<p>Sequential targets are the modules which determine the granularity of error propagation and activation offloading when performing forward passes of the model. These are typically the \"transformer blocks\" of the model, also referred to as \"layers\" with llm-compressor.</p> <p>Choosing sequential targets with higher granularity (for example \"Linear\" instead of \"LlamaDecoderLayer\") will result in fewer hessians being allocated at the same time, decreasing the memory requirements for compression. This may also increase the recovered accuracy of the model, as compression error is propagated at a higher granularity. However, using higher granularity sequential targets may also increase compression time, as more time is spent offloading and onloading activations.</p>"},{"location":"examples/multimodal_vision/#ignore","title":"Ignore","text":"<p>If your model is not traceable for your desired dataset, first consider adding any problematic modules to the ignore list. Doing this prevents the model tracer from tracing the internals of those modules, thereby avoid the untraceable operations.</p>"},{"location":"examples/multimodal_vision/#tracing-errors","title":"Tracing Errors","text":"<p>Because the architectures of vision-language models is often times more complex than those of typical decoder-only text models, you may encounter <code>torch.fx.TraceError</code>s when attempting to quantize your model. For more information on <code>torch.fx.TraceError</code>s, why they occur, and how to resolve them, please see the Model Tracing Guide.</p>"},{"location":"examples/multimodal_vision/#adding-your-own-smoothquant-mappings","title":"Adding Your Own Smoothquant Mappings","text":"<p>For a guide on adding smoothquant mappings for your dataset, see the SmoothQuant Guide.</p>"},{"location":"examples/multimodal_vision/#adding-your-own-data-collator","title":"Adding Your Own Data Collator","text":"<p>Most examples utilize a generic <code>data_collator</code> which correctly correlates data for most multimodal datasets. If you find that your model needs custom data collation (as is the case with pixtral), you can modify this function to reflect these model-specific requirements.</p>"},{"location":"examples/multimodal_vision/#sample-image-provided-under-a-creative-commons-attribution-license","title":"Sample Image Provided Under a Creative Commons Attribution License","text":"<p>https://creativecommons.org/licenses/by/4.0/legalcode <pre><code>@article{cocodataset,\n  author    = {Tsung{-}Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{'{a} }r and C. Lawrence Zitnick},\n  title     = {Microsoft {COCO:} Common Objects in Context},\n  journal   = {CoRR},\n  volume    = {abs/1405.0312},\n  year      = {2014},\n  url       = {http://arxiv.org/abs/1405.0312},\n  archivePrefix = {arXiv},\n  eprint    = {1405.0312},\n  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre></p>"},{"location":"examples/quantization_2of4_sparse_w4a16/","title":"<code>int4</code> Weight Quantization of a 2:4 Sparse Model","text":"<p><code>llm-compressor</code> supports quantizing weights while maintaining sparsity patterns for memory savings and inference acceleration with <code>vLLM</code></p> <p><code>2:4 sparisty + int4/int8</code> mixed precision computation is supported in vLLM on Nvidia capability &gt; 8.0 (Ampere, Ada Lovelace, Hopper).</p>"},{"location":"examples/quantization_2of4_sparse_w4a16/#installation","title":"Installation","text":"<p>To get started, install:</p> <pre><code>git clone https://github.com/vllm-project/llm-compressor.git\ncd llm-compressor\npip install -e .\n</code></pre>"},{"location":"examples/quantization_2of4_sparse_w4a16/#quickstart","title":"Quickstart","text":"<p>The example includes an end-to-end script for applying the quantization algorithm.</p> <pre><code>python3 llama7b_sparse_w4a16.py\n</code></pre>"},{"location":"examples/quantization_2of4_sparse_w4a16/#creating-a-sparse-quantized-llama7b-model","title":"Creating a Sparse Quantized Llama7b Model","text":"<p>This example uses LLMCompressor and Compressed-Tensors to create a 2:4 sparse and quantized Llama2-7b model. The model is calibrated and trained with the ultachat200k dataset. At least 75GB of GPU memory is required to run this example.</p> <p>Follow the steps below, or to run the example as <code>python examples/quantization_2of4_sparse_w4a16/llama7b_sparse_w4a16.py</code></p>"},{"location":"examples/quantization_2of4_sparse_w4a16/#step-1-select-a-model-dataset-and-recipe","title":"Step 1: Select a model, dataset, and recipe","text":"<p>In this step, we select which model to use as a baseline for sparsification, a dataset to use for calibration and finetuning, and a recipe.</p> <p>Models can reference a local directory, or a model in the huggingface hub.</p> <p>Datasets can be from a local compatible directory or the huggingface hub.</p> <p>Recipes are YAML files that describe how a model should be optimized during or after training. The recipe used for this flow is located in 2of4_w4a16_recipe.yaml. It contains instructions to prune the model to 2:4 sparsity, run one epoch of recovery finetuning, and quantize to 4 bits in one show using GPTQ.</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM\n\nmodel_stub = \"neuralmagic/Llama-2-7b-ultrachat200k\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_stub, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n\ndataset = \"ultrachat-200k\"\nsplits = {\"calibration\": \"train_gen[:5%]\", \"train\": \"train_gen\"}\n\nrecipe = \"2of4_w4a16_recipe.yaml\"\n</code></pre>"},{"location":"examples/quantization_2of4_sparse_w4a16/#step-2-run-sparsification-using-apply","title":"Step 2: Run sparsification using <code>apply</code>","text":"<p>The <code>apply</code> function applies the given recipe to our model and dataset. The hardcoded kwargs may be altered based on each model's needs. After running, the sparsified model will be saved to <code>output_llama7b_2of4_w4a16_channel</code>.</p> <pre><code>from llmcompressor.transformers import apply\n\noutput_dir = \"output_llama7b_2of4_w4a16_channel\"\n\napply(\n    model=model,\n    dataset=dataset,\n    recipe=recipe,\n    bf16=False,  # use full precision for training\n    output_dir=output_dir,\n    splits=splits,\n    max_seq_length=512,\n    num_calibration_samples=512,\n    num_train_epochs=0.5,\n    logging_steps=500,\n    save_steps=5000,\n    gradient_checkpointing=True,\n    learning_rate=0.0001,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n)\n</code></pre>"},{"location":"examples/quantization_2of4_sparse_w4a16/#custom-quantization","title":"Custom Quantization","text":"<p>The current repo supports multiple quantization techniques configured using a recipe. Supported strategies are <code>tensor</code>, <code>group</code> and <code>channel</code>.  The above recipe (<code>2of4_w4a16_recipe.yaml</code>) uses channel-wise quantization specified by <code>strategy: \"channel\"</code> in its config group.  To use quantize per tensor, change strategy from <code>channel</code> to <code>tensor</code>. To use group size quantization, change from <code>channel</code> to <code>group</code> and specify its value, say 128, by including <code>group_size: 128</code>. A group size quantization example is shown in <code>2of4_w4a16_group-128_recipe.yaml</code>.</p>"},{"location":"examples/quantization_kv_cache/","title":"<code>fp8</code> Weight, Activation, and KV Cache Quantization","text":"<p><code>llmcompressor</code> now supports quantizing weights, activations, and KV cache to <code>fp8</code> for memory savings and inference acceleration with <code>vllm</code>.</p> <p><code>fp8</code> computation is supported on NVIDIA GPUs with compute capability &gt; 8.9 (Ada Lovelace, Hopper).</p>"},{"location":"examples/quantization_kv_cache/#installation","title":"Installation","text":"<p>To get started, install llmcompressor from source as this feature is new:</p> <pre><code>pip install git+https://github.com/vllm-project/llm-compressor.git@cb98f34d4ec9dd175e6995d12fb02dec39c6f27a\n</code></pre>"},{"location":"examples/quantization_kv_cache/#quickstart","title":"Quickstart","text":"<p>The example includes an end-to-end script for applying the quantization algorithm:</p> <pre><code>python3 llama3_fp8_kv_example.py\n</code></pre> <p>The resulting model <code>Meta-Llama-3-8B-Instruct-FP8-KV</code> is ready to be loaded into vLLM.</p>"},{"location":"examples/quantization_kv_cache/#code-walkthrough","title":"Code Walkthrough","text":"<p>Let's walk through the main steps of the quantization process:</p> <ol> <li>Load model</li> <li>Prepare calibration data</li> <li>Apply quantization</li> <li>Evaluate and save the model</li> </ol>"},{"location":"examples/quantization_kv_cache/#1-load-model","title":"1. Load Model","text":"<p>Load the model using <code>AutoModelForCausalLM</code>:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n</code></pre>"},{"location":"examples/quantization_kv_cache/#2-prepare-calibration-data","title":"2. Prepare Calibration Data","text":"<p>Prepare the calibration data using the <code>ultrachat</code> dataset:</p> <pre><code>from datasets import load_dataset\n\nDATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\nDATASET_SPLIT = \"train_sft\"\nNUM_CALIBRATION_SAMPLES = 512\nMAX_SEQUENCE_LENGTH = 2048\n\nds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\nds = ds.shuffle(seed=42)\n\ndef process_and_tokenize(example):\n    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n    return tokenizer(text, padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\n\nds = ds.map(process_and_tokenize, remove_columns=ds.column_names)\n</code></pre>"},{"location":"examples/quantization_kv_cache/#3-apply-quantization","title":"3. Apply Quantization","text":"<p>Configure and apply the FP8 quantization for weights, activations, and KV cache. Notice the new <code>kv_cache_scheme</code> section:</p> <pre><code>from llmcompressor import oneshot\n\nrecipe = \"\"\"\nquant_stage:\n    quant_modifiers:\n        QuantizationModifier:\n            ignore: [\"lm_head\"]\n            config_groups:\n                group_0:\n                    weights:\n                        num_bits: 8\n                        type: float\n                        strategy: tensor\n                        dynamic: false\n                        symmetric: true\n                    input_activations:\n                        num_bits: 8\n                        type: float\n                        strategy: tensor\n                        dynamic: false\n                        symmetric: true\n                    targets: [\"Linear\"]\n            kv_cache_scheme:\n                num_bits: 8\n                type: float\n                strategy: tensor\n                dynamic: false\n                symmetric: true\n\"\"\"\n\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    max_seq_length=MAX_SEQUENCE_LENGTH,\n    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n</code></pre>"},{"location":"examples/quantization_kv_cache/#4-evaluate-and-save-the-model","title":"4. Evaluate and Save the Model","text":"<p>Test the quantized model with a sample generation:</p> <pre><code>input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\noutput = model.generate(input_ids, max_new_tokens=100)\nprint(tokenizer.decode(output[0]))\n</code></pre> <p>Save the quantized model:</p> <pre><code>SAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-KV\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n</code></pre> <p>For running the model in vLLM, make sure to specify the <code>kv_cache_dtype=\"fp8\"</code> argument to enable quantization of the kv cache, and thus usage of your calibrated scales.</p>"},{"location":"examples/quantization_kv_cache/#evaluating-accuracy","title":"Evaluating Accuracy","text":"<p>To evaluate the accuracy of your quantized model:</p> <ol> <li>Install <code>vllm</code> and <code>lm-evaluation-harness</code>:</li> </ol> <pre><code>pip install \"vllm&gt;=0.5.5\" lm_eval==0.4.3\n</code></pre> <ol> <li>Run an evaluation (e.g., on GSM-8K):</li> </ol> <pre><code>MODEL=$PWD/Meta-Llama-3-8B-Instruct-FP8-KV\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=$MODEL,kv_cache_dtype=fp8,add_bos_token=True \\\n  --tasks gsm8k --num_fewshot 5 --batch_size auto\n</code></pre> <pre><code>vllm (pretrained=Meta-Llama-3-8B-Instruct-FP8-KV,kv_cache_dtype=fp8,add_bos_token=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.7748|\u00b1  |0.0115|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.7763|\u00b1  |0.0115|\n</code></pre> <p>Note: Include <code>add_bos_token=True</code> as quantized models can be sensitive to the presence of the <code>bos</code> token.</p>"},{"location":"examples/quantization_kv_cache/#questions-or-feature-requests","title":"Questions or Feature Requests?","text":"<p>Please open an issue on <code>vllm-project/llm-compressor</code>.</p>"},{"location":"examples/quantization_w4a16/","title":"<code>int4</code> Weight Quantization","text":"<p><code>llm-compressor</code> supports quantizing weights to <code>int4</code> for memory savings and inference acceleration with <code>vLLM</code></p> <p><code>int4</code> mixed precision computation is supported on Nvidia GPUs with compute capability &gt; 8.0 (Ampere, Ada Lovelace, Hopper).</p>"},{"location":"examples/quantization_w4a16/#installation","title":"Installation","text":"<p>To get started, install:</p> <pre><code>git clone https://github.com/vllm-project/llm-compressor.git\ncd llm-compressor\npip install -e .\n</code></pre>"},{"location":"examples/quantization_w4a16/#quickstart","title":"Quickstart","text":"<p>The example includes an end-to-end script for applying the quantization algorithm.</p> <pre><code>python3 llama3_example.py\n</code></pre> <p>The resulting model <code>Meta-Llama-3-8B-Instruct-W4A16-G128</code> is ready to be loaded into vLLM.</p>"},{"location":"examples/quantization_w4a16/#code-walkthough","title":"Code Walkthough","text":"<p>Now, we will step though the code in the example. There are four steps: 1) Load model 2) Prepare calibration data 3) Apply quantization 4) Evaluate accuracy in vLLM</p>"},{"location":"examples/quantization_w4a16/#1-load-model","title":"1) Load Model","text":"<p>Load the model using <code>AutoModelForCausalLM</code> for handling quantized saving and loading. </p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n</code></pre>"},{"location":"examples/quantization_w4a16/#2-prepare-calibration-data","title":"2) Prepare Calibration Data","text":"<p>Prepare the calibration data. When quantizing weigths of a model to <code>int4</code> using GPTQ, we need some sample data to run the GPTQ algorithms. As a result, it is very useful to use calibration data that closely matches the type of data used in deployment. If you have fine-tuned a model, using a sample of your training data is a good idea.</p> <p>In our case, we are quantizing an Instruction tuned generic model, so we will use the <code>ultrachat</code> dataset. Some best practices include: * 512 samples is a good place to start (increase if accuracy drops) * 2048 sequence length is a good place to start * Use the chat template or instrucion template that the model is trained with</p> <pre><code>from datasets import load_dataset\n\nNUM_CALIBRATION_SAMPLES=512\nMAX_SEQUENCE_LENGTH=2048\n\n# Load dataset.\nds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\nds = ds.shuffle(seed=42)\n\n# Preprocess the data into the format the model is trained with.\ndef preprocess(example):\n    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False,)}\nds = ds.map(preprocess)\n\n# Tokenize the data (be careful with bos tokens - we need add_special_tokens=False since the chat_template already added it).\ndef tokenize(sample):\n    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\nds = ds.map(tokenize, remove_columns=ds.column_names)\n</code></pre>"},{"location":"examples/quantization_w4a16/#3-apply-quantization","title":"3) Apply Quantization","text":"<p>With the dataset ready, we will now apply quantization.</p> <p>We first select the quantization algorithm.</p> <p>In our case, we will apply the default GPTQ recipe for <code>int4</code> (which uses static group size 128 scales) to all linear layers.</p> <p>See the <code>Recipes</code> documentation for more information on making complex recipes</p> <pre><code>from llmcompressor import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\n\n# Configure the quantization algorithm to run.\nrecipe = GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"])\n\n# Apply quantization.\noneshot(\n    model=model, dataset=ds,\n    recipe=recipe,\n    max_seq_length=MAX_SEQUENCE_LENGTH,\n    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\n# Save to disk compressed.\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-W4A16-G128\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n</code></pre> <p>We have successfully created an <code>int4</code> model!</p>"},{"location":"examples/quantization_w4a16/#4-evaluate-accuracy","title":"4) Evaluate Accuracy","text":"<p>With the model created, we can now load and run in vLLM (after installing).</p> <pre><code>from vllm import LLM\nmodel = LLM(\"./Meta-Llama-3-8B-Instruct-W4A16-G128\")\n</code></pre> <p>We can evaluate accuracy with <code>lm_eval</code> (<code>pip install lm_eval==v0.4.3</code>):</p> <p>Note: quantized models can be sensitive to the presence of the <code>bos</code> token. <code>lm_eval</code> does not add a <code>bos</code> token by default, so make sure to include the <code>add_bos_token=True</code> argument when running your evaluations.</p> <p>Run the following to test accuracy on GSM-8K:</p> <pre><code>lm_eval --model vllm \\\n  --model_args pretrained=\"./Meta-Llama-3-8B-Instruct-W4A16-G128\",add_bos_token=true \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --limit 250 \\\n  --batch_size 'auto'\n</code></pre> <p>We can see the resulting scores look good!</p> <pre><code>|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.728|\u00b1  |0.0282|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.720|\u00b1  |0.0285|\n</code></pre>"},{"location":"examples/quantization_w4a16/#questions-or-feature-request","title":"Questions or Feature Request?","text":"<p>Please open up an issue on <code>vllm-project/llm-compressor</code></p>"},{"location":"examples/quantization_w8a8_fp8/","title":"<code>fp8</code> Weight and Activation Quantization","text":"<p><code>llmcompressor</code> supports quantizing weights and activations to <code>fp8</code> for memory savings and inference acceleration with <code>vllm</code></p> <p><code>fp8</code> compuation is supported on Nvidia GPUs with compute capability &gt; 8.9 (Ada Lovelace, Hopper).</p>"},{"location":"examples/quantization_w8a8_fp8/#installation","title":"Installation","text":"<p>To get started, install:</p> <pre><code>pip install llmcompressor\n</code></pre>"},{"location":"examples/quantization_w8a8_fp8/#quickstart","title":"Quickstart","text":"<p>The example includes an end-to-end script for applying the quantization algorithm.</p> <pre><code>python3 llama3_example.py\n</code></pre> <p>The resulting model <code>Meta-Llama-3-8B-Instruct-FP8-Dynamic</code> is ready to be loaded into vLLM.</p>"},{"location":"examples/quantization_w8a8_fp8/#code-walkthough","title":"Code Walkthough","text":"<p>Now, we will step though the code in the example. There are three steps: 1) Load model 2) Apply quantization 3) Evaluate accuracy in vLLM</p>"},{"location":"examples/quantization_w8a8_fp8/#1-load-model","title":"1) Load Model","text":"<p>Load the model using <code>AutoModelForCausalLM</code></p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n  MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n</code></pre>"},{"location":"examples/quantization_w8a8_fp8/#2-apply-quantization","title":"2) Apply Quantization","text":"<p>For <code>fp8</code> quantization, we can recover accuracy with simple PTQ quantization.</p> <p>We recommend targeting all <code>Linear</code> layers using the <code>FP8_DYNAMIC</code> scheme, which uses: - Static, per-channel quantization on the weights - Dynamic, per-token quantization on the activations</p> <p>Since simple PTQ does not require data for weight quantization and the activations are quantized dynamically, we do not need any calibration data for this quantization flow.</p> <pre><code>from llmcompressor import oneshot\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\n\n# Configure the simple PTQ quantization\nrecipe = QuantizationModifier(\n  targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n\n# Apply the quantization algorithm.\noneshot(model=model, recipe=recipe)\n\n# Save the model.\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-Dynamic\"\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n</code></pre> <p>We have successfully created an <code>fp8</code> model!</p>"},{"location":"examples/quantization_w8a8_fp8/#3-evaluate-accuracy","title":"3) Evaluate Accuracy","text":"<p>Install <code>vllm</code> and <code>lm-evaluation-harness</code>:</p> <pre><code>pip install vllm lm_eval==0.4.3\n</code></pre> <p>Load and run the model in <code>vllm</code>:</p> <pre><code>from vllm import LLM\nmodel = LLM(\"./Meta-Llama-3-8B-Instruct-FP8-Dynamic\")\nmodel.generate(\"Hello my name is\")\n</code></pre> <p>Evaluate accuracy with <code>lm_eval</code> (for example on 250 samples of <code>gsm8k</code>):</p> <p>Note: quantized models can be sensitive to the presence of the <code>bos</code> token. <code>lm_eval</code> does not add a <code>bos</code> token by default, so make sure to include the <code>add_bos_token=True</code> argument when running your evaluations.</p> <pre><code>MODEL=$PWD/Meta-Llama-3-8B-Instruct-FP8-Dynamic \nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=$MODEL,add_bos_token=True \\\n  --tasks gsm8k  --num_fewshot 5 --batch_size auto --limit 250\n</code></pre> <p>We can see the resulting scores look good:</p> <pre><code>|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.768|\u00b1  |0.0268|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.768|\u00b1  |0.0268|\n</code></pre>"},{"location":"examples/quantization_w8a8_fp8/#questions-or-feature-request","title":"Questions or Feature Request?","text":"<p>Please open up an issue on <code>vllm-project/llm-compressor</code></p>"},{"location":"examples/quantization_w8a8_int8/","title":"<code>int8</code> Weight and Activation Quantization","text":"<p><code>llm-compressor</code> supports quantizing weights and activations to <code>int</code> for memory savings and inference acceleration with <code>vLLM</code></p> <p><code>int8</code> compuation is supported on Nvidia GPUs with compute capability &gt; 7.5 (Turing, Ampere, Ada Lovelace, Hopper).</p>"},{"location":"examples/quantization_w8a8_int8/#installation","title":"Installation","text":"<p>To get started, install:</p> <pre><code>pip install llmcompressor\n</code></pre>"},{"location":"examples/quantization_w8a8_int8/#quickstart","title":"Quickstart","text":"<p>The example includes an end-to-end script for applying the quantization algorithm.</p> <pre><code>python3 llama3_example.py\n</code></pre> <p>The resulting model <code>Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token</code> is ready to be loaded into vLLM.</p>"},{"location":"examples/quantization_w8a8_int8/#code-walkthough","title":"Code Walkthough","text":"<p>Now, we will step though the code in the example. There are four steps: 1) Load model 2) Prepare calibration data 3) Apply quantization 4) Evaluate accuracy in vLLM</p>"},{"location":"examples/quantization_w8a8_int8/#1-load-model","title":"1) Load Model","text":"<p>Load the model using <code>AutoModelForCausalLM</code> for handling quantized saving and loading.</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n</code></pre>"},{"location":"examples/quantization_w8a8_int8/#2-prepare-calibration-data","title":"2) Prepare Calibration Data","text":"<p>Prepare the calibration data. When quantizing activations of a model to <code>int8</code>, we need some sample data to estimate the activation scales. As a result, it is very useful to use calibration data that closely matches the type of data used in deployment. If you have fine-tuned a model, using a sample of your training data is a good idea.</p> <p>In our case, we are quantizing an Instruction tuned generic model, so we will use the <code>ultrachat</code> dataset. Some best practices include: * 512 samples is a good place to start (increase if accuracy drops) * 2048 sequence length is a good place to start * Use the chat template or instrucion template that the model is trained with</p> <pre><code>from datasets import load_dataset\n\nNUM_CALIBRATION_SAMPLES=512\nMAX_SEQUENCE_LENGTH=2048\n\n# Load dataset.\nds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\nds = ds.shuffle(seed=42)\n\n# Preprocess the data into the format the model is trained with.\ndef preprocess(example):\n    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False,)}\nds = ds.map(preprocess)\n\n# Tokenize the data (be careful with bos tokens - we need add_special_tokens=False since the chat_template already added it).\ndef tokenize(sample):\n    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\nds = ds.map(tokenize, remove_columns=ds.column_names)\n</code></pre>"},{"location":"examples/quantization_w8a8_int8/#3-apply-quantization","title":"3) Apply Quantization","text":"<p>With the dataset ready, we will now apply quantization.</p> <p>We first select the quantization algorithm. For W8A8, we want to: * Run SmoothQuant to make the activations easier to quantize * Quantize the weights to 8 bits with channelwise scales using GPTQ * Quantize the activations with dynamic per token strategy</p> <p>See the <code>Recipes</code> documentation for more information on recipes</p> <pre><code>from llmcompressor import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n\n# Configure the quantization algorithms to run.\nrecipe = [\n    SmoothQuantModifier(smoothing_strength=0.8),\n    GPTQModifier(targets=\"Linear\", scheme=\"W8A8\", ignore=[\"lm_head\"]),\n]\n\n# Apply quantization.\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    max_seq_length=MAX_SEQUENCE_LENGTH,\n    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\n# Save to disk compressed.\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-W8A8-Dynamic-Per-Token\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n</code></pre> <p>We have successfully created an <code>w8a8</code> model with weights and activations quantized to 8-bit integers!</p>"},{"location":"examples/quantization_w8a8_int8/#4-evaluate-accuracy","title":"4) Evaluate Accuracy","text":"<p>With the model created, we can now load and run in vLLM (after installing).</p> <pre><code>from vllm import LLM\nmodel = LLM(\"./Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\")\n</code></pre> <p>We can evaluate accuracy with <code>lm_eval</code> (<code>pip install lm_eval==v0.4.3</code>):</p> <p>Note: quantized models can be sensitive to the presence of the <code>bos</code> token. <code>lm_eval</code> does not add a <code>bos</code> token by default, so make sure to include the <code>add_bos_token=True</code> argument when running your evaluations.</p> <p>Run the following to test accuracy on GSM-8K:</p> <pre><code>lm_eval --model vllm \\\n  --model_args pretrained=\"./Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\",add_bos_token=true \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --limit 250 \\\n  --batch_size 'auto'\n</code></pre> <p>We can see the resulting scores look good!</p> <pre><code>|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.752|\u00b1  |0.0274|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.756|\u00b1  |0.0272|\n</code></pre>"},{"location":"examples/quantization_w8a8_int8/#questions-or-feature-request","title":"Questions or Feature Request?","text":"<p>Please open up an issue on <code>vllm-project/llm-compressor</code></p>"},{"location":"examples/quantizing_moe/","title":"Quantizing Mixtral-8x7B-Instruct-v0.1 Model with FP8","text":"<p>This directory contains an example script for quantizing the <code>Mixtral-8x7B-Instruct-v0.1</code> model using the static per-tensor FP8 quantization scheme.</p>"},{"location":"examples/quantizing_moe/#installation","title":"Installation","text":"<p>To get started, install the necessary dependencies by executing the following commands:</p> <pre><code>git clone https://github.com/vllm-project/llm-compressor.git\ncd llm-compressor\npip install -e .\n</code></pre>"},{"location":"examples/quantizing_moe/#quickstart","title":"Quickstart","text":"<p>The provided example script demonstrates an end-to-end process for applying the quantization algorithm:</p> <pre><code>python3 mixtral_moe_w8a8_fp8.py\n</code></pre>"},{"location":"examples/quantizing_moe/#creating-a-quantized-moe-model","title":"Creating a Quantized MoE Model","text":"<p>This example leverages <code>llm-compressor</code> and <code>compressed-tensors</code> to create an FP8-quantized <code>Mixtral-8x7B-Instruct-v0.1</code> model. The model is calibrated and trained using the <code>open_platypus</code> dataset.</p> <p>You can follow the detailed steps below or simply run the example script with:</p> <pre><code>python mixtral_moe_w8a8_fp8.py\n</code></pre>"},{"location":"examples/quantizing_moe/#step-1-select-a-model-dataset-and-recipe","title":"Step 1: Select a Model, Dataset, and Recipe","text":"<p>In this step, you'll choose a baseline model for quantization, a dataset for calibration, and a quantization recipe.</p> <ul> <li>Models: Can be referenced from a local directory or retrieved from the Hugging Face Hub.</li> <li>Datasets: Can also be from a local directory or the Hugging Face Hub.</li> <li>Recipes: These are YAML files or Python modifier objects that describe how a model should be optimized during or after training. In this example, we use a <code>QuantizationModifier</code> object with the scheme set to <code>FP8</code>.</li> </ul> <pre><code>from llmcompressor.modifiers.quantization import QuantizationModifier\n\nrecipe = QuantizationModifier(scheme=\"FP8\", targets=\"Linear\", ignore=[\"lm_head\", \"re:.*block_sparse_moe.gate\"])\n</code></pre> <p>NOTE: <code>.*block_sparse_moe.gate</code> layers do not quantize well, hence they are ignored!</p>"},{"location":"examples/quantizing_moe/#step-2-run-quantization-using-oneshot","title":"Step 2: Run Quantization Using Oneshot","text":"<p>The <code>oneshot</code> method applies the selected recipe to your model and dataset without requiring any fine-tuning. The model will be sparsified and saved to <code>Mixtral-8x7B-Instruct-v0.1-FP8</code>.</p> <pre><code>from llmcompressor import oneshot\n\noutput_dir = \"Mixtral-8x7B-Instruct-v0.1-FP8\"\n\noneshot(\n    model=model,\n    dataset=dataset,\n    recipe=recipe,\n    save_compressed=True,\n    output_dir=output_dir,\n\n    max_seq_length=2048,\n    num_calibration_samples=512,\n)\n</code></pre>"},{"location":"examples/quantizing_moe/#custom-quantization","title":"Custom Quantization","text":"<p>NOTE: Only per-tensor quantization is supported in vLLM as of now (<code>vllm==0.6.1</code>)</p> <p>The repository supports multiple quantization techniques configured via a recipe. Supported strategies include <code>tensor</code>, <code>group</code>, and <code>channel</code> quantization.</p> <p>In the above example, FP8 per-tensor quantization is used as specified by the <code>FP8</code> scheme. For other preset schemes, refer to the quantization schemes in the <code>compressed-tensors</code> library.</p> <p>A custom scheme can also be specified using <code>config_groups</code>:</p> <pre><code># Example of defining a custom quantization scheme\n\nfrom llmcompressor.modifiers.quantization.gptq import GPTQModifier\n\nconfig_groups = {\n                \"group_0\": {\n                    \"targets\": [\"Linear\"],\n                    \"input_activations\": None,\n                    \"output_activations\": None,\n                    \"weights\": {\n                        \"num_bits\": 8,\n                        \"type\": \"int\",\n                        \"symmetric\": true,\n                        \"strategy\": \"group\",\n                        \"group_size\": 128, \n                    }\n               }\n}\n\nrecipe = GPTQModifier(config_groups=config_groups)\n</code></pre>"},{"location":"examples/sparse_2of4_quantization_fp8/","title":"Applying 2:4 Sparsity with Optional FP8 Quantization","text":"<p>This script demonstrates how to apply 2:4 structured sparsity with and without FP8 quantization to the <code>Meta-Llama-3-8B-Instruct</code> model using the <code>llm-compressor</code> library. The compressed model is optimized for memory efficiency and faster inference on supported GPUs.</p> <p>Note: FP8 dynamic precision computation is supported on Nvidia GPUs with CUDA Compute Capability 9.0 and above.</p>"},{"location":"examples/sparse_2of4_quantization_fp8/#installation","title":"Installation","text":"<p>To get started, install the <code>llm-compressor</code> library and its dependencies:</p> <pre><code>git clone https://github.com/vllm-project/llm-compressor.git\ncd llm-compressor\npip install -e .\n</code></pre>"},{"location":"examples/sparse_2of4_quantization_fp8/#quickstart","title":"Quickstart","text":"<p>Run the script with the following commands:</p> <ul> <li>Without FP8 Quantization:</li> </ul> <pre><code>python3 llama3_8b_2of4.py\n</code></pre> <ul> <li>With FP8 Quantization:</li> </ul> <pre><code>python3 llama3_8b_2of4.py --fp8\n</code></pre> <p>The script compresses the Meta-Llama-3-8B-Instruct model using:</p> <ul> <li>2:4 Structured Sparsity: Applies structured pruning to reduce weights by 50%.</li> <li>FP8 Quantization (Optional): Enables dynamic quantization for further memory savings.</li> </ul>"},{"location":"examples/sparse_2of4_quantization_fp8/#configuration","title":"Configuration","text":"<ul> <li>Model: meta-llama/Meta-Llama-3-8B-Instruct</li> <li>Dataset: HuggingFaceH4/ultrachat_200k (train split)</li> <li>Calibration Samples: 512</li> <li>Maximum Sequence Length: 2048</li> </ul>"},{"location":"examples/sparse_2of4_quantization_fp8/#steps-to-run","title":"Steps to Run","text":"<ol> <li>Select Model, Dataset, and Recipe</li> </ol> <p>The model and dataset are predefined in the script. The recipe dynamically adjusts the recipe based on  whether FP8 quantization is enabled (--fp8 flag).</p> <p>Example Recipe:</p> <pre><code>recipe = [\n    SparseGPTModifier(\n        sparsity=0.5,\n        mask_structure=\"2:4\",\n        sequential_update=True,\n        targets=[r\"re:model.layers.\\d*$\"],\n    )\n]\n\nif fp8_enabled:\n    recipe.append(\n        QuantizationModifier(\n            targets=[\"Linear\"],\n            ignore=[\"lm_head\"],\n            scheme=\"FP8_DYNAMIC\",\n        ),\n    )\n</code></pre> <ol> <li>Apply Compression The script applies compression using the oneshot function:</li> </ol> <pre><code>oneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    max_seq_length=2048,\n    num_calibration_samples=512,\n)\n</code></pre>"},{"location":"examples/sparse_2of4_quantization_fp8/#saving-the-compressed-model","title":"Saving the Compressed Model","text":"<p>The compressed model and tokenizer are saved to the output directory:</p> <pre><code>model.save_pretrained(save_dir, save_compressed=True)\ntokenizer.save_pretrained(save_dir)\n</code></pre> <p>Output Directories: - Without FP8: <code>Meta-Llama-3-8B-Instruct-2of4-sparse</code> - With FP8: <code>Meta-Llama-3-8B-Instruct-2of4-W8A8-FP8-Dynamic-Per-Token</code></p>"},{"location":"examples/sparse_2of4_quantization_fp8/#saving-without-sparse-compression","title":"Saving Without Sparse Compression","text":"<p>To save the model on disk without sparse compression:</p> <pre><code>model.save_pretrained(save_dir, save_compressed=True, disable_sparse_compression=True)\ntokenizer.save_pretrained(save_dir)\n</code></pre> <p>Note: Saving a model with both the <code>save_compressed</code> and <code>disable_sparse_compression</code> options will compress the model using the quantization compressor; however, instead of using the more disk-efficient sparsity compressor(s), the dense sparsity compressor will be used. The <code>dense</code> sparsity compressor saves model params as is, and does not leverage sparsity for disk-efficient storage. These options only affect how the model(s) are saved on disk and do not impact the actual pruning or quantization processes.</p>"},{"location":"examples/sparse_2of4_quantization_fp8/#validation","title":"Validation","text":"<p>After compression, the script validates the model by generating a sample output:</p> <pre><code>========== SAMPLE GENERATION ============\nHello my name is ...\n=========================================\n</code></pre> <p>Notes:  - Ensure your GPU has sufficient memory (at least ~25GB) to run compression script. - Use the <code>--fp8</code> flag to enable FP8 quantization.</p> <p>Modify the <code>MODEL_ID</code> and <code>DATASET_ID</code> variables to use other models or datasets.</p>"},{"location":"examples/sparse_2of4_quantization_fp8/#running-in-vllm","title":"Running in vLLM","text":"<p>Install vLLM using <code>pip install vllm</code></p> <pre><code># run_model.py\n\nimport argparse\nfrom vllm import LLM, SamplingParams\n\ndef run_inference(model_path, tensor_parallel_size, prompt=\"Hello my name is:\"):\n    \"\"\"\n    Loads a model and performs inference using LLM.\n    \"\"\"\n    # Define sampling parameters\n    sampling_params = SamplingParams(\n        temperature=0.8,\n        top_p=0.95,\n    )\n    # Load the model\n    model = LLM(\n        model=model_path, \n        enforce_eager=True,\n        dtype=\"auto\",\n        tensor_parallel_size=tensor_parallel_size,\n    )\n\n    # Generate inference\n    outputs = model.generate(prompt, sampling_params=sampling_params)\n    return outputs[0].outputs[0].text\n\ndef main():\n    \"\"\" Main function to handle CLI and process the model. \"\"\"\n    # Argument parsing\n    parser = argparse.ArgumentParser(description=\"Run inference on a single model and print results.\")\n    parser.add_argument(\n        \"model_path\",\n        type=str,\n        help=\"Path to the model to perform inference.\"\n    )\n    parser.add_argument(\n        \"--tensor_parallel_size\",\n        type=int,\n        default=1,\n        help=\"Tensor parallel size for the model. Default is 1.\"\n    )\n\n    args = parser.parse_args()\n    model_path = args.model_path\n    tensor_parallel_size = args.tensor_parallel_size\n\n    prompt = \"Hello my name is:\"\n\n    # Load model and perform inference\n    inference_result = run_inference(model_path, tensor_parallel_size)\n    print(\"=\"* 20)\n    print(\"Model:\", model_path)\n    print(prompt, inference_result)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Command to run model: <pre><code>python3 run_model.py &lt;MODEL_PATH&gt;\n</code></pre></p> <p>Example: <pre><code>python3 run_model.py Meta-Llama-3-8B-Instruct2of4-W8A8-FP8-Dynamic-Per-Token\n</code></pre></p>"},{"location":"examples/trl_mixin/","title":"Sparse Finetuning with TRL's SFTTrainer","text":"<p>The <code>SessionManagerMixin</code> can be added to other Trainer classes that inherit from  Hugging Face's Trainer.</p> <p>For example, we can add LLM Compressor support to TRL's SFTTrainer like so: </p> <p>Note: install <code>trl</code> using <code>pip install trl</code></p> <pre><code>from trl import SFTTrainer as TRLSFTTrainer\n\nclass SFTTrainer(SessionManagerMixIn, TRLSFTTrainer):\n    ...\n</code></pre> <p>The new <code>SFTTrainer</code> class can now apply LLM Compressor recipes and modifiers during  supervised finetuning, will full support for all of the original TRL features. The full class is defined in the script <code>sft_trainer.py</code> and requires very minimal  additional code: just a dataset load override to support passing in tokenized datasets  to the Trainer. </p>"},{"location":"examples/trl_mixin/#examples","title":"Examples","text":"<ul> <li> <p>Script <code>ex_trl_constant.py</code>: finetunes a 50% sparse Llama-7b model, using TRL's dataset preprocessing. Sparsity is maintained throughout training by  applying a <code>ConstantPruningModifier</code> recipe to the <code>SFTTrainer</code> </p> </li> <li> <p>Script <code>ex_trl_distillation.py</code>: finetunes a 50% sparse Llama-7b  model using knowledge distillation from a dense Llama-7b model. Sparsity is maintained  throughout training with a <code>ConstantPruningModifier</code> and layer-wise knowledge  distillation is handled by the <code>OutputDistillationModifier</code></p> </li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to LLM Compressor! This section will guide you through the process of installing the library, compressing your first model, and deploying it with vLLM for faster, more efficient inference.</p> <p>LLM Compressor makes it simple to optimize large language models for deployment, offering various quantization techniques that help you find the perfect balance between model quality, performance, and resource efficiency.</p>"},{"location":"getting-started/#quick-start-guides","title":"Quick Start Guides","text":"<p>Follow the guides below to get started with LLM Compressor and optimize your models for production deployment.</p> <ul> <li> <p> Installation</p> <p>Learn how to install LLM Compressor using pip or from source.</p> <p> Installation Guide</p> </li> <li> <p> Compress Your Model</p> <p>Learn how to apply quantization to your models using different algorithms and formats.</p> <p> Compression Guide</p> </li> <li> <p> Deploy with vLLM</p> <p>Deploy your compressed model for efficient inference using vLLM.</p> <p> Deployment Guide</p> </li> </ul>"},{"location":"getting-started/compress/","title":"Compress Your Model","text":"<p>LLM Compressor provides a straightforward way to compress your models using various optimization techniques. This guide will walk you through the process of compressing a model using different quantization methods.</p>"},{"location":"getting-started/compress/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites: - Operating System: Linux (recommended for GPU support) - Python Version: 3.9 or newer - Available GPU: For optimal performance, it's recommended to use a GPU. LLM Compressor supports the latest PyTorch and CUDA versions for compatability with NVIDIA GPUs.</p>"},{"location":"getting-started/compress/#select-a-model-and-dataset","title":"Select a Model and Dataset","text":"<p>Before you start compressing, select the model you'd like to compress and a calibration dataset that is representative of your use case. LLM Compressor supports a variety of models and integrates natively with Hugging Face Transformers and Model Hub, so a great starting point is to use a model from the Hugging Face Model Hub. LLM Compressor also supports many datasets from the Hugging Face Datasets library, making it easy to find a suitable dataset for calibration.</p> <p>For this guide, we'll use the <code>TinyLlama</code> model and the <code>open_platypus</code> dataset for calibration. You can replace these with your own model and dataset as needed.</p>"},{"location":"getting-started/compress/#select-a-quantization-method-and-scheme","title":"Select a Quantization Method and Scheme","text":"<p>LLM Compressor supports several quantization methods and schemes, each with its own strengths and weaknesses. The choice of method and scheme will depend on your specific use case, hardware capabilities, and desired trade-offs between model size, speed, and accuracy.</p> <p>Some common quantization schemes include:</p> Scheme Description Hardware Compatibility FP W8A8 8-bit floating point (FP8) quantization for weights and activations, providing ~2X smaller weights with 8-bit arithmetic operations. Good for general performance and compression, especially for server and batch inference. Latest NVIDIA GPUs (Ada Lovelace, Hopper, and later) and latest AMD GPUs INT W8A8 8-bit integer (INT8) quantization for weights and activations, providing ~2X smaller weights with 8-bit arithmetic operations. Good for general performance and compression, especially for server and batch inference. All NVIDIA GPUs, AMD GPUs, TPUs, CPUs, and other accelerators W4A16 4-bit integer (INT4) weights with 16-bit floating point (FP16) activations, providing ~3.7X smaller weights but requiring 16-bit arithmetic operations. Maximum compression for latency-sensitive applications with limited memory. All NVIDIA GPUs, AMD GPUs, TPUs, CPUs, and other accelerators <p>Some common quantization methods include:</p> Method Description Accuracy Recovery vs. Time GPTQ Utilizes second-order layer-wise optimizations to prioritize important weights/activations and enables updates to remaining weights High accuracy recovery but more expensive/slower to run AWQ Uses channelwise scaling to better preserve important outliers in weights and activations Moderate accuracy recovery with faster runtime than GPTQ SmoothQuant Smooths outliers in activations by folding them into weights, ensuring better accuracy for weight and activation quantized models Good accuracy recovery with minimal calibration time; composable with other methods <p>For this guide, we'll use <code>GPTQ</code> composed with <code>SmoothQuant</code> to create an <code>INT W8A8</code> quantized model. This combination provides a good balance for performance, accuracy, and compatability across a wide range of hardware.</p>"},{"location":"getting-started/compress/#apply-the-recipe","title":"Apply the Recipe","text":"<p>LLM Compressor provides the <code>oneshot</code> API for simple and straightforward model compression. This API allows you to apply a pre-defined recipe to your model and dataset, making it easy to get started with compression. To apply what we discussed above, we'll import the necessary modifiers and create a recipe to apply to our model and dataset:</p> <pre><code>from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor import oneshot\n\nrecipe = [\n    SmoothQuantModifier(smoothing_strength=0.8),\n    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n]\noneshot(\n    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    dataset=\"open_platypus\",\n    recipe=recipe,\n    output_dir=\"TinyLlama-1.1B-Chat-v1.0-INT8\",\n    max_seq_length=2048,\n    num_calibration_samples=512,\n)\n</code></pre> <p>Once the above code is run, it will save the compressed model to the specified output directory: <code>TinyLlama-1.1B-Chat-v1.0-INT8</code>. You can then load this model using the Hugging Face Transformers library or vLLM for inference and testing.</p>"},{"location":"getting-started/deploy/","title":"Deploy with vLLM","text":"<p>Once you've compressed your model using LLM Compressor, you can deploy it for efficient inference using vLLM. This guide walks you through the deployment process, using the output from the Compress Your Model guide. If you haven't completed that step, change the model arguments in the code snippets below to point to your desired model.</p> <p>vLLM is a high-performance inference engine designed for large language models, providing support for various quantization formats and optimized for both single and multi-GPU setups. It also offers an OpenAI-compatible API for easy integration with existing applications.</p>"},{"location":"getting-started/deploy/#prerequisites","title":"Prerequisites","text":"<p>Before deploying your model, ensure you have the following prerequisites: - Operating System: Linux (recommended for GPU support) - Python Version: 3.9 or newer - Available GPU: For optimal performance, it's recommended to use a GPU. vLLM supports a range of accelerators, including NVIDIA GPUs, AMD GPUs, TPUs, and other accelerators. - vLLM Installed: Ensure you have vLLM installed. You can install it using pip:   <pre><code>pip install vllm\n</code></pre></p>"},{"location":"getting-started/deploy/#python-api","title":"Python API","text":"<p>vLLM provides a Python API for easy integration with your applications, enabling you to load and use your compressed model directly in your Python code. To test the compressed model, use the following code:</p> <pre><code>from vllm import LLM\n\nmodel = LLM(\"./TinyLlama-1.1B-Chat-v1.0-INT8\")\noutput = model.generate(\"What is machine learning?\", max_tokens=256)\nprint(output)\n</code></pre> <p>After running the above code, you should see the generated output from your compressed model. This confirms that the model is loaded and ready for inference.</p>"},{"location":"getting-started/deploy/#http-server","title":"HTTP Server","text":"<p>vLLM also provides an HTTP server for serving your model via a RESTful API that is compatible with OpenAI's API definitions. This allows you to easily integrate your model into existing applications or services. To start the HTTP server, use the following command:</p> <pre><code>vllm serve \"./TinyLlama-1.1B-Chat-v1.0-INT8\"\n</code></pre> <p>By default, the server will run on <code>localhost:8000</code>. You can change the host and port by using the <code>--host</code> and <code>--port</code> flags. Now that the server is running, you can send requests to it using any HTTP client. For example, you can use <code>curl</code> to send a request:</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama-1.1B-Chat-v1.0-INT8\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n        \"max_tokens\": 256\n    }'\n</code></pre> <p>This will return a JSON response with the generated text from your model. You can also use any HTTP client library in your programming language of choice to send requests to the server.</p>"},{"location":"getting-started/install/","title":"Installation","text":"<p>LLM Compressor can be installed using several methods depending on your requirements. Below are the detailed instructions for each installation pathway.</p>"},{"location":"getting-started/install/#prerequisites","title":"Prerequisites","text":"<p>Before installing LLM Compressor, ensure you have the following prerequisites:</p> <ul> <li>Operating System: Linux (recommended for GPU support)</li> <li>Python Version: 3.9 or newer</li> <li>Pip Version: Ensure you have the latest version of pip installed. You can upgrade pip using the following command:</li> </ul> <pre><code>python -m pip install --upgrade pip\n</code></pre>"},{"location":"getting-started/install/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/install/#install-from-pypi","title":"Install from PyPI","text":"<p>The simplest way to install LLM Compressor is via pip from the Python Package Index (PyPI):</p> <pre><code>pip install llmcompressor\n</code></pre> <p>This will install the latest stable release of LLM Compressor.</p>"},{"location":"getting-started/install/#install-a-specific-version-from-pypi","title":"Install a Specific Version from PyPI","text":"<p>If you need a specific version of LLM Compressor, you can specify the version number during installation:</p> <pre><code>pip install llmcompressor==0.5.1\n</code></pre> <p>Replace <code>0.1.0</code> with your desired version number.</p>"},{"location":"getting-started/install/#install-from-source","title":"Install from Source","text":"<p>To install the latest development version of LLM Compressor from the main branch, use the following command:</p> <pre><code>pip install git+https://github.com/vllm-project/llm-compressor.git\n</code></pre> <p>This will clone the repository and install LLM Compressor directly from the main branch.</p>"},{"location":"getting-started/install/#install-from-a-local-clone","title":"Install from a Local Clone","text":"<p>If you have cloned the LLM Compressor repository locally and want to install it, navigate to the repository directory and run:</p> <pre><code>pip install .\n</code></pre> <p>For development purposes, you can install it in editable mode with the <code>dev</code> extra:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>This allows you to make changes to the source code and have them reflected immediately without reinstalling.</p>"},{"location":"guides/","title":"Guides","text":"<p>Welcome to the LLM Compressor guides section! Here you'll find comprehensive documentation covering key components and concepts of LLM Compressor. These guides will help you understand the various compression options available, how to apply them effectively, and how to deploy your optimized models for maximum performance.</p>"},{"location":"guides/#key-guides","title":"Key Guides","text":"<ul> <li> <p> Compression Schemes</p> <p>Explore the available compression schemes for Quantization and Pruning to determine which is best for your use case.</p> <p> Compression Schemes</p> </li> <li> <p> Saving Models</p> <p>Learn the enhanced ways to save your compressed models with the library's extended <code>save_pretrained</code> functionality for compatibility with vLLM deployment.</p> <p> Saving a Model</p> </li> </ul>"},{"location":"guides/compression_schemes/","title":"Compression Schemes","text":""},{"location":"guides/compression_schemes/#ptq","title":"PTQ","text":"<p>PTQ is performed to reduce the precision of quantizable weights (e.g., linear layers) to a lower bit-width. Supported formats are:</p>"},{"location":"guides/compression_schemes/#w4a16","title":"W4A16","text":"<ul> <li>Uses GPTQ to compress weights to 4 bits. Requires calibration dataset.</li> <li>Optionally, AWQ can also be leveraged for W4A16 quantization</li> <li>Useful speed ups in low QPS regimes with more weight compression. </li> <li>Recommended for any GPUs types.</li> </ul>"},{"location":"guides/compression_schemes/#w8a8-int8","title":"W8A8-INT8","text":"<ul> <li>Uses channel-wise quantization to compress weights to 8 bits using GPTQ, and uses dynamic per-token quantization to compress activations to 8 bits. Requires calibration dataset for weight quantization. Activation quantization is carried out during inference on vLLM.</li> <li>Useful for speed ups in high QPS regimes or offline serving on vLLM. </li> <li>Recommended for NVIDIA GPUs with compute capability &lt;8.9 (Ampere, Turing, Volta, Pascal, or older).</li> </ul>"},{"location":"guides/compression_schemes/#w8a8-fp8","title":"W8A8-FP8","text":"<ul> <li>Uses channel-wise quantization to compress weights to 8 bits, and uses dynamic per-token quantization to compress activations to 8 bits. Does not require calibration dataset. Activation quantization is carried out during inference on vLLM.</li> <li>Useful for speed ups in high QPS regimes or offline serving on vLLM. </li> <li>Recommended for NVIDIA GPUs with compute capability &gt;=9.0 (Hopper and Blackwell).</li> </ul>"},{"location":"guides/compression_schemes/#sparsification","title":"Sparsification","text":"<p>Sparsification reduces model complexity by pruning selected weight values to zero while retaining essential weights in a subset of parameters. Supported formats include:</p>"},{"location":"guides/compression_schemes/#24-sparsity-with-fp8-weight-fp8-input-activation","title":"2:4-Sparsity with FP8 Weight, FP8 Input Activation","text":"<ul> <li>Uses (1) semi-structured sparsity (SparseGPT), where, for every four contiguous weights in a tensor, two are set to zero. (2) Uses channel-wise quantization to compress weights to 8 bits and dynamic per-token quantization to compress activations to 8 bits.</li> <li>Useful for better inference than W8A8-fp8, with almost no drop in its evaluation score blog. Note: Small models may experience accuracy drops when the remaining non-zero weights are insufficient to recapitulate the original distribution.</li> <li>Recommended for compute capability &gt;=9.0 (Hopper and Blackwell).</li> </ul>"},{"location":"guides/saving_a_model/","title":"Saving a Model","text":"<p>The <code>llmcompressor</code> library extends Hugging Face's <code>save_pretrained</code> method with additional arguments to support model compression functionality. This document explains these extra arguments and how to use them effectively.</p>"},{"location":"guides/saving_a_model/#how-it-works","title":"How It Works","text":"<p>When you import <code>llmcompressor</code>, it automatically wraps the model's original <code>save_pretrained</code> method with an enhanced version that supports compression. This happens in two ways:</p> <ol> <li>Direct modification: When you call <code>modify_save_pretrained(model)</code> directly</li> <li>Automatic wrapping: When you call <code>oneshot(...)</code>, which wraps <code>save_pretrained</code> under the hood</li> </ol> <p>This means that after applying compression with <code>oneshot</code>, your model's <code>save_pretrained</code> method is already enhanced with compression capabilities, and you can use the additional arguments described below.</p>"},{"location":"guides/saving_a_model/#additional-arguments","title":"Additional Arguments","text":"<p>When saving your compressed models, you can use the following extra arguments with the <code>save_pretrained</code> method:</p> Parameter Type Default Description <code>sparsity_config</code> <code>Optional[SparsityCompressionConfig]</code> <code>None</code> Optional configuration for sparsity compression. This should be provided if there's existing sparsity in the model. If None and <code>skip_sparsity_compression_stats</code> is False, configuration will be automatically inferred from the model. <code>quantization_format</code> <code>Optional[str]</code> <code>None</code> Optional format string for quantization. If not provided, it will be inferred from the model. <code>save_compressed</code> <code>bool</code> <code>True</code> Controls whether to save the model in a compressed format. Set to <code>False</code> to save in the original dense format. <code>skip_sparsity_compression_stats</code> <code>bool</code> <code>True</code> Controls whether to skip calculating sparsity statistics (e.g., global sparsity and structure) when saving the model. Set to <code>False</code> to include these statistics. If you are not providing a <code>sparsity_config</code>, you should set this to <code>False</code> to automatically generate the config for you. <code>disable_sparse_compression</code> <code>bool</code> <code>False</code> When set to <code>True</code>, skips any sparse compression during save, even if the model has been previously compressed."},{"location":"guides/saving_a_model/#workflow-for-models-with-existing-sparsity","title":"Workflow for Models with Existing Sparsity","text":"<p>When working with models that already have sparsity:</p> <ol> <li>If you know the sparsity configuration, provide it directly via <code>sparsity_config</code></li> <li>If you don't know the sparsity configuration, set <code>skip_sparsity_compression_stats</code> to <code>False</code> to automatically infer it from the model</li> </ol> <p>This workflow ensures that the correct sparsity configuration is either provided or generated when saving models with existing sparsity.</p>"},{"location":"guides/saving_a_model/#examples","title":"Examples","text":""},{"location":"guides/saving_a_model/#applying-compression-with-oneshot","title":"Applying Compression with oneshot","text":"<p>The simplest approach is to use <code>oneshot</code>, which handles both compression and wrapping <code>save_pretrained</code>:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom llmcompressor import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"your-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"your-model\")\n\n# Apply compression - this also wraps save_pretrained\noneshot(\n    model=model,\n    recipe=[GPTQModifier(targets=\"Linear\", scheme=\"W8A8\", ignore=[\"lm_head\"])],\n    # Other oneshot parameters...\n)\n\n# Now you can use the enhanced save_pretrained\nSAVE_DIR = \"your-model-W8A8-compressed\"\nmodel.save_pretrained(\n    SAVE_DIR,\n    save_compressed=True  # Use the enhanced functionality\n)\ntokenizer.save_pretrained(SAVE_DIR)\n</code></pre>"},{"location":"guides/saving_a_model/#manual-approach-without-oneshot","title":"Manual Approach (Without oneshot)","text":"<p>If you need more control, you can wrap <code>save_pretrained</code> manually:</p> <pre><code>from transformers import AutoModelForCausalLM\nfrom llmcompressor.transformers.sparsification import modify_save_pretrained\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"your-model\")\n\n# Manually wrap save_pretrained\nmodify_save_pretrained(model)\n\n# Now you can use the enhanced save_pretrained\nmodel.save_pretrained(\n    \"your-model-path\",\n    save_compressed=True,\n    skip_sparsity_compression_stats=False  # To automatically infer sparsity config\n)\n</code></pre>"},{"location":"guides/saving_a_model/#saving-with-custom-sparsity-configuration","title":"Saving with Custom Sparsity Configuration","text":"<pre><code>from compressed_tensors.sparsification import SparsityCompressionConfig\n\n# Create custom sparsity config\ncustom_config = SparsityCompressionConfig(\n    format=\"2:4\",\n    block_size=16\n)\n\n# Save with custom config\nmodel.save_pretrained(\n    \"your-model-custom-sparse\",\n    sparsity_config=custom_config,\n)\n</code></pre>"},{"location":"guides/saving_a_model/#notes","title":"Notes","text":"<ul> <li>When loading compressed models with <code>from_pretrained</code>, the compression format is automatically detected.</li> <li>To use compressed models with vLLM, simply load them as you would any model:   <pre><code>from vllm import LLM\nmodel = LLM(\"./your-model-compressed\")\n</code></pre></li> <li>Compression configurations are saved in the model's config file and are automatically applied when loading.</li> </ul> <p>For more information about compression algorithms and formats, please refer to the documentation and examples in the llmcompressor repository.</p>"},{"location":"reference/llmcompressor/","title":"llmcompressor","text":"<p>A library for compressing large language models utilizing the latest techniques and research in the field for both training aware and post training techniques.</p> <p>The library is designed to be flexible and easy to use on top of PyTorch and HuggingFace Transformers, allowing for quick experimentation.</p>"},{"location":"reference/llmcompressor/#llmcompressor.configure_logger","title":"<code>configure_logger(config=None)</code>","text":"<p>Configure the logger for LLM Compressor. This function sets up the console and file logging as per the specified or default parameters.</p> <p>Note: Environment variables take precedence over the function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[LoggerConfig]</code> <p>The configuration for the logger to use.</p> <code>None</code> Source code in <code>src/llmcompressor/logger.py</code> <pre><code>def configure_logger(config: Optional[LoggerConfig] = None) -&gt; None:\n    \"\"\"\n    Configure the logger for LLM Compressor.\n    This function sets up the console and file logging\n    as per the specified or default parameters.\n\n    Note: Environment variables take precedence over the function parameters.\n\n    :param config: The configuration for the logger to use.\n    :type config: LoggerConfig\n    \"\"\"\n    logger_config = config or LoggerConfig()\n\n    # env vars get priority\n    if (disabled := os.getenv(\"LLM_COMPRESSOR_LOG_DISABLED\")) is not None:\n        logger_config.disabled = disabled.lower() == \"true\"\n    if (clear_loggers := os.getenv(\"LLM_COMPRESSOR_CLEAR_LOGGERS\")) is not None:\n        logger_config.clear_loggers = clear_loggers.lower() == \"true\"\n    if (console_log_level := os.getenv(\"LLM_COMPRESSOR_LOG_LEVEL\")) is not None:\n        logger_config.console_log_level = console_log_level.upper()\n    if (log_file := os.getenv(\"LLM_COMPRESSOR_LOG_FILE\")) is not None:\n        logger_config.log_file = log_file\n    if (log_file_level := os.getenv(\"LLM_COMPRESSOR_LOG_FILE_LEVEL\")) is not None:\n        logger_config.log_file_level = log_file_level.upper()\n\n    if logger_config.disabled:\n        logger.disable(\"llmcompressor\")\n        return\n\n    logger.enable(\"llmcompressor\")\n\n    if logger_config.clear_loggers:\n        logger.remove()\n\n    if logger_config.console_log_level:\n        # log as a human readable string with the time, function, level, and message\n        logger.add(\n            sys.stdout,\n            level=logger_config.console_log_level.upper(),\n            format=\"{time} | {function} | {level} - {message}\",\n            filter=support_log_once,\n        )\n\n    if logger_config.log_file or logger_config.log_file_level:\n        log_file = logger_config.log_file or \"llmcompressor.log\"\n        log_file_level = logger_config.log_file_level or \"INFO\"\n        # log as json to the file for easier parsing\n        logger.add(\n            log_file,\n            level=log_file_level.upper(),\n            serialize=True,\n            filter=support_log_once,\n        )\n\n    if logger_config.metrics_disabled or \"METRIC\" in logger._core.levels.keys():\n        return\n\n    # initialize metric logger on loguru\n    logger.level(\"METRIC\", no=38, color=\"&lt;yellow&gt;\", icon=\"\ud83d\udcc8\")\n</code></pre>"},{"location":"reference/llmcompressor/logger/","title":"llmcompressor.logger","text":"<p>Logger configuration for LLM Compressor.</p> <p>This module provides a flexible logging configuration using the loguru library. It supports console and file logging with options to configure via environment variables or direct function calls.</p> <p>Environment Variables:     - LLM_COMPRESSOR_LOG_DISABLED: Disable logging (default: false).     - LLM_COMPRESSOR_CLEAR_LOGGERS: Clear existing loggers from loguru (default: true).     - LLM_COMPRESSOR_LOG_LEVEL: Log level for console logging         (default: none, options: DEBUG, INFO, WARNING, ERROR, CRITICAL).     - LLM_COMPRESSOR_LOG_FILE: Path to the log file for file logging         (default: llm-compressor.log if log file level set else none)     - LLM_COMPRESSOR_LOG_FILE_LEVEL: Log level for file logging         (default: INFO if log file set else none).</p> <p>Usage:     from llmcompressor import logger, configure_logger, LoggerConfig</p> <pre><code># Configure metrics with default settings\nconfigure_logger(\n    config=LoggerConfig(\n        disabled=False,\n        clear_loggers=True,\n        console_log_level=\"DEBUG\",\n        log_file=None,\n        log_file_level=None,\n    )\n)\n\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\n</code></pre>"},{"location":"reference/llmcompressor/logger/#llmcompressor.logger.configure_logger","title":"<code>configure_logger(config=None)</code>","text":"<p>Configure the logger for LLM Compressor. This function sets up the console and file logging as per the specified or default parameters.</p> <p>Note: Environment variables take precedence over the function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[LoggerConfig]</code> <p>The configuration for the logger to use.</p> <code>None</code> Source code in <code>src/llmcompressor/logger.py</code> <pre><code>def configure_logger(config: Optional[LoggerConfig] = None) -&gt; None:\n    \"\"\"\n    Configure the logger for LLM Compressor.\n    This function sets up the console and file logging\n    as per the specified or default parameters.\n\n    Note: Environment variables take precedence over the function parameters.\n\n    :param config: The configuration for the logger to use.\n    :type config: LoggerConfig\n    \"\"\"\n    logger_config = config or LoggerConfig()\n\n    # env vars get priority\n    if (disabled := os.getenv(\"LLM_COMPRESSOR_LOG_DISABLED\")) is not None:\n        logger_config.disabled = disabled.lower() == \"true\"\n    if (clear_loggers := os.getenv(\"LLM_COMPRESSOR_CLEAR_LOGGERS\")) is not None:\n        logger_config.clear_loggers = clear_loggers.lower() == \"true\"\n    if (console_log_level := os.getenv(\"LLM_COMPRESSOR_LOG_LEVEL\")) is not None:\n        logger_config.console_log_level = console_log_level.upper()\n    if (log_file := os.getenv(\"LLM_COMPRESSOR_LOG_FILE\")) is not None:\n        logger_config.log_file = log_file\n    if (log_file_level := os.getenv(\"LLM_COMPRESSOR_LOG_FILE_LEVEL\")) is not None:\n        logger_config.log_file_level = log_file_level.upper()\n\n    if logger_config.disabled:\n        logger.disable(\"llmcompressor\")\n        return\n\n    logger.enable(\"llmcompressor\")\n\n    if logger_config.clear_loggers:\n        logger.remove()\n\n    if logger_config.console_log_level:\n        # log as a human readable string with the time, function, level, and message\n        logger.add(\n            sys.stdout,\n            level=logger_config.console_log_level.upper(),\n            format=\"{time} | {function} | {level} - {message}\",\n            filter=support_log_once,\n        )\n\n    if logger_config.log_file or logger_config.log_file_level:\n        log_file = logger_config.log_file or \"llmcompressor.log\"\n        log_file_level = logger_config.log_file_level or \"INFO\"\n        # log as json to the file for easier parsing\n        logger.add(\n            log_file,\n            level=log_file_level.upper(),\n            serialize=True,\n            filter=support_log_once,\n        )\n\n    if logger_config.metrics_disabled or \"METRIC\" in logger._core.levels.keys():\n        return\n\n    # initialize metric logger on loguru\n    logger.level(\"METRIC\", no=38, color=\"&lt;yellow&gt;\", icon=\"\ud83d\udcc8\")\n</code></pre>"},{"location":"reference/llmcompressor/logger/#llmcompressor.logger.support_log_once","title":"<code>support_log_once(record)</code>","text":"<p>Support logging only once using <code>.bind(log_once=True)</code></p> <pre><code>logger.bind(log_once=False).info(\"This will log multiple times\")\nlogger.bind(log_once=False).info(\"This will log multiple times\")\nlogger.bind(log_once=True).info(\"This will only log once\")\nlogger.bind(log_once=True).info(\"This will only log once\")  # skipped\n</code></pre> Source code in <code>src/llmcompressor/logger.py</code> <pre><code>def support_log_once(record: Dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Support logging only once using `.bind(log_once=True)`\n\n    ```\n    logger.bind(log_once=False).info(\"This will log multiple times\")\n    logger.bind(log_once=False).info(\"This will log multiple times\")\n    logger.bind(log_once=True).info(\"This will only log once\")\n    logger.bind(log_once=True).info(\"This will only log once\")  # skipped\n    ```\n    \"\"\"\n    log_once = record[\"extra\"].get(\"log_once\", False)\n    level = getattr(record[\"level\"], \"name\", \"none\")\n    message = str(level) + record[\"message\"]\n\n    if log_once and message in _logged_once:\n        return False\n\n    if log_once:\n        _logged_once.add(message)\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/typing/","title":"llmcompressor.typing","text":""},{"location":"reference/llmcompressor/version/","title":"llmcompressor.version","text":""},{"location":"reference/llmcompressor/args/","title":"llmcompressor.args","text":""},{"location":"reference/llmcompressor/args/dataset_arguments/","title":"llmcompressor.args.dataset_arguments","text":""},{"location":"reference/llmcompressor/args/dataset_arguments/#llmcompressor.args.dataset_arguments.CustomDatasetArguments","title":"<code>CustomDatasetArguments</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DVCDatasetArguments</code></p> <p>Arguments for training using custom datasets</p> Source code in <code>src/llmcompressor/args/dataset_arguments.py</code> <pre><code>@dataclass\nclass CustomDatasetArguments(DVCDatasetArguments):\n    \"\"\"\n    Arguments for training using custom datasets\n    \"\"\"\n\n    dataset_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Path to the custom dataset. Supports json, csv, dvc. \"\n                \"For DVC, the to dvc dataset to load, of format dvc://path. \"\n                \"For csv or json, the path containing the dataset. \"\n            ),\n        },\n    )\n\n    text_column: str = field(\n        default=\"text\",\n        metadata={\n            \"help\": (\n                \"Optional key to be used as the `text` input to tokenizer/processor \"\n                \"after dataset preprocesssing\"\n            )\n        },\n    )\n\n    remove_columns: Union[None, str, List] = field(\n        default=None,\n        metadata={\"help\": \"Column names to remove after preprocessing (deprecated)\"},\n    )\n\n    preprocessing_func: Union[None, str, Callable] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Typically a function which applies a chat template. Can take the form \"\n                \"of either a function to apply to the dataset, a name defined in \"\n                \"src/llmcompressor/transformers/utils/preprocessing_functions.py, or \"\n                \"a path to a function definition of the form /path/to/file.py:func\"\n            )\n        },\n    )\n\n    data_collator: Callable[[Any], Any] = field(\n        default_factory=lambda: DefaultDataCollator(),\n        metadata={\"help\": \"The function to used to form a batch from the dataset\"},\n    )\n</code></pre>"},{"location":"reference/llmcompressor/args/dataset_arguments/#llmcompressor.args.dataset_arguments.DVCDatasetArguments","title":"<code>DVCDatasetArguments</code>  <code>dataclass</code>","text":"<p>Arguments for training using DVC</p> Source code in <code>src/llmcompressor/args/dataset_arguments.py</code> <pre><code>@dataclass\nclass DVCDatasetArguments:\n    \"\"\"\n    Arguments for training using DVC\n    \"\"\"\n\n    dvc_data_repository: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to repository used for dvc_dataset_path\"},\n    )\n</code></pre>"},{"location":"reference/llmcompressor/args/dataset_arguments/#llmcompressor.args.dataset_arguments.DatasetArguments","title":"<code>DatasetArguments</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CustomDatasetArguments</code></p> <p>Arguments pertaining to what data we are going to input our model for calibration, training</p> <p>Using <code>HfArgumentParser</code> we can turn this class into argparse arguments to be able to specify them on the command line</p> Source code in <code>src/llmcompressor/args/dataset_arguments.py</code> <pre><code>@dataclass\nclass DatasetArguments(CustomDatasetArguments):\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for\n    calibration, training\n\n    Using `HfArgumentParser` we can turn this class into argparse\n    arguments to be able to specify them on the command line\n    \"\"\"\n\n    dataset: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The name of the dataset to use (via the datasets library). \"\n                \"Supports input as a string or DatasetDict from HF\"\n            )\n        },\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\"The configuration name of the dataset to use\"),\n        },\n    )\n    max_seq_length: int = field(\n        default=384,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. \"\n            \"Sequences longer  than this will be truncated, sequences shorter will \"\n            \"be padded.\"\n        },\n    )\n    concatenate_data: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether or not to concatenate datapoints to fill max_seq_length\"\n        },\n    )\n    raw_kwargs: Dict = field(\n        default_factory=dict,\n        metadata={\"help\": \"Additional keyboard args to pass to datasets load_data\"},\n    )\n    splits: Union[None, str, List, Dict] = field(\n        default=None,\n        metadata={\"help\": \"Optional percentages of each split to download\"},\n    )\n    num_calibration_samples: Optional[int] = field(\n        default=512,\n        metadata={\"help\": \"Number of samples to use for one-shot calibration\"},\n    )\n    shuffle_calibration_samples: Optional[bool] = field(\n        default=True,\n        metadata={\n            \"help\": \"whether to shuffle the dataset before selecting calibration data\"\n        },\n    )\n    streaming: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"True to stream data from a cloud dataset\"},\n    )\n    overwrite_cache: bool = field(\n        default=False,\n        metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"},\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    pad_to_max_length: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to pad all samples to `max_seq_length`. If False, \"\n            \"will pad the samples dynamically when batching to the maximum length \"\n            \"in the batch (which can be faster on GPU but will be slower on TPU).\"\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number \"\n            \"of training examples to this value if set.\"\n        },\n    )\n    min_tokens_per_module: Optional[float] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The minimum percentage of tokens (out of the total number) \"\n                \"that the module should 'receive' throughout the forward \"\n                \"pass of the calibration. If a module receives fewer tokens, \"\n                \"a warning will be logged. Defaults to 1/num_of_experts.\"\n                \"note: this argument is only relevant for MoE models\"\n            ),\n        },\n    )\n    trust_remote_code_data: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether or not to allow for datasets defined on the Hub using \"\n            \"a dataset script. This option should only be set to True for \"\n            \"repositories you trust and in which you have read the code, as it \"\n            \"will execute code present on the Hub on your local machine.\"\n        },\n    )\n    pipeline: Optional[str] = field(\n        default=\"independent\",\n        metadata={\n            \"help\": \"Calibration pipeline used to calibrate model\"\n            \"Options: ['basic', 'datafree', 'sequential', 'layer_sequential', \"\n            \"independent]\"\n        },\n    )\n</code></pre>"},{"location":"reference/llmcompressor/args/model_arguments/","title":"llmcompressor.args.model_arguments","text":""},{"location":"reference/llmcompressor/args/model_arguments/#llmcompressor.args.model_arguments.ModelArguments","title":"<code>ModelArguments</code>  <code>dataclass</code>","text":"<p>Model variables used for oneshot calibration, finetuning and stage runners (sequential run of oneshot and finetune).</p> Source code in <code>src/llmcompressor/args/model_arguments.py</code> <pre><code>@dataclass\nclass ModelArguments:\n    \"\"\"\n    Model variables used for oneshot calibration, finetuning and\n    stage runners (sequential run of oneshot and finetune).\n\n    \"\"\"\n\n    model: str = field(\n        metadata={\n            \"help\": (\n                \"A pretrained model or a string as a path to pretrained model, \"\n                \"HF stub, or model identifier from huggingface.co/models.\"\n            )\n        },\n    )\n    distill_teacher: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Teacher model (a trained text generation model)\",\n        },\n    )\n    config_name: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Pretrained config name or path if not the same as model_name\"\n        },\n    )\n    tokenizer: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n        },\n    )\n    processor: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Pretrained processor name or path if not the same as model_name\"\n        },\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where to store the pretrained data from huggingface.co\"},\n    )\n\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use token generated when running `transformers-cli login` \"\n            \"(necessary to use this script with private models)\"\n        },\n    )\n    precision: str = field(\n        default=\"auto\",\n        metadata={\"help\": \"Precision to cast model weights to, default to auto\"},\n    )\n\n    tie_word_embeddings: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether the model's input and output word embeddings \"\n            \"should be tied. Note that this is only relevant if the \"\n            \"model has a output word embedding layer.\"\n        },\n    )\n    trust_remote_code_model: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether or not to allow for custom models to execute their \"\n            \"own modeling files. This option should only be set to True for \"\n            \"repositories you trust and in which you have read the code\"\n        },\n    )\n    # TODO: potentialy separate out/expand to additional saving args\n    save_compressed: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Whether to compress sparse models during save\"},\n    )\n    oneshot_device: Optional[str] = field(\n        default=\"cuda:0\",\n        metadata={\"help\": \"Device to run oneshot calibration on\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\n            \"help\": \"The specific model version to use \"\n            \"(can be a branch name, tag name or commit id)\"\n        },\n    )\n</code></pre>"},{"location":"reference/llmcompressor/args/recipe_arguments/","title":"llmcompressor.args.recipe_arguments","text":""},{"location":"reference/llmcompressor/args/recipe_arguments/#llmcompressor.args.recipe_arguments.RecipeArguments","title":"<code>RecipeArguments</code>  <code>dataclass</code>","text":"<p>Recipe and session variables</p> Source code in <code>src/llmcompressor/args/recipe_arguments.py</code> <pre><code>@dataclass\nclass RecipeArguments:\n    \"\"\"Recipe and session variables\"\"\"\n\n    recipe: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to a LLM Compressor sparsification recipe\",\n        },\n    )\n    recipe_args: Optional[List[str]] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"List of recipe arguments to evaluate, of the format key1=value1 \"\n                \"key2=value2\"\n            )\n        },\n    )\n    clear_sparse_session: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Whether to clear CompressionSession/CompressionLifecycle \",\n                \"data between runs.\",\n            )\n        },\n    )\n    stage: Optional[str] = field(\n        default=None,\n        metadata={\"help\": (\"The stage of the recipe to use for oneshot / train.\",)},\n    )\n</code></pre>"},{"location":"reference/llmcompressor/args/training_arguments/","title":"llmcompressor.args.training_arguments","text":""},{"location":"reference/llmcompressor/args/training_arguments/#llmcompressor.args.training_arguments.TrainingArguments","title":"<code>TrainingArguments</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingArguments</code></p> <p>Training arguments specific to LLM Compressor Transformers workflow using HFTrainingArgs as base class</p> Source code in <code>src/llmcompressor/args/training_arguments.py</code> <pre><code>@dataclass\nclass TrainingArguments(HFTrainingArgs):\n    \"\"\"\n    Training arguments specific to LLM Compressor Transformers workflow using\n    HFTrainingArgs as base class\n\n    \"\"\"\n\n    do_oneshot: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to run one-shot calibration in stages\"},\n    )\n    run_stages: Optional[bool] = field(\n        default=False, metadata={\"help\": \"Whether to trigger recipe stage by stage\"}\n    )\n    output_dir: str = field(\n        default=\"./output\",\n        metadata={\n            \"help\": \"The output directory where the model safetensors, \"\n            \"recipe, config, and optionally checkpoints will be written.\"\n        },\n    )\n\n    @property\n    def place_model_on_device(self):\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/args/utils/","title":"llmcompressor.args.utils","text":""},{"location":"reference/llmcompressor/args/utils/#llmcompressor.args.utils.parse_args","title":"<code>parse_args(include_training_args=False, **kwargs)</code>","text":"<p>Keyword arguments passed in from <code>oneshot</code> or <code>train</code> will separate the arguments into the following:</p> <pre><code>* ModelArguments in\n    src/llmcompressor/args/model_args.py\n* DatasetArguments in\n    src/llmcompressor/args/dataset_args.py\n* RecipeArguments in\n    src/llmcompressor/args/recipe_args.py\n* TrainingArguments in\n    src/llmcompressor/args/training_args.py\n</code></pre> <p>ModelArguments, DatasetArguments, and RecipeArguments are used for both <code>oneshot</code> and <code>train</code>. TrainingArguments is only used for <code>train</code>.</p> Source code in <code>src/llmcompressor/args/utils.py</code> <pre><code>def parse_args(\n    include_training_args: bool = False, **kwargs\n) -&gt; Tuple[ModelArguments, DatasetArguments, RecipeArguments, TrainingArguments, str]:\n    \"\"\"\n    Keyword arguments passed in from `oneshot` or `train` will\n    separate the arguments into the following:\n\n        * ModelArguments in\n            src/llmcompressor/args/model_args.py\n        * DatasetArguments in\n            src/llmcompressor/args/dataset_args.py\n        * RecipeArguments in\n            src/llmcompressor/args/recipe_args.py\n        * TrainingArguments in\n            src/llmcompressor/args/training_args.py\n\n    ModelArguments, DatasetArguments, and RecipeArguments are used for both\n    `oneshot` and `train`. TrainingArguments is only used for `train`.\n\n    \"\"\"\n\n    # pop output_dir, used as an attr in TrainingArguments, where oneshot is not used\n    output_dir = kwargs.pop(\"output_dir\", None)\n\n    parser_args = (ModelArguments, DatasetArguments, RecipeArguments)\n    if include_training_args:\n        parser_args += (TrainingArguments,)\n\n    parser = HfArgumentParser(parser_args)\n    parsed_args = parser.parse_dict(kwargs)\n\n    training_args = None\n    if include_training_args:\n        model_args, dataset_args, recipe_args, training_args = parsed_args\n        if output_dir is not None:\n            training_args.output_dir = output_dir\n    else:\n        model_args, dataset_args, recipe_args = parsed_args\n\n    if recipe_args.recipe_args is not None:\n        if not isinstance(recipe_args.recipe_args, dict):\n            arg_dict = {}\n            for recipe_arg in recipe_args.recipe_args:\n                key, value = recipe_arg.split(\"=\")\n                arg_dict[key] = value\n            recipe_args.recipe_args = arg_dict\n\n    # raise depreciation warnings\n    if dataset_args.remove_columns is not None:\n        logger.warn(\n            \"`remove_columns` argument is depreciated. When tokenizing datasets, all \"\n            \"columns which are invalid inputs the tokenizer will be removed\",\n            DeprecationWarning,\n        )\n\n    # silently assign tokenizer to processor\n    resolve_processor_from_model_args(model_args)\n\n    return model_args, dataset_args, recipe_args, training_args, output_dir\n</code></pre>"},{"location":"reference/llmcompressor/core/","title":"llmcompressor.core","text":""},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionLifecycle","title":"<code>CompressionLifecycle</code>  <code>dataclass</code>","text":"<p>A class for managing the lifecycle of compression events in the LLM Compressor.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the compression process</p> <code>State()</code> <code>recipe_container</code> <code>RecipeContainer</code> <p>The container for the compression recipe</p> <code>RecipeContainer()</code> <code>modifiers</code> <code>List[StageModifiers]</code> <p>The list of stage modifiers</p> <code>list()</code> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>@dataclass\nclass CompressionLifecycle:\n    \"\"\"\n    A class for managing the lifecycle of compression events in the LLM Compressor.\n\n    :param state: The current state of the compression process\n    :type state: Optional[State]\n    :param recipe_container: The container for the compression recipe\n    :type recipe_container: RecipeContainer\n    :param modifiers: The list of stage modifiers\n    :type modifiers: List[StageModifiers]\n    \"\"\"\n\n    state: State = field(default_factory=State)\n    recipe_container: RecipeContainer = field(default_factory=RecipeContainer)\n    modifiers: List[StageModifiers] = field(default_factory=list)\n\n    initialized_: bool = False\n    finalized: bool = False\n\n    # event order validation\n    _last_event_type: Optional[EventType] = EventType.BATCH_END\n    _event_order: List[EventType] = field(\n        default_factory=lambda: [\n            EventType.BATCH_START,\n            EventType.LOSS_CALCULATED,\n            EventType.OPTIM_PRE_STEP,\n            EventType.OPTIM_POST_STEP,\n            EventType.BATCH_END,\n        ]\n    )\n\n    # track global step in training (could be epoch/batch)\n    global_step: int = 0\n\n    def reset(self):\n        \"\"\"\n        Reset the compression lifecycle, finalizing any active modifiers\n        and resetting all attributes.\n        \"\"\"\n        logger.debug(\"Resetting compression lifecycle\")\n\n        for mod in self.modifiers:\n            if not mod.initialized or mod.finalized:\n                continue\n            try:\n                mod.finalize(self.state)\n                logger.debug(\"Finalized modifier: {}\", mod)\n            except Exception as e:\n                logger.warning(f\"Exception during finalizing modifier: {e}\")\n\n        self.__init__()\n        logger.info(\"Compression lifecycle reset\")\n\n    def initialize(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n        **kwargs,\n    ) -&gt; List[Any]:\n        \"\"\"\n        Initialize the compression lifecycle.\n\n        :param kwargs: Additional arguments to update the state with\n        :return: List of data returned from initialization of modifiers\n        :rtype: List[Any]\n        \"\"\"\n        self.state.update(**kwargs)\n        if self.initialized_:  # TODO: do not initialize twice\n            return\n\n        logger.debug(\"Initializing compression lifecycle\")\n        self.recipe_container.append(recipe, recipe_stage, recipe_args)\n        self.modifiers = self.recipe_container.get_modifiers()\n\n        mod_data = []\n        for mod in self.modifiers:\n            data = mod.initialize(state=self.state, **kwargs)\n            logger.debug(\"Initialized modifier: {}\", mod)\n            if data is not None:\n                mod_data.append(data)\n\n        self.initialized_ = True\n        logger.info(\n            \"Compression lifecycle initialized for {} modifiers\", len(self.modifiers)\n        )\n\n        return mod_data\n\n    def finalize(self, **kwargs) -&gt; List[Any]:\n        \"\"\"\n        Finalize the compression lifecycle.\n\n        :param kwargs: Additional arguments to update the state with\n        :return: List of data returned from finalizing modifiers\n        :rtype: List[Any]\n        :raises ValueError: If called before initialization or more than once\n        \"\"\"\n        if not self.initialized_:\n            logger.error(\"Cannot finalize before initializing\")\n            raise ValueError(\"Cannot finalize before initializing\")\n\n        if self.finalized:\n            logger.error(\"Cannot finalize more than once\")\n            raise ValueError(\"Cannot finalize more than once\")\n\n        logger.debug(\"Finalizing compression lifecycle\")\n        mod_data = []\n        for mod in self.modifiers:\n            data = mod.finalize(state=self.state, **kwargs)\n            logger.debug(\"Finalized modifier: {}\", mod)\n            if data is not None:\n                mod_data.append(data)\n\n        self.finalized = True\n        applied_stage_names = [mod.unique_id for mod in self.modifiers if mod.applied]\n        self.recipe_container.update_applied_stages(applied_stage_names)\n\n        logger.info(\n            \"Compression lifecycle finalized for {} modifiers\", len(self.modifiers)\n        )\n\n        return mod_data\n\n    def event(\n        self, event_type: EventType, global_step: Optional[int] = 0, **kwargs\n    ) -&gt; List[Any]:\n        \"\"\"\n        Handle a compression event.\n\n        :param event_type: The type of event to handle\n        :type event_type: EventType\n        :param kwargs: Additional arguments to pass to the event handlers\n        :return: List of data returned from handling the event by modifiers\n        :rtype: List[Any]\n        :raises ValueError: If called before initialization, after finalization,\n            or for an invalid event type\n        \"\"\"\n        if not self.initialized_:\n            logger.error(\"Cannot invoke event before initializing\")\n            raise ValueError(\"Cannot invoke event before initializing\")\n\n        if self.finalized:\n            logger.error(\"Cannot invoke event after finalizing\")\n            raise ValueError(\"Cannot invoke event after finalizing\")\n\n        if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n            logger.error(\n                \"Cannot invoke {} event. Use the corresponding method instead.\",\n                event_type,\n            )\n            raise ValueError(\n                f\"Cannot invoke {event_type} event. \"\n                f\"Use the corresponding method instead.\"\n            )\n\n        if not self._validate_event_order(event_type):\n            raise ValueError(\n                f\"Lifecycle events must appear following order: {self._event_order}. \"\n                f\"Instead, {self._last_event_type} was called before {event_type}\"\n            )\n\n        if event_type == EventType.LOSS_CALCULATED and (\n            \"loss\" not in kwargs or kwargs[\"loss\"] is None\n        ):\n            logger.error(\"Loss must be provided for loss calculated event\")\n            raise ValueError(\"Loss must be provided for loss calculated event\")\n\n        logger.debug(\"Handling event: {}\", event_type)\n\n        # update global step\n        if global_step is not None:\n            self.global_step = global_step\n\n        event = Event(type_=event_type)\n        mod_data = []\n        for mod in self.modifiers:\n            data = mod.update_event(state=self.state, event=event, **kwargs)\n            logger.debug(\"Updated event with modifier: {}\", mod)\n            if data is not None:\n                mod_data.append(data)\n\n        assert (\n            event is not None\n        ), f\"Event lifecycle did not return an event for {event_type}\"\n\n        return mod_data\n\n    def _validate_event_order(self, event_type: EventType) -&gt; bool:\n        if event_type not in self._event_order:\n            # for unhandled events, do not save last event\n            return True\n\n        if event_type == EventType.BATCH_START:\n            valid = self._last_event_type != EventType.BATCH_START\n\n        else:\n            last_event_index = self._event_order.index(self._last_event_type)\n            curr_event_index = self._event_order.index(event_type)\n            valid = last_event_index &lt;= curr_event_index\n\n        if valid:\n            self._last_event_type = event_type\n        return valid\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionLifecycle.event","title":"<code>event(event_type, global_step=0, **kwargs)</code>","text":"<p>Handle a compression event.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>The type of event to handle</p> required <code>kwargs</code> <p>Additional arguments to pass to the event handlers</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of data returned from handling the event by modifiers</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before initialization, after finalization, or for an invalid event type</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def event(\n    self, event_type: EventType, global_step: Optional[int] = 0, **kwargs\n) -&gt; List[Any]:\n    \"\"\"\n    Handle a compression event.\n\n    :param event_type: The type of event to handle\n    :type event_type: EventType\n    :param kwargs: Additional arguments to pass to the event handlers\n    :return: List of data returned from handling the event by modifiers\n    :rtype: List[Any]\n    :raises ValueError: If called before initialization, after finalization,\n        or for an invalid event type\n    \"\"\"\n    if not self.initialized_:\n        logger.error(\"Cannot invoke event before initializing\")\n        raise ValueError(\"Cannot invoke event before initializing\")\n\n    if self.finalized:\n        logger.error(\"Cannot invoke event after finalizing\")\n        raise ValueError(\"Cannot invoke event after finalizing\")\n\n    if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n        logger.error(\n            \"Cannot invoke {} event. Use the corresponding method instead.\",\n            event_type,\n        )\n        raise ValueError(\n            f\"Cannot invoke {event_type} event. \"\n            f\"Use the corresponding method instead.\"\n        )\n\n    if not self._validate_event_order(event_type):\n        raise ValueError(\n            f\"Lifecycle events must appear following order: {self._event_order}. \"\n            f\"Instead, {self._last_event_type} was called before {event_type}\"\n        )\n\n    if event_type == EventType.LOSS_CALCULATED and (\n        \"loss\" not in kwargs or kwargs[\"loss\"] is None\n    ):\n        logger.error(\"Loss must be provided for loss calculated event\")\n        raise ValueError(\"Loss must be provided for loss calculated event\")\n\n    logger.debug(\"Handling event: {}\", event_type)\n\n    # update global step\n    if global_step is not None:\n        self.global_step = global_step\n\n    event = Event(type_=event_type)\n    mod_data = []\n    for mod in self.modifiers:\n        data = mod.update_event(state=self.state, event=event, **kwargs)\n        logger.debug(\"Updated event with modifier: {}\", mod)\n        if data is not None:\n            mod_data.append(data)\n\n    assert (\n        event is not None\n    ), f\"Event lifecycle did not return an event for {event_type}\"\n\n    return mod_data\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionLifecycle.finalize","title":"<code>finalize(**kwargs)</code>","text":"<p>Finalize the compression lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Additional arguments to update the state with</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of data returned from finalizing modifiers</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before initialization or more than once</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def finalize(self, **kwargs) -&gt; List[Any]:\n    \"\"\"\n    Finalize the compression lifecycle.\n\n    :param kwargs: Additional arguments to update the state with\n    :return: List of data returned from finalizing modifiers\n    :rtype: List[Any]\n    :raises ValueError: If called before initialization or more than once\n    \"\"\"\n    if not self.initialized_:\n        logger.error(\"Cannot finalize before initializing\")\n        raise ValueError(\"Cannot finalize before initializing\")\n\n    if self.finalized:\n        logger.error(\"Cannot finalize more than once\")\n        raise ValueError(\"Cannot finalize more than once\")\n\n    logger.debug(\"Finalizing compression lifecycle\")\n    mod_data = []\n    for mod in self.modifiers:\n        data = mod.finalize(state=self.state, **kwargs)\n        logger.debug(\"Finalized modifier: {}\", mod)\n        if data is not None:\n            mod_data.append(data)\n\n    self.finalized = True\n    applied_stage_names = [mod.unique_id for mod in self.modifiers if mod.applied]\n    self.recipe_container.update_applied_stages(applied_stage_names)\n\n    logger.info(\n        \"Compression lifecycle finalized for {} modifiers\", len(self.modifiers)\n    )\n\n    return mod_data\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionLifecycle.initialize","title":"<code>initialize(recipe=None, recipe_stage=None, recipe_args=None, **kwargs)</code>","text":"<p>Initialize the compression lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Additional arguments to update the state with</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of data returned from initialization of modifiers</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def initialize(\n    self,\n    recipe: Optional[RecipeInput] = None,\n    recipe_stage: Optional[RecipeStageInput] = None,\n    recipe_args: Optional[RecipeArgsInput] = None,\n    **kwargs,\n) -&gt; List[Any]:\n    \"\"\"\n    Initialize the compression lifecycle.\n\n    :param kwargs: Additional arguments to update the state with\n    :return: List of data returned from initialization of modifiers\n    :rtype: List[Any]\n    \"\"\"\n    self.state.update(**kwargs)\n    if self.initialized_:  # TODO: do not initialize twice\n        return\n\n    logger.debug(\"Initializing compression lifecycle\")\n    self.recipe_container.append(recipe, recipe_stage, recipe_args)\n    self.modifiers = self.recipe_container.get_modifiers()\n\n    mod_data = []\n    for mod in self.modifiers:\n        data = mod.initialize(state=self.state, **kwargs)\n        logger.debug(\"Initialized modifier: {}\", mod)\n        if data is not None:\n            mod_data.append(data)\n\n    self.initialized_ = True\n    logger.info(\n        \"Compression lifecycle initialized for {} modifiers\", len(self.modifiers)\n    )\n\n    return mod_data\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionLifecycle.reset","title":"<code>reset()</code>","text":"<p>Reset the compression lifecycle, finalizing any active modifiers and resetting all attributes.</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the compression lifecycle, finalizing any active modifiers\n    and resetting all attributes.\n    \"\"\"\n    logger.debug(\"Resetting compression lifecycle\")\n\n    for mod in self.modifiers:\n        if not mod.initialized or mod.finalized:\n            continue\n        try:\n            mod.finalize(self.state)\n            logger.debug(\"Finalized modifier: {}\", mod)\n        except Exception as e:\n            logger.warning(f\"Exception during finalizing modifier: {e}\")\n\n    self.__init__()\n    logger.info(\"Compression lifecycle reset\")\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession","title":"<code>CompressionSession</code>","text":"<p>A session for compression that holds the lifecycle and state for the current compression session</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>class CompressionSession:\n    \"\"\"\n    A session for compression that holds the lifecycle\n    and state for the current compression session\n    \"\"\"\n\n    def __init__(self):\n        self._lifecycle = CompressionLifecycle()\n\n    @property\n    def lifecycle(self) -&gt; CompressionLifecycle:\n        \"\"\"\n        Lifecycle is used to keep track of where we are in the compression\n        process and what modifiers are active. It also provides the ability\n        to invoke events on the lifecycle.\n\n        :return: the lifecycle for the session\n        \"\"\"\n        return self._lifecycle\n\n    @property\n    def state(self) -&gt; State:\n        \"\"\"\n        State of the current compression session. State instance\n        is used to store all information such as the recipe, model\n        optimizer, data, etc. that is needed for compression.\n\n        :return: the current state of the session\n        \"\"\"\n        return self._lifecycle.state\n\n    def initialize(\n        self,\n        recipe: Union[str, List[str], \"Recipe\", List[\"Recipe\"], None] = None,\n        recipe_stage: Union[str, List[str], None] = None,\n        recipe_args: Union[Dict[str, Any], None] = None,\n        model: Optional[Any] = None,\n        teacher_model: Optional[Any] = None,\n        optimizer: Optional[Any] = None,\n        attach_optim_callbacks: bool = True,\n        train_data: Optional[Any] = None,\n        val_data: Optional[Any] = None,\n        test_data: Optional[Any] = None,\n        calib_data: Optional[Any] = None,\n        copy_data: bool = True,\n        start: Optional[float] = None,\n        steps_per_epoch: Optional[int] = None,\n        batches_per_step: Optional[int] = None,\n        loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n        **kwargs,\n    ) -&gt; ModifiedState:\n        \"\"\"\n        Initialize the session for compression. This will run the initialize method\n        for each modifier in the session's lifecycle. This will also set the session's\n        state to the initialized state.\n\n        :param recipe: the recipe to use for the compression, can be a path to a\n            recipe file, a raw recipe string, a recipe object, or a list\n            of recipe objects.\n        :param recipe_stage: the stage to target for the compression\n        :param recipe_args: the args to use for overriding the recipe defaults\n        :param model: the model to compress\n        :param teacher_model: the teacher model to use for knowledge distillation\n        :param optimizer: the optimizer to use for the compression\n        :param attach_optim_callbacks: True to attach the optimizer callbacks to the\n            compression lifecycle, False otherwise\n        :param train_data: the training data to use for the compression\n        :param val_data: the validation data to use for the compression\n        :param test_data: the testing data to use for the compression\n        :param calib_data: the calibration data to use for the compression\n        :param copy_data: True to copy the data, False otherwise\n        :param start: the start epoch to use for the compression\n        :param steps_per_epoch: the number of steps per epoch to use for the\n            compression\n        :param batches_per_step: the number of batches per step to use for\n            compression\n        :param loggers: the metrics manager to setup logging important info\n            and milestones to, also accepts a list of BaseLogger(s)\n        :param kwargs: additional kwargs to pass to the lifecycle's initialize method\n        :return: the modified state of the session after initializing\n        \"\"\"\n        mod_data = self._lifecycle.initialize(\n            recipe=recipe,\n            recipe_stage=recipe_stage,\n            recipe_args=recipe_args,\n            model=model,\n            teacher_model=teacher_model,\n            optimizer=optimizer,\n            attach_optim_callbacks=attach_optim_callbacks,\n            train_data=train_data,\n            val_data=val_data,\n            test_data=test_data,\n            calib_data=calib_data,\n            copy_data=copy_data,\n            start=start,\n            steps_per_epoch=steps_per_epoch,\n            batches_per_step=batches_per_step,\n            loggers=loggers,\n            **kwargs,\n        )\n\n        return ModifiedState(\n            model=self.state.model,\n            optimizer=self.state.optimizer,\n            loss=self.state.loss,\n            modifier_data=mod_data,\n        )\n\n    def finalize(self, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Finalize the session for compression. This will run the finalize method\n        for each modifier in the session's lifecycle. This will also set the session's\n        state to the finalized state.\n\n        :param kwargs: additional kwargs to pass to the lifecycle's finalize method\n        :return: the modified state of the session after finalizing\n        \"\"\"\n        mod_data = self._lifecycle.finalize(**kwargs)\n\n        return ModifiedState(\n            model=self.state.model,\n            optimizer=self.state.optimizer,\n            loss=self.state.loss,\n            modifier_data=mod_data,\n        )\n\n    def event(\n        self,\n        event_type: EventType,\n        batch_data: Optional[Any] = None,\n        loss: Optional[Any] = None,\n        **kwargs,\n    ) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an event for current CompressionSession.\n\n        :param event_type: the event type to invoke\n        :param batch_data: the batch data to use for the event\n        :param loss: the loss to use for the event if any\n        :param kwargs: additional kwargs to pass to the lifecycle's event method\n        :return: the modified state of the session after invoking the event\n        \"\"\"\n        mod_data = self._lifecycle.event(\n            event_type=event_type, batch_data=batch_data, loss=loss, **kwargs\n        )\n        return ModifiedState(\n            model=self.state.model,\n            optimizer=self.state.optimizer,\n            loss=self.state.loss,  # TODO: is this supposed to be a different type?\n            modifier_data=mod_data,\n        )\n\n    def log(self, event_type: EventType, loss: Optional[Any] = None):\n        \"\"\"\n        Log model and loss information for the current event type\n\n        :param event_type: the event type to log for\n        :param loss: the loss to log if any\n        \"\"\"\n        self._log_model_info()\n        self._log_loss(event_type=event_type, loss=loss)\n\n    def reset(self):\n        \"\"\"\n        Reset the session to its initial state\n        \"\"\"\n        self._lifecycle.reset()\n\n    def reset_stage(self):\n        \"\"\"\n        Reset the session for starting a new stage, recipe and model stays intact\n        \"\"\"\n        self.lifecycle.initialized_ = False\n        self.lifecycle.finalized = False\n\n    def get_serialized_recipe(self) -&gt; Optional[str]:\n        \"\"\"\n        :return: serialized string of the current compiled recipe\n        \"\"\"\n        recipe = self.lifecycle.recipe_container.compiled_recipe\n\n        if recipe is not None and hasattr(recipe, \"yaml\"):\n            return recipe.yaml()\n\n        logger.warning(\"Recipe not found in session - it may have been reset\")\n\n    def get_modifiers(self):\n        \"\"\"\n        Get all modifiers across all stages\n        \"\"\"\n        stage_modifiers = self.lifecycle.modifiers\n        return [\n            modifier\n            for stage_modifier in stage_modifiers\n            for modifier in stage_modifier.modifiers\n        ]  # noqa: E127\n\n    def _log_model_info(self):\n        # Log model level logs if cadence reached\n        current_index = self._lifecycle.global_step\n\n        if (\n            should_log_model_info(\n                model=self.state.model,\n                loggers=self.state.loggers,\n                current_log_step=current_index,\n                last_log_step=self.state._last_log_step,\n            )\n            and self.state.loggers.frequency_manager.is_epoch_frequency_manager\n        ):\n            log_model_info(\n                state=self.state,\n                current_log_step=current_index,\n            )\n            # update last log epoch\n            self.state.loggers.log_written(current_index)\n\n    def _log_loss(self, event_type: EventType, loss: Any):\n        if event_type != EventType.LOSS_CALCULATED:\n            # only log loss when loss is calculated\n            return\n\n        current_index = self._lifecycle.global_step\n\n        # always log loss if available\n        if loss is not None:\n            loss = loss if isinstance(loss, dict) else {\"loss\": loss}\n            self.state.loggers.metric.log_scalars(\n                tag=\"Loss\", values=loss, step=current_index\n            )\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.lifecycle","title":"<code>lifecycle</code>  <code>property</code>","text":"<p>Lifecycle is used to keep track of where we are in the compression process and what modifiers are active. It also provides the ability to invoke events on the lifecycle.</p> <p>Returns:</p> Type Description <code>CompressionLifecycle</code> <p>the lifecycle for the session</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.state","title":"<code>state</code>  <code>property</code>","text":"<p>State of the current compression session. State instance is used to store all information such as the recipe, model optimizer, data, etc. that is needed for compression.</p> <p>Returns:</p> Type Description <code>State</code> <p>the current state of the session</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.event","title":"<code>event(event_type, batch_data=None, loss=None, **kwargs)</code>","text":"<p>Invoke an event for current CompressionSession.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>the event type to invoke</p> required <code>batch_data</code> <code>Optional[Any]</code> <p>the batch data to use for the event</p> <code>None</code> <code>loss</code> <code>Optional[Any]</code> <p>the loss to use for the event if any</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the lifecycle's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the session after invoking the event</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def event(\n    self,\n    event_type: EventType,\n    batch_data: Optional[Any] = None,\n    loss: Optional[Any] = None,\n    **kwargs,\n) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an event for current CompressionSession.\n\n    :param event_type: the event type to invoke\n    :param batch_data: the batch data to use for the event\n    :param loss: the loss to use for the event if any\n    :param kwargs: additional kwargs to pass to the lifecycle's event method\n    :return: the modified state of the session after invoking the event\n    \"\"\"\n    mod_data = self._lifecycle.event(\n        event_type=event_type, batch_data=batch_data, loss=loss, **kwargs\n    )\n    return ModifiedState(\n        model=self.state.model,\n        optimizer=self.state.optimizer,\n        loss=self.state.loss,  # TODO: is this supposed to be a different type?\n        modifier_data=mod_data,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.finalize","title":"<code>finalize(**kwargs)</code>","text":"<p>Finalize the session for compression. This will run the finalize method for each modifier in the session's lifecycle. This will also set the session's state to the finalized state.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the lifecycle's finalize method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the session after finalizing</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def finalize(self, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Finalize the session for compression. This will run the finalize method\n    for each modifier in the session's lifecycle. This will also set the session's\n    state to the finalized state.\n\n    :param kwargs: additional kwargs to pass to the lifecycle's finalize method\n    :return: the modified state of the session after finalizing\n    \"\"\"\n    mod_data = self._lifecycle.finalize(**kwargs)\n\n    return ModifiedState(\n        model=self.state.model,\n        optimizer=self.state.optimizer,\n        loss=self.state.loss,\n        modifier_data=mod_data,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.get_modifiers","title":"<code>get_modifiers()</code>","text":"<p>Get all modifiers across all stages</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def get_modifiers(self):\n    \"\"\"\n    Get all modifiers across all stages\n    \"\"\"\n    stage_modifiers = self.lifecycle.modifiers\n    return [\n        modifier\n        for stage_modifier in stage_modifiers\n        for modifier in stage_modifier.modifiers\n    ]  # noqa: E127\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.get_serialized_recipe","title":"<code>get_serialized_recipe()</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>serialized string of the current compiled recipe</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def get_serialized_recipe(self) -&gt; Optional[str]:\n    \"\"\"\n    :return: serialized string of the current compiled recipe\n    \"\"\"\n    recipe = self.lifecycle.recipe_container.compiled_recipe\n\n    if recipe is not None and hasattr(recipe, \"yaml\"):\n        return recipe.yaml()\n\n    logger.warning(\"Recipe not found in session - it may have been reset\")\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.initialize","title":"<code>initialize(recipe=None, recipe_stage=None, recipe_args=None, model=None, teacher_model=None, optimizer=None, attach_optim_callbacks=True, train_data=None, val_data=None, test_data=None, calib_data=None, copy_data=True, start=None, steps_per_epoch=None, batches_per_step=None, loggers=None, **kwargs)</code>","text":"<p>Initialize the session for compression. This will run the initialize method for each modifier in the session's lifecycle. This will also set the session's state to the initialized state.</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>Union[str, List[str], Recipe, List[Recipe], None]</code> <p>the recipe to use for the compression, can be a path to a recipe file, a raw recipe string, a recipe object, or a list of recipe objects.</p> <code>None</code> <code>recipe_stage</code> <code>Union[str, List[str], None]</code> <p>the stage to target for the compression</p> <code>None</code> <code>recipe_args</code> <code>Union[Dict[str, Any], None]</code> <p>the args to use for overriding the recipe defaults</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>the model to compress</p> <code>None</code> <code>teacher_model</code> <code>Optional[Any]</code> <p>the teacher model to use for knowledge distillation</p> <code>None</code> <code>optimizer</code> <code>Optional[Any]</code> <p>the optimizer to use for the compression</p> <code>None</code> <code>attach_optim_callbacks</code> <code>bool</code> <p>True to attach the optimizer callbacks to the compression lifecycle, False otherwise</p> <code>True</code> <code>train_data</code> <code>Optional[Any]</code> <p>the training data to use for the compression</p> <code>None</code> <code>val_data</code> <code>Optional[Any]</code> <p>the validation data to use for the compression</p> <code>None</code> <code>test_data</code> <code>Optional[Any]</code> <p>the testing data to use for the compression</p> <code>None</code> <code>calib_data</code> <code>Optional[Any]</code> <p>the calibration data to use for the compression</p> <code>None</code> <code>copy_data</code> <code>bool</code> <p>True to copy the data, False otherwise</p> <code>True</code> <code>start</code> <code>Optional[float]</code> <p>the start epoch to use for the compression</p> <code>None</code> <code>steps_per_epoch</code> <code>Optional[int]</code> <p>the number of steps per epoch to use for the compression</p> <code>None</code> <code>batches_per_step</code> <code>Optional[int]</code> <p>the number of batches per step to use for compression</p> <code>None</code> <code>loggers</code> <code>Union[None, LoggerManager, List[BaseLogger]]</code> <p>the metrics manager to setup logging important info and milestones to, also accepts a list of BaseLogger(s)</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the lifecycle's initialize method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the session after initializing</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def initialize(\n    self,\n    recipe: Union[str, List[str], \"Recipe\", List[\"Recipe\"], None] = None,\n    recipe_stage: Union[str, List[str], None] = None,\n    recipe_args: Union[Dict[str, Any], None] = None,\n    model: Optional[Any] = None,\n    teacher_model: Optional[Any] = None,\n    optimizer: Optional[Any] = None,\n    attach_optim_callbacks: bool = True,\n    train_data: Optional[Any] = None,\n    val_data: Optional[Any] = None,\n    test_data: Optional[Any] = None,\n    calib_data: Optional[Any] = None,\n    copy_data: bool = True,\n    start: Optional[float] = None,\n    steps_per_epoch: Optional[int] = None,\n    batches_per_step: Optional[int] = None,\n    loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n    **kwargs,\n) -&gt; ModifiedState:\n    \"\"\"\n    Initialize the session for compression. This will run the initialize method\n    for each modifier in the session's lifecycle. This will also set the session's\n    state to the initialized state.\n\n    :param recipe: the recipe to use for the compression, can be a path to a\n        recipe file, a raw recipe string, a recipe object, or a list\n        of recipe objects.\n    :param recipe_stage: the stage to target for the compression\n    :param recipe_args: the args to use for overriding the recipe defaults\n    :param model: the model to compress\n    :param teacher_model: the teacher model to use for knowledge distillation\n    :param optimizer: the optimizer to use for the compression\n    :param attach_optim_callbacks: True to attach the optimizer callbacks to the\n        compression lifecycle, False otherwise\n    :param train_data: the training data to use for the compression\n    :param val_data: the validation data to use for the compression\n    :param test_data: the testing data to use for the compression\n    :param calib_data: the calibration data to use for the compression\n    :param copy_data: True to copy the data, False otherwise\n    :param start: the start epoch to use for the compression\n    :param steps_per_epoch: the number of steps per epoch to use for the\n        compression\n    :param batches_per_step: the number of batches per step to use for\n        compression\n    :param loggers: the metrics manager to setup logging important info\n        and milestones to, also accepts a list of BaseLogger(s)\n    :param kwargs: additional kwargs to pass to the lifecycle's initialize method\n    :return: the modified state of the session after initializing\n    \"\"\"\n    mod_data = self._lifecycle.initialize(\n        recipe=recipe,\n        recipe_stage=recipe_stage,\n        recipe_args=recipe_args,\n        model=model,\n        teacher_model=teacher_model,\n        optimizer=optimizer,\n        attach_optim_callbacks=attach_optim_callbacks,\n        train_data=train_data,\n        val_data=val_data,\n        test_data=test_data,\n        calib_data=calib_data,\n        copy_data=copy_data,\n        start=start,\n        steps_per_epoch=steps_per_epoch,\n        batches_per_step=batches_per_step,\n        loggers=loggers,\n        **kwargs,\n    )\n\n    return ModifiedState(\n        model=self.state.model,\n        optimizer=self.state.optimizer,\n        loss=self.state.loss,\n        modifier_data=mod_data,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.log","title":"<code>log(event_type, loss=None)</code>","text":"<p>Log model and loss information for the current event type</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>the event type to log for</p> required <code>loss</code> <code>Optional[Any]</code> <p>the loss to log if any</p> <code>None</code> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def log(self, event_type: EventType, loss: Optional[Any] = None):\n    \"\"\"\n    Log model and loss information for the current event type\n\n    :param event_type: the event type to log for\n    :param loss: the loss to log if any\n    \"\"\"\n    self._log_model_info()\n    self._log_loss(event_type=event_type, loss=loss)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.reset","title":"<code>reset()</code>","text":"<p>Reset the session to its initial state</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the session to its initial state\n    \"\"\"\n    self._lifecycle.reset()\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.CompressionSession.reset_stage","title":"<code>reset_stage()</code>","text":"<p>Reset the session for starting a new stage, recipe and model stays intact</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def reset_stage(self):\n    \"\"\"\n    Reset the session for starting a new stage, recipe and model stays intact\n    \"\"\"\n    self.lifecycle.initialized_ = False\n    self.lifecycle.finalized = False\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Data","title":"<code>Data</code>  <code>dataclass</code>","text":"<p>A dataclass to hold different data sets for training, validation, testing, and/or calibration. Each data set is a ModifiableData instance.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>Optional[Any]</code> <p>The training data set</p> <code>None</code> <code>val</code> <code>Optional[Any]</code> <p>The validation data set</p> <code>None</code> <code>test</code> <code>Optional[Any]</code> <p>The testing data set</p> <code>None</code> <code>calib</code> <code>Optional[Any]</code> <p>The calibration data set</p> <code>None</code> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass Data:\n    \"\"\"\n    A dataclass to hold different data sets for training, validation,\n    testing, and/or calibration. Each data set is a ModifiableData instance.\n\n    :param train: The training data set\n    :type train: Optional[Any]\n    :param val: The validation data set\n    :type val: Optional[Any]\n    :param test: The testing data set\n    :type test: Optional[Any]\n    :param calib: The calibration data set\n    :type calib: Optional[Any]\n    \"\"\"\n\n    train: Optional[Any] = None\n    val: Optional[Any] = None\n    test: Optional[Any] = None\n    calib: Optional[Any] = None\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event","title":"<code>Event</code>  <code>dataclass</code>","text":"<p>A class for defining an event that can be triggered during sparsification.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Optional[EventType]</code> <p>The type of event.</p> <code>None</code> <code>steps_per_epoch</code> <code>Optional[int]</code> <p>The number of steps per epoch.</p> <code>None</code> <code>batches_per_step</code> <code>Optional[int]</code> <p>The number of batches per step where step is an optimizer step invocation. For most pathways, these are the same. See the invocations_per_step parameter for more details when they are not.</p> <code>None</code> <code>invocations_per_step</code> <code>int</code> <p>The number of invocations of the step wrapper before optimizer.step was called. Generally can be left as 1 (default). For older amp pathways, this is the number of times the scaler wrapper was invoked before the wrapped optimizer step function was called to handle accumulation in fp16.</p> <code>1</code> <code>global_step</code> <code>int</code> <p>The current global step.</p> <code>0</code> <code>global_batch</code> <code>int</code> <p>The current global batch.</p> <code>0</code> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>@dataclass\nclass Event:\n    \"\"\"\n    A class for defining an event that can be triggered during sparsification.\n\n    :param type_: The type of event.\n    :type type_: Optional[EventType]\n    :param steps_per_epoch: The number of steps per epoch.\n    :type steps_per_epoch: Optional[int]\n    :param batches_per_step: The number of batches per step where step is an\n        optimizer step invocation. For most pathways, these are the same.\n        See the invocations_per_step parameter for more details when they are not.\n    :type batches_per_step: Optional[int]\n    :param invocations_per_step: The number of invocations of the step wrapper\n        before optimizer.step was called. Generally can be left as 1 (default).\n        For older amp pathways, this is the number of times the scaler wrapper\n        was invoked before the wrapped optimizer step function was called to\n        handle accumulation in fp16.\n    :type invocations_per_step: Optional[int]\n    :param global_step: The current global step.\n    :type global_step: int\n    :param global_batch: The current global batch.\n    :type global_batch: int\n    \"\"\"\n\n    type_: Optional[EventType] = None\n    steps_per_epoch: Optional[int] = None\n    batches_per_step: Optional[int] = None\n    invocations_per_step: int = 1\n    global_step: int = 0\n    global_batch: int = 0\n\n    @property\n    def epoch_based(self) -&gt; bool:\n        \"\"\"\n        Determines if the event is based on epochs.\n\n        :return: True if the event is based on epochs, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.steps_per_epoch is not None\n\n    @property\n    def epoch(self) -&gt; int:\n        \"\"\"\n        Calculates the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step // self.steps_per_epoch\n\n    @property\n    def epoch_full(self) -&gt; float:\n        \"\"\"\n        Calculates the current epoch with the fraction of the current step.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current epoch with the fraction of the current step.\n        :rtype: float\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_full for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step / float(self.steps_per_epoch)\n\n    @property\n    def epoch_step(self) -&gt; int:\n        \"\"\"\n        Calculates the current step within the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current step within the current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_step for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step % self.steps_per_epoch\n\n    @property\n    def epoch_batch(self) -&gt; int:\n        \"\"\"\n        Calculates the current batch within the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current batch within the current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_batch for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        batches_per_epoch = (\n            self.steps_per_epoch * self.batches_per_step\n            if self.batches_per_step\n            else self.steps_per_epoch\n        )\n        return self.global_batch % batches_per_epoch\n\n    @property\n    def current_index(self) -&gt; float:\n        \"\"\"\n        Calculates the current index of the event.\n\n        :raises ValueError: if the event is not epoch based or\n            if the steps per epoch are too many.\n        :return: The current index of the event, which is either the global step\n            or the epoch with the fraction of the current step.\n        :rtype: float\n        \"\"\"\n        if not self.epoch_based:\n            return self.global_step\n        epoch_full = self.epoch_full\n        if epoch_full - self.epoch &gt; 1.0:\n            logger.error(\"Too many steps per epoch for epoch based event\")\n            raise ValueError(\"Too many steps per epoch for epoch based event\")\n        return epoch_full\n\n    @current_index.setter\n    def current_index(self, value: float):\n        \"\"\"\n        Sets the current index of the event.\n\n        :param value: The current index value.\n        :type value: float\n        \"\"\"\n        logger.debug(\"Setting current index: {}\", value)\n        if not self.epoch_based:\n            self.global_step = int(value)\n            self.global_batch = (\n                self.global_step\n                if self.batches_per_step is None or self.batches_per_step &lt; 2\n                else self.global_step * self.batches_per_step\n            )\n        else:\n            self.global_step = int(value * self.steps_per_epoch)\n            self.global_batch = (\n                self.global_step\n                if self.batches_per_step is None or self.batches_per_step &lt; 2\n                else self.global_step * self.batches_per_step\n            )\n\n    def should_update(\n        self, start: Optional[float], end: Optional[float], update: Optional[float]\n    ) -&gt; bool:\n        \"\"\"\n        Determines if the event should trigger an update.\n\n        :param start: The start index to check against, set to None to ignore start.\n        :type start: Optional[float]\n        :param end: The end index to check against, set to None to ignore end.\n        :type end: Optional[float]\n        :param update: The update interval, set to None or 0.0 to always update,\n            otherwise must be greater than 0.0, defaults to None.\n        :type update: Optional[float]\n        :return: True if the event should trigger an update, False otherwise.\n        :rtype: bool\n        \"\"\"\n        current = self.current_index\n        logger.debug(\n            \"Checking if event should update: \"\n            \"current_index={}, start={}, end={}, update={}\",\n            current,\n            start,\n            end,\n            update,\n        )\n        if start is not None and current &lt; start:\n            return False\n        if end is not None and current &gt; end:\n            return False\n        return update is None or update &lt;= 0.0 or current % update &lt; 1e-10\n\n    def new_instance(self, **kwargs) -&gt; \"Event\":\n        \"\"\"\n        Creates a new instance of the event with the provided keyword arguments.\n\n        :param kwargs: Keyword arguments to set in the new instance.\n        :return: A new instance of the event with the provided kwargs.\n        :rtype: Event\n        \"\"\"\n        logger.debug(\"Creating new instance of event with kwargs: {}\", kwargs)\n        instance = deepcopy(self)\n        for key, value in kwargs.items():\n            setattr(instance, key, value)\n        return instance\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.current_index","title":"<code>current_index</code>  <code>property</code> <code>writable</code>","text":"<p>Calculates the current index of the event.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current index of the event, which is either the global step or the epoch with the fraction of the current step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based or if the steps per epoch are too many.</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.epoch","title":"<code>epoch</code>  <code>property</code>","text":"<p>Calculates the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.epoch_based","title":"<code>epoch_based</code>  <code>property</code>","text":"<p>Determines if the event is based on epochs.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the event is based on epochs, False otherwise.</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.epoch_batch","title":"<code>epoch_batch</code>  <code>property</code>","text":"<p>Calculates the current batch within the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current batch within the current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.epoch_full","title":"<code>epoch_full</code>  <code>property</code>","text":"<p>Calculates the current epoch with the fraction of the current step.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current epoch with the fraction of the current step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.epoch_step","title":"<code>epoch_step</code>  <code>property</code>","text":"<p>Calculates the current step within the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current step within the current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.new_instance","title":"<code>new_instance(**kwargs)</code>","text":"<p>Creates a new instance of the event with the provided keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments to set in the new instance.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Event</code> <p>A new instance of the event with the provided kwargs.</p> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>def new_instance(self, **kwargs) -&gt; \"Event\":\n    \"\"\"\n    Creates a new instance of the event with the provided keyword arguments.\n\n    :param kwargs: Keyword arguments to set in the new instance.\n    :return: A new instance of the event with the provided kwargs.\n    :rtype: Event\n    \"\"\"\n    logger.debug(\"Creating new instance of event with kwargs: {}\", kwargs)\n    instance = deepcopy(self)\n    for key, value in kwargs.items():\n        setattr(instance, key, value)\n    return instance\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Event.should_update","title":"<code>should_update(start, end, update)</code>","text":"<p>Determines if the event should trigger an update.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[float]</code> <p>The start index to check against, set to None to ignore start.</p> required <code>end</code> <code>Optional[float]</code> <p>The end index to check against, set to None to ignore end.</p> required <code>update</code> <code>Optional[float]</code> <p>The update interval, set to None or 0.0 to always update, otherwise must be greater than 0.0, defaults to None.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event should trigger an update, False otherwise.</p> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>def should_update(\n    self, start: Optional[float], end: Optional[float], update: Optional[float]\n) -&gt; bool:\n    \"\"\"\n    Determines if the event should trigger an update.\n\n    :param start: The start index to check against, set to None to ignore start.\n    :type start: Optional[float]\n    :param end: The end index to check against, set to None to ignore end.\n    :type end: Optional[float]\n    :param update: The update interval, set to None or 0.0 to always update,\n        otherwise must be greater than 0.0, defaults to None.\n    :type update: Optional[float]\n    :return: True if the event should trigger an update, False otherwise.\n    :rtype: bool\n    \"\"\"\n    current = self.current_index\n    logger.debug(\n        \"Checking if event should update: \"\n        \"current_index={}, start={}, end={}, update={}\",\n        current,\n        start,\n        end,\n        update,\n    )\n    if start is not None and current &lt; start:\n        return False\n    if end is not None and current &gt; end:\n        return False\n    return update is None or update &lt;= 0.0 or current % update &lt; 1e-10\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.EventType","title":"<code>EventType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An Enum for defining the different types of events that can be triggered during model compression lifecycles. The purpose of each EventType is to trigger the corresponding modifier callback during training or post training pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>INITIALIZE</code> <p>Event type for initialization.</p> required <code>FINALIZE</code> <p>Event type for finalization.</p> required <code>BATCH_START</code> <p>Event type for the start of a batch.</p> required <code>LOSS_CALCULATED</code> <p>Event type for when loss is calculated.</p> required <code>BATCH_END</code> <p>Event type for the end of a batch.</p> required <code>CALIBRATION_EPOCH_START</code> <p>Event type for the start of a calibration epoch.</p> required <code>SEQUENTIAL_EPOCH_END</code> <p>Event type for the end of a layer calibration epoch, specifically used by <code>src/llmcompressor/pipelines/sequential/pipeline.py</code></p> required <code>CALIBRATION_EPOCH_END</code> <p>Event type for the end of a calibration epoch.</p> required <code>OPTIM_PRE_STEP</code> <p>Event type for pre-optimization step.</p> required <code>OPTIM_POST_STEP</code> <p>Event type for post-optimization step.</p> required Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>@unique\nclass EventType(Enum):\n    \"\"\"\n    An Enum for defining the different types of events that can be triggered\n    during model compression lifecycles.\n    The purpose of each EventType is to trigger the corresponding\n    modifier callback during training or post training pipelines.\n\n    :param INITIALIZE: Event type for initialization.\n    :param FINALIZE: Event type for finalization.\n    :param BATCH_START: Event type for the start of a batch.\n    :param LOSS_CALCULATED: Event type for when loss is calculated.\n    :param BATCH_END: Event type for the end of a batch.\n    :param CALIBRATION_EPOCH_START: Event type for the start of a calibration epoch.\n    :param SEQUENTIAL_EPOCH_END: Event type for the end of a layer calibration epoch,\n        specifically used by `src/llmcompressor/pipelines/sequential/pipeline.py`\n    :param CALIBRATION_EPOCH_END: Event type for the end of a calibration epoch.\n    :param OPTIM_PRE_STEP: Event type for pre-optimization step.\n    :param OPTIM_POST_STEP: Event type for post-optimization step.\n    \"\"\"\n\n    # training lifecycle\n    INITIALIZE = \"initialize\"\n    FINALIZE = \"finalize\"\n\n    # batch lifecycle\n    BATCH_START = \"batch_start\"\n    LOSS_CALCULATED = \"loss_calculated\"\n    BATCH_END = \"batch_end\"\n\n    # calibration lifecycle\n    CALIBRATION_EPOCH_START = \"calibration_epoch_start\"\n    SEQUENTIAL_EPOCH_END = \"sequential_epoch_end\"\n    CALIBRATION_EPOCH_END = \"calibration_epoch_end\"\n\n    # step lifecycle\n    OPTIM_PRE_STEP = \"optim_pre_step\"\n    OPTIM_POST_STEP = \"optim_post_step\"\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.Hardware","title":"<code>Hardware</code>  <code>dataclass</code>","text":"<p>A dataclass to hold information about the hardware being used.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>The current device being used for training</p> <code>None</code> <code>devices</code> <code>Optional[List[str]]</code> <p>List of all devices to be used for training</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>The rank of the current device</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>The total number of devices being used</p> <code>None</code> <code>local_rank</code> <code>Optional[int]</code> <p>The local rank of the current device</p> <code>None</code> <code>local_world_size</code> <code>Optional[int]</code> <p>The total number of devices being used on the local machine</p> <code>None</code> <code>distributed</code> <code>Optional[bool]</code> <p>Whether or not distributed training is being used</p> <code>None</code> <code>distributed_strategy</code> <code>Optional[str]</code> <p>The distributed strategy being used</p> <code>None</code> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass Hardware:\n    \"\"\"\n    A dataclass to hold information about the hardware being used.\n\n    :param device: The current device being used for training\n    :type device: Optional[str]\n    :param devices: List of all devices to be used for training\n    :type devices: Optional[List[str]]\n    :param rank: The rank of the current device\n    :type rank: Optional[int]\n    :param world_size: The total number of devices being used\n    :type world_size: Optional[int]\n    :param local_rank: The local rank of the current device\n    :type local_rank: Optional[int]\n    :param local_world_size: The total number of devices being used on the local machine\n    :type local_world_size: Optional[int]\n    :param distributed: Whether or not distributed training is being used\n    :type distributed: Optional[bool]\n    :param distributed_strategy: The distributed strategy being used\n    :type distributed_strategy: Optional[str]\n    \"\"\"\n\n    device: Optional[str] = None\n    devices: Optional[List[str]] = None\n    rank: Optional[int] = None\n    world_size: Optional[int] = None\n    local_rank: Optional[int] = None\n    local_world_size: Optional[int] = None\n    distributed: Optional[bool] = None\n    distributed_strategy: Optional[str] = None\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks","title":"<code>LifecycleCallbacks</code>","text":"<p>A class for invoking lifecycle events for the active session</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>class LifecycleCallbacks:\n    \"\"\"\n    A class for invoking lifecycle events for the active session\n    \"\"\"\n\n    @classmethod\n    def event(cls, event_type: EventType, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an event for the active session\n\n        :param event_type: the event type to invoke\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n            raise ValueError(\n                f\"Cannot invoke {event_type} event. \"\n                f\"Use the corresponding method instead.\"\n            )\n\n        # skip event callbacks if no recipe was provided\n        if not active_session().lifecycle.recipe_container.check_any_recipe_exists():\n            return\n\n        return active_session().event(event_type, **kwargs)\n\n    @classmethod\n    def batch_start(cls, batch_data: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a batch start event for the active session\n\n        :param batch_data: the batch data to use for the event\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        return cls.event(EventType.BATCH_START, batch_data=batch_data, **kwargs)\n\n    @classmethod\n    def loss_calculated(cls, loss: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a loss calculated event for the active session\n\n        :param loss: the loss to use for the event\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        # log loss if loss calculated\n        active_session()._log_loss(event_type=EventType.LOSS_CALCULATED, loss=loss)\n        return cls.event(EventType.LOSS_CALCULATED, loss=loss, **kwargs)\n\n    @classmethod\n    def optim_pre_step(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an optimizer pre-step event for the active session\n\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        return cls.event(EventType.OPTIM_PRE_STEP, **kwargs)\n\n    @classmethod\n    def optim_post_step(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an optimizer post-step event for the active session\n\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        return cls.event(EventType.OPTIM_POST_STEP, **kwargs)\n\n    @classmethod\n    def batch_end(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a batch end event for the active session\n\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        active_session()._log_model_info()\n        return cls.event(EventType.BATCH_END, **kwargs)\n\n    @classmethod\n    def calibration_epoch_start(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a epoch start event for the active session during calibration. This event\n        should be called before calibration starts for one epoch\n\n        see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n        \"\"\"\n        return cls.event(EventType.CALIBRATION_EPOCH_START, **kwargs)\n\n    @classmethod\n    def sequential_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a sequential epoch end event for the active session. This event should be\n        called after one sequential layer has been calibrated/trained for one epoch\n\n        This is called after a sequential layer has been calibrated with one batch, see\n        `src/llmcompressor/pipelines/sequential/pipeline.py` for usage example\n        \"\"\"\n        return cls.event(EventType.SEQUENTIAL_EPOCH_END, **kwargs)\n\n    @classmethod\n    def calibration_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a epoch end event for the active session during calibration. This event\n        should be called after the model has been calibrated for one epoch\n\n        see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n        \"\"\"\n        return cls.event(EventType.CALIBRATION_EPOCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.batch_end","title":"<code>batch_end(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a batch end event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef batch_end(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a batch end event for the active session\n\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    active_session()._log_model_info()\n    return cls.event(EventType.BATCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.batch_start","title":"<code>batch_start(batch_data=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a batch start event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Optional[Any]</code> <p>the batch data to use for the event</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef batch_start(cls, batch_data: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a batch start event for the active session\n\n    :param batch_data: the batch data to use for the event\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    return cls.event(EventType.BATCH_START, batch_data=batch_data, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.calibration_epoch_end","title":"<code>calibration_epoch_end(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a epoch end event for the active session during calibration. This event should be called after the model has been calibrated for one epoch</p> <p>see <code>src/llmcompressor/pipelines/basic/pipeline.py</code> for usage example</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef calibration_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a epoch end event for the active session during calibration. This event\n    should be called after the model has been calibrated for one epoch\n\n    see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n    \"\"\"\n    return cls.event(EventType.CALIBRATION_EPOCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.calibration_epoch_start","title":"<code>calibration_epoch_start(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a epoch start event for the active session during calibration. This event should be called before calibration starts for one epoch</p> <p>see <code>src/llmcompressor/pipelines/basic/pipeline.py</code> for usage example</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef calibration_epoch_start(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a epoch start event for the active session during calibration. This event\n    should be called before calibration starts for one epoch\n\n    see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n    \"\"\"\n    return cls.event(EventType.CALIBRATION_EPOCH_START, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.event","title":"<code>event(event_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke an event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>the event type to invoke</p> required <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef event(cls, event_type: EventType, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an event for the active session\n\n    :param event_type: the event type to invoke\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n        raise ValueError(\n            f\"Cannot invoke {event_type} event. \"\n            f\"Use the corresponding method instead.\"\n        )\n\n    # skip event callbacks if no recipe was provided\n    if not active_session().lifecycle.recipe_container.check_any_recipe_exists():\n        return\n\n    return active_session().event(event_type, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.loss_calculated","title":"<code>loss_calculated(loss=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a loss calculated event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Optional[Any]</code> <p>the loss to use for the event</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef loss_calculated(cls, loss: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a loss calculated event for the active session\n\n    :param loss: the loss to use for the event\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    # log loss if loss calculated\n    active_session()._log_loss(event_type=EventType.LOSS_CALCULATED, loss=loss)\n    return cls.event(EventType.LOSS_CALCULATED, loss=loss, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.optim_post_step","title":"<code>optim_post_step(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke an optimizer post-step event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef optim_post_step(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an optimizer post-step event for the active session\n\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    return cls.event(EventType.OPTIM_POST_STEP, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.optim_pre_step","title":"<code>optim_pre_step(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke an optimizer pre-step event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef optim_pre_step(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an optimizer pre-step event for the active session\n\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    return cls.event(EventType.OPTIM_PRE_STEP, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.LifecycleCallbacks.sequential_epoch_end","title":"<code>sequential_epoch_end(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a sequential epoch end event for the active session. This event should be called after one sequential layer has been calibrated/trained for one epoch</p> <p>This is called after a sequential layer has been calibrated with one batch, see <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> for usage example</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef sequential_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a sequential epoch end event for the active session. This event should be\n    called after one sequential layer has been calibrated/trained for one epoch\n\n    This is called after a sequential layer has been calibrated with one batch, see\n    `src/llmcompressor/pipelines/sequential/pipeline.py` for usage example\n    \"\"\"\n    return cls.event(EventType.SEQUENTIAL_EPOCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.ModelParameterizedLayer","title":"<code>ModelParameterizedLayer</code>  <code>dataclass</code>","text":"<p>A dataclass for holding a parameter and its layer</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>the name of the layer</p> required <code>layer</code> <code>Any</code> <p>the layer object</p> required <code>param_name</code> <code>str</code> <p>the name of the parameter</p> required <code>param</code> <code>Any</code> <p>the parameter object</p> required Source code in <code>src/llmcompressor/core/model_layer.py</code> <pre><code>@dataclass\nclass ModelParameterizedLayer:\n    \"\"\"\n    A dataclass for holding a parameter and its layer\n\n    :param layer_name: the name of the layer\n    :param layer: the layer object\n    :param param_name: the name of the parameter\n    :param param: the parameter object\n    \"\"\"\n\n    layer_name: str\n    layer: Any\n    param_name: str\n    param: Any\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.ModifiedState","title":"<code>ModifiedState</code>  <code>dataclass</code>","text":"<p>A dataclass to represent a modified model, optimizer, and loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Any]</code> <p>The modified model</p> required <code>optimizer</code> <code>Optional[Any]</code> <p>The modified optimizer</p> required <code>loss</code> <code>Optional[Any]</code> <p>The modified loss</p> required <code>modifier_data</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The modifier data used to modify the model, optimizer, and loss</p> required Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass ModifiedState:\n    \"\"\"\n    A dataclass to represent a modified model, optimizer, and loss.\n\n    :param model: The modified model\n    :type model: Optional[Any]\n    :param optimizer: The modified optimizer\n    :type optimizer: Optional[Any]\n    :param loss: The modified loss\n    :type loss: Optional[Any]\n    :param modifier_data: The modifier data used to modify the\n        model, optimizer, and loss\n    :type modifier_data: Optional[List[Dict[str, Any]]]\n    \"\"\"\n\n    model: Optional[Any] = None\n    optimizer: Optional[Any] = None\n    loss: Optional[Any] = None\n    modifier_data: Optional[List[Dict[str, Any]]] = None\n\n    def __init__(self, model, optimizer, loss, modifier_data):\n        \"\"\"\n        Initialize the ModifiedState with the given parameters.\n\n        :param model: The modified model\n        :type model: Any\n        :param optimizer: The modified optimizer\n        :type optimizer: Any\n        :param loss: The modified loss\n        :type loss: Any\n        :param modifier_data: The modifier data used to modify the model, optimizer,\n            and loss\n        :type modifier_data: List[Dict[str, Any]]\n        \"\"\"\n        self.model = model\n        self.optimizer = optimizer\n        self.loss = loss\n        self.modifier_data = modifier_data\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.ModifiedState.__init__","title":"<code>__init__(model, optimizer, loss, modifier_data)</code>","text":"<p>Initialize the ModifiedState with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The modified model</p> required <code>optimizer</code> <code>Any</code> <p>The modified optimizer</p> required <code>loss</code> <code>Any</code> <p>The modified loss</p> required <code>modifier_data</code> <code>List[Dict[str, Any]]</code> <p>The modifier data used to modify the model, optimizer, and loss</p> required Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>def __init__(self, model, optimizer, loss, modifier_data):\n    \"\"\"\n    Initialize the ModifiedState with the given parameters.\n\n    :param model: The modified model\n    :type model: Any\n    :param optimizer: The modified optimizer\n    :type optimizer: Any\n    :param loss: The modified loss\n    :type loss: Any\n    :param modifier_data: The modifier data used to modify the model, optimizer,\n        and loss\n    :type modifier_data: List[Dict[str, Any]]\n    \"\"\"\n    self.model = model\n    self.optimizer = optimizer\n    self.loss = loss\n    self.modifier_data = modifier_data\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.State","title":"<code>State</code>  <code>dataclass</code>","text":"<p>State class holds information about the current compression state.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model being used for compression</p> <code>None</code> <code>teacher_model</code> <code>Any</code> <p>The teacher model being used for compression</p> <code>None</code> <code>optimizer</code> <code>Any</code> <p>The optimizer being used for training</p> <code>None</code> <code>optim_wrapped</code> <code>bool</code> <p>Whether or not the optimizer has been wrapped</p> <code>None</code> <code>loss</code> <code>Any</code> <p>The loss function being used for training</p> <code>None</code> <code>batch_data</code> <code>Any</code> <p>The current batch of data being used for compression</p> <code>None</code> <code>data</code> <code>Data</code> <p>The data sets being used for training, validation, testing, and/or calibration, wrapped in a Data instance</p> <code>Data()</code> <code>hardware</code> <code>Hardware</code> <p>Hardware instance holding info about the target hardware being used</p> <code>Hardware()</code> <code>loggers</code> <code>Optional[LoggerManager]</code> <p>LoggerManager instance holding all the loggers to log</p> <code>None</code> <code>model_log_cadence</code> <code>Optional[float]</code> <p>The cadence to log model information w.r.t epochs. If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.</p> <code>None</code> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass State:\n    \"\"\"\n    State class holds information about the current compression state.\n\n    :param model: The model being used for compression\n    :type model: Any\n    :param teacher_model: The teacher model being used for compression\n    :type teacher_model: Any\n    :param optimizer: The optimizer being used for training\n    :type optimizer: Any\n    :param optim_wrapped: Whether or not the optimizer has been wrapped\n    :type optim_wrapped: bool\n    :param loss: The loss function being used for training\n    :type loss: Any\n    :param batch_data: The current batch of data being used for compression\n    :type batch_data: Any\n    :param data: The data sets being used for training, validation, testing,\n        and/or calibration, wrapped in a Data instance\n    :type data: Data\n    :param hardware: Hardware instance holding info about the target hardware being used\n    :type hardware: Hardware\n    :param loggers: LoggerManager instance holding all the loggers to log\n    :type loggers: Optional[LoggerManager]\n    :param model_log_cadence: The cadence to log model information w.r.t epochs.\n        If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.\n    :type model_log_cadence: Optional[float]\n    \"\"\"\n\n    model: Any = None\n    teacher_model: Any = None\n    optimizer: Any = None\n    optim_wrapped: bool = None\n    loss: Any = None\n    batch_data: Any = None\n    data: Data = field(default_factory=Data)\n    hardware: Hardware = field(default_factory=Hardware)\n    loggers: Optional[LoggerManager] = None\n    model_log_cadence: Optional[float] = None\n    _last_log_step: Union[float, int, None] = None\n\n    @property\n    def compression_ready(self) -&gt; bool:\n        \"\"\"\n        Check if the model and optimizer are set for compression.\n\n        :return: True if model and optimizer are set, False otherwise\n        :rtype: bool\n        \"\"\"\n        ready = self.model is not None and self.optimizer is not None\n        logger.debug(\"Compression ready: {}\", ready)\n        return ready\n\n    def update(\n        self,\n        model: Any = None,\n        teacher_model: Any = None,\n        optimizer: Any = None,\n        attach_optim_callbacks: bool = True,\n        train_data: Any = None,\n        val_data: Any = None,\n        test_data: Any = None,\n        calib_data: Any = None,\n        copy_data: bool = True,\n        start: float = None,\n        steps_per_epoch: int = None,\n        batches_per_step: int = None,\n        loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n        model_log_cadence: Optional[float] = None,\n        **kwargs,\n    ) -&gt; Dict:\n        \"\"\"\n        Update the state with the given parameters.\n\n        :param model: The model to update the state with\n        :type model: Any\n        :param teacher_model: The teacher model to update the state with\n        :type teacher_model: Any\n        :param optimizer: The optimizer to update the state with\n        :type optimizer: Any\n        :param attach_optim_callbacks: Whether or not to attach optimizer callbacks\n        :type attach_optim_callbacks: bool\n        :param train_data: The training data to update the state with\n        :type train_data: Any\n        :param val_data: The validation data to update the state with\n        :type val_data: Any\n        :param test_data: The testing data to update the state with\n        :type test_data: Any\n        :param calib_data: The calibration data to update the state with\n        :type calib_data: Any\n        :param copy_data: Whether or not to copy the data\n        :type copy_data: bool\n        :param start: The start index to update the state with\n        :type start: float\n        :param steps_per_epoch: The steps per epoch to update the state with\n        :type steps_per_epoch: int\n        :param batches_per_step: The batches per step to update the state with\n        :type batches_per_step: int\n        :param loggers: The metrics manager to setup logging important info and\n            milestones to, also accepts a list of BaseLogger(s)\n        :type loggers: Union[None, LoggerManager, List[BaseLogger]]\n        :param model_log_cadence: The cadence to log model information w.r.t epochs.\n            If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.\n        :type model_log_cadence: Optional[float]\n        :param kwargs: Additional keyword arguments to update the state with\n        :return: The updated state as a dictionary\n        :rtype: Dict\n        \"\"\"\n        logger.debug(\n            \"Updating state with provided parameters: {}\",\n            {\n                \"model\": model,\n                \"teacher_model\": teacher_model,\n                \"optimizer\": optimizer,\n                \"attach_optim_callbacks\": attach_optim_callbacks,\n                \"train_data\": train_data,\n                \"val_data\": val_data,\n                \"test_data\": test_data,\n                \"calib_data\": calib_data,\n                \"copy_data\": copy_data,\n                \"start\": start,\n                \"steps_per_epoch\": steps_per_epoch,\n                \"batches_per_step\": batches_per_step,\n                \"loggers\": loggers,\n                \"model_log_cadence\": model_log_cadence,\n                \"kwargs\": kwargs,\n            },\n        )\n\n        if model is not None:\n            self.model = model\n        if teacher_model is not None:\n            self.teacher_model = teacher_model\n        if optimizer is not None:\n            self.optim_wrapped = attach_optim_callbacks\n            self.optimizer = optimizer\n        if train_data is not None:\n            self.data.train = train_data if not copy_data else deepcopy(train_data)\n        if val_data is not None:\n            self.data.val = val_data if not copy_data else deepcopy(val_data)\n        if test_data is not None:\n            self.data.test = test_data if not copy_data else deepcopy(test_data)\n        if calib_data is not None:\n            self.data.calib = calib_data if not copy_data else deepcopy(calib_data)\n\n        if \"device\" in kwargs:\n            self.hardware.device = kwargs[\"device\"]\n\n        loggers = loggers or []\n        if isinstance(loggers, list):\n            loggers = LoggerManager(loggers)\n        self.loggers = loggers\n\n        if model_log_cadence is not None:\n            self.model_log_cadence = model_log_cadence\n\n        return kwargs\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.State.compression_ready","title":"<code>compression_ready</code>  <code>property</code>","text":"<p>Check if the model and optimizer are set for compression.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model and optimizer are set, False otherwise</p>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.State.update","title":"<code>update(model=None, teacher_model=None, optimizer=None, attach_optim_callbacks=True, train_data=None, val_data=None, test_data=None, calib_data=None, copy_data=True, start=None, steps_per_epoch=None, batches_per_step=None, loggers=None, model_log_cadence=None, **kwargs)</code>","text":"<p>Update the state with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model to update the state with</p> <code>None</code> <code>teacher_model</code> <code>Any</code> <p>The teacher model to update the state with</p> <code>None</code> <code>optimizer</code> <code>Any</code> <p>The optimizer to update the state with</p> <code>None</code> <code>attach_optim_callbacks</code> <code>bool</code> <p>Whether or not to attach optimizer callbacks</p> <code>True</code> <code>train_data</code> <code>Any</code> <p>The training data to update the state with</p> <code>None</code> <code>val_data</code> <code>Any</code> <p>The validation data to update the state with</p> <code>None</code> <code>test_data</code> <code>Any</code> <p>The testing data to update the state with</p> <code>None</code> <code>calib_data</code> <code>Any</code> <p>The calibration data to update the state with</p> <code>None</code> <code>copy_data</code> <code>bool</code> <p>Whether or not to copy the data</p> <code>True</code> <code>start</code> <code>float</code> <p>The start index to update the state with</p> <code>None</code> <code>steps_per_epoch</code> <code>int</code> <p>The steps per epoch to update the state with</p> <code>None</code> <code>batches_per_step</code> <code>int</code> <p>The batches per step to update the state with</p> <code>None</code> <code>loggers</code> <code>Union[None, LoggerManager, List[BaseLogger]]</code> <p>The metrics manager to setup logging important info and milestones to, also accepts a list of BaseLogger(s)</p> <code>None</code> <code>model_log_cadence</code> <code>Optional[float]</code> <p>The cadence to log model information w.r.t epochs. If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to update the state with</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>The updated state as a dictionary</p> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>def update(\n    self,\n    model: Any = None,\n    teacher_model: Any = None,\n    optimizer: Any = None,\n    attach_optim_callbacks: bool = True,\n    train_data: Any = None,\n    val_data: Any = None,\n    test_data: Any = None,\n    calib_data: Any = None,\n    copy_data: bool = True,\n    start: float = None,\n    steps_per_epoch: int = None,\n    batches_per_step: int = None,\n    loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n    model_log_cadence: Optional[float] = None,\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"\n    Update the state with the given parameters.\n\n    :param model: The model to update the state with\n    :type model: Any\n    :param teacher_model: The teacher model to update the state with\n    :type teacher_model: Any\n    :param optimizer: The optimizer to update the state with\n    :type optimizer: Any\n    :param attach_optim_callbacks: Whether or not to attach optimizer callbacks\n    :type attach_optim_callbacks: bool\n    :param train_data: The training data to update the state with\n    :type train_data: Any\n    :param val_data: The validation data to update the state with\n    :type val_data: Any\n    :param test_data: The testing data to update the state with\n    :type test_data: Any\n    :param calib_data: The calibration data to update the state with\n    :type calib_data: Any\n    :param copy_data: Whether or not to copy the data\n    :type copy_data: bool\n    :param start: The start index to update the state with\n    :type start: float\n    :param steps_per_epoch: The steps per epoch to update the state with\n    :type steps_per_epoch: int\n    :param batches_per_step: The batches per step to update the state with\n    :type batches_per_step: int\n    :param loggers: The metrics manager to setup logging important info and\n        milestones to, also accepts a list of BaseLogger(s)\n    :type loggers: Union[None, LoggerManager, List[BaseLogger]]\n    :param model_log_cadence: The cadence to log model information w.r.t epochs.\n        If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.\n    :type model_log_cadence: Optional[float]\n    :param kwargs: Additional keyword arguments to update the state with\n    :return: The updated state as a dictionary\n    :rtype: Dict\n    \"\"\"\n    logger.debug(\n        \"Updating state with provided parameters: {}\",\n        {\n            \"model\": model,\n            \"teacher_model\": teacher_model,\n            \"optimizer\": optimizer,\n            \"attach_optim_callbacks\": attach_optim_callbacks,\n            \"train_data\": train_data,\n            \"val_data\": val_data,\n            \"test_data\": test_data,\n            \"calib_data\": calib_data,\n            \"copy_data\": copy_data,\n            \"start\": start,\n            \"steps_per_epoch\": steps_per_epoch,\n            \"batches_per_step\": batches_per_step,\n            \"loggers\": loggers,\n            \"model_log_cadence\": model_log_cadence,\n            \"kwargs\": kwargs,\n        },\n    )\n\n    if model is not None:\n        self.model = model\n    if teacher_model is not None:\n        self.teacher_model = teacher_model\n    if optimizer is not None:\n        self.optim_wrapped = attach_optim_callbacks\n        self.optimizer = optimizer\n    if train_data is not None:\n        self.data.train = train_data if not copy_data else deepcopy(train_data)\n    if val_data is not None:\n        self.data.val = val_data if not copy_data else deepcopy(val_data)\n    if test_data is not None:\n        self.data.test = test_data if not copy_data else deepcopy(test_data)\n    if calib_data is not None:\n        self.data.calib = calib_data if not copy_data else deepcopy(calib_data)\n\n    if \"device\" in kwargs:\n        self.hardware.device = kwargs[\"device\"]\n\n    loggers = loggers or []\n    if isinstance(loggers, list):\n        loggers = LoggerManager(loggers)\n    self.loggers = loggers\n\n    if model_log_cadence is not None:\n        self.model_log_cadence = model_log_cadence\n\n    return kwargs\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.active_session","title":"<code>active_session()</code>","text":"<p>Returns:</p> Type Description <code>CompressionSession</code> <p>the active session for sparsification</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>def active_session() -&gt; CompressionSession:\n    \"\"\"\n    :return: the active session for sparsification\n    \"\"\"\n    global _local_storage\n    return getattr(_local_storage, \"session\", _global_session)\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.create_session","title":"<code>create_session()</code>","text":"<p>Context manager to create and yield a new session for sparsification. This will set the active session to the new session for the duration of the context.</p> <p>Returns:</p> Type Description <code>Generator[CompressionSession, None, None]</code> <p>the new session</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@contextmanager\ndef create_session() -&gt; Generator[CompressionSession, None, None]:\n    \"\"\"\n    Context manager to create and yield a new session for sparsification.\n    This will set the active session to the new session for the duration\n    of the context.\n\n    :return: the new session\n    \"\"\"\n    global _local_storage\n    orig_session = getattr(_local_storage, \"session\", None)\n    new_session = CompressionSession()\n    _local_storage.session = new_session\n    try:\n        yield new_session\n    finally:\n        _local_storage.session = orig_session\n</code></pre>"},{"location":"reference/llmcompressor/core/#llmcompressor.core.reset_session","title":"<code>reset_session()</code>","text":"<p>Reset the currently active session to its initial state</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>def reset_session():\n    \"\"\"\n    Reset the currently active session to its initial state\n    \"\"\"\n    session = active_session()\n    session._lifecycle.reset()\n</code></pre>"},{"location":"reference/llmcompressor/core/helpers/","title":"llmcompressor.core.helpers","text":""},{"location":"reference/llmcompressor/core/helpers/#llmcompressor.core.helpers.log_model_info","title":"<code>log_model_info(state, current_log_step)</code>","text":"<p>Log model level info to the metrics Relies on <code>state.model</code> having a <code>loggable_items</code> method that returns a generator of tuples of the loggable item name and value. Also relies on <code>state.loggers</code> being a <code>LoggerManager</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of sparsification</p> required <code>current_log_step</code> <p>The current log step to log model info at</p> required Source code in <code>src/llmcompressor/core/helpers.py</code> <pre><code>def log_model_info(state: State, current_log_step):\n    \"\"\"\n    Log model level info to the metrics\n    Relies on `state.model` having a `loggable_items` method\n    that returns a generator of tuples of the loggable item\n    name and value. Also relies on `state.loggers` being a\n    `LoggerManager` instance.\n\n    :param state: The current state of sparsification\n    :param current_log_step: The current log step to log\n        model info at\n    \"\"\"\n    _log_current_step(logger_manager=state.loggers, current_log_step=current_log_step)\n    _log_model_loggable_items(\n        logger_manager=state.loggers,\n        loggable_items=state.model.loggable_items(),\n        epoch=current_log_step,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/helpers/#llmcompressor.core.helpers.should_log_model_info","title":"<code>should_log_model_info(model, loggers, current_log_step, last_log_step=None)</code>","text":"<p>Check if we should log model level info Criteria:     - model has a loggable_items method     - state has a metrics manager     - metrics manager is ready to log based on cadence and last log epoch</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model whose info we want to log</p> required <code>loggers</code> <code>LoggerManager</code> <p>The metrics manager to log to</p> required <code>current_log_step</code> <code>float</code> <p>The current epoch</p> required <code>last_log_step</code> <code>Optional[float]</code> <p>The last step we logged model info at</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if we should log model level info, False otherwise</p> Source code in <code>src/llmcompressor/core/helpers.py</code> <pre><code>def should_log_model_info(\n    model: Any,\n    loggers: LoggerManager,\n    current_log_step: float,\n    last_log_step: Optional[float] = None,\n) -&gt; bool:\n    \"\"\"\n    Check if we should log model level info\n    Criteria:\n        - model has a loggable_items method\n        - state has a metrics manager\n        - metrics manager is ready to log based on cadence and last log epoch\n\n    :param model: The model whose info we want to log\n    :param loggers: The metrics manager to log to\n    :param current_log_step: The current epoch\n    :param last_log_step: The last step we logged model info at\n    :return: True if we should log model level info, False otherwise\n    \"\"\"\n    return (\n        hasattr(model, \"loggable_items\")\n        and isinstance(loggers, LoggerManager)\n        and loggers.log_ready(\n            current_log_step=current_log_step, last_log_step=last_log_step\n        )\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/lifecycle/","title":"llmcompressor.core.lifecycle","text":"<p>Module for managing the compression lifecycle in the LLM Compressor.</p> <p>This module provides a class for defining and managing the lifecycle of compression events, including initialization, finalization, and event handling.</p>"},{"location":"reference/llmcompressor/core/lifecycle/#llmcompressor.core.lifecycle.CompressionLifecycle","title":"<code>CompressionLifecycle</code>  <code>dataclass</code>","text":"<p>A class for managing the lifecycle of compression events in the LLM Compressor.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the compression process</p> <code>State()</code> <code>recipe_container</code> <code>RecipeContainer</code> <p>The container for the compression recipe</p> <code>RecipeContainer()</code> <code>modifiers</code> <code>List[StageModifiers]</code> <p>The list of stage modifiers</p> <code>list()</code> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>@dataclass\nclass CompressionLifecycle:\n    \"\"\"\n    A class for managing the lifecycle of compression events in the LLM Compressor.\n\n    :param state: The current state of the compression process\n    :type state: Optional[State]\n    :param recipe_container: The container for the compression recipe\n    :type recipe_container: RecipeContainer\n    :param modifiers: The list of stage modifiers\n    :type modifiers: List[StageModifiers]\n    \"\"\"\n\n    state: State = field(default_factory=State)\n    recipe_container: RecipeContainer = field(default_factory=RecipeContainer)\n    modifiers: List[StageModifiers] = field(default_factory=list)\n\n    initialized_: bool = False\n    finalized: bool = False\n\n    # event order validation\n    _last_event_type: Optional[EventType] = EventType.BATCH_END\n    _event_order: List[EventType] = field(\n        default_factory=lambda: [\n            EventType.BATCH_START,\n            EventType.LOSS_CALCULATED,\n            EventType.OPTIM_PRE_STEP,\n            EventType.OPTIM_POST_STEP,\n            EventType.BATCH_END,\n        ]\n    )\n\n    # track global step in training (could be epoch/batch)\n    global_step: int = 0\n\n    def reset(self):\n        \"\"\"\n        Reset the compression lifecycle, finalizing any active modifiers\n        and resetting all attributes.\n        \"\"\"\n        logger.debug(\"Resetting compression lifecycle\")\n\n        for mod in self.modifiers:\n            if not mod.initialized or mod.finalized:\n                continue\n            try:\n                mod.finalize(self.state)\n                logger.debug(\"Finalized modifier: {}\", mod)\n            except Exception as e:\n                logger.warning(f\"Exception during finalizing modifier: {e}\")\n\n        self.__init__()\n        logger.info(\"Compression lifecycle reset\")\n\n    def initialize(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n        **kwargs,\n    ) -&gt; List[Any]:\n        \"\"\"\n        Initialize the compression lifecycle.\n\n        :param kwargs: Additional arguments to update the state with\n        :return: List of data returned from initialization of modifiers\n        :rtype: List[Any]\n        \"\"\"\n        self.state.update(**kwargs)\n        if self.initialized_:  # TODO: do not initialize twice\n            return\n\n        logger.debug(\"Initializing compression lifecycle\")\n        self.recipe_container.append(recipe, recipe_stage, recipe_args)\n        self.modifiers = self.recipe_container.get_modifiers()\n\n        mod_data = []\n        for mod in self.modifiers:\n            data = mod.initialize(state=self.state, **kwargs)\n            logger.debug(\"Initialized modifier: {}\", mod)\n            if data is not None:\n                mod_data.append(data)\n\n        self.initialized_ = True\n        logger.info(\n            \"Compression lifecycle initialized for {} modifiers\", len(self.modifiers)\n        )\n\n        return mod_data\n\n    def finalize(self, **kwargs) -&gt; List[Any]:\n        \"\"\"\n        Finalize the compression lifecycle.\n\n        :param kwargs: Additional arguments to update the state with\n        :return: List of data returned from finalizing modifiers\n        :rtype: List[Any]\n        :raises ValueError: If called before initialization or more than once\n        \"\"\"\n        if not self.initialized_:\n            logger.error(\"Cannot finalize before initializing\")\n            raise ValueError(\"Cannot finalize before initializing\")\n\n        if self.finalized:\n            logger.error(\"Cannot finalize more than once\")\n            raise ValueError(\"Cannot finalize more than once\")\n\n        logger.debug(\"Finalizing compression lifecycle\")\n        mod_data = []\n        for mod in self.modifiers:\n            data = mod.finalize(state=self.state, **kwargs)\n            logger.debug(\"Finalized modifier: {}\", mod)\n            if data is not None:\n                mod_data.append(data)\n\n        self.finalized = True\n        applied_stage_names = [mod.unique_id for mod in self.modifiers if mod.applied]\n        self.recipe_container.update_applied_stages(applied_stage_names)\n\n        logger.info(\n            \"Compression lifecycle finalized for {} modifiers\", len(self.modifiers)\n        )\n\n        return mod_data\n\n    def event(\n        self, event_type: EventType, global_step: Optional[int] = 0, **kwargs\n    ) -&gt; List[Any]:\n        \"\"\"\n        Handle a compression event.\n\n        :param event_type: The type of event to handle\n        :type event_type: EventType\n        :param kwargs: Additional arguments to pass to the event handlers\n        :return: List of data returned from handling the event by modifiers\n        :rtype: List[Any]\n        :raises ValueError: If called before initialization, after finalization,\n            or for an invalid event type\n        \"\"\"\n        if not self.initialized_:\n            logger.error(\"Cannot invoke event before initializing\")\n            raise ValueError(\"Cannot invoke event before initializing\")\n\n        if self.finalized:\n            logger.error(\"Cannot invoke event after finalizing\")\n            raise ValueError(\"Cannot invoke event after finalizing\")\n\n        if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n            logger.error(\n                \"Cannot invoke {} event. Use the corresponding method instead.\",\n                event_type,\n            )\n            raise ValueError(\n                f\"Cannot invoke {event_type} event. \"\n                f\"Use the corresponding method instead.\"\n            )\n\n        if not self._validate_event_order(event_type):\n            raise ValueError(\n                f\"Lifecycle events must appear following order: {self._event_order}. \"\n                f\"Instead, {self._last_event_type} was called before {event_type}\"\n            )\n\n        if event_type == EventType.LOSS_CALCULATED and (\n            \"loss\" not in kwargs or kwargs[\"loss\"] is None\n        ):\n            logger.error(\"Loss must be provided for loss calculated event\")\n            raise ValueError(\"Loss must be provided for loss calculated event\")\n\n        logger.debug(\"Handling event: {}\", event_type)\n\n        # update global step\n        if global_step is not None:\n            self.global_step = global_step\n\n        event = Event(type_=event_type)\n        mod_data = []\n        for mod in self.modifiers:\n            data = mod.update_event(state=self.state, event=event, **kwargs)\n            logger.debug(\"Updated event with modifier: {}\", mod)\n            if data is not None:\n                mod_data.append(data)\n\n        assert (\n            event is not None\n        ), f\"Event lifecycle did not return an event for {event_type}\"\n\n        return mod_data\n\n    def _validate_event_order(self, event_type: EventType) -&gt; bool:\n        if event_type not in self._event_order:\n            # for unhandled events, do not save last event\n            return True\n\n        if event_type == EventType.BATCH_START:\n            valid = self._last_event_type != EventType.BATCH_START\n\n        else:\n            last_event_index = self._event_order.index(self._last_event_type)\n            curr_event_index = self._event_order.index(event_type)\n            valid = last_event_index &lt;= curr_event_index\n\n        if valid:\n            self._last_event_type = event_type\n        return valid\n</code></pre>"},{"location":"reference/llmcompressor/core/lifecycle/#llmcompressor.core.lifecycle.CompressionLifecycle.event","title":"<code>event(event_type, global_step=0, **kwargs)</code>","text":"<p>Handle a compression event.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>The type of event to handle</p> required <code>kwargs</code> <p>Additional arguments to pass to the event handlers</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of data returned from handling the event by modifiers</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before initialization, after finalization, or for an invalid event type</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def event(\n    self, event_type: EventType, global_step: Optional[int] = 0, **kwargs\n) -&gt; List[Any]:\n    \"\"\"\n    Handle a compression event.\n\n    :param event_type: The type of event to handle\n    :type event_type: EventType\n    :param kwargs: Additional arguments to pass to the event handlers\n    :return: List of data returned from handling the event by modifiers\n    :rtype: List[Any]\n    :raises ValueError: If called before initialization, after finalization,\n        or for an invalid event type\n    \"\"\"\n    if not self.initialized_:\n        logger.error(\"Cannot invoke event before initializing\")\n        raise ValueError(\"Cannot invoke event before initializing\")\n\n    if self.finalized:\n        logger.error(\"Cannot invoke event after finalizing\")\n        raise ValueError(\"Cannot invoke event after finalizing\")\n\n    if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n        logger.error(\n            \"Cannot invoke {} event. Use the corresponding method instead.\",\n            event_type,\n        )\n        raise ValueError(\n            f\"Cannot invoke {event_type} event. \"\n            f\"Use the corresponding method instead.\"\n        )\n\n    if not self._validate_event_order(event_type):\n        raise ValueError(\n            f\"Lifecycle events must appear following order: {self._event_order}. \"\n            f\"Instead, {self._last_event_type} was called before {event_type}\"\n        )\n\n    if event_type == EventType.LOSS_CALCULATED and (\n        \"loss\" not in kwargs or kwargs[\"loss\"] is None\n    ):\n        logger.error(\"Loss must be provided for loss calculated event\")\n        raise ValueError(\"Loss must be provided for loss calculated event\")\n\n    logger.debug(\"Handling event: {}\", event_type)\n\n    # update global step\n    if global_step is not None:\n        self.global_step = global_step\n\n    event = Event(type_=event_type)\n    mod_data = []\n    for mod in self.modifiers:\n        data = mod.update_event(state=self.state, event=event, **kwargs)\n        logger.debug(\"Updated event with modifier: {}\", mod)\n        if data is not None:\n            mod_data.append(data)\n\n    assert (\n        event is not None\n    ), f\"Event lifecycle did not return an event for {event_type}\"\n\n    return mod_data\n</code></pre>"},{"location":"reference/llmcompressor/core/lifecycle/#llmcompressor.core.lifecycle.CompressionLifecycle.finalize","title":"<code>finalize(**kwargs)</code>","text":"<p>Finalize the compression lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Additional arguments to update the state with</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of data returned from finalizing modifiers</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before initialization or more than once</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def finalize(self, **kwargs) -&gt; List[Any]:\n    \"\"\"\n    Finalize the compression lifecycle.\n\n    :param kwargs: Additional arguments to update the state with\n    :return: List of data returned from finalizing modifiers\n    :rtype: List[Any]\n    :raises ValueError: If called before initialization or more than once\n    \"\"\"\n    if not self.initialized_:\n        logger.error(\"Cannot finalize before initializing\")\n        raise ValueError(\"Cannot finalize before initializing\")\n\n    if self.finalized:\n        logger.error(\"Cannot finalize more than once\")\n        raise ValueError(\"Cannot finalize more than once\")\n\n    logger.debug(\"Finalizing compression lifecycle\")\n    mod_data = []\n    for mod in self.modifiers:\n        data = mod.finalize(state=self.state, **kwargs)\n        logger.debug(\"Finalized modifier: {}\", mod)\n        if data is not None:\n            mod_data.append(data)\n\n    self.finalized = True\n    applied_stage_names = [mod.unique_id for mod in self.modifiers if mod.applied]\n    self.recipe_container.update_applied_stages(applied_stage_names)\n\n    logger.info(\n        \"Compression lifecycle finalized for {} modifiers\", len(self.modifiers)\n    )\n\n    return mod_data\n</code></pre>"},{"location":"reference/llmcompressor/core/lifecycle/#llmcompressor.core.lifecycle.CompressionLifecycle.initialize","title":"<code>initialize(recipe=None, recipe_stage=None, recipe_args=None, **kwargs)</code>","text":"<p>Initialize the compression lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Additional arguments to update the state with</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of data returned from initialization of modifiers</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def initialize(\n    self,\n    recipe: Optional[RecipeInput] = None,\n    recipe_stage: Optional[RecipeStageInput] = None,\n    recipe_args: Optional[RecipeArgsInput] = None,\n    **kwargs,\n) -&gt; List[Any]:\n    \"\"\"\n    Initialize the compression lifecycle.\n\n    :param kwargs: Additional arguments to update the state with\n    :return: List of data returned from initialization of modifiers\n    :rtype: List[Any]\n    \"\"\"\n    self.state.update(**kwargs)\n    if self.initialized_:  # TODO: do not initialize twice\n        return\n\n    logger.debug(\"Initializing compression lifecycle\")\n    self.recipe_container.append(recipe, recipe_stage, recipe_args)\n    self.modifiers = self.recipe_container.get_modifiers()\n\n    mod_data = []\n    for mod in self.modifiers:\n        data = mod.initialize(state=self.state, **kwargs)\n        logger.debug(\"Initialized modifier: {}\", mod)\n        if data is not None:\n            mod_data.append(data)\n\n    self.initialized_ = True\n    logger.info(\n        \"Compression lifecycle initialized for {} modifiers\", len(self.modifiers)\n    )\n\n    return mod_data\n</code></pre>"},{"location":"reference/llmcompressor/core/lifecycle/#llmcompressor.core.lifecycle.CompressionLifecycle.reset","title":"<code>reset()</code>","text":"<p>Reset the compression lifecycle, finalizing any active modifiers and resetting all attributes.</p> Source code in <code>src/llmcompressor/core/lifecycle.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the compression lifecycle, finalizing any active modifiers\n    and resetting all attributes.\n    \"\"\"\n    logger.debug(\"Resetting compression lifecycle\")\n\n    for mod in self.modifiers:\n        if not mod.initialized or mod.finalized:\n            continue\n        try:\n            mod.finalize(self.state)\n            logger.debug(\"Finalized modifier: {}\", mod)\n        except Exception as e:\n            logger.warning(f\"Exception during finalizing modifier: {e}\")\n\n    self.__init__()\n    logger.info(\"Compression lifecycle reset\")\n</code></pre>"},{"location":"reference/llmcompressor/core/model_layer/","title":"llmcompressor.core.model_layer","text":""},{"location":"reference/llmcompressor/core/model_layer/#llmcompressor.core.model_layer.ModelParameterizedLayer","title":"<code>ModelParameterizedLayer</code>  <code>dataclass</code>","text":"<p>A dataclass for holding a parameter and its layer</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>the name of the layer</p> required <code>layer</code> <code>Any</code> <p>the layer object</p> required <code>param_name</code> <code>str</code> <p>the name of the parameter</p> required <code>param</code> <code>Any</code> <p>the parameter object</p> required Source code in <code>src/llmcompressor/core/model_layer.py</code> <pre><code>@dataclass\nclass ModelParameterizedLayer:\n    \"\"\"\n    A dataclass for holding a parameter and its layer\n\n    :param layer_name: the name of the layer\n    :param layer: the layer object\n    :param param_name: the name of the parameter\n    :param param: the parameter object\n    \"\"\"\n\n    layer_name: str\n    layer: Any\n    param_name: str\n    param: Any\n</code></pre>"},{"location":"reference/llmcompressor/core/session/","title":"llmcompressor.core.session","text":""},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession","title":"<code>CompressionSession</code>","text":"<p>A session for compression that holds the lifecycle and state for the current compression session</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>class CompressionSession:\n    \"\"\"\n    A session for compression that holds the lifecycle\n    and state for the current compression session\n    \"\"\"\n\n    def __init__(self):\n        self._lifecycle = CompressionLifecycle()\n\n    @property\n    def lifecycle(self) -&gt; CompressionLifecycle:\n        \"\"\"\n        Lifecycle is used to keep track of where we are in the compression\n        process and what modifiers are active. It also provides the ability\n        to invoke events on the lifecycle.\n\n        :return: the lifecycle for the session\n        \"\"\"\n        return self._lifecycle\n\n    @property\n    def state(self) -&gt; State:\n        \"\"\"\n        State of the current compression session. State instance\n        is used to store all information such as the recipe, model\n        optimizer, data, etc. that is needed for compression.\n\n        :return: the current state of the session\n        \"\"\"\n        return self._lifecycle.state\n\n    def initialize(\n        self,\n        recipe: Union[str, List[str], \"Recipe\", List[\"Recipe\"], None] = None,\n        recipe_stage: Union[str, List[str], None] = None,\n        recipe_args: Union[Dict[str, Any], None] = None,\n        model: Optional[Any] = None,\n        teacher_model: Optional[Any] = None,\n        optimizer: Optional[Any] = None,\n        attach_optim_callbacks: bool = True,\n        train_data: Optional[Any] = None,\n        val_data: Optional[Any] = None,\n        test_data: Optional[Any] = None,\n        calib_data: Optional[Any] = None,\n        copy_data: bool = True,\n        start: Optional[float] = None,\n        steps_per_epoch: Optional[int] = None,\n        batches_per_step: Optional[int] = None,\n        loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n        **kwargs,\n    ) -&gt; ModifiedState:\n        \"\"\"\n        Initialize the session for compression. This will run the initialize method\n        for each modifier in the session's lifecycle. This will also set the session's\n        state to the initialized state.\n\n        :param recipe: the recipe to use for the compression, can be a path to a\n            recipe file, a raw recipe string, a recipe object, or a list\n            of recipe objects.\n        :param recipe_stage: the stage to target for the compression\n        :param recipe_args: the args to use for overriding the recipe defaults\n        :param model: the model to compress\n        :param teacher_model: the teacher model to use for knowledge distillation\n        :param optimizer: the optimizer to use for the compression\n        :param attach_optim_callbacks: True to attach the optimizer callbacks to the\n            compression lifecycle, False otherwise\n        :param train_data: the training data to use for the compression\n        :param val_data: the validation data to use for the compression\n        :param test_data: the testing data to use for the compression\n        :param calib_data: the calibration data to use for the compression\n        :param copy_data: True to copy the data, False otherwise\n        :param start: the start epoch to use for the compression\n        :param steps_per_epoch: the number of steps per epoch to use for the\n            compression\n        :param batches_per_step: the number of batches per step to use for\n            compression\n        :param loggers: the metrics manager to setup logging important info\n            and milestones to, also accepts a list of BaseLogger(s)\n        :param kwargs: additional kwargs to pass to the lifecycle's initialize method\n        :return: the modified state of the session after initializing\n        \"\"\"\n        mod_data = self._lifecycle.initialize(\n            recipe=recipe,\n            recipe_stage=recipe_stage,\n            recipe_args=recipe_args,\n            model=model,\n            teacher_model=teacher_model,\n            optimizer=optimizer,\n            attach_optim_callbacks=attach_optim_callbacks,\n            train_data=train_data,\n            val_data=val_data,\n            test_data=test_data,\n            calib_data=calib_data,\n            copy_data=copy_data,\n            start=start,\n            steps_per_epoch=steps_per_epoch,\n            batches_per_step=batches_per_step,\n            loggers=loggers,\n            **kwargs,\n        )\n\n        return ModifiedState(\n            model=self.state.model,\n            optimizer=self.state.optimizer,\n            loss=self.state.loss,\n            modifier_data=mod_data,\n        )\n\n    def finalize(self, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Finalize the session for compression. This will run the finalize method\n        for each modifier in the session's lifecycle. This will also set the session's\n        state to the finalized state.\n\n        :param kwargs: additional kwargs to pass to the lifecycle's finalize method\n        :return: the modified state of the session after finalizing\n        \"\"\"\n        mod_data = self._lifecycle.finalize(**kwargs)\n\n        return ModifiedState(\n            model=self.state.model,\n            optimizer=self.state.optimizer,\n            loss=self.state.loss,\n            modifier_data=mod_data,\n        )\n\n    def event(\n        self,\n        event_type: EventType,\n        batch_data: Optional[Any] = None,\n        loss: Optional[Any] = None,\n        **kwargs,\n    ) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an event for current CompressionSession.\n\n        :param event_type: the event type to invoke\n        :param batch_data: the batch data to use for the event\n        :param loss: the loss to use for the event if any\n        :param kwargs: additional kwargs to pass to the lifecycle's event method\n        :return: the modified state of the session after invoking the event\n        \"\"\"\n        mod_data = self._lifecycle.event(\n            event_type=event_type, batch_data=batch_data, loss=loss, **kwargs\n        )\n        return ModifiedState(\n            model=self.state.model,\n            optimizer=self.state.optimizer,\n            loss=self.state.loss,  # TODO: is this supposed to be a different type?\n            modifier_data=mod_data,\n        )\n\n    def log(self, event_type: EventType, loss: Optional[Any] = None):\n        \"\"\"\n        Log model and loss information for the current event type\n\n        :param event_type: the event type to log for\n        :param loss: the loss to log if any\n        \"\"\"\n        self._log_model_info()\n        self._log_loss(event_type=event_type, loss=loss)\n\n    def reset(self):\n        \"\"\"\n        Reset the session to its initial state\n        \"\"\"\n        self._lifecycle.reset()\n\n    def reset_stage(self):\n        \"\"\"\n        Reset the session for starting a new stage, recipe and model stays intact\n        \"\"\"\n        self.lifecycle.initialized_ = False\n        self.lifecycle.finalized = False\n\n    def get_serialized_recipe(self) -&gt; Optional[str]:\n        \"\"\"\n        :return: serialized string of the current compiled recipe\n        \"\"\"\n        recipe = self.lifecycle.recipe_container.compiled_recipe\n\n        if recipe is not None and hasattr(recipe, \"yaml\"):\n            return recipe.yaml()\n\n        logger.warning(\"Recipe not found in session - it may have been reset\")\n\n    def get_modifiers(self):\n        \"\"\"\n        Get all modifiers across all stages\n        \"\"\"\n        stage_modifiers = self.lifecycle.modifiers\n        return [\n            modifier\n            for stage_modifier in stage_modifiers\n            for modifier in stage_modifier.modifiers\n        ]  # noqa: E127\n\n    def _log_model_info(self):\n        # Log model level logs if cadence reached\n        current_index = self._lifecycle.global_step\n\n        if (\n            should_log_model_info(\n                model=self.state.model,\n                loggers=self.state.loggers,\n                current_log_step=current_index,\n                last_log_step=self.state._last_log_step,\n            )\n            and self.state.loggers.frequency_manager.is_epoch_frequency_manager\n        ):\n            log_model_info(\n                state=self.state,\n                current_log_step=current_index,\n            )\n            # update last log epoch\n            self.state.loggers.log_written(current_index)\n\n    def _log_loss(self, event_type: EventType, loss: Any):\n        if event_type != EventType.LOSS_CALCULATED:\n            # only log loss when loss is calculated\n            return\n\n        current_index = self._lifecycle.global_step\n\n        # always log loss if available\n        if loss is not None:\n            loss = loss if isinstance(loss, dict) else {\"loss\": loss}\n            self.state.loggers.metric.log_scalars(\n                tag=\"Loss\", values=loss, step=current_index\n            )\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.lifecycle","title":"<code>lifecycle</code>  <code>property</code>","text":"<p>Lifecycle is used to keep track of where we are in the compression process and what modifiers are active. It also provides the ability to invoke events on the lifecycle.</p> <p>Returns:</p> Type Description <code>CompressionLifecycle</code> <p>the lifecycle for the session</p>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.state","title":"<code>state</code>  <code>property</code>","text":"<p>State of the current compression session. State instance is used to store all information such as the recipe, model optimizer, data, etc. that is needed for compression.</p> <p>Returns:</p> Type Description <code>State</code> <p>the current state of the session</p>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.event","title":"<code>event(event_type, batch_data=None, loss=None, **kwargs)</code>","text":"<p>Invoke an event for current CompressionSession.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>the event type to invoke</p> required <code>batch_data</code> <code>Optional[Any]</code> <p>the batch data to use for the event</p> <code>None</code> <code>loss</code> <code>Optional[Any]</code> <p>the loss to use for the event if any</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the lifecycle's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the session after invoking the event</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def event(\n    self,\n    event_type: EventType,\n    batch_data: Optional[Any] = None,\n    loss: Optional[Any] = None,\n    **kwargs,\n) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an event for current CompressionSession.\n\n    :param event_type: the event type to invoke\n    :param batch_data: the batch data to use for the event\n    :param loss: the loss to use for the event if any\n    :param kwargs: additional kwargs to pass to the lifecycle's event method\n    :return: the modified state of the session after invoking the event\n    \"\"\"\n    mod_data = self._lifecycle.event(\n        event_type=event_type, batch_data=batch_data, loss=loss, **kwargs\n    )\n    return ModifiedState(\n        model=self.state.model,\n        optimizer=self.state.optimizer,\n        loss=self.state.loss,  # TODO: is this supposed to be a different type?\n        modifier_data=mod_data,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.finalize","title":"<code>finalize(**kwargs)</code>","text":"<p>Finalize the session for compression. This will run the finalize method for each modifier in the session's lifecycle. This will also set the session's state to the finalized state.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the lifecycle's finalize method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the session after finalizing</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def finalize(self, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Finalize the session for compression. This will run the finalize method\n    for each modifier in the session's lifecycle. This will also set the session's\n    state to the finalized state.\n\n    :param kwargs: additional kwargs to pass to the lifecycle's finalize method\n    :return: the modified state of the session after finalizing\n    \"\"\"\n    mod_data = self._lifecycle.finalize(**kwargs)\n\n    return ModifiedState(\n        model=self.state.model,\n        optimizer=self.state.optimizer,\n        loss=self.state.loss,\n        modifier_data=mod_data,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.get_modifiers","title":"<code>get_modifiers()</code>","text":"<p>Get all modifiers across all stages</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def get_modifiers(self):\n    \"\"\"\n    Get all modifiers across all stages\n    \"\"\"\n    stage_modifiers = self.lifecycle.modifiers\n    return [\n        modifier\n        for stage_modifier in stage_modifiers\n        for modifier in stage_modifier.modifiers\n    ]  # noqa: E127\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.get_serialized_recipe","title":"<code>get_serialized_recipe()</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>serialized string of the current compiled recipe</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def get_serialized_recipe(self) -&gt; Optional[str]:\n    \"\"\"\n    :return: serialized string of the current compiled recipe\n    \"\"\"\n    recipe = self.lifecycle.recipe_container.compiled_recipe\n\n    if recipe is not None and hasattr(recipe, \"yaml\"):\n        return recipe.yaml()\n\n    logger.warning(\"Recipe not found in session - it may have been reset\")\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.initialize","title":"<code>initialize(recipe=None, recipe_stage=None, recipe_args=None, model=None, teacher_model=None, optimizer=None, attach_optim_callbacks=True, train_data=None, val_data=None, test_data=None, calib_data=None, copy_data=True, start=None, steps_per_epoch=None, batches_per_step=None, loggers=None, **kwargs)</code>","text":"<p>Initialize the session for compression. This will run the initialize method for each modifier in the session's lifecycle. This will also set the session's state to the initialized state.</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>Union[str, List[str], Recipe, List[Recipe], None]</code> <p>the recipe to use for the compression, can be a path to a recipe file, a raw recipe string, a recipe object, or a list of recipe objects.</p> <code>None</code> <code>recipe_stage</code> <code>Union[str, List[str], None]</code> <p>the stage to target for the compression</p> <code>None</code> <code>recipe_args</code> <code>Union[Dict[str, Any], None]</code> <p>the args to use for overriding the recipe defaults</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>the model to compress</p> <code>None</code> <code>teacher_model</code> <code>Optional[Any]</code> <p>the teacher model to use for knowledge distillation</p> <code>None</code> <code>optimizer</code> <code>Optional[Any]</code> <p>the optimizer to use for the compression</p> <code>None</code> <code>attach_optim_callbacks</code> <code>bool</code> <p>True to attach the optimizer callbacks to the compression lifecycle, False otherwise</p> <code>True</code> <code>train_data</code> <code>Optional[Any]</code> <p>the training data to use for the compression</p> <code>None</code> <code>val_data</code> <code>Optional[Any]</code> <p>the validation data to use for the compression</p> <code>None</code> <code>test_data</code> <code>Optional[Any]</code> <p>the testing data to use for the compression</p> <code>None</code> <code>calib_data</code> <code>Optional[Any]</code> <p>the calibration data to use for the compression</p> <code>None</code> <code>copy_data</code> <code>bool</code> <p>True to copy the data, False otherwise</p> <code>True</code> <code>start</code> <code>Optional[float]</code> <p>the start epoch to use for the compression</p> <code>None</code> <code>steps_per_epoch</code> <code>Optional[int]</code> <p>the number of steps per epoch to use for the compression</p> <code>None</code> <code>batches_per_step</code> <code>Optional[int]</code> <p>the number of batches per step to use for compression</p> <code>None</code> <code>loggers</code> <code>Union[None, LoggerManager, List[BaseLogger]]</code> <p>the metrics manager to setup logging important info and milestones to, also accepts a list of BaseLogger(s)</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the lifecycle's initialize method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the session after initializing</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def initialize(\n    self,\n    recipe: Union[str, List[str], \"Recipe\", List[\"Recipe\"], None] = None,\n    recipe_stage: Union[str, List[str], None] = None,\n    recipe_args: Union[Dict[str, Any], None] = None,\n    model: Optional[Any] = None,\n    teacher_model: Optional[Any] = None,\n    optimizer: Optional[Any] = None,\n    attach_optim_callbacks: bool = True,\n    train_data: Optional[Any] = None,\n    val_data: Optional[Any] = None,\n    test_data: Optional[Any] = None,\n    calib_data: Optional[Any] = None,\n    copy_data: bool = True,\n    start: Optional[float] = None,\n    steps_per_epoch: Optional[int] = None,\n    batches_per_step: Optional[int] = None,\n    loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n    **kwargs,\n) -&gt; ModifiedState:\n    \"\"\"\n    Initialize the session for compression. This will run the initialize method\n    for each modifier in the session's lifecycle. This will also set the session's\n    state to the initialized state.\n\n    :param recipe: the recipe to use for the compression, can be a path to a\n        recipe file, a raw recipe string, a recipe object, or a list\n        of recipe objects.\n    :param recipe_stage: the stage to target for the compression\n    :param recipe_args: the args to use for overriding the recipe defaults\n    :param model: the model to compress\n    :param teacher_model: the teacher model to use for knowledge distillation\n    :param optimizer: the optimizer to use for the compression\n    :param attach_optim_callbacks: True to attach the optimizer callbacks to the\n        compression lifecycle, False otherwise\n    :param train_data: the training data to use for the compression\n    :param val_data: the validation data to use for the compression\n    :param test_data: the testing data to use for the compression\n    :param calib_data: the calibration data to use for the compression\n    :param copy_data: True to copy the data, False otherwise\n    :param start: the start epoch to use for the compression\n    :param steps_per_epoch: the number of steps per epoch to use for the\n        compression\n    :param batches_per_step: the number of batches per step to use for\n        compression\n    :param loggers: the metrics manager to setup logging important info\n        and milestones to, also accepts a list of BaseLogger(s)\n    :param kwargs: additional kwargs to pass to the lifecycle's initialize method\n    :return: the modified state of the session after initializing\n    \"\"\"\n    mod_data = self._lifecycle.initialize(\n        recipe=recipe,\n        recipe_stage=recipe_stage,\n        recipe_args=recipe_args,\n        model=model,\n        teacher_model=teacher_model,\n        optimizer=optimizer,\n        attach_optim_callbacks=attach_optim_callbacks,\n        train_data=train_data,\n        val_data=val_data,\n        test_data=test_data,\n        calib_data=calib_data,\n        copy_data=copy_data,\n        start=start,\n        steps_per_epoch=steps_per_epoch,\n        batches_per_step=batches_per_step,\n        loggers=loggers,\n        **kwargs,\n    )\n\n    return ModifiedState(\n        model=self.state.model,\n        optimizer=self.state.optimizer,\n        loss=self.state.loss,\n        modifier_data=mod_data,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.log","title":"<code>log(event_type, loss=None)</code>","text":"<p>Log model and loss information for the current event type</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>the event type to log for</p> required <code>loss</code> <code>Optional[Any]</code> <p>the loss to log if any</p> <code>None</code> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def log(self, event_type: EventType, loss: Optional[Any] = None):\n    \"\"\"\n    Log model and loss information for the current event type\n\n    :param event_type: the event type to log for\n    :param loss: the loss to log if any\n    \"\"\"\n    self._log_model_info()\n    self._log_loss(event_type=event_type, loss=loss)\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.reset","title":"<code>reset()</code>","text":"<p>Reset the session to its initial state</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the session to its initial state\n    \"\"\"\n    self._lifecycle.reset()\n</code></pre>"},{"location":"reference/llmcompressor/core/session/#llmcompressor.core.session.CompressionSession.reset_stage","title":"<code>reset_stage()</code>","text":"<p>Reset the session for starting a new stage, recipe and model stays intact</p> Source code in <code>src/llmcompressor/core/session.py</code> <pre><code>def reset_stage(self):\n    \"\"\"\n    Reset the session for starting a new stage, recipe and model stays intact\n    \"\"\"\n    self.lifecycle.initialized_ = False\n    self.lifecycle.finalized = False\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/","title":"llmcompressor.core.session_functions","text":""},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks","title":"<code>LifecycleCallbacks</code>","text":"<p>A class for invoking lifecycle events for the active session</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>class LifecycleCallbacks:\n    \"\"\"\n    A class for invoking lifecycle events for the active session\n    \"\"\"\n\n    @classmethod\n    def event(cls, event_type: EventType, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an event for the active session\n\n        :param event_type: the event type to invoke\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n            raise ValueError(\n                f\"Cannot invoke {event_type} event. \"\n                f\"Use the corresponding method instead.\"\n            )\n\n        # skip event callbacks if no recipe was provided\n        if not active_session().lifecycle.recipe_container.check_any_recipe_exists():\n            return\n\n        return active_session().event(event_type, **kwargs)\n\n    @classmethod\n    def batch_start(cls, batch_data: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a batch start event for the active session\n\n        :param batch_data: the batch data to use for the event\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        return cls.event(EventType.BATCH_START, batch_data=batch_data, **kwargs)\n\n    @classmethod\n    def loss_calculated(cls, loss: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a loss calculated event for the active session\n\n        :param loss: the loss to use for the event\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        # log loss if loss calculated\n        active_session()._log_loss(event_type=EventType.LOSS_CALCULATED, loss=loss)\n        return cls.event(EventType.LOSS_CALCULATED, loss=loss, **kwargs)\n\n    @classmethod\n    def optim_pre_step(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an optimizer pre-step event for the active session\n\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        return cls.event(EventType.OPTIM_PRE_STEP, **kwargs)\n\n    @classmethod\n    def optim_post_step(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke an optimizer post-step event for the active session\n\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        return cls.event(EventType.OPTIM_POST_STEP, **kwargs)\n\n    @classmethod\n    def batch_end(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a batch end event for the active session\n\n        :param kwargs: additional kwargs to pass to the current session's event method\n        :return: the modified state of the active session after invoking the event\n        \"\"\"\n        active_session()._log_model_info()\n        return cls.event(EventType.BATCH_END, **kwargs)\n\n    @classmethod\n    def calibration_epoch_start(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a epoch start event for the active session during calibration. This event\n        should be called before calibration starts for one epoch\n\n        see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n        \"\"\"\n        return cls.event(EventType.CALIBRATION_EPOCH_START, **kwargs)\n\n    @classmethod\n    def sequential_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a sequential epoch end event for the active session. This event should be\n        called after one sequential layer has been calibrated/trained for one epoch\n\n        This is called after a sequential layer has been calibrated with one batch, see\n        `src/llmcompressor/pipelines/sequential/pipeline.py` for usage example\n        \"\"\"\n        return cls.event(EventType.SEQUENTIAL_EPOCH_END, **kwargs)\n\n    @classmethod\n    def calibration_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n        \"\"\"\n        Invoke a epoch end event for the active session during calibration. This event\n        should be called after the model has been calibrated for one epoch\n\n        see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n        \"\"\"\n        return cls.event(EventType.CALIBRATION_EPOCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.batch_end","title":"<code>batch_end(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a batch end event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef batch_end(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a batch end event for the active session\n\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    active_session()._log_model_info()\n    return cls.event(EventType.BATCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.batch_start","title":"<code>batch_start(batch_data=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a batch start event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Optional[Any]</code> <p>the batch data to use for the event</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef batch_start(cls, batch_data: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a batch start event for the active session\n\n    :param batch_data: the batch data to use for the event\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    return cls.event(EventType.BATCH_START, batch_data=batch_data, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.calibration_epoch_end","title":"<code>calibration_epoch_end(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a epoch end event for the active session during calibration. This event should be called after the model has been calibrated for one epoch</p> <p>see <code>src/llmcompressor/pipelines/basic/pipeline.py</code> for usage example</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef calibration_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a epoch end event for the active session during calibration. This event\n    should be called after the model has been calibrated for one epoch\n\n    see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n    \"\"\"\n    return cls.event(EventType.CALIBRATION_EPOCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.calibration_epoch_start","title":"<code>calibration_epoch_start(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a epoch start event for the active session during calibration. This event should be called before calibration starts for one epoch</p> <p>see <code>src/llmcompressor/pipelines/basic/pipeline.py</code> for usage example</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef calibration_epoch_start(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a epoch start event for the active session during calibration. This event\n    should be called before calibration starts for one epoch\n\n    see `src/llmcompressor/pipelines/basic/pipeline.py` for usage example\n    \"\"\"\n    return cls.event(EventType.CALIBRATION_EPOCH_START, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.event","title":"<code>event(event_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke an event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>EventType</code> <p>the event type to invoke</p> required <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef event(cls, event_type: EventType, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an event for the active session\n\n    :param event_type: the event type to invoke\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    if event_type in [EventType.INITIALIZE, EventType.FINALIZE]:\n        raise ValueError(\n            f\"Cannot invoke {event_type} event. \"\n            f\"Use the corresponding method instead.\"\n        )\n\n    # skip event callbacks if no recipe was provided\n    if not active_session().lifecycle.recipe_container.check_any_recipe_exists():\n        return\n\n    return active_session().event(event_type, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.loss_calculated","title":"<code>loss_calculated(loss=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a loss calculated event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Optional[Any]</code> <p>the loss to use for the event</p> <code>None</code> <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef loss_calculated(cls, loss: Optional[Any] = None, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a loss calculated event for the active session\n\n    :param loss: the loss to use for the event\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    # log loss if loss calculated\n    active_session()._log_loss(event_type=EventType.LOSS_CALCULATED, loss=loss)\n    return cls.event(EventType.LOSS_CALCULATED, loss=loss, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.optim_post_step","title":"<code>optim_post_step(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke an optimizer post-step event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef optim_post_step(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an optimizer post-step event for the active session\n\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    return cls.event(EventType.OPTIM_POST_STEP, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.optim_pre_step","title":"<code>optim_pre_step(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke an optimizer pre-step event for the active session</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the current session's event method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModifiedState</code> <p>the modified state of the active session after invoking the event</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef optim_pre_step(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke an optimizer pre-step event for the active session\n\n    :param kwargs: additional kwargs to pass to the current session's event method\n    :return: the modified state of the active session after invoking the event\n    \"\"\"\n    return cls.event(EventType.OPTIM_PRE_STEP, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.LifecycleCallbacks.sequential_epoch_end","title":"<code>sequential_epoch_end(**kwargs)</code>  <code>classmethod</code>","text":"<p>Invoke a sequential epoch end event for the active session. This event should be called after one sequential layer has been calibrated/trained for one epoch</p> <p>This is called after a sequential layer has been calibrated with one batch, see <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> for usage example</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@classmethod\ndef sequential_epoch_end(cls, **kwargs) -&gt; ModifiedState:\n    \"\"\"\n    Invoke a sequential epoch end event for the active session. This event should be\n    called after one sequential layer has been calibrated/trained for one epoch\n\n    This is called after a sequential layer has been calibrated with one batch, see\n    `src/llmcompressor/pipelines/sequential/pipeline.py` for usage example\n    \"\"\"\n    return cls.event(EventType.SEQUENTIAL_EPOCH_END, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.active_session","title":"<code>active_session()</code>","text":"<p>Returns:</p> Type Description <code>CompressionSession</code> <p>the active session for sparsification</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>def active_session() -&gt; CompressionSession:\n    \"\"\"\n    :return: the active session for sparsification\n    \"\"\"\n    global _local_storage\n    return getattr(_local_storage, \"session\", _global_session)\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.create_session","title":"<code>create_session()</code>","text":"<p>Context manager to create and yield a new session for sparsification. This will set the active session to the new session for the duration of the context.</p> <p>Returns:</p> Type Description <code>Generator[CompressionSession, None, None]</code> <p>the new session</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>@contextmanager\ndef create_session() -&gt; Generator[CompressionSession, None, None]:\n    \"\"\"\n    Context manager to create and yield a new session for sparsification.\n    This will set the active session to the new session for the duration\n    of the context.\n\n    :return: the new session\n    \"\"\"\n    global _local_storage\n    orig_session = getattr(_local_storage, \"session\", None)\n    new_session = CompressionSession()\n    _local_storage.session = new_session\n    try:\n        yield new_session\n    finally:\n        _local_storage.session = orig_session\n</code></pre>"},{"location":"reference/llmcompressor/core/session_functions/#llmcompressor.core.session_functions.reset_session","title":"<code>reset_session()</code>","text":"<p>Reset the currently active session to its initial state</p> Source code in <code>src/llmcompressor/core/session_functions.py</code> <pre><code>def reset_session():\n    \"\"\"\n    Reset the currently active session to its initial state\n    \"\"\"\n    session = active_session()\n    session._lifecycle.reset()\n</code></pre>"},{"location":"reference/llmcompressor/core/state/","title":"llmcompressor.core.state","text":"<p>Module for managing the state of the LLM Compressor.</p> <p>This module provides classes for holding and updating the state information related to data, hardware, and model compression.</p>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.Data","title":"<code>Data</code>  <code>dataclass</code>","text":"<p>A dataclass to hold different data sets for training, validation, testing, and/or calibration. Each data set is a ModifiableData instance.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>Optional[Any]</code> <p>The training data set</p> <code>None</code> <code>val</code> <code>Optional[Any]</code> <p>The validation data set</p> <code>None</code> <code>test</code> <code>Optional[Any]</code> <p>The testing data set</p> <code>None</code> <code>calib</code> <code>Optional[Any]</code> <p>The calibration data set</p> <code>None</code> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass Data:\n    \"\"\"\n    A dataclass to hold different data sets for training, validation,\n    testing, and/or calibration. Each data set is a ModifiableData instance.\n\n    :param train: The training data set\n    :type train: Optional[Any]\n    :param val: The validation data set\n    :type val: Optional[Any]\n    :param test: The testing data set\n    :type test: Optional[Any]\n    :param calib: The calibration data set\n    :type calib: Optional[Any]\n    \"\"\"\n\n    train: Optional[Any] = None\n    val: Optional[Any] = None\n    test: Optional[Any] = None\n    calib: Optional[Any] = None\n</code></pre>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.Hardware","title":"<code>Hardware</code>  <code>dataclass</code>","text":"<p>A dataclass to hold information about the hardware being used.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>The current device being used for training</p> <code>None</code> <code>devices</code> <code>Optional[List[str]]</code> <p>List of all devices to be used for training</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>The rank of the current device</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>The total number of devices being used</p> <code>None</code> <code>local_rank</code> <code>Optional[int]</code> <p>The local rank of the current device</p> <code>None</code> <code>local_world_size</code> <code>Optional[int]</code> <p>The total number of devices being used on the local machine</p> <code>None</code> <code>distributed</code> <code>Optional[bool]</code> <p>Whether or not distributed training is being used</p> <code>None</code> <code>distributed_strategy</code> <code>Optional[str]</code> <p>The distributed strategy being used</p> <code>None</code> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass Hardware:\n    \"\"\"\n    A dataclass to hold information about the hardware being used.\n\n    :param device: The current device being used for training\n    :type device: Optional[str]\n    :param devices: List of all devices to be used for training\n    :type devices: Optional[List[str]]\n    :param rank: The rank of the current device\n    :type rank: Optional[int]\n    :param world_size: The total number of devices being used\n    :type world_size: Optional[int]\n    :param local_rank: The local rank of the current device\n    :type local_rank: Optional[int]\n    :param local_world_size: The total number of devices being used on the local machine\n    :type local_world_size: Optional[int]\n    :param distributed: Whether or not distributed training is being used\n    :type distributed: Optional[bool]\n    :param distributed_strategy: The distributed strategy being used\n    :type distributed_strategy: Optional[str]\n    \"\"\"\n\n    device: Optional[str] = None\n    devices: Optional[List[str]] = None\n    rank: Optional[int] = None\n    world_size: Optional[int] = None\n    local_rank: Optional[int] = None\n    local_world_size: Optional[int] = None\n    distributed: Optional[bool] = None\n    distributed_strategy: Optional[str] = None\n</code></pre>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.ModifiedState","title":"<code>ModifiedState</code>  <code>dataclass</code>","text":"<p>A dataclass to represent a modified model, optimizer, and loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Any]</code> <p>The modified model</p> required <code>optimizer</code> <code>Optional[Any]</code> <p>The modified optimizer</p> required <code>loss</code> <code>Optional[Any]</code> <p>The modified loss</p> required <code>modifier_data</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The modifier data used to modify the model, optimizer, and loss</p> required Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass ModifiedState:\n    \"\"\"\n    A dataclass to represent a modified model, optimizer, and loss.\n\n    :param model: The modified model\n    :type model: Optional[Any]\n    :param optimizer: The modified optimizer\n    :type optimizer: Optional[Any]\n    :param loss: The modified loss\n    :type loss: Optional[Any]\n    :param modifier_data: The modifier data used to modify the\n        model, optimizer, and loss\n    :type modifier_data: Optional[List[Dict[str, Any]]]\n    \"\"\"\n\n    model: Optional[Any] = None\n    optimizer: Optional[Any] = None\n    loss: Optional[Any] = None\n    modifier_data: Optional[List[Dict[str, Any]]] = None\n\n    def __init__(self, model, optimizer, loss, modifier_data):\n        \"\"\"\n        Initialize the ModifiedState with the given parameters.\n\n        :param model: The modified model\n        :type model: Any\n        :param optimizer: The modified optimizer\n        :type optimizer: Any\n        :param loss: The modified loss\n        :type loss: Any\n        :param modifier_data: The modifier data used to modify the model, optimizer,\n            and loss\n        :type modifier_data: List[Dict[str, Any]]\n        \"\"\"\n        self.model = model\n        self.optimizer = optimizer\n        self.loss = loss\n        self.modifier_data = modifier_data\n</code></pre>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.ModifiedState.__init__","title":"<code>__init__(model, optimizer, loss, modifier_data)</code>","text":"<p>Initialize the ModifiedState with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The modified model</p> required <code>optimizer</code> <code>Any</code> <p>The modified optimizer</p> required <code>loss</code> <code>Any</code> <p>The modified loss</p> required <code>modifier_data</code> <code>List[Dict[str, Any]]</code> <p>The modifier data used to modify the model, optimizer, and loss</p> required Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>def __init__(self, model, optimizer, loss, modifier_data):\n    \"\"\"\n    Initialize the ModifiedState with the given parameters.\n\n    :param model: The modified model\n    :type model: Any\n    :param optimizer: The modified optimizer\n    :type optimizer: Any\n    :param loss: The modified loss\n    :type loss: Any\n    :param modifier_data: The modifier data used to modify the model, optimizer,\n        and loss\n    :type modifier_data: List[Dict[str, Any]]\n    \"\"\"\n    self.model = model\n    self.optimizer = optimizer\n    self.loss = loss\n    self.modifier_data = modifier_data\n</code></pre>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.State","title":"<code>State</code>  <code>dataclass</code>","text":"<p>State class holds information about the current compression state.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model being used for compression</p> <code>None</code> <code>teacher_model</code> <code>Any</code> <p>The teacher model being used for compression</p> <code>None</code> <code>optimizer</code> <code>Any</code> <p>The optimizer being used for training</p> <code>None</code> <code>optim_wrapped</code> <code>bool</code> <p>Whether or not the optimizer has been wrapped</p> <code>None</code> <code>loss</code> <code>Any</code> <p>The loss function being used for training</p> <code>None</code> <code>batch_data</code> <code>Any</code> <p>The current batch of data being used for compression</p> <code>None</code> <code>data</code> <code>Data</code> <p>The data sets being used for training, validation, testing, and/or calibration, wrapped in a Data instance</p> <code>Data()</code> <code>hardware</code> <code>Hardware</code> <p>Hardware instance holding info about the target hardware being used</p> <code>Hardware()</code> <code>loggers</code> <code>Optional[LoggerManager]</code> <p>LoggerManager instance holding all the loggers to log</p> <code>None</code> <code>model_log_cadence</code> <code>Optional[float]</code> <p>The cadence to log model information w.r.t epochs. If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.</p> <code>None</code> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>@dataclass\nclass State:\n    \"\"\"\n    State class holds information about the current compression state.\n\n    :param model: The model being used for compression\n    :type model: Any\n    :param teacher_model: The teacher model being used for compression\n    :type teacher_model: Any\n    :param optimizer: The optimizer being used for training\n    :type optimizer: Any\n    :param optim_wrapped: Whether or not the optimizer has been wrapped\n    :type optim_wrapped: bool\n    :param loss: The loss function being used for training\n    :type loss: Any\n    :param batch_data: The current batch of data being used for compression\n    :type batch_data: Any\n    :param data: The data sets being used for training, validation, testing,\n        and/or calibration, wrapped in a Data instance\n    :type data: Data\n    :param hardware: Hardware instance holding info about the target hardware being used\n    :type hardware: Hardware\n    :param loggers: LoggerManager instance holding all the loggers to log\n    :type loggers: Optional[LoggerManager]\n    :param model_log_cadence: The cadence to log model information w.r.t epochs.\n        If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.\n    :type model_log_cadence: Optional[float]\n    \"\"\"\n\n    model: Any = None\n    teacher_model: Any = None\n    optimizer: Any = None\n    optim_wrapped: bool = None\n    loss: Any = None\n    batch_data: Any = None\n    data: Data = field(default_factory=Data)\n    hardware: Hardware = field(default_factory=Hardware)\n    loggers: Optional[LoggerManager] = None\n    model_log_cadence: Optional[float] = None\n    _last_log_step: Union[float, int, None] = None\n\n    @property\n    def compression_ready(self) -&gt; bool:\n        \"\"\"\n        Check if the model and optimizer are set for compression.\n\n        :return: True if model and optimizer are set, False otherwise\n        :rtype: bool\n        \"\"\"\n        ready = self.model is not None and self.optimizer is not None\n        logger.debug(\"Compression ready: {}\", ready)\n        return ready\n\n    def update(\n        self,\n        model: Any = None,\n        teacher_model: Any = None,\n        optimizer: Any = None,\n        attach_optim_callbacks: bool = True,\n        train_data: Any = None,\n        val_data: Any = None,\n        test_data: Any = None,\n        calib_data: Any = None,\n        copy_data: bool = True,\n        start: float = None,\n        steps_per_epoch: int = None,\n        batches_per_step: int = None,\n        loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n        model_log_cadence: Optional[float] = None,\n        **kwargs,\n    ) -&gt; Dict:\n        \"\"\"\n        Update the state with the given parameters.\n\n        :param model: The model to update the state with\n        :type model: Any\n        :param teacher_model: The teacher model to update the state with\n        :type teacher_model: Any\n        :param optimizer: The optimizer to update the state with\n        :type optimizer: Any\n        :param attach_optim_callbacks: Whether or not to attach optimizer callbacks\n        :type attach_optim_callbacks: bool\n        :param train_data: The training data to update the state with\n        :type train_data: Any\n        :param val_data: The validation data to update the state with\n        :type val_data: Any\n        :param test_data: The testing data to update the state with\n        :type test_data: Any\n        :param calib_data: The calibration data to update the state with\n        :type calib_data: Any\n        :param copy_data: Whether or not to copy the data\n        :type copy_data: bool\n        :param start: The start index to update the state with\n        :type start: float\n        :param steps_per_epoch: The steps per epoch to update the state with\n        :type steps_per_epoch: int\n        :param batches_per_step: The batches per step to update the state with\n        :type batches_per_step: int\n        :param loggers: The metrics manager to setup logging important info and\n            milestones to, also accepts a list of BaseLogger(s)\n        :type loggers: Union[None, LoggerManager, List[BaseLogger]]\n        :param model_log_cadence: The cadence to log model information w.r.t epochs.\n            If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.\n        :type model_log_cadence: Optional[float]\n        :param kwargs: Additional keyword arguments to update the state with\n        :return: The updated state as a dictionary\n        :rtype: Dict\n        \"\"\"\n        logger.debug(\n            \"Updating state with provided parameters: {}\",\n            {\n                \"model\": model,\n                \"teacher_model\": teacher_model,\n                \"optimizer\": optimizer,\n                \"attach_optim_callbacks\": attach_optim_callbacks,\n                \"train_data\": train_data,\n                \"val_data\": val_data,\n                \"test_data\": test_data,\n                \"calib_data\": calib_data,\n                \"copy_data\": copy_data,\n                \"start\": start,\n                \"steps_per_epoch\": steps_per_epoch,\n                \"batches_per_step\": batches_per_step,\n                \"loggers\": loggers,\n                \"model_log_cadence\": model_log_cadence,\n                \"kwargs\": kwargs,\n            },\n        )\n\n        if model is not None:\n            self.model = model\n        if teacher_model is not None:\n            self.teacher_model = teacher_model\n        if optimizer is not None:\n            self.optim_wrapped = attach_optim_callbacks\n            self.optimizer = optimizer\n        if train_data is not None:\n            self.data.train = train_data if not copy_data else deepcopy(train_data)\n        if val_data is not None:\n            self.data.val = val_data if not copy_data else deepcopy(val_data)\n        if test_data is not None:\n            self.data.test = test_data if not copy_data else deepcopy(test_data)\n        if calib_data is not None:\n            self.data.calib = calib_data if not copy_data else deepcopy(calib_data)\n\n        if \"device\" in kwargs:\n            self.hardware.device = kwargs[\"device\"]\n\n        loggers = loggers or []\n        if isinstance(loggers, list):\n            loggers = LoggerManager(loggers)\n        self.loggers = loggers\n\n        if model_log_cadence is not None:\n            self.model_log_cadence = model_log_cadence\n\n        return kwargs\n</code></pre>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.State.compression_ready","title":"<code>compression_ready</code>  <code>property</code>","text":"<p>Check if the model and optimizer are set for compression.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model and optimizer are set, False otherwise</p>"},{"location":"reference/llmcompressor/core/state/#llmcompressor.core.state.State.update","title":"<code>update(model=None, teacher_model=None, optimizer=None, attach_optim_callbacks=True, train_data=None, val_data=None, test_data=None, calib_data=None, copy_data=True, start=None, steps_per_epoch=None, batches_per_step=None, loggers=None, model_log_cadence=None, **kwargs)</code>","text":"<p>Update the state with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model to update the state with</p> <code>None</code> <code>teacher_model</code> <code>Any</code> <p>The teacher model to update the state with</p> <code>None</code> <code>optimizer</code> <code>Any</code> <p>The optimizer to update the state with</p> <code>None</code> <code>attach_optim_callbacks</code> <code>bool</code> <p>Whether or not to attach optimizer callbacks</p> <code>True</code> <code>train_data</code> <code>Any</code> <p>The training data to update the state with</p> <code>None</code> <code>val_data</code> <code>Any</code> <p>The validation data to update the state with</p> <code>None</code> <code>test_data</code> <code>Any</code> <p>The testing data to update the state with</p> <code>None</code> <code>calib_data</code> <code>Any</code> <p>The calibration data to update the state with</p> <code>None</code> <code>copy_data</code> <code>bool</code> <p>Whether or not to copy the data</p> <code>True</code> <code>start</code> <code>float</code> <p>The start index to update the state with</p> <code>None</code> <code>steps_per_epoch</code> <code>int</code> <p>The steps per epoch to update the state with</p> <code>None</code> <code>batches_per_step</code> <code>int</code> <p>The batches per step to update the state with</p> <code>None</code> <code>loggers</code> <code>Union[None, LoggerManager, List[BaseLogger]]</code> <p>The metrics manager to setup logging important info and milestones to, also accepts a list of BaseLogger(s)</p> <code>None</code> <code>model_log_cadence</code> <code>Optional[float]</code> <p>The cadence to log model information w.r.t epochs. If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to update the state with</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>The updated state as a dictionary</p> Source code in <code>src/llmcompressor/core/state.py</code> <pre><code>def update(\n    self,\n    model: Any = None,\n    teacher_model: Any = None,\n    optimizer: Any = None,\n    attach_optim_callbacks: bool = True,\n    train_data: Any = None,\n    val_data: Any = None,\n    test_data: Any = None,\n    calib_data: Any = None,\n    copy_data: bool = True,\n    start: float = None,\n    steps_per_epoch: int = None,\n    batches_per_step: int = None,\n    loggers: Union[None, LoggerManager, List[BaseLogger]] = None,\n    model_log_cadence: Optional[float] = None,\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"\n    Update the state with the given parameters.\n\n    :param model: The model to update the state with\n    :type model: Any\n    :param teacher_model: The teacher model to update the state with\n    :type teacher_model: Any\n    :param optimizer: The optimizer to update the state with\n    :type optimizer: Any\n    :param attach_optim_callbacks: Whether or not to attach optimizer callbacks\n    :type attach_optim_callbacks: bool\n    :param train_data: The training data to update the state with\n    :type train_data: Any\n    :param val_data: The validation data to update the state with\n    :type val_data: Any\n    :param test_data: The testing data to update the state with\n    :type test_data: Any\n    :param calib_data: The calibration data to update the state with\n    :type calib_data: Any\n    :param copy_data: Whether or not to copy the data\n    :type copy_data: bool\n    :param start: The start index to update the state with\n    :type start: float\n    :param steps_per_epoch: The steps per epoch to update the state with\n    :type steps_per_epoch: int\n    :param batches_per_step: The batches per step to update the state with\n    :type batches_per_step: int\n    :param loggers: The metrics manager to setup logging important info and\n        milestones to, also accepts a list of BaseLogger(s)\n    :type loggers: Union[None, LoggerManager, List[BaseLogger]]\n    :param model_log_cadence: The cadence to log model information w.r.t epochs.\n        If 1, logs every epoch. If 2, logs every other epoch, etc. Default is 1.\n    :type model_log_cadence: Optional[float]\n    :param kwargs: Additional keyword arguments to update the state with\n    :return: The updated state as a dictionary\n    :rtype: Dict\n    \"\"\"\n    logger.debug(\n        \"Updating state with provided parameters: {}\",\n        {\n            \"model\": model,\n            \"teacher_model\": teacher_model,\n            \"optimizer\": optimizer,\n            \"attach_optim_callbacks\": attach_optim_callbacks,\n            \"train_data\": train_data,\n            \"val_data\": val_data,\n            \"test_data\": test_data,\n            \"calib_data\": calib_data,\n            \"copy_data\": copy_data,\n            \"start\": start,\n            \"steps_per_epoch\": steps_per_epoch,\n            \"batches_per_step\": batches_per_step,\n            \"loggers\": loggers,\n            \"model_log_cadence\": model_log_cadence,\n            \"kwargs\": kwargs,\n        },\n    )\n\n    if model is not None:\n        self.model = model\n    if teacher_model is not None:\n        self.teacher_model = teacher_model\n    if optimizer is not None:\n        self.optim_wrapped = attach_optim_callbacks\n        self.optimizer = optimizer\n    if train_data is not None:\n        self.data.train = train_data if not copy_data else deepcopy(train_data)\n    if val_data is not None:\n        self.data.val = val_data if not copy_data else deepcopy(val_data)\n    if test_data is not None:\n        self.data.test = test_data if not copy_data else deepcopy(test_data)\n    if calib_data is not None:\n        self.data.calib = calib_data if not copy_data else deepcopy(calib_data)\n\n    if \"device\" in kwargs:\n        self.hardware.device = kwargs[\"device\"]\n\n    loggers = loggers or []\n    if isinstance(loggers, list):\n        loggers = LoggerManager(loggers)\n    self.loggers = loggers\n\n    if model_log_cadence is not None:\n        self.model_log_cadence = model_log_cadence\n\n    return kwargs\n</code></pre>"},{"location":"reference/llmcompressor/core/events/","title":"llmcompressor.core.events","text":"<p>LLM Compressor Core Events Package</p> <p>This package provides the core components and lifecycle management for events used in the LLM Compressor framework. It includes definitions for various event types and lifecycles that are critical for managing the state and execution flow of the model compression and training processes.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event","title":"<code>Event</code>  <code>dataclass</code>","text":"<p>A class for defining an event that can be triggered during sparsification.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Optional[EventType]</code> <p>The type of event.</p> <code>None</code> <code>steps_per_epoch</code> <code>Optional[int]</code> <p>The number of steps per epoch.</p> <code>None</code> <code>batches_per_step</code> <code>Optional[int]</code> <p>The number of batches per step where step is an optimizer step invocation. For most pathways, these are the same. See the invocations_per_step parameter for more details when they are not.</p> <code>None</code> <code>invocations_per_step</code> <code>int</code> <p>The number of invocations of the step wrapper before optimizer.step was called. Generally can be left as 1 (default). For older amp pathways, this is the number of times the scaler wrapper was invoked before the wrapped optimizer step function was called to handle accumulation in fp16.</p> <code>1</code> <code>global_step</code> <code>int</code> <p>The current global step.</p> <code>0</code> <code>global_batch</code> <code>int</code> <p>The current global batch.</p> <code>0</code> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>@dataclass\nclass Event:\n    \"\"\"\n    A class for defining an event that can be triggered during sparsification.\n\n    :param type_: The type of event.\n    :type type_: Optional[EventType]\n    :param steps_per_epoch: The number of steps per epoch.\n    :type steps_per_epoch: Optional[int]\n    :param batches_per_step: The number of batches per step where step is an\n        optimizer step invocation. For most pathways, these are the same.\n        See the invocations_per_step parameter for more details when they are not.\n    :type batches_per_step: Optional[int]\n    :param invocations_per_step: The number of invocations of the step wrapper\n        before optimizer.step was called. Generally can be left as 1 (default).\n        For older amp pathways, this is the number of times the scaler wrapper\n        was invoked before the wrapped optimizer step function was called to\n        handle accumulation in fp16.\n    :type invocations_per_step: Optional[int]\n    :param global_step: The current global step.\n    :type global_step: int\n    :param global_batch: The current global batch.\n    :type global_batch: int\n    \"\"\"\n\n    type_: Optional[EventType] = None\n    steps_per_epoch: Optional[int] = None\n    batches_per_step: Optional[int] = None\n    invocations_per_step: int = 1\n    global_step: int = 0\n    global_batch: int = 0\n\n    @property\n    def epoch_based(self) -&gt; bool:\n        \"\"\"\n        Determines if the event is based on epochs.\n\n        :return: True if the event is based on epochs, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.steps_per_epoch is not None\n\n    @property\n    def epoch(self) -&gt; int:\n        \"\"\"\n        Calculates the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step // self.steps_per_epoch\n\n    @property\n    def epoch_full(self) -&gt; float:\n        \"\"\"\n        Calculates the current epoch with the fraction of the current step.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current epoch with the fraction of the current step.\n        :rtype: float\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_full for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step / float(self.steps_per_epoch)\n\n    @property\n    def epoch_step(self) -&gt; int:\n        \"\"\"\n        Calculates the current step within the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current step within the current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_step for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step % self.steps_per_epoch\n\n    @property\n    def epoch_batch(self) -&gt; int:\n        \"\"\"\n        Calculates the current batch within the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current batch within the current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_batch for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        batches_per_epoch = (\n            self.steps_per_epoch * self.batches_per_step\n            if self.batches_per_step\n            else self.steps_per_epoch\n        )\n        return self.global_batch % batches_per_epoch\n\n    @property\n    def current_index(self) -&gt; float:\n        \"\"\"\n        Calculates the current index of the event.\n\n        :raises ValueError: if the event is not epoch based or\n            if the steps per epoch are too many.\n        :return: The current index of the event, which is either the global step\n            or the epoch with the fraction of the current step.\n        :rtype: float\n        \"\"\"\n        if not self.epoch_based:\n            return self.global_step\n        epoch_full = self.epoch_full\n        if epoch_full - self.epoch &gt; 1.0:\n            logger.error(\"Too many steps per epoch for epoch based event\")\n            raise ValueError(\"Too many steps per epoch for epoch based event\")\n        return epoch_full\n\n    @current_index.setter\n    def current_index(self, value: float):\n        \"\"\"\n        Sets the current index of the event.\n\n        :param value: The current index value.\n        :type value: float\n        \"\"\"\n        logger.debug(\"Setting current index: {}\", value)\n        if not self.epoch_based:\n            self.global_step = int(value)\n            self.global_batch = (\n                self.global_step\n                if self.batches_per_step is None or self.batches_per_step &lt; 2\n                else self.global_step * self.batches_per_step\n            )\n        else:\n            self.global_step = int(value * self.steps_per_epoch)\n            self.global_batch = (\n                self.global_step\n                if self.batches_per_step is None or self.batches_per_step &lt; 2\n                else self.global_step * self.batches_per_step\n            )\n\n    def should_update(\n        self, start: Optional[float], end: Optional[float], update: Optional[float]\n    ) -&gt; bool:\n        \"\"\"\n        Determines if the event should trigger an update.\n\n        :param start: The start index to check against, set to None to ignore start.\n        :type start: Optional[float]\n        :param end: The end index to check against, set to None to ignore end.\n        :type end: Optional[float]\n        :param update: The update interval, set to None or 0.0 to always update,\n            otherwise must be greater than 0.0, defaults to None.\n        :type update: Optional[float]\n        :return: True if the event should trigger an update, False otherwise.\n        :rtype: bool\n        \"\"\"\n        current = self.current_index\n        logger.debug(\n            \"Checking if event should update: \"\n            \"current_index={}, start={}, end={}, update={}\",\n            current,\n            start,\n            end,\n            update,\n        )\n        if start is not None and current &lt; start:\n            return False\n        if end is not None and current &gt; end:\n            return False\n        return update is None or update &lt;= 0.0 or current % update &lt; 1e-10\n\n    def new_instance(self, **kwargs) -&gt; \"Event\":\n        \"\"\"\n        Creates a new instance of the event with the provided keyword arguments.\n\n        :param kwargs: Keyword arguments to set in the new instance.\n        :return: A new instance of the event with the provided kwargs.\n        :rtype: Event\n        \"\"\"\n        logger.debug(\"Creating new instance of event with kwargs: {}\", kwargs)\n        instance = deepcopy(self)\n        for key, value in kwargs.items():\n            setattr(instance, key, value)\n        return instance\n</code></pre>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.current_index","title":"<code>current_index</code>  <code>property</code> <code>writable</code>","text":"<p>Calculates the current index of the event.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current index of the event, which is either the global step or the epoch with the fraction of the current step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based or if the steps per epoch are too many.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.epoch","title":"<code>epoch</code>  <code>property</code>","text":"<p>Calculates the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.epoch_based","title":"<code>epoch_based</code>  <code>property</code>","text":"<p>Determines if the event is based on epochs.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the event is based on epochs, False otherwise.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.epoch_batch","title":"<code>epoch_batch</code>  <code>property</code>","text":"<p>Calculates the current batch within the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current batch within the current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.epoch_full","title":"<code>epoch_full</code>  <code>property</code>","text":"<p>Calculates the current epoch with the fraction of the current step.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current epoch with the fraction of the current step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.epoch_step","title":"<code>epoch_step</code>  <code>property</code>","text":"<p>Calculates the current step within the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current step within the current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.new_instance","title":"<code>new_instance(**kwargs)</code>","text":"<p>Creates a new instance of the event with the provided keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments to set in the new instance.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Event</code> <p>A new instance of the event with the provided kwargs.</p> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>def new_instance(self, **kwargs) -&gt; \"Event\":\n    \"\"\"\n    Creates a new instance of the event with the provided keyword arguments.\n\n    :param kwargs: Keyword arguments to set in the new instance.\n    :return: A new instance of the event with the provided kwargs.\n    :rtype: Event\n    \"\"\"\n    logger.debug(\"Creating new instance of event with kwargs: {}\", kwargs)\n    instance = deepcopy(self)\n    for key, value in kwargs.items():\n        setattr(instance, key, value)\n    return instance\n</code></pre>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.Event.should_update","title":"<code>should_update(start, end, update)</code>","text":"<p>Determines if the event should trigger an update.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[float]</code> <p>The start index to check against, set to None to ignore start.</p> required <code>end</code> <code>Optional[float]</code> <p>The end index to check against, set to None to ignore end.</p> required <code>update</code> <code>Optional[float]</code> <p>The update interval, set to None or 0.0 to always update, otherwise must be greater than 0.0, defaults to None.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event should trigger an update, False otherwise.</p> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>def should_update(\n    self, start: Optional[float], end: Optional[float], update: Optional[float]\n) -&gt; bool:\n    \"\"\"\n    Determines if the event should trigger an update.\n\n    :param start: The start index to check against, set to None to ignore start.\n    :type start: Optional[float]\n    :param end: The end index to check against, set to None to ignore end.\n    :type end: Optional[float]\n    :param update: The update interval, set to None or 0.0 to always update,\n        otherwise must be greater than 0.0, defaults to None.\n    :type update: Optional[float]\n    :return: True if the event should trigger an update, False otherwise.\n    :rtype: bool\n    \"\"\"\n    current = self.current_index\n    logger.debug(\n        \"Checking if event should update: \"\n        \"current_index={}, start={}, end={}, update={}\",\n        current,\n        start,\n        end,\n        update,\n    )\n    if start is not None and current &lt; start:\n        return False\n    if end is not None and current &gt; end:\n        return False\n    return update is None or update &lt;= 0.0 or current % update &lt; 1e-10\n</code></pre>"},{"location":"reference/llmcompressor/core/events/#llmcompressor.core.events.EventType","title":"<code>EventType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An Enum for defining the different types of events that can be triggered during model compression lifecycles. The purpose of each EventType is to trigger the corresponding modifier callback during training or post training pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>INITIALIZE</code> <p>Event type for initialization.</p> required <code>FINALIZE</code> <p>Event type for finalization.</p> required <code>BATCH_START</code> <p>Event type for the start of a batch.</p> required <code>LOSS_CALCULATED</code> <p>Event type for when loss is calculated.</p> required <code>BATCH_END</code> <p>Event type for the end of a batch.</p> required <code>CALIBRATION_EPOCH_START</code> <p>Event type for the start of a calibration epoch.</p> required <code>SEQUENTIAL_EPOCH_END</code> <p>Event type for the end of a layer calibration epoch, specifically used by <code>src/llmcompressor/pipelines/sequential/pipeline.py</code></p> required <code>CALIBRATION_EPOCH_END</code> <p>Event type for the end of a calibration epoch.</p> required <code>OPTIM_PRE_STEP</code> <p>Event type for pre-optimization step.</p> required <code>OPTIM_POST_STEP</code> <p>Event type for post-optimization step.</p> required Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>@unique\nclass EventType(Enum):\n    \"\"\"\n    An Enum for defining the different types of events that can be triggered\n    during model compression lifecycles.\n    The purpose of each EventType is to trigger the corresponding\n    modifier callback during training or post training pipelines.\n\n    :param INITIALIZE: Event type for initialization.\n    :param FINALIZE: Event type for finalization.\n    :param BATCH_START: Event type for the start of a batch.\n    :param LOSS_CALCULATED: Event type for when loss is calculated.\n    :param BATCH_END: Event type for the end of a batch.\n    :param CALIBRATION_EPOCH_START: Event type for the start of a calibration epoch.\n    :param SEQUENTIAL_EPOCH_END: Event type for the end of a layer calibration epoch,\n        specifically used by `src/llmcompressor/pipelines/sequential/pipeline.py`\n    :param CALIBRATION_EPOCH_END: Event type for the end of a calibration epoch.\n    :param OPTIM_PRE_STEP: Event type for pre-optimization step.\n    :param OPTIM_POST_STEP: Event type for post-optimization step.\n    \"\"\"\n\n    # training lifecycle\n    INITIALIZE = \"initialize\"\n    FINALIZE = \"finalize\"\n\n    # batch lifecycle\n    BATCH_START = \"batch_start\"\n    LOSS_CALCULATED = \"loss_calculated\"\n    BATCH_END = \"batch_end\"\n\n    # calibration lifecycle\n    CALIBRATION_EPOCH_START = \"calibration_epoch_start\"\n    SEQUENTIAL_EPOCH_END = \"sequential_epoch_end\"\n    CALIBRATION_EPOCH_END = \"calibration_epoch_end\"\n\n    # step lifecycle\n    OPTIM_PRE_STEP = \"optim_pre_step\"\n    OPTIM_POST_STEP = \"optim_post_step\"\n</code></pre>"},{"location":"reference/llmcompressor/core/events/event/","title":"llmcompressor.core.events.event","text":"<p>Module for defining and managing events in the LLM Compressor.</p> <p>This module provides an Enum for different event types and a class for creating and managing events, including methods for calculating event properties and triggering updates based on specified intervals.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event","title":"<code>Event</code>  <code>dataclass</code>","text":"<p>A class for defining an event that can be triggered during sparsification.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Optional[EventType]</code> <p>The type of event.</p> <code>None</code> <code>steps_per_epoch</code> <code>Optional[int]</code> <p>The number of steps per epoch.</p> <code>None</code> <code>batches_per_step</code> <code>Optional[int]</code> <p>The number of batches per step where step is an optimizer step invocation. For most pathways, these are the same. See the invocations_per_step parameter for more details when they are not.</p> <code>None</code> <code>invocations_per_step</code> <code>int</code> <p>The number of invocations of the step wrapper before optimizer.step was called. Generally can be left as 1 (default). For older amp pathways, this is the number of times the scaler wrapper was invoked before the wrapped optimizer step function was called to handle accumulation in fp16.</p> <code>1</code> <code>global_step</code> <code>int</code> <p>The current global step.</p> <code>0</code> <code>global_batch</code> <code>int</code> <p>The current global batch.</p> <code>0</code> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>@dataclass\nclass Event:\n    \"\"\"\n    A class for defining an event that can be triggered during sparsification.\n\n    :param type_: The type of event.\n    :type type_: Optional[EventType]\n    :param steps_per_epoch: The number of steps per epoch.\n    :type steps_per_epoch: Optional[int]\n    :param batches_per_step: The number of batches per step where step is an\n        optimizer step invocation. For most pathways, these are the same.\n        See the invocations_per_step parameter for more details when they are not.\n    :type batches_per_step: Optional[int]\n    :param invocations_per_step: The number of invocations of the step wrapper\n        before optimizer.step was called. Generally can be left as 1 (default).\n        For older amp pathways, this is the number of times the scaler wrapper\n        was invoked before the wrapped optimizer step function was called to\n        handle accumulation in fp16.\n    :type invocations_per_step: Optional[int]\n    :param global_step: The current global step.\n    :type global_step: int\n    :param global_batch: The current global batch.\n    :type global_batch: int\n    \"\"\"\n\n    type_: Optional[EventType] = None\n    steps_per_epoch: Optional[int] = None\n    batches_per_step: Optional[int] = None\n    invocations_per_step: int = 1\n    global_step: int = 0\n    global_batch: int = 0\n\n    @property\n    def epoch_based(self) -&gt; bool:\n        \"\"\"\n        Determines if the event is based on epochs.\n\n        :return: True if the event is based on epochs, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.steps_per_epoch is not None\n\n    @property\n    def epoch(self) -&gt; int:\n        \"\"\"\n        Calculates the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step // self.steps_per_epoch\n\n    @property\n    def epoch_full(self) -&gt; float:\n        \"\"\"\n        Calculates the current epoch with the fraction of the current step.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current epoch with the fraction of the current step.\n        :rtype: float\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_full for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step / float(self.steps_per_epoch)\n\n    @property\n    def epoch_step(self) -&gt; int:\n        \"\"\"\n        Calculates the current step within the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current step within the current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_step for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        return self.global_step % self.steps_per_epoch\n\n    @property\n    def epoch_batch(self) -&gt; int:\n        \"\"\"\n        Calculates the current batch within the current epoch.\n\n        :raises ValueError: if the event is not epoch based.\n        :return: The current batch within the current epoch.\n        :rtype: int\n        \"\"\"\n        if not self.epoch_based:\n            logger.error(\"Attempt to access epoch_batch for a non-epoch based event\")\n            raise ValueError(\"Event is not epoch based\")\n        batches_per_epoch = (\n            self.steps_per_epoch * self.batches_per_step\n            if self.batches_per_step\n            else self.steps_per_epoch\n        )\n        return self.global_batch % batches_per_epoch\n\n    @property\n    def current_index(self) -&gt; float:\n        \"\"\"\n        Calculates the current index of the event.\n\n        :raises ValueError: if the event is not epoch based or\n            if the steps per epoch are too many.\n        :return: The current index of the event, which is either the global step\n            or the epoch with the fraction of the current step.\n        :rtype: float\n        \"\"\"\n        if not self.epoch_based:\n            return self.global_step\n        epoch_full = self.epoch_full\n        if epoch_full - self.epoch &gt; 1.0:\n            logger.error(\"Too many steps per epoch for epoch based event\")\n            raise ValueError(\"Too many steps per epoch for epoch based event\")\n        return epoch_full\n\n    @current_index.setter\n    def current_index(self, value: float):\n        \"\"\"\n        Sets the current index of the event.\n\n        :param value: The current index value.\n        :type value: float\n        \"\"\"\n        logger.debug(\"Setting current index: {}\", value)\n        if not self.epoch_based:\n            self.global_step = int(value)\n            self.global_batch = (\n                self.global_step\n                if self.batches_per_step is None or self.batches_per_step &lt; 2\n                else self.global_step * self.batches_per_step\n            )\n        else:\n            self.global_step = int(value * self.steps_per_epoch)\n            self.global_batch = (\n                self.global_step\n                if self.batches_per_step is None or self.batches_per_step &lt; 2\n                else self.global_step * self.batches_per_step\n            )\n\n    def should_update(\n        self, start: Optional[float], end: Optional[float], update: Optional[float]\n    ) -&gt; bool:\n        \"\"\"\n        Determines if the event should trigger an update.\n\n        :param start: The start index to check against, set to None to ignore start.\n        :type start: Optional[float]\n        :param end: The end index to check against, set to None to ignore end.\n        :type end: Optional[float]\n        :param update: The update interval, set to None or 0.0 to always update,\n            otherwise must be greater than 0.0, defaults to None.\n        :type update: Optional[float]\n        :return: True if the event should trigger an update, False otherwise.\n        :rtype: bool\n        \"\"\"\n        current = self.current_index\n        logger.debug(\n            \"Checking if event should update: \"\n            \"current_index={}, start={}, end={}, update={}\",\n            current,\n            start,\n            end,\n            update,\n        )\n        if start is not None and current &lt; start:\n            return False\n        if end is not None and current &gt; end:\n            return False\n        return update is None or update &lt;= 0.0 or current % update &lt; 1e-10\n\n    def new_instance(self, **kwargs) -&gt; \"Event\":\n        \"\"\"\n        Creates a new instance of the event with the provided keyword arguments.\n\n        :param kwargs: Keyword arguments to set in the new instance.\n        :return: A new instance of the event with the provided kwargs.\n        :rtype: Event\n        \"\"\"\n        logger.debug(\"Creating new instance of event with kwargs: {}\", kwargs)\n        instance = deepcopy(self)\n        for key, value in kwargs.items():\n            setattr(instance, key, value)\n        return instance\n</code></pre>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.current_index","title":"<code>current_index</code>  <code>property</code> <code>writable</code>","text":"<p>Calculates the current index of the event.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current index of the event, which is either the global step or the epoch with the fraction of the current step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based or if the steps per epoch are too many.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.epoch","title":"<code>epoch</code>  <code>property</code>","text":"<p>Calculates the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.epoch_based","title":"<code>epoch_based</code>  <code>property</code>","text":"<p>Determines if the event is based on epochs.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the event is based on epochs, False otherwise.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.epoch_batch","title":"<code>epoch_batch</code>  <code>property</code>","text":"<p>Calculates the current batch within the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current batch within the current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.epoch_full","title":"<code>epoch_full</code>  <code>property</code>","text":"<p>Calculates the current epoch with the fraction of the current step.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current epoch with the fraction of the current step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.epoch_step","title":"<code>epoch_step</code>  <code>property</code>","text":"<p>Calculates the current step within the current epoch.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current step within the current epoch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the event is not epoch based.</p>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.new_instance","title":"<code>new_instance(**kwargs)</code>","text":"<p>Creates a new instance of the event with the provided keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments to set in the new instance.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Event</code> <p>A new instance of the event with the provided kwargs.</p> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>def new_instance(self, **kwargs) -&gt; \"Event\":\n    \"\"\"\n    Creates a new instance of the event with the provided keyword arguments.\n\n    :param kwargs: Keyword arguments to set in the new instance.\n    :return: A new instance of the event with the provided kwargs.\n    :rtype: Event\n    \"\"\"\n    logger.debug(\"Creating new instance of event with kwargs: {}\", kwargs)\n    instance = deepcopy(self)\n    for key, value in kwargs.items():\n        setattr(instance, key, value)\n    return instance\n</code></pre>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.Event.should_update","title":"<code>should_update(start, end, update)</code>","text":"<p>Determines if the event should trigger an update.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[float]</code> <p>The start index to check against, set to None to ignore start.</p> required <code>end</code> <code>Optional[float]</code> <p>The end index to check against, set to None to ignore end.</p> required <code>update</code> <code>Optional[float]</code> <p>The update interval, set to None or 0.0 to always update, otherwise must be greater than 0.0, defaults to None.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event should trigger an update, False otherwise.</p> Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>def should_update(\n    self, start: Optional[float], end: Optional[float], update: Optional[float]\n) -&gt; bool:\n    \"\"\"\n    Determines if the event should trigger an update.\n\n    :param start: The start index to check against, set to None to ignore start.\n    :type start: Optional[float]\n    :param end: The end index to check against, set to None to ignore end.\n    :type end: Optional[float]\n    :param update: The update interval, set to None or 0.0 to always update,\n        otherwise must be greater than 0.0, defaults to None.\n    :type update: Optional[float]\n    :return: True if the event should trigger an update, False otherwise.\n    :rtype: bool\n    \"\"\"\n    current = self.current_index\n    logger.debug(\n        \"Checking if event should update: \"\n        \"current_index={}, start={}, end={}, update={}\",\n        current,\n        start,\n        end,\n        update,\n    )\n    if start is not None and current &lt; start:\n        return False\n    if end is not None and current &gt; end:\n        return False\n    return update is None or update &lt;= 0.0 or current % update &lt; 1e-10\n</code></pre>"},{"location":"reference/llmcompressor/core/events/event/#llmcompressor.core.events.event.EventType","title":"<code>EventType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An Enum for defining the different types of events that can be triggered during model compression lifecycles. The purpose of each EventType is to trigger the corresponding modifier callback during training or post training pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>INITIALIZE</code> <p>Event type for initialization.</p> required <code>FINALIZE</code> <p>Event type for finalization.</p> required <code>BATCH_START</code> <p>Event type for the start of a batch.</p> required <code>LOSS_CALCULATED</code> <p>Event type for when loss is calculated.</p> required <code>BATCH_END</code> <p>Event type for the end of a batch.</p> required <code>CALIBRATION_EPOCH_START</code> <p>Event type for the start of a calibration epoch.</p> required <code>SEQUENTIAL_EPOCH_END</code> <p>Event type for the end of a layer calibration epoch, specifically used by <code>src/llmcompressor/pipelines/sequential/pipeline.py</code></p> required <code>CALIBRATION_EPOCH_END</code> <p>Event type for the end of a calibration epoch.</p> required <code>OPTIM_PRE_STEP</code> <p>Event type for pre-optimization step.</p> required <code>OPTIM_POST_STEP</code> <p>Event type for post-optimization step.</p> required Source code in <code>src/llmcompressor/core/events/event.py</code> <pre><code>@unique\nclass EventType(Enum):\n    \"\"\"\n    An Enum for defining the different types of events that can be triggered\n    during model compression lifecycles.\n    The purpose of each EventType is to trigger the corresponding\n    modifier callback during training or post training pipelines.\n\n    :param INITIALIZE: Event type for initialization.\n    :param FINALIZE: Event type for finalization.\n    :param BATCH_START: Event type for the start of a batch.\n    :param LOSS_CALCULATED: Event type for when loss is calculated.\n    :param BATCH_END: Event type for the end of a batch.\n    :param CALIBRATION_EPOCH_START: Event type for the start of a calibration epoch.\n    :param SEQUENTIAL_EPOCH_END: Event type for the end of a layer calibration epoch,\n        specifically used by `src/llmcompressor/pipelines/sequential/pipeline.py`\n    :param CALIBRATION_EPOCH_END: Event type for the end of a calibration epoch.\n    :param OPTIM_PRE_STEP: Event type for pre-optimization step.\n    :param OPTIM_POST_STEP: Event type for post-optimization step.\n    \"\"\"\n\n    # training lifecycle\n    INITIALIZE = \"initialize\"\n    FINALIZE = \"finalize\"\n\n    # batch lifecycle\n    BATCH_START = \"batch_start\"\n    LOSS_CALCULATED = \"loss_calculated\"\n    BATCH_END = \"batch_end\"\n\n    # calibration lifecycle\n    CALIBRATION_EPOCH_START = \"calibration_epoch_start\"\n    SEQUENTIAL_EPOCH_END = \"sequential_epoch_end\"\n    CALIBRATION_EPOCH_END = \"calibration_epoch_end\"\n\n    # step lifecycle\n    OPTIM_PRE_STEP = \"optim_pre_step\"\n    OPTIM_POST_STEP = \"optim_post_step\"\n</code></pre>"},{"location":"reference/llmcompressor/datasets/","title":"llmcompressor.datasets","text":""},{"location":"reference/llmcompressor/datasets/utils/","title":"llmcompressor.datasets.utils","text":""},{"location":"reference/llmcompressor/datasets/utils/#llmcompressor.datasets.utils.format_calibration_data","title":"<code>format_calibration_data(tokenized_dataset, num_calibration_samples=None, do_shuffle=True, collate_fn=default_data_collator)</code>","text":"<p>Creates a dataloader out of the calibration dataset split, trimming it to the desired number of calibration samples</p> <p>Parameters:</p> Name Type Description Default <code>tokenized_dataset</code> <code>Dataset</code> <p>dataset to convert to dataloader</p> required <code>num_calibration_samples</code> <code>Optional[int]</code> <p>number of data samples to convert</p> <code>None</code> <code>do_shuffle</code> <code>bool</code> <p>whether to shuffle the dataset before selecting calibration samples, true by default</p> <code>True</code> <code>collate_fn</code> <code>Callable</code> <p>optional custom collate function, or use default</p> <code>default_data_collator</code> <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>list of trimmed calibration data tensors</p> Source code in <code>src/llmcompressor/datasets/utils.py</code> <pre><code>def format_calibration_data(\n    tokenized_dataset: Dataset,\n    num_calibration_samples: Optional[int] = None,\n    do_shuffle: bool = True,\n    collate_fn: Callable = default_data_collator,\n) -&gt; List[torch.Tensor]:\n    \"\"\"\n    Creates a dataloader out of the calibration dataset split, trimming it to\n    the desired number of calibration samples\n    :param tokenized_dataset: dataset to convert to dataloader\n    :param num_calibration_samples: number of data samples to convert\n    :param do_shuffle: whether to shuffle the dataset before selecting calibration\n        samples, true by default\n    :param collate_fn: optional custom collate function, or use default\n    :return: list of trimmed calibration data tensors\n    \"\"\"\n    safe_calibration_samples = len(tokenized_dataset)\n    if num_calibration_samples is not None:\n        safe_calibration_samples = min(len(tokenized_dataset), num_calibration_samples)\n        if safe_calibration_samples != num_calibration_samples:\n            logger.warning(\n                f\"Requested {num_calibration_samples} calibration samples but \"\n                f\"the provided dataset only has {safe_calibration_samples}. \"\n            )\n\n    if do_shuffle:\n        tokenized_dataset = tokenized_dataset.shuffle()\n    tokenized_calibration = tokenized_dataset.select(range(safe_calibration_samples))\n\n    dataloader_params = {\n        \"batch_size\": 1,\n        \"sampler\": RandomSampler(tokenized_calibration)\n        if do_shuffle\n        else SequentialSampler(tokenized_calibration),\n        \"collate_fn\": collate_fn,\n        \"pin_memory\": True,\n    }\n\n    calibration_dataloader = DataLoader(tokenized_calibration, **dataloader_params)\n\n    return calibration_dataloader\n</code></pre>"},{"location":"reference/llmcompressor/datasets/utils/#llmcompressor.datasets.utils.get_calibration_dataloader","title":"<code>get_calibration_dataloader(dataset_args, processor)</code>","text":"<p>Get the dataloader used for oneshot calibration.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>DatasetArguments that contains the dataset parameters.</p> required <code>processor</code> <code>Processor</code> <p>Processor or the tokenizer of the model.</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>PyTorch dataloader object that contains the calibration dataset.</p> Source code in <code>src/llmcompressor/datasets/utils.py</code> <pre><code>def get_calibration_dataloader(\n    dataset_args: DatasetArguments,\n    processor: Processor,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"\n    Get the dataloader used for oneshot calibration.\n    :param dataset_args: DatasetArguments that contains the dataset parameters.\n    :param processor: Processor or the tokenizer of the model.\n    :return: PyTorch dataloader object that contains the calibration dataset.\n    \"\"\"\n    if dataset_args.dataset is None:\n        # weight-only quantization or dynamic quantization\n        return\n\n    datasets = get_processed_dataset(\n        dataset_args=dataset_args,\n        processor=processor,\n        do_oneshot=True,\n        do_train=False,\n    )\n\n    calibration_dataset = datasets.get(\"calibration\")\n\n    return format_calibration_data(\n        tokenized_dataset=calibration_dataset,\n        num_calibration_samples=dataset_args.num_calibration_samples,\n        do_shuffle=dataset_args.shuffle_calibration_samples,\n        collate_fn=dataset_args.data_collator,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/datasets/utils/#llmcompressor.datasets.utils.get_processed_dataset","title":"<code>get_processed_dataset(dataset_args, processor=None, do_oneshot=False, do_train=True)</code>","text":"<p>Loads datasets for each flow based on dataset_args, stores a Dataset for each enabled flow in datasets</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>DatasetArguments that contain dataset loading and processing params</p> required <code>processor</code> <code>Optional[Processor]</code> <p>processor or tokenizer to use for dataset tokenization</p> <code>None</code> <code>do_oneshot</code> <code>bool</code> <p>True for oneshot pathway</p> <code>False</code> <code>do_train</code> <code>bool</code> <p>True for train pathway</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Dataset]]</code> <p>A dataset corresponding to either train or calibration (oneshot)</p> Source code in <code>src/llmcompressor/datasets/utils.py</code> <pre><code>def get_processed_dataset(\n    dataset_args: DatasetArguments,\n    processor: Optional[Processor] = None,\n    do_oneshot: bool = False,\n    do_train: bool = True,\n) -&gt; Optional[Dict[str, Dataset]]:\n    \"\"\"\n    Loads datasets for each flow based on dataset_args, stores a Dataset for each\n    enabled flow in datasets\n    :param dataset_args: DatasetArguments that contain dataset loading and\n        processing params\n    :param processor: processor or tokenizer to use for dataset tokenization\n    :param do_oneshot: True for oneshot pathway\n    :param do_train: True for train pathway\n    :return: A dataset corresponding to either train or calibration (oneshot)\n    \"\"\"\n    if dataset_args.dataset is None:\n        logger.warning(\n            \"Running oneshot without calibration data. This is expected for \"\n            \"weight-only and dynamic quantization\"\n        )\n        return\n\n    splits = dataset_args.splits\n    tokenized_datasets = {}\n\n    def _get_split_name(inp_str):\n        # strip out split name, for ex train[60%:] -&gt; train\n        match = re.match(r\"(\\w*)\\[.*\\]\", inp_str)\n        if match is not None:\n            return match.group(1)\n        return inp_str\n\n    if splits is None:\n        splits = {\"all\": None}\n    elif isinstance(splits, str):\n        splits = {_get_split_name(splits): splits}\n    elif isinstance(splits, List):\n        splits = {_get_split_name(s): s for s in splits}\n\n    # default to custom dataset if dataset provided isn't a string\n    registry_id = (\n        dataset_args.dataset if isinstance(dataset_args.dataset, str) else \"custom\"\n    )\n    for split_name, split_str in splits.items():\n        dataset = dataset_args.dataset\n        if hasattr(dataset, \"column_names\") and \"input_ids\" in dataset.column_names:\n            # dataset is already tokenized\n            tokenized_datasets[split_name] = dataset\n        else:\n            # dataset needs to be tokenized\n            dataset_manager = TextGenerationDataset.load_from_registry(\n                registry_id,\n                dataset_args=dataset_args,\n                split=split_str,\n                processor=processor,\n            )\n            tokenized_datasets[split_name] = dataset_manager(add_labels=do_train)\n\n    return make_dataset_splits(\n        tokenized_datasets,\n        do_oneshot=do_oneshot,\n        do_train=do_train,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/datasets/utils/#llmcompressor.datasets.utils.make_dataset_splits","title":"<code>make_dataset_splits(tokenized_datasets, do_oneshot=True, do_train=False)</code>","text":"<p>Restructures the datasets dictionary based on what tasks will be run train</p> <p>Parameters:</p> Name Type Description Default <code>tokenized_datasets</code> <code>Dict[str, Any]</code> <p>dictionary of processed datasets</p> required <code>do_oneshot</code> <code>bool</code> <p>Whether to store the calibration dataset</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Dataset]</code> <p>A dataset corresponding to either train or calibration (oneshot)</p> Source code in <code>src/llmcompressor/datasets/utils.py</code> <pre><code>def make_dataset_splits(\n    tokenized_datasets: Dict[str, Any],\n    do_oneshot: bool = True,\n    do_train: bool = False,\n) -&gt; Dict[str, Dataset]:\n    \"\"\"\n    Restructures the datasets dictionary based on what tasks will be run\n    train\n    :param tokenized_datasets: dictionary of processed datasets\n    :param do_oneshot: Whether to store the calibration dataset\n    :return: A dataset corresponding to either train or calibration (oneshot)\n    \"\"\"\n\n    # handles case where all splits are contained in a single dataset\n    if \"all\" in tokenized_datasets and len(tokenized_datasets) == 1:\n        tokenized_datasets = tokenized_datasets.get(\"all\")\n        if isinstance(tokenized_datasets, Dataset):\n            tokenized_datasets = {\"train\": tokenized_datasets}\n\n    train_split = calib_split = None\n\n    if do_train:\n        if \"train\" not in tokenized_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_split = tokenized_datasets[\"train\"]\n    if do_oneshot:\n        calib_split = tokenized_datasets.get(\"calibration\")\n        if calib_split is None:\n            if \"train\" not in tokenized_datasets:\n                raise ValueError(\"--do_oneshot requires a calibration dataset\")\n            calib_split = tokenized_datasets[\"train\"]\n\n    split_datasets = {\n        \"train\": train_split,\n        \"calibration\": calib_split,\n    }\n    return split_datasets\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/","title":"llmcompressor.entrypoints","text":""},{"location":"reference/llmcompressor/entrypoints/oneshot/","title":"llmcompressor.entrypoints.oneshot","text":""},{"location":"reference/llmcompressor/entrypoints/oneshot/#llmcompressor.entrypoints.oneshot.Oneshot","title":"<code>Oneshot</code>","text":"<p>Class responsible for carrying out one-shot calibration on a pretrained model.</p> <p>This class handles the entire lifecycle of one-shot calibration, including preprocessing (model and tokenizer/processor initialization), model optimization (quantization or sparsification), and postprocessing (saving outputs). The intructions for model optimization can be specified by using a recipe.</p> <ul> <li> <p>Input Keyword Arguments: <code>kwargs</code> are parsed into:</p> <ul> <li><code>model_args</code>: Arguments for loading and configuring a pretrained model   (e.g., <code>AutoModelForCausalLM</code>).</li> <li><code>dataset_args</code>: Arguments for dataset-related configurations, such as   calibration dataloaders.</li> <li><code>recipe_args</code>: Arguments for defining and configuring recipes that specify   optimization actions.</li> </ul> <p>Parsers are defined in <code>src/llmcompressor/args/</code>.</p> </li> <li> <p>Lifecycle Overview:     The oneshot calibration lifecycle consists of three steps:</p> <ol> <li>Preprocessing:<ul> <li>Instantiates a pretrained model and tokenizer/processor.</li> <li>Ensures input and output embedding layers are untied if they share   tensors.</li> <li>Patches the model to include additional functionality for saving with   quantization configurations.</li> </ul> </li> <li>Oneshot Calibration:<ul> <li>Optimizes the model using a global <code>CompressionSession</code> and applies   recipe-defined modifiers (e.g., <code>GPTQModifier</code>, <code>SparseGPTModifier</code>)</li> </ul> </li> <li>Postprocessing:<ul> <li>Saves the model, tokenizer/processor, and configuration to the specified   <code>output_dir</code>.</li> </ul> </li> </ol> </li> <li> <p>Usage: <pre><code>oneshot = Oneshot(model=model, recipe=recipe, dataset=dataset)\noneshot()\n\n# Access the processed components\nmodel = oneshot.model\nprocessor = oneshot.processor\nrecipe = oneshot.recipe\n</code></pre></p> </li> </ul> <p>Methods:     init(**kwargs):         Initializes the <code>Oneshot</code> object by parsing input arguments, performing         preprocessing, and setting instance attributes.</p> <pre><code>__call__(**kwargs):\n    Performs the one-shot calibration process by preparing a calibration\n    dataloader, applying recipe modifiers to the model, and executing\n    postprocessing steps.\n\nsave():\n    Saves the calibrated model and tokenizer/processor to the specified\n    `output_dir`. Supports saving in compressed formats based on model\n    arguments.\n\napply_recipe_modifiers(calibration_dataloader, **kwargs):\n    Applies lifecycle actions (e.g., `initialize`, `finalize`) using modifiers\n    defined in the recipe. Each action is executed via the global\n    `CompressionSession`.\n</code></pre> Source code in <code>src/llmcompressor/entrypoints/oneshot.py</code> <pre><code>class Oneshot:\n    \"\"\"\n    Class responsible for carrying out one-shot calibration on a pretrained model.\n\n    This class handles the entire lifecycle of one-shot calibration, including\n    preprocessing (model and tokenizer/processor initialization), model optimization\n    (quantization or sparsification), and postprocessing (saving outputs). The\n    intructions for model optimization can be specified by using a recipe.\n\n    - **Input Keyword Arguments:**\n        `kwargs` are parsed into:\n        - `model_args`: Arguments for loading and configuring a pretrained model\n          (e.g., `AutoModelForCausalLM`).\n        - `dataset_args`: Arguments for dataset-related configurations, such as\n          calibration dataloaders.\n        - `recipe_args`: Arguments for defining and configuring recipes that specify\n          optimization actions.\n\n        Parsers are defined in `src/llmcompressor/args/`.\n\n    - **Lifecycle Overview:**\n        The oneshot calibration lifecycle consists of three steps:\n        1. **Preprocessing**:\n            - Instantiates a pretrained model and tokenizer/processor.\n            - Ensures input and output embedding layers are untied if they share\n              tensors.\n            - Patches the model to include additional functionality for saving with\n              quantization configurations.\n        2. **Oneshot Calibration**:\n            - Optimizes the model using a global `CompressionSession` and applies\n              recipe-defined modifiers (e.g., `GPTQModifier`, `SparseGPTModifier`)\n        3. **Postprocessing**:\n            - Saves the model, tokenizer/processor, and configuration to the specified\n              `output_dir`.\n\n    - **Usage:**\n        ```python\n        oneshot = Oneshot(model=model, recipe=recipe, dataset=dataset)\n        oneshot()\n\n        # Access the processed components\n        model = oneshot.model\n        processor = oneshot.processor\n        recipe = oneshot.recipe\n        ```\n\n    Methods:\n        __init__(**kwargs):\n            Initializes the `Oneshot` object by parsing input arguments, performing\n            preprocessing, and setting instance attributes.\n\n        __call__(**kwargs):\n            Performs the one-shot calibration process by preparing a calibration\n            dataloader, applying recipe modifiers to the model, and executing\n            postprocessing steps.\n\n        save():\n            Saves the calibrated model and tokenizer/processor to the specified\n            `output_dir`. Supports saving in compressed formats based on model\n            arguments.\n\n        apply_recipe_modifiers(calibration_dataloader, **kwargs):\n            Applies lifecycle actions (e.g., `initialize`, `finalize`) using modifiers\n            defined in the recipe. Each action is executed via the global\n            `CompressionSession`.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the `Oneshot` class with provided arguments.\n\n        Parses the input keyword arguments into `model_args`, `dataset_args`, and\n        `recipe_args`. Performs preprocessing to initialize the model and\n        tokenizer/processor.\n\n        :param model_args: ModelArguments parameters, responsible for controlling\n            model loading and saving logic\n        :param dataset_args: DatasetArguments parameters, responsible for controlling\n            dataset loading, preprocessing and dataloader loading\n        :param recipe_args: RecipeArguments parameters, responsible for containing\n            recipe-related parameters\n        :param output_dir: Path to save the output model after carrying out oneshot\n\n        \"\"\"\n        model_args, dataset_args, recipe_args, _, output_dir = parse_args(**kwargs)\n\n        self.model_args = model_args\n        self.dataset_args = dataset_args\n        self.recipe_args = recipe_args\n        self.output_dir = output_dir\n\n        # initialize the model and processor\n        pre_process(model_args)\n\n        # Set instance attributes\n        self.model = self.model_args.model\n        self.processor = self.model_args.processor\n        self.recipe = self.recipe_args.recipe\n\n    def __call__(self):\n        \"\"\"\n        Performs one-shot calibration.\n\n        This method prepares a calibration dataloader using dataset arguments and\n        applies recipe-based modifiers to optimize the model. The lifecycle actions\n        are executed sequentially, and the modified model is saved during\n        postprocessing.\n\n        \"\"\"\n\n        calibration_dataloader = get_calibration_dataloader(\n            self.dataset_args, self.processor\n        )\n        self.apply_recipe_modifiers(\n            calibration_dataloader=calibration_dataloader,\n            recipe_stage=self.recipe_args.stage,\n        )\n        post_process(\n            model_args=self.model_args,\n            recipe_args=self.recipe_args,\n            output_dir=self.output_dir,\n        )\n\n    def apply_recipe_modifiers(\n        self,\n        calibration_dataloader: Optional[DataLoader],\n        recipe_stage: Optional[str] = None,\n    ):\n        \"\"\"\n        Applies recipe modifiers to the model during the lifecycle.\n\n        The modifiers are defined in the recipe and executed via lifecycle actions\n        (`initialize`, `finalize`) through the global `CompressionSession`.\n\n\n        :param: calibration_dataloader: Dataloader for calibration data.\n\n        Raises:\n            RuntimeError: If any modifier fails during execution.\n        \"\"\"\n\n        session = active_session()\n        session.reset()\n\n        # (Helen INFERENG-661): validate recipe modifiers before intialization\n        session.initialize(\n            model=self.model,\n            start=-1,\n            recipe=self.recipe,\n            recipe_stage=recipe_stage,\n            recipe_args=self.recipe_args.recipe_args,\n            calib_data=calibration_dataloader,  # only used by AWQModifier, remove once\n            # AWQModifier supports calibration pipelines\n        )\n\n        user_pipeline = self.dataset_args.pipeline\n        modifiers = session.get_modifiers()\n        pipeline = CalibrationPipeline.from_modifiers(modifiers, user=user_pipeline)\n        pipeline(self.model, calibration_dataloader, self.dataset_args)\n\n        session.finalize()\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/oneshot/#llmcompressor.entrypoints.oneshot.Oneshot.__call__","title":"<code>__call__()</code>","text":"<p>Performs one-shot calibration.</p> <p>This method prepares a calibration dataloader using dataset arguments and applies recipe-based modifiers to optimize the model. The lifecycle actions are executed sequentially, and the modified model is saved during postprocessing.</p> Source code in <code>src/llmcompressor/entrypoints/oneshot.py</code> <pre><code>def __call__(self):\n    \"\"\"\n    Performs one-shot calibration.\n\n    This method prepares a calibration dataloader using dataset arguments and\n    applies recipe-based modifiers to optimize the model. The lifecycle actions\n    are executed sequentially, and the modified model is saved during\n    postprocessing.\n\n    \"\"\"\n\n    calibration_dataloader = get_calibration_dataloader(\n        self.dataset_args, self.processor\n    )\n    self.apply_recipe_modifiers(\n        calibration_dataloader=calibration_dataloader,\n        recipe_stage=self.recipe_args.stage,\n    )\n    post_process(\n        model_args=self.model_args,\n        recipe_args=self.recipe_args,\n        output_dir=self.output_dir,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/oneshot/#llmcompressor.entrypoints.oneshot.Oneshot.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the <code>Oneshot</code> class with provided arguments.</p> <p>Parses the input keyword arguments into <code>model_args</code>, <code>dataset_args</code>, and <code>recipe_args</code>. Performs preprocessing to initialize the model and tokenizer/processor.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <p>ModelArguments parameters, responsible for controlling model loading and saving logic</p> required <code>dataset_args</code> <p>DatasetArguments parameters, responsible for controlling dataset loading, preprocessing and dataloader loading</p> required <code>recipe_args</code> <p>RecipeArguments parameters, responsible for containing recipe-related parameters</p> required <code>output_dir</code> <p>Path to save the output model after carrying out oneshot</p> required Source code in <code>src/llmcompressor/entrypoints/oneshot.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the `Oneshot` class with provided arguments.\n\n    Parses the input keyword arguments into `model_args`, `dataset_args`, and\n    `recipe_args`. Performs preprocessing to initialize the model and\n    tokenizer/processor.\n\n    :param model_args: ModelArguments parameters, responsible for controlling\n        model loading and saving logic\n    :param dataset_args: DatasetArguments parameters, responsible for controlling\n        dataset loading, preprocessing and dataloader loading\n    :param recipe_args: RecipeArguments parameters, responsible for containing\n        recipe-related parameters\n    :param output_dir: Path to save the output model after carrying out oneshot\n\n    \"\"\"\n    model_args, dataset_args, recipe_args, _, output_dir = parse_args(**kwargs)\n\n    self.model_args = model_args\n    self.dataset_args = dataset_args\n    self.recipe_args = recipe_args\n    self.output_dir = output_dir\n\n    # initialize the model and processor\n    pre_process(model_args)\n\n    # Set instance attributes\n    self.model = self.model_args.model\n    self.processor = self.model_args.processor\n    self.recipe = self.recipe_args.recipe\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/oneshot/#llmcompressor.entrypoints.oneshot.Oneshot.apply_recipe_modifiers","title":"<code>apply_recipe_modifiers(calibration_dataloader, recipe_stage=None)</code>","text":"<p>Applies recipe modifiers to the model during the lifecycle.</p> <p>The modifiers are defined in the recipe and executed via lifecycle actions (<code>initialize</code>, <code>finalize</code>) through the global <code>CompressionSession</code>.</p> Source code in <code>src/llmcompressor/entrypoints/oneshot.py</code> <pre><code>def apply_recipe_modifiers(\n    self,\n    calibration_dataloader: Optional[DataLoader],\n    recipe_stage: Optional[str] = None,\n):\n    \"\"\"\n    Applies recipe modifiers to the model during the lifecycle.\n\n    The modifiers are defined in the recipe and executed via lifecycle actions\n    (`initialize`, `finalize`) through the global `CompressionSession`.\n\n\n    :param: calibration_dataloader: Dataloader for calibration data.\n\n    Raises:\n        RuntimeError: If any modifier fails during execution.\n    \"\"\"\n\n    session = active_session()\n    session.reset()\n\n    # (Helen INFERENG-661): validate recipe modifiers before intialization\n    session.initialize(\n        model=self.model,\n        start=-1,\n        recipe=self.recipe,\n        recipe_stage=recipe_stage,\n        recipe_args=self.recipe_args.recipe_args,\n        calib_data=calibration_dataloader,  # only used by AWQModifier, remove once\n        # AWQModifier supports calibration pipelines\n    )\n\n    user_pipeline = self.dataset_args.pipeline\n    modifiers = session.get_modifiers()\n    pipeline = CalibrationPipeline.from_modifiers(modifiers, user=user_pipeline)\n    pipeline(self.model, calibration_dataloader, self.dataset_args)\n\n    session.finalize()\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/train/","title":"llmcompressor.entrypoints.train","text":""},{"location":"reference/llmcompressor/entrypoints/train/#llmcompressor.entrypoints.train.train","title":"<code>train(**kwargs)</code>","text":"<p>Fine-tuning entrypoint that supports vanilla fine-tuning and knowledge distillation for compressed model using <code>oneshot</code>.</p> <p>This entrypoint is responsible the entire fine-tuning lifecycle, including preprocessing (model and tokenizer/processor initialization), fine-tuning, and postprocessing (saving outputs). The intructions for fine-tuning compressed model can be specified by using a recipe.</p> <ul> <li> <p>Input Keyword Arguments: <code>kwargs</code> are parsed into:</p> <ul> <li><code>model_args</code>: Arguments for loading and configuring a pretrained model   (e.g., <code>AutoModelForCausalLM</code>).</li> <li><code>dataset_args</code>: Arguments for dataset-related configurations, such as   calibration dataloaders.</li> <li><code>recipe_args</code>: Arguments for defining and configuring recipes that specify   optimization actions.</li> <li><code>training_args</code>: rguments for defining and configuring training parameters</li> </ul> <p>Parsers are defined in <code>src/llmcompressor/args/</code>.</p> </li> <li> <p>Lifecycle Overview:     The fine-tuning lifecycle consists of three steps:</p> <ol> <li>Preprocessing:<ul> <li>Instantiates a pretrained model and tokenizer/processor.</li> <li>Ensures input and output embedding layers are untied if they share   tensors.</li> <li>Patches the model to include additional functionality for saving with   quantization configurations.</li> </ul> </li> <li>Training:<ul> <li>Finetunes the model using a global <code>CompressionSession</code> and applies   recipe-defined modifiers (e.g., <code>ConstantPruningModifier</code>,     <code>OutputDistillationModifier</code>)</li> </ul> </li> <li>Postprocessing:<ul> <li>Saves the model, tokenizer/processor, and configuration to the specified   <code>output_dir</code>.</li> </ul> </li> </ol> </li> <li> <p>Usage: <pre><code>train(model=model, recipe=recipe, dataset=dataset)\n</code></pre></p> </li> </ul> Source code in <code>src/llmcompressor/entrypoints/train.py</code> <pre><code>def train(**kwargs) -&gt; PreTrainedModel:\n    \"\"\"\n    Fine-tuning entrypoint that supports vanilla fine-tuning and\n    knowledge distillation for compressed model using `oneshot`.\n\n\n    This entrypoint is responsible the entire fine-tuning lifecycle, including\n    preprocessing (model and tokenizer/processor initialization), fine-tuning,\n    and postprocessing (saving outputs). The intructions for fine-tuning compressed\n    model can be specified by using a recipe.\n\n    - **Input Keyword Arguments:**\n        `kwargs` are parsed into:\n        - `model_args`: Arguments for loading and configuring a pretrained model\n          (e.g., `AutoModelForCausalLM`).\n        - `dataset_args`: Arguments for dataset-related configurations, such as\n          calibration dataloaders.\n        - `recipe_args`: Arguments for defining and configuring recipes that specify\n          optimization actions.\n        - `training_args`: rguments for defining and configuring training parameters\n\n        Parsers are defined in `src/llmcompressor/args/`.\n\n    - **Lifecycle Overview:**\n        The fine-tuning lifecycle consists of three steps:\n        1. **Preprocessing**:\n            - Instantiates a pretrained model and tokenizer/processor.\n            - Ensures input and output embedding layers are untied if they share\n              tensors.\n            - Patches the model to include additional functionality for saving with\n              quantization configurations.\n        2. **Training**:\n            - Finetunes the model using a global `CompressionSession` and applies\n              recipe-defined modifiers (e.g., `ConstantPruningModifier`,\n                `OutputDistillationModifier`)\n        3. **Postprocessing**:\n            - Saves the model, tokenizer/processor, and configuration to the specified\n              `output_dir`.\n\n    - **Usage:**\n        ```python\n        train(model=model, recipe=recipe, dataset=dataset)\n\n        ```\n\n    \"\"\"\n    model_args, dataset_args, recipe_args, training_args, _ = parse_args(\n        include_training_args=True, **kwargs\n    )\n\n    pre_process(model_args)\n\n    processed_dataset = get_processed_dataset(\n        dataset_args=dataset_args,\n        processor=model_args.processor,\n    )\n    training_dataset = processed_dataset.get(\"train\")\n\n    # create output dir for stages\n    original_output_dir = output_dir = training_args.output_dir\n    if all([output_dir, recipe_args, getattr(recipe_args, \"stage\", None)]):\n        output_dir = os.path.join(original_output_dir, recipe_args.stage)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        # update output dir in training args\n        logger.info(\n            f\"Stage detected for training. Updating output dir to: {output_dir}\"\n        )\n        training_args.output_dir = output_dir\n\n    trainer = Trainer(\n        model=model_args.model,\n        teacher=model_args.distill_teacher,\n        recipe=recipe_args.recipe,\n        recipe_args=recipe_args.recipe_args,\n        args=training_args,\n        model_args=model_args,\n        dataset_args=dataset_args,\n        train_dataset=training_dataset,\n        processing_class=model_args.processor,\n        data_collator=dataset_args.data_collator,\n    )\n\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n\n    logger.info(\"*** Train ***\")\n\n    session = active_session()\n    session.reset()\n    train_result = trainer.train(\n        resume_from_checkpoint=checkpoint,\n        stage=recipe_args.stage,\n    )\n\n    # return output\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(training_dataset)\n    metrics[\"perplexity\"] = math.exp(metrics[\"train_loss\"])\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n\n    # this includes saving the state, optimizer and scheduler\n    # TODO: support all save args, not just skip_sparsity_compression_stats\n    trainer.save_model(\n        output_dir=training_args.output_dir, skip_sparsity_compression_stats=False\n    )\n\n    post_process(recipe_args=recipe_args)\n    training_args.output_dir = original_output_dir\n\n    return model_args.model\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/utils/","title":"llmcompressor.entrypoints.utils","text":""},{"location":"reference/llmcompressor/entrypoints/utils/#llmcompressor.entrypoints.utils.get_processor_name_from_model","title":"<code>get_processor_name_from_model(student, teacher)</code>","text":"<p>Get a processor/tokenizer source used for both student and teacher, assuming that they could be shared</p> <p>Parameters:</p> Name Type Description Default <code>student</code> <code>Module</code> <p>the student model</p> required <code>teacher</code> <code>Optional[Module]</code> <p>the teacher model</p> required <p>Returns:</p> Type Description <code>str</code> <p>the source for the processor/tokenizer shared between teacher and model</p> Source code in <code>src/llmcompressor/entrypoints/utils.py</code> <pre><code>def get_processor_name_from_model(student: Module, teacher: Optional[Module]) -&gt; str:\n    \"\"\"\n    Get a processor/tokenizer source used for both student and teacher, assuming\n    that they could be shared\n\n    :param student: the student model\n    :param teacher: the teacher model\n    :return: the source for the processor/tokenizer shared between teacher and model\n    \"\"\"\n    if teacher is not None and teacher not in (\"disable\", \"self\"):\n        student_forward_params = list(\n            inspect.signature(student.forward).parameters.keys()\n        )\n        teacher_forward_params = list(\n            inspect.signature(teacher.forward).parameters.keys()\n        )\n        diff = [p for p in student_forward_params if p not in teacher_forward_params]\n        if diff:\n            raise RuntimeError(\n                \"Teacher tokenizer cannot be used for student \"\n                f\"due to missing args: {diff}\"\n            )\n        src_model = teacher\n    else:\n        src_model = student\n    return src_model.config._name_or_path\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/utils/#llmcompressor.entrypoints.utils.post_process","title":"<code>post_process(model_args=None, recipe_args=None, output_dir=None)</code>","text":"<p>Saves the model and tokenizer/processor to the output directory if model_args, output_dir is provided.</p> <p>Save is skipped for stage runs for <code>train</code> - saves using the trainer.save_model()</p> <p>If the <code>output_dir</code> is not the default directory, the method resets lifecycle actions. The model is saved in a compressed format if specified in <code>model_args</code>. Additionally, the tokenizer or processor, if available, is also saved.</p> <p>Raises:     ValueError: If saving fails due to an invalid <code>output_dir</code> or other issues.</p> Source code in <code>src/llmcompressor/entrypoints/utils.py</code> <pre><code>def post_process(\n    model_args: Optional[\"ModelArguments\"] = None,\n    recipe_args: Optional[\"RecipeArguments\"] = None,\n    output_dir: Optional[str] = None,\n):\n    \"\"\"\n    Saves the model and tokenizer/processor to the output directory if model_args,\n    output_dir is provided.\n\n    Save is skipped for stage runs for `train` - saves using the trainer.save_model()\n\n    If the `output_dir` is not the default directory, the method resets lifecycle\n    actions. The model is saved in a compressed format if specified in `model_args`.\n    Additionally, the tokenizer or processor, if available, is also saved.\n\n    Raises:\n        ValueError: If saving fails due to an invalid `output_dir` or other issues.\n    \"\"\"\n    if model_args is not None and output_dir is not None:\n        if recipe_args is not None and getattr(recipe_args, \"stage\", None) is not None:\n            output_dir = os.path.join(output_dir, recipe_args.stage)\n            os.makedirs(output_dir, exist_ok=True)\n            logger.info(f\"[Save] Stage detected. Updating output_dir to {output_dir}\")\n\n        # TODO: support general saving parameters, beyond save_compressed\n        model_args.model.save_pretrained(\n            output_dir, save_compressed=model_args.save_compressed\n        )\n\n        if model_args.processor is not None:\n            model_args.processor.save_pretrained(output_dir)\n\n    else:\n        logger.warning(\n            \"Optimized model is not saved. To save, please provide\"\n            \"`output_dir` as input arg.\"\n            \"Ex. `oneshot(..., output_dir=...)`\"\n        )\n\n    # Reset the one-time-use session upon completion\n    if recipe_args is not None and recipe_args.clear_sparse_session:\n        reset_session()\n</code></pre>"},{"location":"reference/llmcompressor/entrypoints/utils/#llmcompressor.entrypoints.utils.pre_process","title":"<code>pre_process(model_args)</code>","text":"<p>Prepares the model and tokenizer/processor for calibration. - Initializes the model if it's specified as a path or string. - Applies patches to fix tied tensor issues and modifies <code>save_pretrained</code>     behavior. - Initializes the processor if specified as a path or <code>None</code>. - Sets the minimum tokens per module if <code>dataset_args</code> are provided. Raises:     FileNotFoundError: If the model or processor path is invalid.</p> Source code in <code>src/llmcompressor/entrypoints/utils.py</code> <pre><code>def pre_process(model_args: \"ModelArguments\"):\n    \"\"\"\n    Prepares the model and tokenizer/processor for calibration.\n    - Initializes the model if it's specified as a path or string.\n    - Applies patches to fix tied tensor issues and modifies `save_pretrained`\n        behavior.\n    - Initializes the processor if specified as a path or `None`.\n    - Sets the minimum tokens per module if `dataset_args` are provided.\n    Raises:\n        FileNotFoundError: If the model or processor path is invalid.\n    \"\"\"\n    _warn_tied_embeddings(model_args.tie_word_embeddings)\n\n    # Initialize model\n    if isinstance(model_args.model, (str, PosixPath)):\n        model, distill_teacher = initialize_model_from_path(model_args)\n        if is_fsdp_model(model):\n            raise NotImplementedError(\n                \"FSDP models are not supported in the current release but will be \"\n                \"suported in future releases of LLM Compressor.\"\n            )\n        model_args.model = model\n        model_args.distill_teacher = distill_teacher\n\n    # Initialize processor\n    if isinstance(model_args.processor, (str, type(None))):\n        model_args.processor = initialize_processor_from_path(\n            model_args, model_args.model\n        )\n\n    # untie tie_word_embeddings weights\n    patch_tied_tensors_bug(model_args.model)\n\n    # wrap model.save_pretrained\n    modify_save_pretrained(model_args.model)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/","title":"llmcompressor.metrics","text":""},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger","title":"<code>BaseLogger</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class that all modifier loggers must implement.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name given to the metrics, used for identification</p> required <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class BaseLogger(ABC):\n    \"\"\"\n    Base class that all modifier loggers must implement.\n\n    :param name: name given to the metrics, used for identification\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(self, name: str, enabled: bool = True):\n        self._name = name\n        self._enabled = enabled\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        :return: name given to the metrics, used for identification\n        \"\"\"\n        return self._name\n\n    @property\n    def enabled(self) -&gt; bool:\n        \"\"\"\n        :return: True to log, False otherwise\n        \"\"\"\n        return self._enabled\n\n    @enabled.setter\n    def enabled(self, value: bool):\n        \"\"\"\n        :param value: True to log, False otherwise\n        \"\"\"\n        self._enabled = value\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(name={self._name}, enabled={self._enabled})\"\n\n    def log_hyperparams(self, params: Dict[str, float]) -&gt; bool:\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def log_string(\n        self,\n        tag: str,\n        string: str,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def save(\n        self,\n        file_path: str,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param file_path: path to a file to be saved\n        :param kwargs: additional arguments that a specific metrics might use\n        :return: True if saved, False otherwise\n        \"\"\"\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.enabled","title":"<code>enabled</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True to log, False otherwise</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>name given to the metrics, used for identification</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.log_hyperparams","title":"<code>log_hyperparams(params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, float]</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(self, params: Dict[str, float]) -&gt; bool:\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.log_string","title":"<code>log_string(tag, string, step=None, wall_time=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: str,\n    string: str,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.BaseLogger.save","title":"<code>save(file_path, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a file to be saved</p> required <code>kwargs</code> <p>additional arguments that a specific metrics might use</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if saved, False otherwise</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def save(\n    self,\n    file_path: str,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param file_path: path to a file to be saved\n    :param kwargs: additional arguments that a specific metrics might use\n    :return: True if saved, False otherwise\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LambdaLogger","title":"<code>LambdaLogger</code>","text":"<p>               Bases: <code>BaseLogger</code></p> <p>Logger that handles calling back to a lambda function with any logs.</p> <p>Parameters:</p> Name Type Description Default <code>lambda_func</code> <code>Callable[[Optional[str], Optional[Union[float, str]], Optional[Dict[str, float]], Optional[int], Optional[float], Optional[int]], bool]</code> <p>the lambda function to call back into with any logs. The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool The return type is True if logged and False otherwise.</p> required <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to lambda</p> <code>'lambda'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class LambdaLogger(BaseLogger):\n    \"\"\"\n    Logger that handles calling back to a lambda function with any logs.\n\n    :param lambda_func: the lambda function to call back into with any logs.\n        The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool\n        The return type is True if logged and False otherwise.\n    :param name: name given to the metrics, used for identification;\n        defaults to lambda\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        lambda_func: Callable[\n            [\n                Optional[str],\n                Optional[Union[float, str]],\n                Optional[Dict[str, float]],\n                Optional[int],\n                Optional[float],\n                Optional[int],\n            ],\n            bool,\n        ],\n        name: str = \"lambda\",\n        enabled: bool = True,\n    ):\n        super().__init__(name, enabled)\n        self._lambda_func = lambda_func\n        assert lambda_func, \"lambda_func must be set to a callable function\"\n\n    @property\n    def lambda_func(\n        self,\n    ) -&gt; Callable[\n        [\n            Optional[str],\n            Optional[Union[float, str]],\n            Optional[Dict[str, float]],\n            Optional[int],\n            Optional[float],\n            Optional[int],\n        ],\n        bool,\n    ]:\n        \"\"\"\n        :return: the lambda function to call back into with any logs.\n            The expected call sequence is (tag, value, values, step, wall_time)\n        \"\"\"\n        return self._lambda_func\n\n    def log_hyperparams(\n        self,\n        params: Dict,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not self.enabled:\n            return False\n\n        return self._lambda_func(\n            tag=None,\n            value=None,\n            values=params,\n            step=None,\n            wall_time=None,\n            level=level,\n        )\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken,\n            defaults to time.time()\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not wall_time:\n            wall_time = time.time()\n\n        return self._lambda_func(\n            tag=tag,\n            value=value,\n            values=None,\n            step=step,\n            wall_time=wall_time,\n            level=level,\n        )\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not wall_time:\n            wall_time = time.time()\n\n        return self._lambda_func(\n            tag=tag,\n            value=None,\n            values=values,\n            step=step,\n            wall_time=wall_time,\n            level=level,\n        )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LambdaLogger.lambda_func","title":"<code>lambda_func</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Callable[[Optional[str], Optional[Union[float, str]], Optional[Dict[str, float]], Optional[int], Optional[float], Optional[int]], bool]</code> <p>the lambda function to call back into with any logs. The expected call sequence is (tag, value, values, step, wall_time)</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LambdaLogger.log_hyperparams","title":"<code>log_hyperparams(params, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(\n    self,\n    params: Dict,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not self.enabled:\n        return False\n\n    return self._lambda_func(\n        tag=None,\n        value=None,\n        values=params,\n        step=None,\n        wall_time=None,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LambdaLogger.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken, defaults to time.time()</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken,\n        defaults to time.time()\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not wall_time:\n        wall_time = time.time()\n\n    return self._lambda_func(\n        tag=tag,\n        value=value,\n        values=None,\n        step=step,\n        wall_time=wall_time,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LambdaLogger.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken, defaults to time.time()</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken,\n        defaults to time.time()\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not wall_time:\n        wall_time = time.time()\n\n    return self._lambda_func(\n        tag=tag,\n        value=None,\n        values=values,\n        step=step,\n        wall_time=wall_time,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager","title":"<code>LoggerManager</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Wrapper around loggers that handles log scheduling and handing off logs to intended loggers.</p> <p>Parameters:</p> Name Type Description Default <code>loggers</code> <code>Optional[List[BaseLogger]]</code> <p>list of loggers assigned to this manager</p> <code>None</code> <code>log_frequency</code> <code>Union[float, None]</code> <p>number of stes or fraction of steps to wait between logs</p> <code>0.1</code> <code>mode</code> <code>LoggingModeType</code> <p>The logging mode to use, either \"on_change\" or \"exact\", \"on_change\" will log when the model has been updated since the last log, \"exact\" will log at the given frequency regardless of model updates. Defaults to \"exact\"</p> <code>'exact'</code> <code>frequency_type</code> <code>FrequencyType</code> <p>The frequency type to use, either \"epoch\", \"step\", or \"batch\" controls what the frequency manager is tracking, e.g. if the frequency type is \"epoch\", then the frequency manager will track the number of epochs that have passed since the last log, if the frequency type is \"step\", then the frequency manager will track the number of optimizer steps</p> <code>'epoch'</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class LoggerManager(ABC):\n    \"\"\"\n    Wrapper around loggers that handles log scheduling and handing off logs to intended\n    loggers.\n\n    :param loggers: list of loggers assigned to this manager\n    :param log_frequency: number of stes or fraction of steps to wait between logs\n    :param mode: The logging mode to use, either \"on_change\" or \"exact\",\n        \"on_change\" will log when the model has been updated since the last log,\n        \"exact\" will log at the given frequency regardless of model updates.\n        Defaults to \"exact\"\n    :param frequency_type: The frequency type to use, either \"epoch\", \"step\", or \"batch\"\n        controls what the frequency manager is tracking, e.g. if the frequency type\n        is \"epoch\", then the frequency manager will track the number of epochs that\n        have passed since the last log, if the frequency type is \"step\", then the\n        frequency manager will track the number of optimizer steps\n    \"\"\"\n\n    def __init__(\n        self,\n        loggers: Optional[List[BaseLogger]] = None,\n        log_frequency: Union[float, None] = 0.1,\n        log_python: bool = True,\n        name: str = \"manager\",\n        mode: LoggingModeType = \"exact\",\n        frequency_type: FrequencyType = \"epoch\",\n    ):\n        self._name = name\n        self._loggers = (\n            loggers\n            or SparsificationGroupLogger(\n                python=log_python,\n                name=name,\n                tensorboard=False,\n                wandb_=False,\n            ).loggers\n        )\n\n        self.frequency_manager = FrequencyManager(\n            mode=mode,\n            frequency_type=frequency_type,\n            log_frequency=log_frequency,\n        )\n\n        self.system = SystemLoggingWraper(\n            loggers=self._loggers, frequency_manager=self.frequency_manager\n        )\n        self.metric = MetricLoggingWrapper(\n            loggers=self._loggers, frequency_manager=self.frequency_manager\n        )\n\n    def __len__(self):\n        return len(self.loggers)\n\n    def __iter__(self):\n        return iter(self.loggers)\n\n    def add_logger(self, logger: BaseLogger):\n        \"\"\"\n        add a BaseLogger implementation to the loggers of this manager\n\n        :param logger: metrics object to add\n        \"\"\"\n        if not isinstance(logger, BaseLogger):\n            raise ValueError(f\"metrics {type(logger)} must be of type BaseLogger\")\n        self._loggers.append(logger)\n\n    def log_ready(\n        self, current_log_step, last_log_step=None, check_model_update: bool = False\n    ):\n        \"\"\"\n        Check if there is a metrics that is ready to accept a log\n\n        :param current_log_step: current step log is requested at\n        :param last_log_step: last time a log was recorder for this object. (Deprecated)\n        :param check_model_update: if True, will check if the model has been updated,\n            if False, will only check the log frequency\n        :return: True if a metrics is ready to accept a log.\n        \"\"\"\n        log_enabled = any(logger.enabled for logger in self.loggers)\n        if last_log_step is not None:\n            self.frequency_manager.log_written(step=last_log_step)\n\n        return log_enabled and self.frequency_manager.log_ready(\n            current_log_step=current_log_step,\n            check_model_update=check_model_update,\n        )\n\n    def log_written(self, step: LogStepType):\n        \"\"\"\n        Update the frequency manager with the last log step written\n\n        :param step: step that was last logged\n        \"\"\"\n        self.frequency_manager.log_written(step=step)\n\n    def model_updated(self, step: LogStepType):\n        \"\"\"\n        Update the frequency manager with the last model update step\n\n        :param step: step that was last logged\n        \"\"\"\n        self.frequency_manager.model_updated(step=step)\n\n    @staticmethod\n    def epoch_to_step(epoch, steps_per_epoch):\n        return round(epoch) if steps_per_epoch &lt;= 0 else round(epoch * steps_per_epoch)\n\n    @property\n    def loggers(self) -&gt; List[BaseLogger]:\n        \"\"\"\n        :return: list of loggers assigned to this manager\n        \"\"\"\n        return self._loggers\n\n    @loggers.setter\n    def loggers(self, value: List[BaseLogger]):\n        \"\"\"\n        :param value: list of loggers assigned to this manager\n        \"\"\"\n        self._loggers = value\n\n    @property\n    def log_frequency(self) -&gt; Union[str, float, None]:\n        \"\"\"\n        :return: number of epochs or fraction of epochs to wait between logs\n        \"\"\"\n        return self.frequency_manager._log_frequency\n\n    @log_frequency.setter\n    def log_frequency(self, value: Union[str, float, None]):\n        \"\"\"\n        :param value: number of epochs or fraction of epochs to wait between logs\n        \"\"\"\n        self.frequency_manager._log_frequency = value\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        :return: name given to the metrics, used for identification\n        \"\"\"\n        return self._name\n\n    @property\n    def wandb(self) -&gt; Optional[ModuleType]:\n        \"\"\"\n        :return: wandb module if initialized\n        \"\"\"\n        for log in self.loggers:\n            if isinstance(log, WANDBLogger) and log.enabled:\n                return log.wandb\n        return None\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().metric.log_scalar instead)\n\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n\n        self.metric.log_scalar(\n            tag=tag,\n            value=value,\n            step=step,\n            wall_time=wall_time,\n            log_types=log_types,\n            level=level,\n        )\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().metric.log_scalars instead)\n\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n\n        self.metric.log_scalars(\n            tag=tag,\n            values=values,\n            step=step,\n            wall_time=wall_time,\n            log_types=log_types,\n            level=level,\n        )\n\n    def log_hyperparams(\n        self,\n        params: Dict,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().metric.log_hyperparams instead)\n\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        \"\"\"\n\n        self.metric.log_hyperparams(\n            params=params,\n            log_types=log_types,\n            level=level,\n        )\n\n    def log_string(\n        self,\n        tag: str,\n        string: str,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().system.log_string instead)\n\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        self.system.log_string(\n            tag=tag,\n            string=string,\n            step=step,\n            wall_time=wall_time,\n            log_types=log_types,\n            level=level,\n        )\n\n    def save(\n        self,\n        file_path: str,\n        **kwargs,\n    ):\n        \"\"\"\n        :param file_path: path to a file to be saved\n        :param kwargs: additional arguments that a specific metrics might use\n        \"\"\"\n        for log in self._loggers:\n            if log.enabled:\n                log.save(file_path, **kwargs)\n\n    @contextmanager\n    def time(self, tag: Optional[str] = None, *args, **kwargs):\n        \"\"\"\n        Context manager to log the time it takes to run the block of code\n\n        Usage:\n        &gt;&gt;&gt; with LoggerManager().time(\"my_block\"):\n        &gt;&gt;&gt;    time.sleep(1)\n\n        :param tag: identifying tag to log the values with\n        \"\"\"\n\n        start = time.time()\n        yield\n        elapsed = time.time() - start\n        if not tag:\n            tag = f\"{DEFAULT_TAG}_time_secs\"\n        self.log_scalar(tag=tag, value=float(f\"{elapsed:.3f}\"), *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_frequency","title":"<code>log_frequency</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>Union[str, float, None]</code> <p>number of epochs or fraction of epochs to wait between logs</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.loggers","title":"<code>loggers</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>List[BaseLogger]</code> <p>list of loggers assigned to this manager</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>name given to the metrics, used for identification</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.wandb","title":"<code>wandb</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[ModuleType]</code> <p>wandb module if initialized</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.add_logger","title":"<code>add_logger(logger)</code>","text":"<p>add a BaseLogger implementation to the loggers of this manager</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>BaseLogger</code> <p>metrics object to add</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def add_logger(self, logger: BaseLogger):\n    \"\"\"\n    add a BaseLogger implementation to the loggers of this manager\n\n    :param logger: metrics object to add\n    \"\"\"\n    if not isinstance(logger, BaseLogger):\n        raise ValueError(f\"metrics {type(logger)} must be of type BaseLogger\")\n    self._loggers.append(logger)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_hyperparams","title":"<code>log_hyperparams(params, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().metric.log_hyperparams instead)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(\n    self,\n    params: Dict,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().metric.log_hyperparams instead)\n\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    \"\"\"\n\n    self.metric.log_hyperparams(\n        params=params,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_ready","title":"<code>log_ready(current_log_step, last_log_step=None, check_model_update=False)</code>","text":"<p>Check if there is a metrics that is ready to accept a log</p> <p>Parameters:</p> Name Type Description Default <code>current_log_step</code> <p>current step log is requested at</p> required <code>last_log_step</code> <p>last time a log was recorder for this object. (Deprecated)</p> <code>None</code> <code>check_model_update</code> <code>bool</code> <p>if True, will check if the model has been updated, if False, will only check the log frequency</p> <code>False</code> <p>Returns:</p> Type Description <p>True if a metrics is ready to accept a log.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_ready(\n    self, current_log_step, last_log_step=None, check_model_update: bool = False\n):\n    \"\"\"\n    Check if there is a metrics that is ready to accept a log\n\n    :param current_log_step: current step log is requested at\n    :param last_log_step: last time a log was recorder for this object. (Deprecated)\n    :param check_model_update: if True, will check if the model has been updated,\n        if False, will only check the log frequency\n    :return: True if a metrics is ready to accept a log.\n    \"\"\"\n    log_enabled = any(logger.enabled for logger in self.loggers)\n    if last_log_step is not None:\n        self.frequency_manager.log_written(step=last_log_step)\n\n    return log_enabled and self.frequency_manager.log_ready(\n        current_log_step=current_log_step,\n        check_model_update=check_model_update,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().metric.log_scalar instead)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().metric.log_scalar instead)\n\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n\n    self.metric.log_scalar(\n        tag=tag,\n        value=value,\n        step=step,\n        wall_time=wall_time,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().metric.log_scalars instead)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().metric.log_scalars instead)\n\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n\n    self.metric.log_scalars(\n        tag=tag,\n        values=values,\n        step=step,\n        wall_time=wall_time,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_string","title":"<code>log_string(tag, string, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().system.log_string instead)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: str,\n    string: str,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().system.log_string instead)\n\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    self.system.log_string(\n        tag=tag,\n        string=string,\n        step=step,\n        wall_time=wall_time,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.log_written","title":"<code>log_written(step)</code>","text":"<p>Update the frequency manager with the last log step written</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>step that was last logged</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_written(self, step: LogStepType):\n    \"\"\"\n    Update the frequency manager with the last log step written\n\n    :param step: step that was last logged\n    \"\"\"\n    self.frequency_manager.log_written(step=step)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.model_updated","title":"<code>model_updated(step)</code>","text":"<p>Update the frequency manager with the last model update step</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>step that was last logged</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def model_updated(self, step: LogStepType):\n    \"\"\"\n    Update the frequency manager with the last model update step\n\n    :param step: step that was last logged\n    \"\"\"\n    self.frequency_manager.model_updated(step=step)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.save","title":"<code>save(file_path, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a file to be saved</p> required <code>kwargs</code> <p>additional arguments that a specific metrics might use</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def save(\n    self,\n    file_path: str,\n    **kwargs,\n):\n    \"\"\"\n    :param file_path: path to a file to be saved\n    :param kwargs: additional arguments that a specific metrics might use\n    \"\"\"\n    for log in self._loggers:\n        if log.enabled:\n            log.save(file_path, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.LoggerManager.time","title":"<code>time(tag=None, *args, **kwargs)</code>","text":"<p>Context manager to log the time it takes to run the block of code</p> <p>Usage:</p> <p>with LoggerManager().time(\"my_block\"):    time.sleep(1)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Optional[str]</code> <p>identifying tag to log the values with</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@contextmanager\ndef time(self, tag: Optional[str] = None, *args, **kwargs):\n    \"\"\"\n    Context manager to log the time it takes to run the block of code\n\n    Usage:\n    &gt;&gt;&gt; with LoggerManager().time(\"my_block\"):\n    &gt;&gt;&gt;    time.sleep(1)\n\n    :param tag: identifying tag to log the values with\n    \"\"\"\n\n    start = time.time()\n    yield\n    elapsed = time.time() - start\n    if not tag:\n        tag = f\"{DEFAULT_TAG}_time_secs\"\n    self.log_scalar(tag=tag, value=float(f\"{elapsed:.3f}\"), *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.PythonLogger","title":"<code>PythonLogger</code>","text":"<p>               Bases: <code>LambdaLogger</code></p> <p>Modifier metrics that handles printing values into a python metrics instance.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>a metrics instance to log to, if None then will create it's own</p> <code>None</code> <code>log_level</code> <code>int</code> <p>default level to log any incoming data at on the logging.Logger instance when an explicit log level isn't provided</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to python</p> <code>'python'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class PythonLogger(LambdaLogger):\n    \"\"\"\n    Modifier metrics that handles printing values into a python metrics instance.\n\n    :param logger: a metrics instance to log to, if None then will create it's own\n    :param log_level: default level to log any incoming data at on the logging.Logger\n        instance when an explicit log level isn't provided\n    :param name: name given to the metrics, used for identification;\n        defaults to python\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        logger: Logger = None,\n        log_level: int = None,\n        name: str = \"python\",\n        enabled: bool = True,\n    ):\n        self._logger = logger or self._create_default_logger(log_level=log_level)\n\n        super().__init__(\n            lambda_func=self._log_lambda,\n            name=name,\n            enabled=enabled,\n        )\n\n    def __getattr__(self, item):\n        return getattr(self._logger, item)\n\n    @property\n    def logger(self) -&gt; Logger:\n        \"\"\"\n        :return: a metrics instance to log to, if None then will create it's own\n        \"\"\"\n        return self._logger\n\n    def _create_default_logger(self, log_level: Optional[int] = None) -&gt; logging.Logger:\n        \"\"\"\n        Create a default modifier metrics,\n        with a file handler logging at the debug level\n        and a stream handler logging to console at the specified level.\n\n        :param log_level: logging level for the console metrics\n        :return: metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n\n        # Console handler, for logging high level modifier logs\n        # must be created before the file handler\n        # as file handler is also a stream handler\n        if not any(\n            isinstance(handler, logging.StreamHandler) for handler in logger.handlers\n        ):\n            stream_handler = logging.StreamHandler()\n            stream_handler.setLevel(\n                log_level or logging.getLogger(\"llmcompressor\").level\n            )\n            logger.addHandler(stream_handler)\n\n        # File handler setup, for logging modifier debug statements\n        if not any(\n            isinstance(handler, logging.FileHandler) for handler in logger.handlers\n        ):\n            base_log_path = (\n                os.environ.get(\"NM_TEST_LOG_DIR\")\n                if os.environ.get(\"NM_TEST_MODE\")\n                else \"sparse_logs\"\n            )\n            now = datetime.now()\n            dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")\n            log_path = os.path.join(base_log_path, f\"{dt_string}.log\")\n            os.makedirs(base_log_path, exist_ok=True)\n            file_handler = logging.FileHandler(\n                log_path,\n                delay=True,\n            )\n            file_handler.setLevel(LOGGING_LEVELS[\"debug\"])\n            logger.addHandler(file_handler)\n            logger.info(f\"Logging all LLM Compressor modifier-level logs to {log_path}\")\n\n        logger.setLevel(LOGGING_LEVELS[\"debug\"])\n        logger.propagate = False\n\n        return logger\n\n    def _log_lambda(\n        self,\n        tag: Optional[str],\n        value: Optional[Union[float, str]],\n        values: Optional[Dict[str, float]],\n        step: Optional[int],\n        wall_time: Optional[float],\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param value: value to save\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        :param level: level to log at. Corresponds to default logging package levels\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not level:\n            level = LOGGING_LEVELS[\"debug\"]\n\n        if level &gt; LOGGING_LEVELS[\"debug\"]:\n            if step is not None:\n                format = \"%s %s step %s: %s\"\n                log_args = [\n                    self.name,\n                    tag,\n                    step,\n                    values or value,\n                ]\n            else:\n                format = \"%s %s: %s\"\n                log_args = [self.name, tag, values or value]\n        else:\n            format = \"%s %s [%s - %s]: %s\"\n            log_args = [self.name, tag, step, wall_time, values or value]\n\n        self._logger.log(level, format, *log_args)\n\n        return True\n\n    def log_string(\n        self,\n        tag: Optional[str],\n        string: Optional[str],\n        step: Optional[int],\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param string: string to log\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        :param level: level to log at. Corresponds to default logging package levels\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not wall_time:\n            wall_time = time.time()\n\n        return self._lambda_func(\n            tag=tag,\n            value=string,\n            values=None,\n            step=step,\n            level=level,\n            wall_time=wall_time,\n        )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.PythonLogger.logger","title":"<code>logger</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Logger</code> <p>a metrics instance to log to, if None then will create it's own</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.PythonLogger.log_string","title":"<code>log_string(tag, string, step, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Optional[str]</code> <p>identifying tag to log the values with</p> required <code>string</code> <code>Optional[str]</code> <p>string to log</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> required <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken, defaults to time.time()</p> <code>None</code> <code>level</code> <code>Optional[int]</code> <p>level to log at. Corresponds to default logging package levels</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: Optional[str],\n    string: Optional[str],\n    step: Optional[int],\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param string: string to log\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken,\n        defaults to time.time()\n    :param level: level to log at. Corresponds to default logging package levels\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not wall_time:\n        wall_time = time.time()\n\n    return self._lambda_func(\n        tag=tag,\n        value=string,\n        values=None,\n        step=step,\n        level=level,\n        wall_time=wall_time,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.SparsificationGroupLogger","title":"<code>SparsificationGroupLogger</code>","text":"<p>               Bases: <code>BaseLogger</code></p> <p>Modifier metrics that handles outputting values to other supported systems. Supported ones include:   - Python logging   - Tensorboard   - Weights and Biases   - Lambda callback</p> <p>All are optional and can be bulk disabled and enabled by this root.</p> <p>Parameters:</p> Name Type Description Default <code>lambda_func</code> <code>Optional[Callable[[Optional[str], Optional[float], Optional[Dict[str, float]], Optional[int], Optional[float]], bool]]</code> <p>an optional lambda function to call back into with any logs. The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool The return type is True if logged and False otherwise.</p> <code>None</code> <code>python</code> <code>Optional[Union[bool, Logger]]</code> <p>an optional argument for logging to a python metrics. May be a logging.Logger instance to log to, True to create a metrics instance, or non truthy to not log anything (False, None)</p> <code>None</code> <code>python_log_level</code> <code>int</code> <p>if python, the level to log any incoming data at on the logging.Logger instance</p> <code>INFO</code> <code>tensorboard</code> <code>Optional[Union[bool, str, SummaryWriter]]</code> <p>an optional argument for logging to a tensorboard writer. May be a SummaryWriter instance to log to, a string representing the directory to create a new SummaryWriter to log to, True to create a new SummaryWriter, or non truthy to not log anything (False, None)</p> <code>None</code> <code>wandb_</code> <code>Optional[Union[bool, Dict]]</code> <p>an optional argument for logging to wandb. May be a dictionary to pass to the init call for wandb, True to log to wandb (will not call init), or non truthy to not log anything (False, None)</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to sparsification</p> <code>'sparsification'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class SparsificationGroupLogger(BaseLogger):\n    \"\"\"\n    Modifier metrics that handles outputting values to other supported systems.\n    Supported ones include:\n      - Python logging\n      - Tensorboard\n      - Weights and Biases\n      - Lambda callback\n\n    All are optional and can be bulk disabled and enabled by this root.\n\n    :param lambda_func: an optional lambda function to call back into with any logs.\n        The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool\n        The return type is True if logged and False otherwise.\n    :param python: an optional argument for logging to a python metrics.\n        May be a logging.Logger instance to log to, True to create a metrics instance,\n        or non truthy to not log anything (False, None)\n    :param python_log_level: if python,\n        the level to log any incoming data at on the logging.Logger instance\n    :param tensorboard: an optional argument for logging to a tensorboard writer.\n        May be a SummaryWriter instance to log to, a string representing the directory\n        to create a new SummaryWriter to log to, True to create a new SummaryWriter,\n        or non truthy to not log anything (False, None)\n    :param wandb_: an optional argument for logging to wandb.\n        May be a dictionary to pass to the init call for wandb,\n        True to log to wandb (will not call init),\n        or non truthy to not log anything (False, None)\n    :param name: name given to the metrics, used for identification;\n        defaults to sparsification\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        lambda_func: Optional[\n            Callable[\n                [\n                    Optional[str],\n                    Optional[float],\n                    Optional[Dict[str, float]],\n                    Optional[int],\n                    Optional[float],\n                ],\n                bool,\n            ]\n        ] = None,\n        python: Optional[Union[bool, Logger]] = None,\n        python_log_level: int = logging.INFO,\n        tensorboard: Optional[Union[bool, str, SummaryWriter]] = None,\n        wandb_: Optional[Union[bool, Dict]] = None,\n        name: str = \"sparsification\",\n        enabled: bool = True,\n    ):\n        super().__init__(name, enabled)\n        self._loggers: List[BaseLogger] = []\n\n        if lambda_func:\n            self._loggers.append(\n                LambdaLogger(lambda_func=lambda_func, name=name, enabled=enabled)\n            )\n\n        if python:\n            self._loggers.append(\n                PythonLogger(\n                    logger=python if isinstance(python, Logger) else None,\n                    log_level=python_log_level,\n                    name=name,\n                    enabled=enabled,\n                )\n            )\n\n        if tensorboard and TensorBoardLogger.available():\n            self._loggers.append(\n                TensorBoardLogger(\n                    log_path=tensorboard if isinstance(tensorboard, str) else None,\n                    writer=(\n                        tensorboard if isinstance(tensorboard, SummaryWriter) else None\n                    ),\n                    name=name,\n                    enabled=enabled,\n                )\n            )\n\n        if wandb_ and WANDBLogger.available():\n            self._loggers.append(\n                WANDBLogger(\n                    init_kwargs=wandb_ if isinstance(wandb_, Dict) else None,\n                    name=name,\n                    enabled=enabled,\n                )\n            )\n\n    @BaseLogger.enabled.setter\n    def enabled(self, value: bool):\n        \"\"\"\n        :param value: True to log, False otherwise\n        \"\"\"\n        self._enabled = value\n\n        for logger in self._loggers:\n            logger.enabled = value\n\n    @property\n    def loggers(self) -&gt; List[BaseLogger]:\n        \"\"\"\n        :return: the created metrics sub instances for this metrics\n        \"\"\"\n        return self._loggers\n\n    def log_hyperparams(self, params: Dict, level: Optional[int] = None):\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        \"\"\"\n        for logger in self._loggers:\n            logger.log_hyperparams(params, level)\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken,\n            defaults to time.time()\n        \"\"\"\n        for logger in self._loggers:\n            logger.log_scalar(tag, value, step, wall_time, level)\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        \"\"\"\n        for logger in self._loggers:\n            logger.log_scalars(tag, values, step, wall_time, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.SparsificationGroupLogger.loggers","title":"<code>loggers</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>List[BaseLogger]</code> <p>the created metrics sub instances for this metrics</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.SparsificationGroupLogger.enabled","title":"<code>enabled(value)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>True to log, False otherwise</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@BaseLogger.enabled.setter\ndef enabled(self, value: bool):\n    \"\"\"\n    :param value: True to log, False otherwise\n    \"\"\"\n    self._enabled = value\n\n    for logger in self._loggers:\n        logger.enabled = value\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.SparsificationGroupLogger.log_hyperparams","title":"<code>log_hyperparams(params, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(self, params: Dict, level: Optional[int] = None):\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    \"\"\"\n    for logger in self._loggers:\n        logger.log_hyperparams(params, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.SparsificationGroupLogger.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken, defaults to time.time()</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken,\n        defaults to time.time()\n    \"\"\"\n    for logger in self._loggers:\n        logger.log_scalar(tag, value, step, wall_time, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.SparsificationGroupLogger.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken, defaults to time.time()</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken,\n        defaults to time.time()\n    \"\"\"\n    for logger in self._loggers:\n        logger.log_scalars(tag, values, step, wall_time, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.TensorBoardLogger","title":"<code>TensorBoardLogger</code>","text":"<p>               Bases: <code>LambdaLogger</code></p> <p>Modifier metrics that handles outputting values into a TensorBoard log directory for viewing in TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>log_path</code> <code>str</code> <p>the path to create a SummaryWriter at. writer must be None to use if not supplied (and writer is None), will create a TensorBoard dir in cwd</p> <code>None</code> <code>writer</code> <code>SummaryWriter</code> <p>the writer to log results to, if none is given creates a new one at the log_path</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to tensorboard</p> <code>'tensorboard'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class TensorBoardLogger(LambdaLogger):\n    \"\"\"\n    Modifier metrics that handles outputting values into a TensorBoard log directory\n    for viewing in TensorBoard.\n\n    :param log_path: the path to create a SummaryWriter at. writer must be None\n        to use if not supplied (and writer is None),\n        will create a TensorBoard dir in cwd\n    :param writer: the writer to log results to,\n        if none is given creates a new one at the log_path\n    :param name: name given to the metrics, used for identification;\n        defaults to tensorboard\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        log_path: str = None,\n        writer: SummaryWriter = None,\n        name: str = \"tensorboard\",\n        enabled: bool = True,\n    ):\n        if tensorboard_import_error:\n            raise tensorboard_import_error\n\n        if writer and log_path:\n            raise ValueError(\n                (\n                    \"log_path given:{} and writer object passed in, \"\n                    \"to create a writer at the log path set writer=None\"\n                ).format(log_path)\n            )\n        elif not writer and not log_path:\n            log_path = os.path.join(\"\", \"tensorboard\")\n\n        if os.environ.get(\"NM_TEST_MODE\"):\n            test_log_root = os.environ.get(\"NM_TEST_LOG_DIR\")\n            log_path = (\n                os.path.join(test_log_root, log_path) if log_path else test_log_root\n            )\n\n        if log_path:\n            _create_dirs(log_path)\n\n        self._writer = writer if writer is not None else SummaryWriter(log_path)\n        super().__init__(\n            lambda_func=self._log_lambda,\n            name=name,\n            enabled=enabled,\n        )\n\n    @staticmethod\n    def available() -&gt; bool:\n        \"\"\"\n        :return: True if tensorboard is available and installed, False, otherwise\n        \"\"\"\n        return not tensorboard_import_error\n\n    @property\n    def writer(self) -&gt; SummaryWriter:\n        \"\"\"\n        :return: the writer to log results to,\n            if none is given creates a new one at the log_path\n        \"\"\"\n        return self._writer\n\n    def _log_lambda(\n        self,\n        tag: Optional[str],\n        value: Optional[float],\n        values: Optional[Dict[str, float]],\n        step: Optional[int],\n        wall_time: Optional[float],\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        if value is not None:\n            self._writer.add_scalar(tag, value, step, wall_time)\n\n        if values and tag:\n            self._writer.add_scalars(tag, values, step, wall_time)\n        elif values:\n            for name, val in values.items():\n                # hyperparameters logging case\n                self._writer.add_scalar(name, val, step, wall_time)\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.TensorBoardLogger.writer","title":"<code>writer</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>SummaryWriter</code> <p>the writer to log results to, if none is given creates a new one at the log_path</p>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.TensorBoardLogger.available","title":"<code>available()</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if tensorboard is available and installed, False, otherwise</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@staticmethod\ndef available() -&gt; bool:\n    \"\"\"\n    :return: True if tensorboard is available and installed, False, otherwise\n    \"\"\"\n    return not tensorboard_import_error\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.WANDBLogger","title":"<code>WANDBLogger</code>","text":"<p>               Bases: <code>LambdaLogger</code></p> <p>Modifier metrics that handles outputting values to Weights and Biases.</p> <p>Parameters:</p> Name Type Description Default <code>init_kwargs</code> <code>Optional[Dict]</code> <p>the args to call into wandb.init with; ex: wandb.init(**init_kwargs). If not supplied, then init will not be called</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to wandb</p> <code>'wandb'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class WANDBLogger(LambdaLogger):\n    \"\"\"\n    Modifier metrics that handles outputting values to Weights and Biases.\n\n    :param init_kwargs: the args to call into wandb.init with;\n        ex: wandb.init(**init_kwargs). If not supplied, then init will not be called\n    :param name: name given to the metrics, used for identification;\n        defaults to wandb\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    @staticmethod\n    def available() -&gt; bool:\n        \"\"\"\n        :return: True if wandb is available and installed, False, otherwise\n        \"\"\"\n        return wandb_available\n\n    def __init__(\n        self,\n        init_kwargs: Optional[Dict] = None,\n        name: str = \"wandb\",\n        enabled: bool = True,\n        wandb_err: Optional[Exception] = wandb_err,\n    ):\n        if wandb_err:\n            raise wandb_err\n\n        super().__init__(\n            lambda_func=self._log_lambda,\n            name=name,\n            enabled=enabled,\n        )\n\n        if os.environ.get(\"NM_TEST_MODE\"):\n            test_log_path = os.environ.get(\"NM_TEST_LOG_DIR\")\n            _create_dirs(test_log_path)\n            if init_kwargs:\n                init_kwargs[\"dir\"] = test_log_path\n            else:\n                init_kwargs = {\"dir\": test_log_path}\n\n        if wandb_err:\n            raise wandb_err\n\n        if init_kwargs:\n            wandb.init(**init_kwargs)\n        else:\n            wandb.init()\n\n        self.wandb = wandb\n\n    def _log_lambda(\n        self,\n        tag: Optional[str],\n        value: Optional[float],\n        values: Optional[Dict[str, float]],\n        step: Optional[int],\n        wall_time: Optional[float],\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        params = {}\n\n        if value:\n            params[tag] = value\n\n        if values:\n            if tag:\n                values = {f\"{tag}/{key}\": val for key, val in values.items()}\n            params.update(values)\n\n        params.update({\"Step\": step})\n        wandb.log(params)\n\n        return True\n\n    def save(\n        self,\n        file_path: str,\n    ) -&gt; bool:\n        \"\"\"\n        :param file_path: path to a file to be saved\n        \"\"\"\n        wandb.save(file_path)\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.WANDBLogger.available","title":"<code>available()</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if wandb is available and installed, False, otherwise</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@staticmethod\ndef available() -&gt; bool:\n    \"\"\"\n    :return: True if wandb is available and installed, False, otherwise\n    \"\"\"\n    return wandb_available\n</code></pre>"},{"location":"reference/llmcompressor/metrics/#llmcompressor.metrics.WANDBLogger.save","title":"<code>save(file_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a file to be saved</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def save(\n    self,\n    file_path: str,\n) -&gt; bool:\n    \"\"\"\n    :param file_path: path to a file to be saved\n    \"\"\"\n    wandb.save(file_path)\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/","title":"llmcompressor.metrics.logger","text":"<p>Contains code for loggers that help visualize the information from each modifier</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger","title":"<code>BaseLogger</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class that all modifier loggers must implement.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name given to the metrics, used for identification</p> required <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class BaseLogger(ABC):\n    \"\"\"\n    Base class that all modifier loggers must implement.\n\n    :param name: name given to the metrics, used for identification\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(self, name: str, enabled: bool = True):\n        self._name = name\n        self._enabled = enabled\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        :return: name given to the metrics, used for identification\n        \"\"\"\n        return self._name\n\n    @property\n    def enabled(self) -&gt; bool:\n        \"\"\"\n        :return: True to log, False otherwise\n        \"\"\"\n        return self._enabled\n\n    @enabled.setter\n    def enabled(self, value: bool):\n        \"\"\"\n        :param value: True to log, False otherwise\n        \"\"\"\n        self._enabled = value\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(name={self._name}, enabled={self._enabled})\"\n\n    def log_hyperparams(self, params: Dict[str, float]) -&gt; bool:\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def log_string(\n        self,\n        tag: str,\n        string: str,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        return False\n\n    def save(\n        self,\n        file_path: str,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        :param file_path: path to a file to be saved\n        :param kwargs: additional arguments that a specific metrics might use\n        :return: True if saved, False otherwise\n        \"\"\"\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.enabled","title":"<code>enabled</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True to log, False otherwise</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>name given to the metrics, used for identification</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.log_hyperparams","title":"<code>log_hyperparams(params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, float]</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(self, params: Dict[str, float]) -&gt; bool:\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.log_string","title":"<code>log_string(tag, string, step=None, wall_time=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: str,\n    string: str,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.BaseLogger.save","title":"<code>save(file_path, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a file to be saved</p> required <code>kwargs</code> <p>additional arguments that a specific metrics might use</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if saved, False otherwise</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def save(\n    self,\n    file_path: str,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    :param file_path: path to a file to be saved\n    :param kwargs: additional arguments that a specific metrics might use\n    :return: True if saved, False otherwise\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LambdaLogger","title":"<code>LambdaLogger</code>","text":"<p>               Bases: <code>BaseLogger</code></p> <p>Logger that handles calling back to a lambda function with any logs.</p> <p>Parameters:</p> Name Type Description Default <code>lambda_func</code> <code>Callable[[Optional[str], Optional[Union[float, str]], Optional[Dict[str, float]], Optional[int], Optional[float], Optional[int]], bool]</code> <p>the lambda function to call back into with any logs. The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool The return type is True if logged and False otherwise.</p> required <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to lambda</p> <code>'lambda'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class LambdaLogger(BaseLogger):\n    \"\"\"\n    Logger that handles calling back to a lambda function with any logs.\n\n    :param lambda_func: the lambda function to call back into with any logs.\n        The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool\n        The return type is True if logged and False otherwise.\n    :param name: name given to the metrics, used for identification;\n        defaults to lambda\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        lambda_func: Callable[\n            [\n                Optional[str],\n                Optional[Union[float, str]],\n                Optional[Dict[str, float]],\n                Optional[int],\n                Optional[float],\n                Optional[int],\n            ],\n            bool,\n        ],\n        name: str = \"lambda\",\n        enabled: bool = True,\n    ):\n        super().__init__(name, enabled)\n        self._lambda_func = lambda_func\n        assert lambda_func, \"lambda_func must be set to a callable function\"\n\n    @property\n    def lambda_func(\n        self,\n    ) -&gt; Callable[\n        [\n            Optional[str],\n            Optional[Union[float, str]],\n            Optional[Dict[str, float]],\n            Optional[int],\n            Optional[float],\n            Optional[int],\n        ],\n        bool,\n    ]:\n        \"\"\"\n        :return: the lambda function to call back into with any logs.\n            The expected call sequence is (tag, value, values, step, wall_time)\n        \"\"\"\n        return self._lambda_func\n\n    def log_hyperparams(\n        self,\n        params: Dict,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not self.enabled:\n            return False\n\n        return self._lambda_func(\n            tag=None,\n            value=None,\n            values=params,\n            step=None,\n            wall_time=None,\n            level=level,\n        )\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken,\n            defaults to time.time()\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not wall_time:\n            wall_time = time.time()\n\n        return self._lambda_func(\n            tag=tag,\n            value=value,\n            values=None,\n            step=step,\n            wall_time=wall_time,\n            level=level,\n        )\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not wall_time:\n            wall_time = time.time()\n\n        return self._lambda_func(\n            tag=tag,\n            value=None,\n            values=values,\n            step=step,\n            wall_time=wall_time,\n            level=level,\n        )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LambdaLogger.lambda_func","title":"<code>lambda_func</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Callable[[Optional[str], Optional[Union[float, str]], Optional[Dict[str, float]], Optional[int], Optional[float], Optional[int]], bool]</code> <p>the lambda function to call back into with any logs. The expected call sequence is (tag, value, values, step, wall_time)</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LambdaLogger.log_hyperparams","title":"<code>log_hyperparams(params, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(\n    self,\n    params: Dict,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not self.enabled:\n        return False\n\n    return self._lambda_func(\n        tag=None,\n        value=None,\n        values=params,\n        step=None,\n        wall_time=None,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LambdaLogger.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken, defaults to time.time()</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken,\n        defaults to time.time()\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not wall_time:\n        wall_time = time.time()\n\n    return self._lambda_func(\n        tag=tag,\n        value=value,\n        values=None,\n        step=step,\n        wall_time=wall_time,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LambdaLogger.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken, defaults to time.time()</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken,\n        defaults to time.time()\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not wall_time:\n        wall_time = time.time()\n\n    return self._lambda_func(\n        tag=tag,\n        value=None,\n        values=values,\n        step=step,\n        wall_time=wall_time,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager","title":"<code>LoggerManager</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Wrapper around loggers that handles log scheduling and handing off logs to intended loggers.</p> <p>Parameters:</p> Name Type Description Default <code>loggers</code> <code>Optional[List[BaseLogger]]</code> <p>list of loggers assigned to this manager</p> <code>None</code> <code>log_frequency</code> <code>Union[float, None]</code> <p>number of stes or fraction of steps to wait between logs</p> <code>0.1</code> <code>mode</code> <code>LoggingModeType</code> <p>The logging mode to use, either \"on_change\" or \"exact\", \"on_change\" will log when the model has been updated since the last log, \"exact\" will log at the given frequency regardless of model updates. Defaults to \"exact\"</p> <code>'exact'</code> <code>frequency_type</code> <code>FrequencyType</code> <p>The frequency type to use, either \"epoch\", \"step\", or \"batch\" controls what the frequency manager is tracking, e.g. if the frequency type is \"epoch\", then the frequency manager will track the number of epochs that have passed since the last log, if the frequency type is \"step\", then the frequency manager will track the number of optimizer steps</p> <code>'epoch'</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class LoggerManager(ABC):\n    \"\"\"\n    Wrapper around loggers that handles log scheduling and handing off logs to intended\n    loggers.\n\n    :param loggers: list of loggers assigned to this manager\n    :param log_frequency: number of stes or fraction of steps to wait between logs\n    :param mode: The logging mode to use, either \"on_change\" or \"exact\",\n        \"on_change\" will log when the model has been updated since the last log,\n        \"exact\" will log at the given frequency regardless of model updates.\n        Defaults to \"exact\"\n    :param frequency_type: The frequency type to use, either \"epoch\", \"step\", or \"batch\"\n        controls what the frequency manager is tracking, e.g. if the frequency type\n        is \"epoch\", then the frequency manager will track the number of epochs that\n        have passed since the last log, if the frequency type is \"step\", then the\n        frequency manager will track the number of optimizer steps\n    \"\"\"\n\n    def __init__(\n        self,\n        loggers: Optional[List[BaseLogger]] = None,\n        log_frequency: Union[float, None] = 0.1,\n        log_python: bool = True,\n        name: str = \"manager\",\n        mode: LoggingModeType = \"exact\",\n        frequency_type: FrequencyType = \"epoch\",\n    ):\n        self._name = name\n        self._loggers = (\n            loggers\n            or SparsificationGroupLogger(\n                python=log_python,\n                name=name,\n                tensorboard=False,\n                wandb_=False,\n            ).loggers\n        )\n\n        self.frequency_manager = FrequencyManager(\n            mode=mode,\n            frequency_type=frequency_type,\n            log_frequency=log_frequency,\n        )\n\n        self.system = SystemLoggingWraper(\n            loggers=self._loggers, frequency_manager=self.frequency_manager\n        )\n        self.metric = MetricLoggingWrapper(\n            loggers=self._loggers, frequency_manager=self.frequency_manager\n        )\n\n    def __len__(self):\n        return len(self.loggers)\n\n    def __iter__(self):\n        return iter(self.loggers)\n\n    def add_logger(self, logger: BaseLogger):\n        \"\"\"\n        add a BaseLogger implementation to the loggers of this manager\n\n        :param logger: metrics object to add\n        \"\"\"\n        if not isinstance(logger, BaseLogger):\n            raise ValueError(f\"metrics {type(logger)} must be of type BaseLogger\")\n        self._loggers.append(logger)\n\n    def log_ready(\n        self, current_log_step, last_log_step=None, check_model_update: bool = False\n    ):\n        \"\"\"\n        Check if there is a metrics that is ready to accept a log\n\n        :param current_log_step: current step log is requested at\n        :param last_log_step: last time a log was recorder for this object. (Deprecated)\n        :param check_model_update: if True, will check if the model has been updated,\n            if False, will only check the log frequency\n        :return: True if a metrics is ready to accept a log.\n        \"\"\"\n        log_enabled = any(logger.enabled for logger in self.loggers)\n        if last_log_step is not None:\n            self.frequency_manager.log_written(step=last_log_step)\n\n        return log_enabled and self.frequency_manager.log_ready(\n            current_log_step=current_log_step,\n            check_model_update=check_model_update,\n        )\n\n    def log_written(self, step: LogStepType):\n        \"\"\"\n        Update the frequency manager with the last log step written\n\n        :param step: step that was last logged\n        \"\"\"\n        self.frequency_manager.log_written(step=step)\n\n    def model_updated(self, step: LogStepType):\n        \"\"\"\n        Update the frequency manager with the last model update step\n\n        :param step: step that was last logged\n        \"\"\"\n        self.frequency_manager.model_updated(step=step)\n\n    @staticmethod\n    def epoch_to_step(epoch, steps_per_epoch):\n        return round(epoch) if steps_per_epoch &lt;= 0 else round(epoch * steps_per_epoch)\n\n    @property\n    def loggers(self) -&gt; List[BaseLogger]:\n        \"\"\"\n        :return: list of loggers assigned to this manager\n        \"\"\"\n        return self._loggers\n\n    @loggers.setter\n    def loggers(self, value: List[BaseLogger]):\n        \"\"\"\n        :param value: list of loggers assigned to this manager\n        \"\"\"\n        self._loggers = value\n\n    @property\n    def log_frequency(self) -&gt; Union[str, float, None]:\n        \"\"\"\n        :return: number of epochs or fraction of epochs to wait between logs\n        \"\"\"\n        return self.frequency_manager._log_frequency\n\n    @log_frequency.setter\n    def log_frequency(self, value: Union[str, float, None]):\n        \"\"\"\n        :param value: number of epochs or fraction of epochs to wait between logs\n        \"\"\"\n        self.frequency_manager._log_frequency = value\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        :return: name given to the metrics, used for identification\n        \"\"\"\n        return self._name\n\n    @property\n    def wandb(self) -&gt; Optional[ModuleType]:\n        \"\"\"\n        :return: wandb module if initialized\n        \"\"\"\n        for log in self.loggers:\n            if isinstance(log, WANDBLogger) and log.enabled:\n                return log.wandb\n        return None\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().metric.log_scalar instead)\n\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n\n        self.metric.log_scalar(\n            tag=tag,\n            value=value,\n            step=step,\n            wall_time=wall_time,\n            log_types=log_types,\n            level=level,\n        )\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().metric.log_scalars instead)\n\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n\n        self.metric.log_scalars(\n            tag=tag,\n            values=values,\n            step=step,\n            wall_time=wall_time,\n            log_types=log_types,\n            level=level,\n        )\n\n    def log_hyperparams(\n        self,\n        params: Dict,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().metric.log_hyperparams instead)\n\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        \"\"\"\n\n        self.metric.log_hyperparams(\n            params=params,\n            log_types=log_types,\n            level=level,\n        )\n\n    def log_string(\n        self,\n        tag: str,\n        string: str,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        (Note: this method is deprecated and will be removed in a future version,\n        use LoggerManager().system.log_string instead)\n\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        self.system.log_string(\n            tag=tag,\n            string=string,\n            step=step,\n            wall_time=wall_time,\n            log_types=log_types,\n            level=level,\n        )\n\n    def save(\n        self,\n        file_path: str,\n        **kwargs,\n    ):\n        \"\"\"\n        :param file_path: path to a file to be saved\n        :param kwargs: additional arguments that a specific metrics might use\n        \"\"\"\n        for log in self._loggers:\n            if log.enabled:\n                log.save(file_path, **kwargs)\n\n    @contextmanager\n    def time(self, tag: Optional[str] = None, *args, **kwargs):\n        \"\"\"\n        Context manager to log the time it takes to run the block of code\n\n        Usage:\n        &gt;&gt;&gt; with LoggerManager().time(\"my_block\"):\n        &gt;&gt;&gt;    time.sleep(1)\n\n        :param tag: identifying tag to log the values with\n        \"\"\"\n\n        start = time.time()\n        yield\n        elapsed = time.time() - start\n        if not tag:\n            tag = f\"{DEFAULT_TAG}_time_secs\"\n        self.log_scalar(tag=tag, value=float(f\"{elapsed:.3f}\"), *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_frequency","title":"<code>log_frequency</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>Union[str, float, None]</code> <p>number of epochs or fraction of epochs to wait between logs</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.loggers","title":"<code>loggers</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>List[BaseLogger]</code> <p>list of loggers assigned to this manager</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>name given to the metrics, used for identification</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.wandb","title":"<code>wandb</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[ModuleType]</code> <p>wandb module if initialized</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.add_logger","title":"<code>add_logger(logger)</code>","text":"<p>add a BaseLogger implementation to the loggers of this manager</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>BaseLogger</code> <p>metrics object to add</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def add_logger(self, logger: BaseLogger):\n    \"\"\"\n    add a BaseLogger implementation to the loggers of this manager\n\n    :param logger: metrics object to add\n    \"\"\"\n    if not isinstance(logger, BaseLogger):\n        raise ValueError(f\"metrics {type(logger)} must be of type BaseLogger\")\n    self._loggers.append(logger)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_hyperparams","title":"<code>log_hyperparams(params, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().metric.log_hyperparams instead)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(\n    self,\n    params: Dict,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().metric.log_hyperparams instead)\n\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    \"\"\"\n\n    self.metric.log_hyperparams(\n        params=params,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_ready","title":"<code>log_ready(current_log_step, last_log_step=None, check_model_update=False)</code>","text":"<p>Check if there is a metrics that is ready to accept a log</p> <p>Parameters:</p> Name Type Description Default <code>current_log_step</code> <p>current step log is requested at</p> required <code>last_log_step</code> <p>last time a log was recorder for this object. (Deprecated)</p> <code>None</code> <code>check_model_update</code> <code>bool</code> <p>if True, will check if the model has been updated, if False, will only check the log frequency</p> <code>False</code> <p>Returns:</p> Type Description <p>True if a metrics is ready to accept a log.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_ready(\n    self, current_log_step, last_log_step=None, check_model_update: bool = False\n):\n    \"\"\"\n    Check if there is a metrics that is ready to accept a log\n\n    :param current_log_step: current step log is requested at\n    :param last_log_step: last time a log was recorder for this object. (Deprecated)\n    :param check_model_update: if True, will check if the model has been updated,\n        if False, will only check the log frequency\n    :return: True if a metrics is ready to accept a log.\n    \"\"\"\n    log_enabled = any(logger.enabled for logger in self.loggers)\n    if last_log_step is not None:\n        self.frequency_manager.log_written(step=last_log_step)\n\n    return log_enabled and self.frequency_manager.log_ready(\n        current_log_step=current_log_step,\n        check_model_update=check_model_update,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().metric.log_scalar instead)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().metric.log_scalar instead)\n\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n\n    self.metric.log_scalar(\n        tag=tag,\n        value=value,\n        step=step,\n        wall_time=wall_time,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().metric.log_scalars instead)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().metric.log_scalars instead)\n\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n\n    self.metric.log_scalars(\n        tag=tag,\n        values=values,\n        step=step,\n        wall_time=wall_time,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_string","title":"<code>log_string(tag, string, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>(Note: this method is deprecated and will be removed in a future version, use LoggerManager().system.log_string instead)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: str,\n    string: str,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    (Note: this method is deprecated and will be removed in a future version,\n    use LoggerManager().system.log_string instead)\n\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    self.system.log_string(\n        tag=tag,\n        string=string,\n        step=step,\n        wall_time=wall_time,\n        log_types=log_types,\n        level=level,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.log_written","title":"<code>log_written(step)</code>","text":"<p>Update the frequency manager with the last log step written</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>step that was last logged</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_written(self, step: LogStepType):\n    \"\"\"\n    Update the frequency manager with the last log step written\n\n    :param step: step that was last logged\n    \"\"\"\n    self.frequency_manager.log_written(step=step)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.model_updated","title":"<code>model_updated(step)</code>","text":"<p>Update the frequency manager with the last model update step</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>step that was last logged</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def model_updated(self, step: LogStepType):\n    \"\"\"\n    Update the frequency manager with the last model update step\n\n    :param step: step that was last logged\n    \"\"\"\n    self.frequency_manager.model_updated(step=step)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.save","title":"<code>save(file_path, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a file to be saved</p> required <code>kwargs</code> <p>additional arguments that a specific metrics might use</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def save(\n    self,\n    file_path: str,\n    **kwargs,\n):\n    \"\"\"\n    :param file_path: path to a file to be saved\n    :param kwargs: additional arguments that a specific metrics might use\n    \"\"\"\n    for log in self._loggers:\n        if log.enabled:\n            log.save(file_path, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggerManager.time","title":"<code>time(tag=None, *args, **kwargs)</code>","text":"<p>Context manager to log the time it takes to run the block of code</p> <p>Usage:</p> <p>with LoggerManager().time(\"my_block\"):    time.sleep(1)</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Optional[str]</code> <p>identifying tag to log the values with</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@contextmanager\ndef time(self, tag: Optional[str] = None, *args, **kwargs):\n    \"\"\"\n    Context manager to log the time it takes to run the block of code\n\n    Usage:\n    &gt;&gt;&gt; with LoggerManager().time(\"my_block\"):\n    &gt;&gt;&gt;    time.sleep(1)\n\n    :param tag: identifying tag to log the values with\n    \"\"\"\n\n    start = time.time()\n    yield\n    elapsed = time.time() - start\n    if not tag:\n        tag = f\"{DEFAULT_TAG}_time_secs\"\n    self.log_scalar(tag=tag, value=float(f\"{elapsed:.3f}\"), *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.LoggingWrapperBase","title":"<code>LoggingWrapperBase</code>","text":"<p>Base class that holds a reference to the loggers and frequency manager</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class LoggingWrapperBase:\n    \"\"\"\n    Base class that holds a reference to the loggers and frequency manager\n    \"\"\"\n\n    def __init__(self, loggers: List[BaseLogger], frequency_manager: FrequencyManager):\n        self.loggers = loggers\n        self._frequency_manager = frequency_manager\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"loggers={self.loggers}, frequency_manager={self._frequency_manager})\"\n        )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper","title":"<code>MetricLoggingWrapper</code>","text":"<p>               Bases: <code>LoggingWrapperBase</code></p> <p>Wraps utilities and convenience methods for logging metrics to the system</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class MetricLoggingWrapper(LoggingWrapperBase):\n    \"\"\"\n    Wraps utilities and convenience methods for logging metrics to the system\n    \"\"\"\n\n    def log_hyperparams(\n        self,\n        params: Dict,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        \"\"\"\n        for log in self.loggers:\n            if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n                log.log_hyperparams(params, level)\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        for log in self.loggers:\n            if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n                log.log_scalar(\n                    tag=tag,\n                    value=value,\n                    step=step,\n                    wall_time=wall_time,\n                    level=level,\n                )\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        for log in self.loggers:\n            if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n                log.log_scalars(\n                    tag=tag,\n                    values=values,\n                    step=step,\n                    wall_time=wall_time,\n                    level=level,\n                )\n\n    def add_scalar(\n        self,\n        value,\n        tag: str = DEFAULT_TAG,\n        step: Optional[int] = None,\n        wall_time: Union[int, float, None] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Add a scalar value to the metrics\n\n        :param value: value to log\n        :param tag: tag to log the value with, defaults to DEFAULT_TAG\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to to pass through to the\n            metrics\n        \"\"\"\n        self.log_scalar(tag=tag, value=value, step=step, wall_time=wall_time, **kwargs)\n\n    def add_scalars(\n        self,\n        values: Dict[str, Any],\n        tag: str = DEFAULT_TAG,\n        step: Optional[int] = None,\n        wall_time: Union[int, float, None] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Adds multiple scalar values to the metrics\n\n        :param values: values to log, must be A dict of serializable\n            python objects i.e `str`, `ints`, `floats`, `Tensors`, `dicts`, etc\n        :param tag: tag to log the value with, defaults to DEFAULT_TAG\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken\n        :param kwargs: additional logging arguments to to pass through to the\n            metrics\n        \"\"\"\n        self.log_scalars(\n            tag=tag, values=values, step=step, wall_time=wall_time, **kwargs\n        )\n\n    def log(\n        self,\n        data: Dict[str, Any],\n        step: Optional[int] = None,\n        tag: Optional[str] = DEFAULT_TAG,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        :param data:  A dict of serializable python objects i.e `str`,\n                `ints`, `floats`, `Tensors`, `dicts`, etc\n        :param step: global step for when the values were taken\n        :param tag: identifying tag to log the values with, defaults to DEFAULT_TAG\n        :param kwargs: additional logging arguments to support\n            Python and custom loggers\n        \"\"\"\n        self.log_scalars(tag=tag, values=data, step=step, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper.add_scalar","title":"<code>add_scalar(value, tag=DEFAULT_TAG, step=None, wall_time=None, **kwargs)</code>","text":"<p>Add a scalar value to the metrics</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>value to log</p> required <code>tag</code> <code>str</code> <p>tag to log the value with, defaults to DEFAULT_TAG</p> <code>DEFAULT_TAG</code> <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Union[int, float, None]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to to pass through to the metrics</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def add_scalar(\n    self,\n    value,\n    tag: str = DEFAULT_TAG,\n    step: Optional[int] = None,\n    wall_time: Union[int, float, None] = None,\n    **kwargs,\n):\n    \"\"\"\n    Add a scalar value to the metrics\n\n    :param value: value to log\n    :param tag: tag to log the value with, defaults to DEFAULT_TAG\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to to pass through to the\n        metrics\n    \"\"\"\n    self.log_scalar(tag=tag, value=value, step=step, wall_time=wall_time, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper.add_scalars","title":"<code>add_scalars(values, tag=DEFAULT_TAG, step=None, wall_time=None, **kwargs)</code>","text":"<p>Adds multiple scalar values to the metrics</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict[str, Any]</code> <p>values to log, must be A dict of serializable python objects i.e <code>str</code>, <code>ints</code>, <code>floats</code>, <code>Tensors</code>, <code>dicts</code>, etc</p> required <code>tag</code> <code>str</code> <p>tag to log the value with, defaults to DEFAULT_TAG</p> <code>DEFAULT_TAG</code> <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Union[int, float, None]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to to pass through to the metrics</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def add_scalars(\n    self,\n    values: Dict[str, Any],\n    tag: str = DEFAULT_TAG,\n    step: Optional[int] = None,\n    wall_time: Union[int, float, None] = None,\n    **kwargs,\n):\n    \"\"\"\n    Adds multiple scalar values to the metrics\n\n    :param values: values to log, must be A dict of serializable\n        python objects i.e `str`, `ints`, `floats`, `Tensors`, `dicts`, etc\n    :param tag: tag to log the value with, defaults to DEFAULT_TAG\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to to pass through to the\n        metrics\n    \"\"\"\n    self.log_scalars(\n        tag=tag, values=values, step=step, wall_time=wall_time, **kwargs\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper.log","title":"<code>log(data, step=None, tag=DEFAULT_TAG, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>A dict of serializable python objects i.e <code>str</code>, <code>ints</code>, <code>floats</code>, <code>Tensors</code>, <code>dicts</code>, etc</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>tag</code> <code>Optional[str]</code> <p>identifying tag to log the values with, defaults to DEFAULT_TAG</p> <code>DEFAULT_TAG</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log(\n    self,\n    data: Dict[str, Any],\n    step: Optional[int] = None,\n    tag: Optional[str] = DEFAULT_TAG,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    :param data:  A dict of serializable python objects i.e `str`,\n            `ints`, `floats`, `Tensors`, `dicts`, etc\n    :param step: global step for when the values were taken\n    :param tag: identifying tag to log the values with, defaults to DEFAULT_TAG\n    :param kwargs: additional logging arguments to support\n        Python and custom loggers\n    \"\"\"\n    self.log_scalars(tag=tag, values=data, step=step, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper.log_hyperparams","title":"<code>log_hyperparams(params, log_types=ALL_TOKEN, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(\n    self,\n    params: Dict,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    \"\"\"\n    for log in self.loggers:\n        if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n            log.log_hyperparams(params, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    for log in self.loggers:\n        if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n            log.log_scalar(\n                tag=tag,\n                value=value,\n                step=step,\n                wall_time=wall_time,\n                level=level,\n            )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.MetricLoggingWrapper.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    for log in self.loggers:\n        if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n            log.log_scalars(\n                tag=tag,\n                values=values,\n                step=step,\n                wall_time=wall_time,\n                level=level,\n            )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.PythonLogger","title":"<code>PythonLogger</code>","text":"<p>               Bases: <code>LambdaLogger</code></p> <p>Modifier metrics that handles printing values into a python metrics instance.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>a metrics instance to log to, if None then will create it's own</p> <code>None</code> <code>log_level</code> <code>int</code> <p>default level to log any incoming data at on the logging.Logger instance when an explicit log level isn't provided</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to python</p> <code>'python'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class PythonLogger(LambdaLogger):\n    \"\"\"\n    Modifier metrics that handles printing values into a python metrics instance.\n\n    :param logger: a metrics instance to log to, if None then will create it's own\n    :param log_level: default level to log any incoming data at on the logging.Logger\n        instance when an explicit log level isn't provided\n    :param name: name given to the metrics, used for identification;\n        defaults to python\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        logger: Logger = None,\n        log_level: int = None,\n        name: str = \"python\",\n        enabled: bool = True,\n    ):\n        self._logger = logger or self._create_default_logger(log_level=log_level)\n\n        super().__init__(\n            lambda_func=self._log_lambda,\n            name=name,\n            enabled=enabled,\n        )\n\n    def __getattr__(self, item):\n        return getattr(self._logger, item)\n\n    @property\n    def logger(self) -&gt; Logger:\n        \"\"\"\n        :return: a metrics instance to log to, if None then will create it's own\n        \"\"\"\n        return self._logger\n\n    def _create_default_logger(self, log_level: Optional[int] = None) -&gt; logging.Logger:\n        \"\"\"\n        Create a default modifier metrics,\n        with a file handler logging at the debug level\n        and a stream handler logging to console at the specified level.\n\n        :param log_level: logging level for the console metrics\n        :return: metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n\n        # Console handler, for logging high level modifier logs\n        # must be created before the file handler\n        # as file handler is also a stream handler\n        if not any(\n            isinstance(handler, logging.StreamHandler) for handler in logger.handlers\n        ):\n            stream_handler = logging.StreamHandler()\n            stream_handler.setLevel(\n                log_level or logging.getLogger(\"llmcompressor\").level\n            )\n            logger.addHandler(stream_handler)\n\n        # File handler setup, for logging modifier debug statements\n        if not any(\n            isinstance(handler, logging.FileHandler) for handler in logger.handlers\n        ):\n            base_log_path = (\n                os.environ.get(\"NM_TEST_LOG_DIR\")\n                if os.environ.get(\"NM_TEST_MODE\")\n                else \"sparse_logs\"\n            )\n            now = datetime.now()\n            dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")\n            log_path = os.path.join(base_log_path, f\"{dt_string}.log\")\n            os.makedirs(base_log_path, exist_ok=True)\n            file_handler = logging.FileHandler(\n                log_path,\n                delay=True,\n            )\n            file_handler.setLevel(LOGGING_LEVELS[\"debug\"])\n            logger.addHandler(file_handler)\n            logger.info(f\"Logging all LLM Compressor modifier-level logs to {log_path}\")\n\n        logger.setLevel(LOGGING_LEVELS[\"debug\"])\n        logger.propagate = False\n\n        return logger\n\n    def _log_lambda(\n        self,\n        tag: Optional[str],\n        value: Optional[Union[float, str]],\n        values: Optional[Dict[str, float]],\n        step: Optional[int],\n        wall_time: Optional[float],\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param value: value to save\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        :param level: level to log at. Corresponds to default logging package levels\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not level:\n            level = LOGGING_LEVELS[\"debug\"]\n\n        if level &gt; LOGGING_LEVELS[\"debug\"]:\n            if step is not None:\n                format = \"%s %s step %s: %s\"\n                log_args = [\n                    self.name,\n                    tag,\n                    step,\n                    values or value,\n                ]\n            else:\n                format = \"%s %s: %s\"\n                log_args = [self.name, tag, values or value]\n        else:\n            format = \"%s %s [%s - %s]: %s\"\n            log_args = [self.name, tag, step, wall_time, values or value]\n\n        self._logger.log(level, format, *log_args)\n\n        return True\n\n    def log_string(\n        self,\n        tag: Optional[str],\n        string: Optional[str],\n        step: Optional[int],\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param string: string to log\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        :param level: level to log at. Corresponds to default logging package levels\n        :return: True if logged, False otherwise.\n        \"\"\"\n        if not wall_time:\n            wall_time = time.time()\n\n        return self._lambda_func(\n            tag=tag,\n            value=string,\n            values=None,\n            step=step,\n            level=level,\n            wall_time=wall_time,\n        )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.PythonLogger.logger","title":"<code>logger</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Logger</code> <p>a metrics instance to log to, if None then will create it's own</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.PythonLogger.log_string","title":"<code>log_string(tag, string, step, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Optional[str]</code> <p>identifying tag to log the values with</p> required <code>string</code> <code>Optional[str]</code> <p>string to log</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> required <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken, defaults to time.time()</p> <code>None</code> <code>level</code> <code>Optional[int]</code> <p>level to log at. Corresponds to default logging package levels</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: Optional[str],\n    string: Optional[str],\n    step: Optional[int],\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param string: string to log\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken,\n        defaults to time.time()\n    :param level: level to log at. Corresponds to default logging package levels\n    :return: True if logged, False otherwise.\n    \"\"\"\n    if not wall_time:\n        wall_time = time.time()\n\n    return self._lambda_func(\n        tag=tag,\n        value=string,\n        values=None,\n        step=step,\n        level=level,\n        wall_time=wall_time,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SparsificationGroupLogger","title":"<code>SparsificationGroupLogger</code>","text":"<p>               Bases: <code>BaseLogger</code></p> <p>Modifier metrics that handles outputting values to other supported systems. Supported ones include:   - Python logging   - Tensorboard   - Weights and Biases   - Lambda callback</p> <p>All are optional and can be bulk disabled and enabled by this root.</p> <p>Parameters:</p> Name Type Description Default <code>lambda_func</code> <code>Optional[Callable[[Optional[str], Optional[float], Optional[Dict[str, float]], Optional[int], Optional[float]], bool]]</code> <p>an optional lambda function to call back into with any logs. The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool The return type is True if logged and False otherwise.</p> <code>None</code> <code>python</code> <code>Optional[Union[bool, Logger]]</code> <p>an optional argument for logging to a python metrics. May be a logging.Logger instance to log to, True to create a metrics instance, or non truthy to not log anything (False, None)</p> <code>None</code> <code>python_log_level</code> <code>int</code> <p>if python, the level to log any incoming data at on the logging.Logger instance</p> <code>INFO</code> <code>tensorboard</code> <code>Optional[Union[bool, str, SummaryWriter]]</code> <p>an optional argument for logging to a tensorboard writer. May be a SummaryWriter instance to log to, a string representing the directory to create a new SummaryWriter to log to, True to create a new SummaryWriter, or non truthy to not log anything (False, None)</p> <code>None</code> <code>wandb_</code> <code>Optional[Union[bool, Dict]]</code> <p>an optional argument for logging to wandb. May be a dictionary to pass to the init call for wandb, True to log to wandb (will not call init), or non truthy to not log anything (False, None)</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to sparsification</p> <code>'sparsification'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class SparsificationGroupLogger(BaseLogger):\n    \"\"\"\n    Modifier metrics that handles outputting values to other supported systems.\n    Supported ones include:\n      - Python logging\n      - Tensorboard\n      - Weights and Biases\n      - Lambda callback\n\n    All are optional and can be bulk disabled and enabled by this root.\n\n    :param lambda_func: an optional lambda function to call back into with any logs.\n        The expected call sequence is (tag, value, values, step, wall_time) -&gt; bool\n        The return type is True if logged and False otherwise.\n    :param python: an optional argument for logging to a python metrics.\n        May be a logging.Logger instance to log to, True to create a metrics instance,\n        or non truthy to not log anything (False, None)\n    :param python_log_level: if python,\n        the level to log any incoming data at on the logging.Logger instance\n    :param tensorboard: an optional argument for logging to a tensorboard writer.\n        May be a SummaryWriter instance to log to, a string representing the directory\n        to create a new SummaryWriter to log to, True to create a new SummaryWriter,\n        or non truthy to not log anything (False, None)\n    :param wandb_: an optional argument for logging to wandb.\n        May be a dictionary to pass to the init call for wandb,\n        True to log to wandb (will not call init),\n        or non truthy to not log anything (False, None)\n    :param name: name given to the metrics, used for identification;\n        defaults to sparsification\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        lambda_func: Optional[\n            Callable[\n                [\n                    Optional[str],\n                    Optional[float],\n                    Optional[Dict[str, float]],\n                    Optional[int],\n                    Optional[float],\n                ],\n                bool,\n            ]\n        ] = None,\n        python: Optional[Union[bool, Logger]] = None,\n        python_log_level: int = logging.INFO,\n        tensorboard: Optional[Union[bool, str, SummaryWriter]] = None,\n        wandb_: Optional[Union[bool, Dict]] = None,\n        name: str = \"sparsification\",\n        enabled: bool = True,\n    ):\n        super().__init__(name, enabled)\n        self._loggers: List[BaseLogger] = []\n\n        if lambda_func:\n            self._loggers.append(\n                LambdaLogger(lambda_func=lambda_func, name=name, enabled=enabled)\n            )\n\n        if python:\n            self._loggers.append(\n                PythonLogger(\n                    logger=python if isinstance(python, Logger) else None,\n                    log_level=python_log_level,\n                    name=name,\n                    enabled=enabled,\n                )\n            )\n\n        if tensorboard and TensorBoardLogger.available():\n            self._loggers.append(\n                TensorBoardLogger(\n                    log_path=tensorboard if isinstance(tensorboard, str) else None,\n                    writer=(\n                        tensorboard if isinstance(tensorboard, SummaryWriter) else None\n                    ),\n                    name=name,\n                    enabled=enabled,\n                )\n            )\n\n        if wandb_ and WANDBLogger.available():\n            self._loggers.append(\n                WANDBLogger(\n                    init_kwargs=wandb_ if isinstance(wandb_, Dict) else None,\n                    name=name,\n                    enabled=enabled,\n                )\n            )\n\n    @BaseLogger.enabled.setter\n    def enabled(self, value: bool):\n        \"\"\"\n        :param value: True to log, False otherwise\n        \"\"\"\n        self._enabled = value\n\n        for logger in self._loggers:\n            logger.enabled = value\n\n    @property\n    def loggers(self) -&gt; List[BaseLogger]:\n        \"\"\"\n        :return: the created metrics sub instances for this metrics\n        \"\"\"\n        return self._loggers\n\n    def log_hyperparams(self, params: Dict, level: Optional[int] = None):\n        \"\"\"\n        :param params: Each key-value pair in the dictionary is the name of the\n            hyper parameter and it's corresponding value.\n        \"\"\"\n        for logger in self._loggers:\n            logger.log_hyperparams(params, level)\n\n    def log_scalar(\n        self,\n        tag: str,\n        value: float,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the value with\n        :param value: value to save\n        :param step: global step for when the value was taken\n        :param wall_time: global wall time for when the value was taken,\n            defaults to time.time()\n        \"\"\"\n        for logger in self._loggers:\n            logger.log_scalar(tag, value, step, wall_time, level)\n\n    def log_scalars(\n        self,\n        tag: str,\n        values: Dict[str, float],\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken,\n            defaults to time.time()\n        \"\"\"\n        for logger in self._loggers:\n            logger.log_scalars(tag, values, step, wall_time, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SparsificationGroupLogger.loggers","title":"<code>loggers</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>List[BaseLogger]</code> <p>the created metrics sub instances for this metrics</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SparsificationGroupLogger.enabled","title":"<code>enabled(value)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>True to log, False otherwise</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@BaseLogger.enabled.setter\ndef enabled(self, value: bool):\n    \"\"\"\n    :param value: True to log, False otherwise\n    \"\"\"\n    self._enabled = value\n\n    for logger in self._loggers:\n        logger.enabled = value\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SparsificationGroupLogger.log_hyperparams","title":"<code>log_hyperparams(params, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict</code> <p>Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value.</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_hyperparams(self, params: Dict, level: Optional[int] = None):\n    \"\"\"\n    :param params: Each key-value pair in the dictionary is the name of the\n        hyper parameter and it's corresponding value.\n    \"\"\"\n    for logger in self._loggers:\n        logger.log_hyperparams(params, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SparsificationGroupLogger.log_scalar","title":"<code>log_scalar(tag, value, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the value with</p> required <code>value</code> <code>float</code> <p>value to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the value was taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the value was taken, defaults to time.time()</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalar(\n    self,\n    tag: str,\n    value: float,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the value with\n    :param value: value to save\n    :param step: global step for when the value was taken\n    :param wall_time: global wall time for when the value was taken,\n        defaults to time.time()\n    \"\"\"\n    for logger in self._loggers:\n        logger.log_scalar(tag, value, step, wall_time, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SparsificationGroupLogger.log_scalars","title":"<code>log_scalars(tag, values, step=None, wall_time=None, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <code>Dict[str, float]</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken, defaults to time.time()</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_scalars(\n    self,\n    tag: str,\n    values: Dict[str, float],\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken,\n        defaults to time.time()\n    \"\"\"\n    for logger in self._loggers:\n        logger.log_scalars(tag, values, step, wall_time, level)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper","title":"<code>SystemLoggingWraper</code>","text":"<p>               Bases: <code>LoggingWrapperBase</code></p> <p>Wraps utilities and convenience methods for logging strings to the system</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class SystemLoggingWraper(LoggingWrapperBase):\n    \"\"\"\n    Wraps utilities and convenience methods for logging strings to the system\n    \"\"\"\n\n    def log_string(\n        self,\n        tag: str,\n        string: str,\n        step: Optional[int] = None,\n        wall_time: Optional[float] = None,\n        log_types: Union[str, List[str]] = ALL_TOKEN,\n        level: Optional[int] = None,\n    ):\n        \"\"\"\n        :param tag: identifying tag to log the values with\n        :param values: values to save\n        :param step: global step for when the values were taken\n        :param wall_time: global wall time for when the values were taken\n        :param kwargs: additional logging arguments to support Python and custom loggers\n        :return: True if logged, False otherwise.\n        \"\"\"\n        for log in self.loggers:\n            if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n                log.log_string(\n                    tag=tag,\n                    string=string,\n                    step=step,\n                    wall_time=wall_time,\n                    level=level,\n                )\n\n    def debug(self, tag, string, *args, **kwargs):\n        \"\"\"\n        logs a string message with level DEBUG on all\n        loggers that are enabled\n\n        :param tag: Identifying tag to log the string with\n        :param string: The string to log\n        :param args: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        :param kwargs: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        \"\"\"\n        kwargs[\"level\"] = logging.DEBUG\n        self.log_string(tag=tag, string=string, *args, **kwargs)\n\n    def info(self, tag, string, *args, **kwargs):\n        \"\"\"\n        logs a string message with level INFO on all\n        loggers that are enabled\n\n        :param tag: Identifying tag to log the string with\n        :param string: The string to log\n        :param args: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        :param kwargs: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        \"\"\"\n        kwargs[\"level\"] = logging.INFO\n        self.log_string(tag=tag, string=string, *args, **kwargs)\n\n    def warning(self, tag, string, *args, **kwargs):\n        \"\"\"\n        logs a string message with level WARNING on all\n        loggers that are enabled\n\n        :param tag: Identifying tag to log the string with\n        :param string: The string to log\n        :param args: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        :param kwargs: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        \"\"\"\n        kwargs[\"level\"] = logging.WARNING\n        self.log_string(tag=tag, string=string, *args, **kwargs)\n\n    def warn(self, tag, string, *args, **kwargs):\n        warnings.warn(\n            \"The 'warn' method is deprecated, use 'warning' instead\",\n            DeprecationWarning,\n            2,\n        )\n        self.warning(tag=tag, string=string, *args, **kwargs)\n\n    def error(self, tag, string, *args, **kwargs):\n        \"\"\"\n        logs a string message with level ERROR on all\n        loggers that are enabled\n\n        :param tag: Identifying tag to log the string with\n        :param string: The string to log\n        :param args: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        :param kwargs: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        \"\"\"\n        kwargs[\"level\"] = logging.ERROR\n        self.log_string(tag=tag, string=string, *args, **kwargs)\n\n    def critical(self, tag, string, *args, **kwargs):\n        \"\"\"\n        logs a string message with level CRITICAL on all\n        loggers that are enabled\n\n        :param tag: Identifying tag to log the string with\n        :param string: The string to log\n        :param args: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        :param kwargs: additional arguments to pass to the metrics,\n            see `log_string` for more details\n        \"\"\"\n        kwargs[\"level\"] = logging.CRITICAL\n        self.log_string(tag=tag, string=string, *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper.critical","title":"<code>critical(tag, string, *args, **kwargs)</code>","text":"<p>logs a string message with level CRITICAL on all loggers that are enabled</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <p>Identifying tag to log the string with</p> required <code>string</code> <p>The string to log</p> required <code>args</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>()</code> <code>kwargs</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def critical(self, tag, string, *args, **kwargs):\n    \"\"\"\n    logs a string message with level CRITICAL on all\n    loggers that are enabled\n\n    :param tag: Identifying tag to log the string with\n    :param string: The string to log\n    :param args: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    :param kwargs: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    \"\"\"\n    kwargs[\"level\"] = logging.CRITICAL\n    self.log_string(tag=tag, string=string, *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper.debug","title":"<code>debug(tag, string, *args, **kwargs)</code>","text":"<p>logs a string message with level DEBUG on all loggers that are enabled</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <p>Identifying tag to log the string with</p> required <code>string</code> <p>The string to log</p> required <code>args</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>()</code> <code>kwargs</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def debug(self, tag, string, *args, **kwargs):\n    \"\"\"\n    logs a string message with level DEBUG on all\n    loggers that are enabled\n\n    :param tag: Identifying tag to log the string with\n    :param string: The string to log\n    :param args: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    :param kwargs: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    \"\"\"\n    kwargs[\"level\"] = logging.DEBUG\n    self.log_string(tag=tag, string=string, *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper.error","title":"<code>error(tag, string, *args, **kwargs)</code>","text":"<p>logs a string message with level ERROR on all loggers that are enabled</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <p>Identifying tag to log the string with</p> required <code>string</code> <p>The string to log</p> required <code>args</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>()</code> <code>kwargs</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def error(self, tag, string, *args, **kwargs):\n    \"\"\"\n    logs a string message with level ERROR on all\n    loggers that are enabled\n\n    :param tag: Identifying tag to log the string with\n    :param string: The string to log\n    :param args: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    :param kwargs: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    \"\"\"\n    kwargs[\"level\"] = logging.ERROR\n    self.log_string(tag=tag, string=string, *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper.info","title":"<code>info(tag, string, *args, **kwargs)</code>","text":"<p>logs a string message with level INFO on all loggers that are enabled</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <p>Identifying tag to log the string with</p> required <code>string</code> <p>The string to log</p> required <code>args</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>()</code> <code>kwargs</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def info(self, tag, string, *args, **kwargs):\n    \"\"\"\n    logs a string message with level INFO on all\n    loggers that are enabled\n\n    :param tag: Identifying tag to log the string with\n    :param string: The string to log\n    :param args: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    :param kwargs: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    \"\"\"\n    kwargs[\"level\"] = logging.INFO\n    self.log_string(tag=tag, string=string, *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper.log_string","title":"<code>log_string(tag, string, step=None, wall_time=None, log_types=ALL_TOKEN, level=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>identifying tag to log the values with</p> required <code>values</code> <p>values to save</p> required <code>step</code> <code>Optional[int]</code> <p>global step for when the values were taken</p> <code>None</code> <code>wall_time</code> <code>Optional[float]</code> <p>global wall time for when the values were taken</p> <code>None</code> <code>kwargs</code> <p>additional logging arguments to support Python and custom loggers</p> required <p>Returns:</p> Type Description <p>True if logged, False otherwise.</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def log_string(\n    self,\n    tag: str,\n    string: str,\n    step: Optional[int] = None,\n    wall_time: Optional[float] = None,\n    log_types: Union[str, List[str]] = ALL_TOKEN,\n    level: Optional[int] = None,\n):\n    \"\"\"\n    :param tag: identifying tag to log the values with\n    :param values: values to save\n    :param step: global step for when the values were taken\n    :param wall_time: global wall time for when the values were taken\n    :param kwargs: additional logging arguments to support Python and custom loggers\n    :return: True if logged, False otherwise.\n    \"\"\"\n    for log in self.loggers:\n        if log.enabled and (log_types == ALL_TOKEN or log.name in log_types):\n            log.log_string(\n                tag=tag,\n                string=string,\n                step=step,\n                wall_time=wall_time,\n                level=level,\n            )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.SystemLoggingWraper.warning","title":"<code>warning(tag, string, *args, **kwargs)</code>","text":"<p>logs a string message with level WARNING on all loggers that are enabled</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <p>Identifying tag to log the string with</p> required <code>string</code> <p>The string to log</p> required <code>args</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>()</code> <code>kwargs</code> <p>additional arguments to pass to the metrics, see <code>log_string</code> for more details</p> <code>{}</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def warning(self, tag, string, *args, **kwargs):\n    \"\"\"\n    logs a string message with level WARNING on all\n    loggers that are enabled\n\n    :param tag: Identifying tag to log the string with\n    :param string: The string to log\n    :param args: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    :param kwargs: additional arguments to pass to the metrics,\n        see `log_string` for more details\n    \"\"\"\n    kwargs[\"level\"] = logging.WARNING\n    self.log_string(tag=tag, string=string, *args, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.TensorBoardLogger","title":"<code>TensorBoardLogger</code>","text":"<p>               Bases: <code>LambdaLogger</code></p> <p>Modifier metrics that handles outputting values into a TensorBoard log directory for viewing in TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>log_path</code> <code>str</code> <p>the path to create a SummaryWriter at. writer must be None to use if not supplied (and writer is None), will create a TensorBoard dir in cwd</p> <code>None</code> <code>writer</code> <code>SummaryWriter</code> <p>the writer to log results to, if none is given creates a new one at the log_path</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to tensorboard</p> <code>'tensorboard'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class TensorBoardLogger(LambdaLogger):\n    \"\"\"\n    Modifier metrics that handles outputting values into a TensorBoard log directory\n    for viewing in TensorBoard.\n\n    :param log_path: the path to create a SummaryWriter at. writer must be None\n        to use if not supplied (and writer is None),\n        will create a TensorBoard dir in cwd\n    :param writer: the writer to log results to,\n        if none is given creates a new one at the log_path\n    :param name: name given to the metrics, used for identification;\n        defaults to tensorboard\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    def __init__(\n        self,\n        log_path: str = None,\n        writer: SummaryWriter = None,\n        name: str = \"tensorboard\",\n        enabled: bool = True,\n    ):\n        if tensorboard_import_error:\n            raise tensorboard_import_error\n\n        if writer and log_path:\n            raise ValueError(\n                (\n                    \"log_path given:{} and writer object passed in, \"\n                    \"to create a writer at the log path set writer=None\"\n                ).format(log_path)\n            )\n        elif not writer and not log_path:\n            log_path = os.path.join(\"\", \"tensorboard\")\n\n        if os.environ.get(\"NM_TEST_MODE\"):\n            test_log_root = os.environ.get(\"NM_TEST_LOG_DIR\")\n            log_path = (\n                os.path.join(test_log_root, log_path) if log_path else test_log_root\n            )\n\n        if log_path:\n            _create_dirs(log_path)\n\n        self._writer = writer if writer is not None else SummaryWriter(log_path)\n        super().__init__(\n            lambda_func=self._log_lambda,\n            name=name,\n            enabled=enabled,\n        )\n\n    @staticmethod\n    def available() -&gt; bool:\n        \"\"\"\n        :return: True if tensorboard is available and installed, False, otherwise\n        \"\"\"\n        return not tensorboard_import_error\n\n    @property\n    def writer(self) -&gt; SummaryWriter:\n        \"\"\"\n        :return: the writer to log results to,\n            if none is given creates a new one at the log_path\n        \"\"\"\n        return self._writer\n\n    def _log_lambda(\n        self,\n        tag: Optional[str],\n        value: Optional[float],\n        values: Optional[Dict[str, float]],\n        step: Optional[int],\n        wall_time: Optional[float],\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        if value is not None:\n            self._writer.add_scalar(tag, value, step, wall_time)\n\n        if values and tag:\n            self._writer.add_scalars(tag, values, step, wall_time)\n        elif values:\n            for name, val in values.items():\n                # hyperparameters logging case\n                self._writer.add_scalar(name, val, step, wall_time)\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.TensorBoardLogger.writer","title":"<code>writer</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>SummaryWriter</code> <p>the writer to log results to, if none is given creates a new one at the log_path</p>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.TensorBoardLogger.available","title":"<code>available()</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if tensorboard is available and installed, False, otherwise</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@staticmethod\ndef available() -&gt; bool:\n    \"\"\"\n    :return: True if tensorboard is available and installed, False, otherwise\n    \"\"\"\n    return not tensorboard_import_error\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.WANDBLogger","title":"<code>WANDBLogger</code>","text":"<p>               Bases: <code>LambdaLogger</code></p> <p>Modifier metrics that handles outputting values to Weights and Biases.</p> <p>Parameters:</p> Name Type Description Default <code>init_kwargs</code> <code>Optional[Dict]</code> <p>the args to call into wandb.init with; ex: wandb.init(**init_kwargs). If not supplied, then init will not be called</p> <code>None</code> <code>name</code> <code>str</code> <p>name given to the metrics, used for identification; defaults to wandb</p> <code>'wandb'</code> <code>enabled</code> <code>bool</code> <p>True to log, False otherwise</p> <code>True</code> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>class WANDBLogger(LambdaLogger):\n    \"\"\"\n    Modifier metrics that handles outputting values to Weights and Biases.\n\n    :param init_kwargs: the args to call into wandb.init with;\n        ex: wandb.init(**init_kwargs). If not supplied, then init will not be called\n    :param name: name given to the metrics, used for identification;\n        defaults to wandb\n    :param enabled: True to log, False otherwise\n    \"\"\"\n\n    @staticmethod\n    def available() -&gt; bool:\n        \"\"\"\n        :return: True if wandb is available and installed, False, otherwise\n        \"\"\"\n        return wandb_available\n\n    def __init__(\n        self,\n        init_kwargs: Optional[Dict] = None,\n        name: str = \"wandb\",\n        enabled: bool = True,\n        wandb_err: Optional[Exception] = wandb_err,\n    ):\n        if wandb_err:\n            raise wandb_err\n\n        super().__init__(\n            lambda_func=self._log_lambda,\n            name=name,\n            enabled=enabled,\n        )\n\n        if os.environ.get(\"NM_TEST_MODE\"):\n            test_log_path = os.environ.get(\"NM_TEST_LOG_DIR\")\n            _create_dirs(test_log_path)\n            if init_kwargs:\n                init_kwargs[\"dir\"] = test_log_path\n            else:\n                init_kwargs = {\"dir\": test_log_path}\n\n        if wandb_err:\n            raise wandb_err\n\n        if init_kwargs:\n            wandb.init(**init_kwargs)\n        else:\n            wandb.init()\n\n        self.wandb = wandb\n\n    def _log_lambda(\n        self,\n        tag: Optional[str],\n        value: Optional[float],\n        values: Optional[Dict[str, float]],\n        step: Optional[int],\n        wall_time: Optional[float],\n        level: Optional[int] = None,\n    ) -&gt; bool:\n        params = {}\n\n        if value:\n            params[tag] = value\n\n        if values:\n            if tag:\n                values = {f\"{tag}/{key}\": val for key, val in values.items()}\n            params.update(values)\n\n        params.update({\"Step\": step})\n        wandb.log(params)\n\n        return True\n\n    def save(\n        self,\n        file_path: str,\n    ) -&gt; bool:\n        \"\"\"\n        :param file_path: path to a file to be saved\n        \"\"\"\n        wandb.save(file_path)\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.WANDBLogger.available","title":"<code>available()</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if wandb is available and installed, False, otherwise</p> Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>@staticmethod\ndef available() -&gt; bool:\n    \"\"\"\n    :return: True if wandb is available and installed, False, otherwise\n    \"\"\"\n    return wandb_available\n</code></pre>"},{"location":"reference/llmcompressor/metrics/logger/#llmcompressor.metrics.logger.WANDBLogger.save","title":"<code>save(file_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a file to be saved</p> required Source code in <code>src/llmcompressor/metrics/logger.py</code> <pre><code>def save(\n    self,\n    file_path: str,\n) -&gt; bool:\n    \"\"\"\n    :param file_path: path to a file to be saved\n    \"\"\"\n    wandb.save(file_path)\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/","title":"llmcompressor.metrics.utils","text":""},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager","title":"<code>FrequencyManager</code>","text":"<p>Class for managing the frequency of logging and model updates</p> <p>Parameters:</p> Name Type Description Default <code>log_frequency</code> <code>LogStepType</code> <p>The frequency to log at</p> <code>None</code> <code>mode</code> <code>LoggingModeType</code> <p>The logging mode to use, either \"on_change\" or \"exact\", \"on_change\" will log when the model has been updated since the last log, \"exact\" will log at the given frequency regardless of model updates</p> <code>DEFAULT_LOGGING_MODE</code> <code>frequency_type</code> <code>FrequencyType</code> <p>The frequency type to use, either \"epoch\", \"step\", or \"batch\" controls what the frequency manager is tracking, e.g. if the frequency type is \"epoch\", then the frequency manager will track the number of epochs that have passed since the last log, if the frequency type is \"step\", then the frequency manager will track the number of optimizer steps</p> <code>DEFAULT_FREQUENCY_TYPE</code> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>class FrequencyManager:\n    \"\"\"\n    Class for managing the frequency of logging and model updates\n\n    :param log_frequency: The frequency to log at\n    :param mode: The logging mode to use, either \"on_change\" or \"exact\",\n        \"on_change\" will log when the model has been updated since the last log,\n        \"exact\" will log at the given frequency regardless of model updates\n    :param frequency_type: The frequency type to use, either \"epoch\", \"step\", or \"batch\"\n        controls what the frequency manager is tracking, e.g. if the frequency type\n        is \"epoch\", then the frequency manager will track the number of epochs that\n        have passed since the last log, if the frequency type is \"step\", then the\n        frequency manager will track the number of optimizer steps\n    \"\"\"\n\n    def __init__(\n        self,\n        log_frequency: LogStepType = None,\n        mode: LoggingModeType = DEFAULT_LOGGING_MODE,\n        frequency_type: FrequencyType = DEFAULT_FREQUENCY_TYPE,\n    ):\n        # sets self._logging_mode and self._check_model_update\n        self._logging_mode = self._set_logging_mode(mode=mode)\n\n        # sets self._frequency_type and self._valid_python_types\n        self.frequency_type = self._set_frequency_type(frequency_type=frequency_type)\n\n        self._validate_log_frequency(log_frequency=log_frequency)\n        self._log_frequency = log_frequency\n\n        self.last_log_step: LogStepType = None\n        self.last_model_update_step: LogStepType = None\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(log_frequency={self.log_frequency}, \"\n            f\"mode={self._logging_mode}, frequency_type={self.frequency_type})\"\n        )\n\n    def log_ready(\n        self,\n        current_log_step: LogStepType,\n        check_model_update: bool = False,\n    ):\n        \"\"\"\n        Check if the frequency manager is ready to log\n        Conditions for readiness:\n            - log frequency is not None\n            - current log step is None\n            - current log step greater than or equal to the last log step\n                plus the log frequency\n            - if check_model_update is True, or self._check_model_update is True,\n                then the last model update step must be greater than or equal\n                to the last log step, and the current log step must be greater\n                than or equal to the last model update step plus the log frequency\n\n        :param current_log_step: The current log step\n        :param check_model_update: If True, will check if the model has been updated\n            since the last log step and if _log_frequency steps have passed since the\n            last model update; Defaults to False.\n        :return: True if the frequency manager is ready to log,\n            False otherwise\n        \"\"\"\n        # check_model_update is used to override self._check_model_update\n        # e.g. if check_model_update is True, then the model update check\n        # will be performed even if self._check_model_update is False\n\n        check_model_update = check_model_update or self._check_model_update\n\n        return log_ready(\n            current_log_step=current_log_step,\n            last_log_step=self.last_log_step,\n            log_frequency=self.log_frequency,\n            last_model_update_step=self.last_model_update_step,\n            check_model_update=check_model_update,\n        )\n\n    def model_updated(self, step: LogStepType = None) -&gt; None:\n        \"\"\"\n        Sets the last model update to the given step\n\n        :param step: The step to set the last model update to\n        :post-cond: The last model update step is set to the given step\n        \"\"\"\n        self._validate_log_step(log_step=step)\n        self.last_model_update_step = step\n\n    def log_written(self, step: LogStepType = None) -&gt; None:\n        \"\"\"\n        Sets the last log step to the given step\n\n        :param step: The step to set the last log step to\n        :post-cond: The last log step is set to the given step\n        \"\"\"\n        self._validate_log_step(log_step=step)\n        self.last_log_step = step\n\n    @property\n    def log_frequency(self) -&gt; LogStepType:\n        \"\"\"\n        :return: The log frequency\n        \"\"\"\n        return self._log_frequency\n\n    @log_frequency.setter\n    def log_frequency(self, log_frequency: LogStepType) -&gt; None:\n        \"\"\"\n        Sets the log frequency to the given value\n\n        :param log_frequency: The log frequency to set\n        :post-cond: The log frequency is set to the given value\n        \"\"\"\n        self._validate_log_frequency(log_frequency=log_frequency)\n        self._log_frequency = log_frequency\n\n    @property\n    def is_optim_frequency_manager(self) -&gt; bool:\n        \"\"\"\n        :return: True if the frequency manager is tracking optimizer steps,\n            False otherwise\n        \"\"\"\n        return self.frequency_type == \"step\"\n\n    @property\n    def is_epoch_frequency_manager(self) -&gt; bool:\n        \"\"\"\n        :return: True if the frequency manager is tracking epochs,\n            False otherwise\n        \"\"\"\n        return self.frequency_type == \"epoch\"\n\n    def _validate_log_frequency(self, log_frequency):\n        # checks that log frequency is a positive number or None\n        # raise TypeError if not a number or None\n        # raises ValueError if not a positive number\n\n        try:\n            self._validate_log_step(log_step=log_frequency)\n            if log_frequency == 0:\n                raise ValueError()\n            # except clauses update the error message\n        except TypeError:\n            raise TypeError(\n                f\"log frequency must be a number or None, given {type(log_frequency)}\"\n            )\n        except ValueError:\n            raise ValueError(\n                f\"log frequency must be greater than 0, given {log_frequency}\"\n            )\n\n    def _validate_log_step(self, log_step):\n        # checks that log step is a non negative number or None\n        # raise TypeError if not a number or None\n        # raises ValueError if negative number\n\n        if not isinstance(log_step, self._valid_python_types) or isinstance(\n            log_step, bool\n        ):\n            raise TypeError(\n                f\"log step must be a number or None, given {type(log_step)}\"\n            )\n\n        if log_step is not None and log_step &lt; 0:\n            raise ValueError(\n                f\"log step must be greater than or equal to 0, given {log_step}\"\n            )\n\n    def _set_logging_mode(self, mode: LoggingModeType) -&gt; LoggingModeType:\n        \"\"\"\n        Set the logging mode for the frequency manager.\n        The logging mode determines how the frequency manager determines\n        if it is ready to log\n\n        :param mode: The logging mode to set\n        :post-cond: The self._logging_mode is set to the given mode\n        :post-cond: The self._check_model_update is set to True if the mode is\n            \"on_change\"\n        :raises ValueError: If the given mode is not one of \"on_change\" or \"exact\"\n        :return: The logging mode that was set\n        \"\"\"\n        mode = _basic_normalization(mode)\n        if mode == \"on_change\":\n            self._check_model_update = True\n            self._logging_mode = \"on_change\"\n        elif mode == \"exact\":\n            self._check_model_update = False\n            self._logging_mode = \"exact\"\n        else:\n            raise ValueError(\n                f\"Invalid logging mode {mode}, must be one of 'on_change', 'exact'\"\n            )\n        return self._logging_mode\n\n    def _set_frequency_type(self, frequency_type: FrequencyType) -&gt; FrequencyType:\n        \"\"\"\n        Set the frequency type for the frequency manager.\n        The frequency type determines what the frequency manager is tracking.\n        For example, if the frequency type is \"epoch\", then the frequency manager\n        will track the number of epochs that have passed since the last log.\n\n        :param frequency_type: The frequency type to set\n        :post-cond: The self._frequency_type is set to the given frequency type\n        :post-cond: The self._valid_python_types is set to the valid python types\n            for the given frequency type, e.g. (int, float, type(None)) for \"epoch\"\n            and (int, type(None)) for \"step\" or \"batch\"\n        :raises ValueError: If the given frequency type is not one of \"epoch\",\n            \"step\"\n        :raises NotImplementedError: If the given frequency type is \"batch\"\n        :return: The frequency type that was set\n        \"\"\"\n        frequency_type = _basic_normalization(frequency_type)\n        if frequency_type == \"epoch\":\n            self.frequency_type = \"epoch\"\n            self._valid_python_types = (int, float, type(None))\n        elif frequency_type == \"step\":\n            self.frequency_type = \"step\"\n            self._valid_python_types = (int, type(None))\n        elif frequency_type == \"batch\":\n            raise NotImplementedError\n        else:\n            raise ValueError(\n                f\"Invalid frequency type {frequency_type}, must be one of \"\n                \"'epoch', 'step'\"\n            )\n        return self.frequency_type\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager.is_epoch_frequency_manager","title":"<code>is_epoch_frequency_manager</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the frequency manager is tracking epochs, False otherwise</p>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager.is_optim_frequency_manager","title":"<code>is_optim_frequency_manager</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the frequency manager is tracking optimizer steps, False otherwise</p>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager.log_frequency","title":"<code>log_frequency</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>LogStepType</code> <p>The log frequency</p>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager.log_ready","title":"<code>log_ready(current_log_step, check_model_update=False)</code>","text":"<p>Check if the frequency manager is ready to log Conditions for readiness:     - log frequency is not None     - current log step is None     - current log step greater than or equal to the last log step         plus the log frequency     - if check_model_update is True, or self._check_model_update is True,         then the last model update step must be greater than or equal         to the last log step, and the current log step must be greater         than or equal to the last model update step plus the log frequency</p> <p>Parameters:</p> Name Type Description Default <code>current_log_step</code> <code>LogStepType</code> <p>The current log step</p> required <code>check_model_update</code> <code>bool</code> <p>If True, will check if the model has been updated since the last log step and if _log_frequency steps have passed since the last model update; Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>True if the frequency manager is ready to log, False otherwise</p> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def log_ready(\n    self,\n    current_log_step: LogStepType,\n    check_model_update: bool = False,\n):\n    \"\"\"\n    Check if the frequency manager is ready to log\n    Conditions for readiness:\n        - log frequency is not None\n        - current log step is None\n        - current log step greater than or equal to the last log step\n            plus the log frequency\n        - if check_model_update is True, or self._check_model_update is True,\n            then the last model update step must be greater than or equal\n            to the last log step, and the current log step must be greater\n            than or equal to the last model update step plus the log frequency\n\n    :param current_log_step: The current log step\n    :param check_model_update: If True, will check if the model has been updated\n        since the last log step and if _log_frequency steps have passed since the\n        last model update; Defaults to False.\n    :return: True if the frequency manager is ready to log,\n        False otherwise\n    \"\"\"\n    # check_model_update is used to override self._check_model_update\n    # e.g. if check_model_update is True, then the model update check\n    # will be performed even if self._check_model_update is False\n\n    check_model_update = check_model_update or self._check_model_update\n\n    return log_ready(\n        current_log_step=current_log_step,\n        last_log_step=self.last_log_step,\n        log_frequency=self.log_frequency,\n        last_model_update_step=self.last_model_update_step,\n        check_model_update=check_model_update,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager.log_written","title":"<code>log_written(step=None)</code>","text":"<p>Sets the last log step to the given step</p> <p>:post-cond: The last log step is set to the given step</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>The step to set the last log step to</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def log_written(self, step: LogStepType = None) -&gt; None:\n    \"\"\"\n    Sets the last log step to the given step\n\n    :param step: The step to set the last log step to\n    :post-cond: The last log step is set to the given step\n    \"\"\"\n    self._validate_log_step(log_step=step)\n    self.last_log_step = step\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.FrequencyManager.model_updated","title":"<code>model_updated(step=None)</code>","text":"<p>Sets the last model update to the given step</p> <p>:post-cond: The last model update step is set to the given step</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>The step to set the last model update to</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def model_updated(self, step: LogStepType = None) -&gt; None:\n    \"\"\"\n    Sets the last model update to the given step\n\n    :param step: The step to set the last model update to\n    :post-cond: The last model update step is set to the given step\n    \"\"\"\n    self._validate_log_step(log_step=step)\n    self.last_model_update_step = step\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/#llmcompressor.metrics.utils.log_ready","title":"<code>log_ready(current_log_step, last_log_step, log_frequency, last_model_update_step=None, check_model_update=False)</code>","text":"<p>Check if we are ready to log again based on the given parameters (Stateless version of FrequencyManager().log_ready)</p> <p>Conditions for readiness:     - log frequency is not None     - current log step is None     - current log step greater than or equal to the last log step         plus the log frequency     - if check_model_update is True, then the last model update step         must be greater than or equal to the last log step, and the         current log step must be greater than or equal to the         last model update step plus the log frequency</p> <p>Parameters:</p> Name Type Description Default <code>current_log_step</code> <code>Optional[LogStepType]</code> <p>The current log step</p> required <code>last_log_step</code> <code>Optional[LogStepType]</code> <p>The last step at which logging occurred</p> required <code>log_frequency</code> <code>Optional[LogStepType]</code> <p>The frequency to log at</p> required <code>last_model_update_step</code> <code>Optional[LogStepType]</code> <p>The last step at which the model was updated</p> <code>None</code> <code>check_model_update</code> <code>bool</code> <p>If True, will check if the model has been updated since the last log step and if log_frequency steps have passed since the last model update; Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>True if logging cadence has been reached again False otherwise</p> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def log_ready(\n    current_log_step: Optional[LogStepType],\n    last_log_step: Optional[LogStepType],\n    log_frequency: Optional[LogStepType],\n    last_model_update_step: Optional[LogStepType] = None,\n    check_model_update: bool = False,\n):\n    \"\"\"\n    Check if we are ready to log again based on the given parameters\n    (Stateless version of FrequencyManager().log_ready)\n\n    Conditions for readiness:\n        - log frequency is not None\n        - current log step is None\n        - current log step greater than or equal to the last log step\n            plus the log frequency\n        - if check_model_update is True, then the last model update step\n            must be greater than or equal to the last log step, and the\n            current log step must be greater than or equal to the\n            last model update step plus the log frequency\n\n    :param current_log_step: The current log step\n    :param last_log_step: The last step at which logging occurred\n    :param log_frequency: The frequency to log at\n    :param last_model_update_step: The last step at which the model was updated\n    :param check_model_update: If True, will check if the model has been updated\n        since the last log step and if log_frequency steps have passed since the\n        last model update; Defaults to False.\n    :return: True if logging cadence has been reached again False otherwise\n    \"\"\"\n    # format is used to avoid floating point errors\n    # e.g. 0.1 + 0.2 != 0.3\n    # format(0.1 + 0.2, \".4f\") == format(0.3, \".4f\")\n\n    cadence_reached: bool = log_frequency is not None and (\n        current_log_step is None\n        or last_log_step is None\n        or current_log_step &gt;= float(format(last_log_step + log_frequency, \".4f\"))\n    )\n\n    if not cadence_reached or not check_model_update:\n        # early return if cadence not reached or,\n        # model update check not requested\n        return cadence_reached\n\n    model_updated_since_last_log: bool = (\n        last_model_update_step is None\n        or last_log_step is None\n        or current_log_step is None\n        or (\n            last_model_update_step &gt;= last_log_step\n            and current_log_step\n            &gt;= float(format(log_frequency + last_model_update_step, \".4f\"))\n        )\n    )\n\n    return cadence_reached and model_updated_since_last_log\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/","title":"llmcompressor.metrics.utils.frequency_manager","text":""},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager","title":"<code>FrequencyManager</code>","text":"<p>Class for managing the frequency of logging and model updates</p> <p>Parameters:</p> Name Type Description Default <code>log_frequency</code> <code>LogStepType</code> <p>The frequency to log at</p> <code>None</code> <code>mode</code> <code>LoggingModeType</code> <p>The logging mode to use, either \"on_change\" or \"exact\", \"on_change\" will log when the model has been updated since the last log, \"exact\" will log at the given frequency regardless of model updates</p> <code>DEFAULT_LOGGING_MODE</code> <code>frequency_type</code> <code>FrequencyType</code> <p>The frequency type to use, either \"epoch\", \"step\", or \"batch\" controls what the frequency manager is tracking, e.g. if the frequency type is \"epoch\", then the frequency manager will track the number of epochs that have passed since the last log, if the frequency type is \"step\", then the frequency manager will track the number of optimizer steps</p> <code>DEFAULT_FREQUENCY_TYPE</code> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>class FrequencyManager:\n    \"\"\"\n    Class for managing the frequency of logging and model updates\n\n    :param log_frequency: The frequency to log at\n    :param mode: The logging mode to use, either \"on_change\" or \"exact\",\n        \"on_change\" will log when the model has been updated since the last log,\n        \"exact\" will log at the given frequency regardless of model updates\n    :param frequency_type: The frequency type to use, either \"epoch\", \"step\", or \"batch\"\n        controls what the frequency manager is tracking, e.g. if the frequency type\n        is \"epoch\", then the frequency manager will track the number of epochs that\n        have passed since the last log, if the frequency type is \"step\", then the\n        frequency manager will track the number of optimizer steps\n    \"\"\"\n\n    def __init__(\n        self,\n        log_frequency: LogStepType = None,\n        mode: LoggingModeType = DEFAULT_LOGGING_MODE,\n        frequency_type: FrequencyType = DEFAULT_FREQUENCY_TYPE,\n    ):\n        # sets self._logging_mode and self._check_model_update\n        self._logging_mode = self._set_logging_mode(mode=mode)\n\n        # sets self._frequency_type and self._valid_python_types\n        self.frequency_type = self._set_frequency_type(frequency_type=frequency_type)\n\n        self._validate_log_frequency(log_frequency=log_frequency)\n        self._log_frequency = log_frequency\n\n        self.last_log_step: LogStepType = None\n        self.last_model_update_step: LogStepType = None\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(log_frequency={self.log_frequency}, \"\n            f\"mode={self._logging_mode}, frequency_type={self.frequency_type})\"\n        )\n\n    def log_ready(\n        self,\n        current_log_step: LogStepType,\n        check_model_update: bool = False,\n    ):\n        \"\"\"\n        Check if the frequency manager is ready to log\n        Conditions for readiness:\n            - log frequency is not None\n            - current log step is None\n            - current log step greater than or equal to the last log step\n                plus the log frequency\n            - if check_model_update is True, or self._check_model_update is True,\n                then the last model update step must be greater than or equal\n                to the last log step, and the current log step must be greater\n                than or equal to the last model update step plus the log frequency\n\n        :param current_log_step: The current log step\n        :param check_model_update: If True, will check if the model has been updated\n            since the last log step and if _log_frequency steps have passed since the\n            last model update; Defaults to False.\n        :return: True if the frequency manager is ready to log,\n            False otherwise\n        \"\"\"\n        # check_model_update is used to override self._check_model_update\n        # e.g. if check_model_update is True, then the model update check\n        # will be performed even if self._check_model_update is False\n\n        check_model_update = check_model_update or self._check_model_update\n\n        return log_ready(\n            current_log_step=current_log_step,\n            last_log_step=self.last_log_step,\n            log_frequency=self.log_frequency,\n            last_model_update_step=self.last_model_update_step,\n            check_model_update=check_model_update,\n        )\n\n    def model_updated(self, step: LogStepType = None) -&gt; None:\n        \"\"\"\n        Sets the last model update to the given step\n\n        :param step: The step to set the last model update to\n        :post-cond: The last model update step is set to the given step\n        \"\"\"\n        self._validate_log_step(log_step=step)\n        self.last_model_update_step = step\n\n    def log_written(self, step: LogStepType = None) -&gt; None:\n        \"\"\"\n        Sets the last log step to the given step\n\n        :param step: The step to set the last log step to\n        :post-cond: The last log step is set to the given step\n        \"\"\"\n        self._validate_log_step(log_step=step)\n        self.last_log_step = step\n\n    @property\n    def log_frequency(self) -&gt; LogStepType:\n        \"\"\"\n        :return: The log frequency\n        \"\"\"\n        return self._log_frequency\n\n    @log_frequency.setter\n    def log_frequency(self, log_frequency: LogStepType) -&gt; None:\n        \"\"\"\n        Sets the log frequency to the given value\n\n        :param log_frequency: The log frequency to set\n        :post-cond: The log frequency is set to the given value\n        \"\"\"\n        self._validate_log_frequency(log_frequency=log_frequency)\n        self._log_frequency = log_frequency\n\n    @property\n    def is_optim_frequency_manager(self) -&gt; bool:\n        \"\"\"\n        :return: True if the frequency manager is tracking optimizer steps,\n            False otherwise\n        \"\"\"\n        return self.frequency_type == \"step\"\n\n    @property\n    def is_epoch_frequency_manager(self) -&gt; bool:\n        \"\"\"\n        :return: True if the frequency manager is tracking epochs,\n            False otherwise\n        \"\"\"\n        return self.frequency_type == \"epoch\"\n\n    def _validate_log_frequency(self, log_frequency):\n        # checks that log frequency is a positive number or None\n        # raise TypeError if not a number or None\n        # raises ValueError if not a positive number\n\n        try:\n            self._validate_log_step(log_step=log_frequency)\n            if log_frequency == 0:\n                raise ValueError()\n            # except clauses update the error message\n        except TypeError:\n            raise TypeError(\n                f\"log frequency must be a number or None, given {type(log_frequency)}\"\n            )\n        except ValueError:\n            raise ValueError(\n                f\"log frequency must be greater than 0, given {log_frequency}\"\n            )\n\n    def _validate_log_step(self, log_step):\n        # checks that log step is a non negative number or None\n        # raise TypeError if not a number or None\n        # raises ValueError if negative number\n\n        if not isinstance(log_step, self._valid_python_types) or isinstance(\n            log_step, bool\n        ):\n            raise TypeError(\n                f\"log step must be a number or None, given {type(log_step)}\"\n            )\n\n        if log_step is not None and log_step &lt; 0:\n            raise ValueError(\n                f\"log step must be greater than or equal to 0, given {log_step}\"\n            )\n\n    def _set_logging_mode(self, mode: LoggingModeType) -&gt; LoggingModeType:\n        \"\"\"\n        Set the logging mode for the frequency manager.\n        The logging mode determines how the frequency manager determines\n        if it is ready to log\n\n        :param mode: The logging mode to set\n        :post-cond: The self._logging_mode is set to the given mode\n        :post-cond: The self._check_model_update is set to True if the mode is\n            \"on_change\"\n        :raises ValueError: If the given mode is not one of \"on_change\" or \"exact\"\n        :return: The logging mode that was set\n        \"\"\"\n        mode = _basic_normalization(mode)\n        if mode == \"on_change\":\n            self._check_model_update = True\n            self._logging_mode = \"on_change\"\n        elif mode == \"exact\":\n            self._check_model_update = False\n            self._logging_mode = \"exact\"\n        else:\n            raise ValueError(\n                f\"Invalid logging mode {mode}, must be one of 'on_change', 'exact'\"\n            )\n        return self._logging_mode\n\n    def _set_frequency_type(self, frequency_type: FrequencyType) -&gt; FrequencyType:\n        \"\"\"\n        Set the frequency type for the frequency manager.\n        The frequency type determines what the frequency manager is tracking.\n        For example, if the frequency type is \"epoch\", then the frequency manager\n        will track the number of epochs that have passed since the last log.\n\n        :param frequency_type: The frequency type to set\n        :post-cond: The self._frequency_type is set to the given frequency type\n        :post-cond: The self._valid_python_types is set to the valid python types\n            for the given frequency type, e.g. (int, float, type(None)) for \"epoch\"\n            and (int, type(None)) for \"step\" or \"batch\"\n        :raises ValueError: If the given frequency type is not one of \"epoch\",\n            \"step\"\n        :raises NotImplementedError: If the given frequency type is \"batch\"\n        :return: The frequency type that was set\n        \"\"\"\n        frequency_type = _basic_normalization(frequency_type)\n        if frequency_type == \"epoch\":\n            self.frequency_type = \"epoch\"\n            self._valid_python_types = (int, float, type(None))\n        elif frequency_type == \"step\":\n            self.frequency_type = \"step\"\n            self._valid_python_types = (int, type(None))\n        elif frequency_type == \"batch\":\n            raise NotImplementedError\n        else:\n            raise ValueError(\n                f\"Invalid frequency type {frequency_type}, must be one of \"\n                \"'epoch', 'step'\"\n            )\n        return self.frequency_type\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager.is_epoch_frequency_manager","title":"<code>is_epoch_frequency_manager</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the frequency manager is tracking epochs, False otherwise</p>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager.is_optim_frequency_manager","title":"<code>is_optim_frequency_manager</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the frequency manager is tracking optimizer steps, False otherwise</p>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager.log_frequency","title":"<code>log_frequency</code>  <code>property</code> <code>writable</code>","text":"<p>Returns:</p> Type Description <code>LogStepType</code> <p>The log frequency</p>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager.log_ready","title":"<code>log_ready(current_log_step, check_model_update=False)</code>","text":"<p>Check if the frequency manager is ready to log Conditions for readiness:     - log frequency is not None     - current log step is None     - current log step greater than or equal to the last log step         plus the log frequency     - if check_model_update is True, or self._check_model_update is True,         then the last model update step must be greater than or equal         to the last log step, and the current log step must be greater         than or equal to the last model update step plus the log frequency</p> <p>Parameters:</p> Name Type Description Default <code>current_log_step</code> <code>LogStepType</code> <p>The current log step</p> required <code>check_model_update</code> <code>bool</code> <p>If True, will check if the model has been updated since the last log step and if _log_frequency steps have passed since the last model update; Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>True if the frequency manager is ready to log, False otherwise</p> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def log_ready(\n    self,\n    current_log_step: LogStepType,\n    check_model_update: bool = False,\n):\n    \"\"\"\n    Check if the frequency manager is ready to log\n    Conditions for readiness:\n        - log frequency is not None\n        - current log step is None\n        - current log step greater than or equal to the last log step\n            plus the log frequency\n        - if check_model_update is True, or self._check_model_update is True,\n            then the last model update step must be greater than or equal\n            to the last log step, and the current log step must be greater\n            than or equal to the last model update step plus the log frequency\n\n    :param current_log_step: The current log step\n    :param check_model_update: If True, will check if the model has been updated\n        since the last log step and if _log_frequency steps have passed since the\n        last model update; Defaults to False.\n    :return: True if the frequency manager is ready to log,\n        False otherwise\n    \"\"\"\n    # check_model_update is used to override self._check_model_update\n    # e.g. if check_model_update is True, then the model update check\n    # will be performed even if self._check_model_update is False\n\n    check_model_update = check_model_update or self._check_model_update\n\n    return log_ready(\n        current_log_step=current_log_step,\n        last_log_step=self.last_log_step,\n        log_frequency=self.log_frequency,\n        last_model_update_step=self.last_model_update_step,\n        check_model_update=check_model_update,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager.log_written","title":"<code>log_written(step=None)</code>","text":"<p>Sets the last log step to the given step</p> <p>:post-cond: The last log step is set to the given step</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>The step to set the last log step to</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def log_written(self, step: LogStepType = None) -&gt; None:\n    \"\"\"\n    Sets the last log step to the given step\n\n    :param step: The step to set the last log step to\n    :post-cond: The last log step is set to the given step\n    \"\"\"\n    self._validate_log_step(log_step=step)\n    self.last_log_step = step\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.FrequencyManager.model_updated","title":"<code>model_updated(step=None)</code>","text":"<p>Sets the last model update to the given step</p> <p>:post-cond: The last model update step is set to the given step</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>LogStepType</code> <p>The step to set the last model update to</p> <code>None</code> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def model_updated(self, step: LogStepType = None) -&gt; None:\n    \"\"\"\n    Sets the last model update to the given step\n\n    :param step: The step to set the last model update to\n    :post-cond: The last model update step is set to the given step\n    \"\"\"\n    self._validate_log_step(log_step=step)\n    self.last_model_update_step = step\n</code></pre>"},{"location":"reference/llmcompressor/metrics/utils/frequency_manager/#llmcompressor.metrics.utils.frequency_manager.log_ready","title":"<code>log_ready(current_log_step, last_log_step, log_frequency, last_model_update_step=None, check_model_update=False)</code>","text":"<p>Check if we are ready to log again based on the given parameters (Stateless version of FrequencyManager().log_ready)</p> <p>Conditions for readiness:     - log frequency is not None     - current log step is None     - current log step greater than or equal to the last log step         plus the log frequency     - if check_model_update is True, then the last model update step         must be greater than or equal to the last log step, and the         current log step must be greater than or equal to the         last model update step plus the log frequency</p> <p>Parameters:</p> Name Type Description Default <code>current_log_step</code> <code>Optional[LogStepType]</code> <p>The current log step</p> required <code>last_log_step</code> <code>Optional[LogStepType]</code> <p>The last step at which logging occurred</p> required <code>log_frequency</code> <code>Optional[LogStepType]</code> <p>The frequency to log at</p> required <code>last_model_update_step</code> <code>Optional[LogStepType]</code> <p>The last step at which the model was updated</p> <code>None</code> <code>check_model_update</code> <code>bool</code> <p>If True, will check if the model has been updated since the last log step and if log_frequency steps have passed since the last model update; Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>True if logging cadence has been reached again False otherwise</p> Source code in <code>src/llmcompressor/metrics/utils/frequency_manager.py</code> <pre><code>def log_ready(\n    current_log_step: Optional[LogStepType],\n    last_log_step: Optional[LogStepType],\n    log_frequency: Optional[LogStepType],\n    last_model_update_step: Optional[LogStepType] = None,\n    check_model_update: bool = False,\n):\n    \"\"\"\n    Check if we are ready to log again based on the given parameters\n    (Stateless version of FrequencyManager().log_ready)\n\n    Conditions for readiness:\n        - log frequency is not None\n        - current log step is None\n        - current log step greater than or equal to the last log step\n            plus the log frequency\n        - if check_model_update is True, then the last model update step\n            must be greater than or equal to the last log step, and the\n            current log step must be greater than or equal to the\n            last model update step plus the log frequency\n\n    :param current_log_step: The current log step\n    :param last_log_step: The last step at which logging occurred\n    :param log_frequency: The frequency to log at\n    :param last_model_update_step: The last step at which the model was updated\n    :param check_model_update: If True, will check if the model has been updated\n        since the last log step and if log_frequency steps have passed since the\n        last model update; Defaults to False.\n    :return: True if logging cadence has been reached again False otherwise\n    \"\"\"\n    # format is used to avoid floating point errors\n    # e.g. 0.1 + 0.2 != 0.3\n    # format(0.1 + 0.2, \".4f\") == format(0.3, \".4f\")\n\n    cadence_reached: bool = log_frequency is not None and (\n        current_log_step is None\n        or last_log_step is None\n        or current_log_step &gt;= float(format(last_log_step + log_frequency, \".4f\"))\n    )\n\n    if not cadence_reached or not check_model_update:\n        # early return if cadence not reached or,\n        # model update check not requested\n        return cadence_reached\n\n    model_updated_since_last_log: bool = (\n        last_model_update_step is None\n        or last_log_step is None\n        or current_log_step is None\n        or (\n            last_model_update_step &gt;= last_log_step\n            and current_log_step\n            &gt;= float(format(log_frequency + last_model_update_step, \".4f\"))\n        )\n    )\n\n    return cadence_reached and model_updated_since_last_log\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/","title":"llmcompressor.modifiers","text":""},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier","title":"<code>Modifier</code>","text":"<p>               Bases: <code>ModifierInterface</code>, <code>HooksMixin</code></p> <p>A base class for all modifiers to inherit from. Modifiers are used to modify the training process for a model. Defines base attributes and methods available to all modifiers</p> <p>Lifecycle: 1. initialize 2. on_event -&gt;     * on_start if self.start &lt;= event.current_index     * on_end if self.end &gt;= event.current_index 5. finalize</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <p>The index of the modifier in the list of modifiers for the model</p> required <code>group</code> <p>The group name for the modifier</p> required <code>start</code> <p>The start step for the modifier</p> required <code>end</code> <p>The end step for the modifier</p> required <code>update</code> <p>The update step for the modifier</p> required Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>class Modifier(ModifierInterface, HooksMixin):\n    \"\"\"\n    A base class for all modifiers to inherit from.\n    Modifiers are used to modify the training process for a model.\n    Defines base attributes and methods available to all modifiers\n\n    Lifecycle:\n    1. initialize\n    2. on_event -&gt;\n        * on_start if self.start &lt;= event.current_index\n        * on_end if self.end &gt;= event.current_index\n    5. finalize\n\n    :param index: The index of the modifier in the list of modifiers\n        for the model\n    :param group: The group name for the modifier\n    :param start: The start step for the modifier\n    :param end: The end step for the modifier\n    :param update: The update step for the modifier\n    \"\"\"\n\n    index: Optional[int] = None\n    group: Optional[str] = None\n    start: Optional[float] = None\n    end: Optional[float] = None\n    update: Optional[float] = None\n\n    initialized_: bool = False\n    finalized_: bool = False\n    started_: bool = False\n    ended_: bool = False\n\n    @property\n    def initialized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been initialized\n        \"\"\"\n        return self.initialized_\n\n    @property\n    def finalized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been finalized\n        \"\"\"\n        return self.finalized_\n\n    def initialize(self, state: State, **kwargs):\n        \"\"\"\n        Initialize the modifier for the given model and state.\n\n        :raises RuntimeError: if the modifier has already been finalized\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for initializing the modifier\n        \"\"\"\n        if self.initialized_:\n            raise RuntimeError(\n                \"Cannot initialize a modifier that has already been initialized\"\n            )\n\n        if self.finalized_:\n            raise RuntimeError(\n                \"Cannot initialize a modifier that has already been finalized\"\n            )\n\n        self.initialized_ = self.on_initialize(state=state, **kwargs)\n\n        # trigger starts\n        fake_start_event = Event(type_=EventType.BATCH_START, global_step=0)\n        if self.should_start(fake_start_event):\n            self.on_start(state, fake_start_event, **kwargs)\n            self.started_ = True\n\n    def finalize(self, state: State, **kwargs):\n        \"\"\"\n        Finalize the modifier for the given model and state.\n\n        :raises RuntimeError: if the modifier has not been initialized\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for finalizing the modifier\n        \"\"\"\n        if self.finalized_:\n            raise RuntimeError(\"cannot finalize a modifier twice\")\n\n        if not self.initialized_:\n            raise RuntimeError(\"cannot finalize an uninitialized modifier\")\n\n        # TODO: all finalization should succeed\n        self.finalized_ = self.on_finalize(state=state, **kwargs)\n\n    def update_event(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Update modifier based on the given event. In turn calls\n        on_start, on_update, and on_end based on the event and\n        modifier settings. Returns immediately if the modifier is\n        not initialized\n\n        :raises RuntimeError: if the modifier has been finalized\n        :param state: The current state of sparsification\n        :param event: The event to update the modifier with\n        :param kwargs: Additional arguments for updating the modifier\n        \"\"\"\n        if not self.initialized_:\n            raise RuntimeError(\"Cannot update an uninitialized modifier\")\n\n        if self.finalized_:\n            raise RuntimeError(\"Cannot update a finalized modifier\")\n\n        self.on_event(state, event, **kwargs)\n\n        # handle starting the modifier if needed\n        if (\n            event.type_ == EventType.BATCH_START\n            and not self.started_\n            and self.should_start(event)\n        ):\n            self.on_start(state, event, **kwargs)\n            self.started_ = True\n            self.on_update(state, event, **kwargs)\n\n            return\n\n        # handle ending the modifier if needed\n        if (\n            event.type_ == EventType.BATCH_END\n            and not self.ended_\n            and self.should_end(event)\n        ):\n            self.on_end(state, event, **kwargs)\n            self.ended_ = True\n            self.on_update(state, event, **kwargs)\n\n            return\n\n        if self.started_ and not self.ended_:\n            self.on_update(state, event, **kwargs)\n\n    def should_start(self, event: Event) -&gt; bool:\n        \"\"\"\n        :param event: The event to check if the modifier should start\n        :return: True if the modifier should start based on the given event\n        \"\"\"\n        if self.start is None:\n            return False\n\n        current = event.current_index\n\n        return self.start &lt;= current and (self.end is None or current &lt; self.end)\n\n    def should_end(self, event: Event):\n        \"\"\"\n        :param event: The event to check if the modifier should end\n        :return: True if the modifier should end based on the given event\n        \"\"\"\n        current = event.current_index\n\n        return self.end is not None and current &gt;= self.end\n\n    @abstractmethod\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        on_initialize is called on modifier initialization and\n        must be implemented by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for initializing the modifier\n        :return: True if the modifier was initialized successfully,\n            False otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        on_finalize is called on modifier finalization and\n        must be implemented by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for finalizing the modifier\n        :return: True if the modifier was finalized successfully,\n            False otherwise\n        \"\"\"\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_start is called when the modifier starts and\n        must be implemented by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param event: The event that triggered the start\n        :param kwargs: Additional arguments for starting the modifier\n        \"\"\"\n        pass\n\n    def on_update(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_update is called when the model in question must be\n        updated based on passed in event. Must be implemented by the\n        inheriting modifier.\n\n        :param state: The current state of the model\n        :param event: The event that triggered the update\n        :param kwargs: Additional arguments for updating the model\n        \"\"\"\n        pass\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_end is called when the modifier ends and must be implemented\n        by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param event: The event that triggered the end\n        :param kwargs: Additional arguments for ending the modifier\n        \"\"\"\n        pass\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_event is called whenever an event is triggered\n\n        :param state: The current state of the model\n        :param event: The event that triggered the update\n        :param kwargs: Additional arguments for updating the model\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.finalized","title":"<code>finalized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been finalized</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.initialized","title":"<code>initialized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been initialized</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.finalize","title":"<code>finalize(state, **kwargs)</code>","text":"<p>Finalize the modifier for the given model and state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for finalizing the modifier</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the modifier has not been initialized</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def finalize(self, state: State, **kwargs):\n    \"\"\"\n    Finalize the modifier for the given model and state.\n\n    :raises RuntimeError: if the modifier has not been initialized\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for finalizing the modifier\n    \"\"\"\n    if self.finalized_:\n        raise RuntimeError(\"cannot finalize a modifier twice\")\n\n    if not self.initialized_:\n        raise RuntimeError(\"cannot finalize an uninitialized modifier\")\n\n    # TODO: all finalization should succeed\n    self.finalized_ = self.on_finalize(state=state, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.initialize","title":"<code>initialize(state, **kwargs)</code>","text":"<p>Initialize the modifier for the given model and state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for initializing the modifier</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the modifier has already been finalized</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def initialize(self, state: State, **kwargs):\n    \"\"\"\n    Initialize the modifier for the given model and state.\n\n    :raises RuntimeError: if the modifier has already been finalized\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for initializing the modifier\n    \"\"\"\n    if self.initialized_:\n        raise RuntimeError(\n            \"Cannot initialize a modifier that has already been initialized\"\n        )\n\n    if self.finalized_:\n        raise RuntimeError(\n            \"Cannot initialize a modifier that has already been finalized\"\n        )\n\n    self.initialized_ = self.on_initialize(state=state, **kwargs)\n\n    # trigger starts\n    fake_start_event = Event(type_=EventType.BATCH_START, global_step=0)\n    if self.should_start(fake_start_event):\n        self.on_start(state, fake_start_event, **kwargs)\n        self.started_ = True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>on_end is called when the modifier ends and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the end</p> required <code>kwargs</code> <p>Additional arguments for ending the modifier</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_end is called when the modifier ends and must be implemented\n    by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param event: The event that triggered the end\n    :param kwargs: Additional arguments for ending the modifier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.on_event","title":"<code>on_event(state, event, **kwargs)</code>","text":"<p>on_event is called whenever an event is triggered</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the update</p> required <code>kwargs</code> <p>Additional arguments for updating the model</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_event(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_event is called whenever an event is triggered\n\n    :param state: The current state of the model\n    :param event: The event that triggered the update\n    :param kwargs: Additional arguments for updating the model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>on_finalize is called on modifier finalization and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for finalizing the modifier</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier was finalized successfully, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    on_finalize is called on modifier finalization and\n    must be implemented by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for finalizing the modifier\n    :return: True if the modifier was finalized successfully,\n        False otherwise\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>on_initialize is called on modifier initialization and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for initializing the modifier</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier was initialized successfully, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>@abstractmethod\ndef on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    on_initialize is called on modifier initialization and\n    must be implemented by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for initializing the modifier\n    :return: True if the modifier was initialized successfully,\n        False otherwise\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.on_start","title":"<code>on_start(state, event, **kwargs)</code>","text":"<p>on_start is called when the modifier starts and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the start</p> required <code>kwargs</code> <p>Additional arguments for starting the modifier</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_start(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_start is called when the modifier starts and\n    must be implemented by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param event: The event that triggered the start\n    :param kwargs: Additional arguments for starting the modifier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.on_update","title":"<code>on_update(state, event, **kwargs)</code>","text":"<p>on_update is called when the model in question must be updated based on passed in event. Must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the update</p> required <code>kwargs</code> <p>Additional arguments for updating the model</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_update(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_update is called when the model in question must be\n    updated based on passed in event. Must be implemented by the\n    inheriting modifier.\n\n    :param state: The current state of the model\n    :param event: The event that triggered the update\n    :param kwargs: Additional arguments for updating the model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.should_end","title":"<code>should_end(event)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to check if the modifier should end</p> required <p>Returns:</p> Type Description <p>True if the modifier should end based on the given event</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def should_end(self, event: Event):\n    \"\"\"\n    :param event: The event to check if the modifier should end\n    :return: True if the modifier should end based on the given event\n    \"\"\"\n    current = event.current_index\n\n    return self.end is not None and current &gt;= self.end\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.should_start","title":"<code>should_start(event)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to check if the modifier should start</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier should start based on the given event</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def should_start(self, event: Event) -&gt; bool:\n    \"\"\"\n    :param event: The event to check if the modifier should start\n    :return: True if the modifier should start based on the given event\n    \"\"\"\n    if self.start is None:\n        return False\n\n    current = event.current_index\n\n    return self.start &lt;= current and (self.end is None or current &lt; self.end)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.Modifier.update_event","title":"<code>update_event(state, event, **kwargs)</code>","text":"<p>Update modifier based on the given event. In turn calls on_start, on_update, and on_end based on the event and modifier settings. Returns immediately if the modifier is not initialized</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of sparsification</p> required <code>event</code> <code>Event</code> <p>The event to update the modifier with</p> required <code>kwargs</code> <p>Additional arguments for updating the modifier</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the modifier has been finalized</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def update_event(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Update modifier based on the given event. In turn calls\n    on_start, on_update, and on_end based on the event and\n    modifier settings. Returns immediately if the modifier is\n    not initialized\n\n    :raises RuntimeError: if the modifier has been finalized\n    :param state: The current state of sparsification\n    :param event: The event to update the modifier with\n    :param kwargs: Additional arguments for updating the modifier\n    \"\"\"\n    if not self.initialized_:\n        raise RuntimeError(\"Cannot update an uninitialized modifier\")\n\n    if self.finalized_:\n        raise RuntimeError(\"Cannot update a finalized modifier\")\n\n    self.on_event(state, event, **kwargs)\n\n    # handle starting the modifier if needed\n    if (\n        event.type_ == EventType.BATCH_START\n        and not self.started_\n        and self.should_start(event)\n    ):\n        self.on_start(state, event, **kwargs)\n        self.started_ = True\n        self.on_update(state, event, **kwargs)\n\n        return\n\n    # handle ending the modifier if needed\n    if (\n        event.type_ == EventType.BATCH_END\n        and not self.ended_\n        and self.should_end(event)\n    ):\n        self.on_end(state, event, **kwargs)\n        self.ended_ = True\n        self.on_update(state, event, **kwargs)\n\n        return\n\n    if self.started_ and not self.ended_:\n        self.on_update(state, event, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierFactory","title":"<code>ModifierFactory</code>","text":"<p>A factory for loading and registering modifiers</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>class ModifierFactory:\n    \"\"\"\n    A factory for loading and registering modifiers\n    \"\"\"\n\n    _MAIN_PACKAGE_PATH = \"llmcompressor.modifiers\"\n    _EXPERIMENTAL_PACKAGE_PATH = \"llmcompressor.modifiers.experimental\"\n\n    _loaded: bool = False\n    _main_registry: Dict[str, Type[Modifier]] = {}\n    _experimental_registry: Dict[str, Type[Modifier]] = {}\n    _registered_registry: Dict[str, Type[Modifier]] = {}\n    _errors: Dict[str, Exception] = {}\n\n    @staticmethod\n    def refresh():\n        \"\"\"\n        A method to refresh the factory by reloading the modifiers\n        Note: this will clear any previously registered modifiers\n        \"\"\"\n        ModifierFactory._main_registry = ModifierFactory.load_from_package(\n            ModifierFactory._MAIN_PACKAGE_PATH\n        )\n        ModifierFactory._experimental_registry = ModifierFactory.load_from_package(\n            ModifierFactory._EXPERIMENTAL_PACKAGE_PATH\n        )\n        ModifierFactory._loaded = True\n\n    @staticmethod\n    def load_from_package(package_path: str) -&gt; Dict[str, Type[Modifier]]:\n        \"\"\"\n        :param package_path: The path to the package to load modifiers from\n        :return: The loaded modifiers, as a mapping of name to class\n        \"\"\"\n        loaded = {}\n        main_package = importlib.import_module(package_path)\n\n        for importer, modname, is_pkg in pkgutil.walk_packages(\n            main_package.__path__, package_path + \".\"\n        ):\n            try:\n                module = importlib.import_module(modname)\n\n                for attribute_name in dir(module):\n                    if not attribute_name.endswith(\"Modifier\"):\n                        continue\n\n                    try:\n                        if attribute_name in loaded:\n                            continue\n\n                        attr = getattr(module, attribute_name)\n\n                        if not isinstance(attr, type):\n                            raise ValueError(\n                                f\"Attribute {attribute_name} is not a type\"\n                            )\n\n                        if not issubclass(attr, Modifier):\n                            raise ValueError(\n                                f\"Attribute {attribute_name} is not a Modifier\"\n                            )\n\n                        loaded[attribute_name] = attr\n                    except Exception as err:\n                        # TODO: log import error\n                        ModifierFactory._errors[attribute_name] = err\n            except Exception as module_err:\n                # TODO: log import error\n                print(module_err)\n\n        return loaded\n\n    @staticmethod\n    def create(\n        type_: str,\n        allow_registered: bool,\n        allow_experimental: bool,\n        **kwargs,\n    ) -&gt; Modifier:\n        \"\"\"\n        Instantiate a modifier of the given type from registered modifiers.\n\n        :raises ValueError: If no modifier of the given type is found\n        :param type_: The type of modifier to create\n        :param framework: The framework the modifier is for\n        :param allow_registered: Whether or not to allow registered modifiers\n        :param allow_experimental: Whether or not to allow experimental modifiers\n        :param kwargs: Additional keyword arguments to pass to the modifier\n            during instantiation\n        :return: The instantiated modifier\n        \"\"\"\n        if type_ in ModifierFactory._errors:\n            raise ModifierFactory._errors[type_]\n\n        if type_ in ModifierFactory._registered_registry:\n            if allow_registered:\n                return ModifierFactory._registered_registry[type_](**kwargs)\n            else:\n                # TODO: log warning that modifier was skipped\n                pass\n\n        if type_ in ModifierFactory._experimental_registry:\n            if allow_experimental:\n                return ModifierFactory._experimental_registry[type_](**kwargs)\n            else:\n                # TODO: log warning that modifier was skipped\n                pass\n\n        if type_ in ModifierFactory._main_registry:\n            return ModifierFactory._main_registry[type_](**kwargs)\n\n        raise ValueError(f\"No modifier of type '{type_}' found.\")\n\n    @staticmethod\n    def register(type_: str, modifier_class: Type[Modifier]):\n        \"\"\"\n        Register a modifier class to be used by the factory.\n\n        :raises ValueError: If the provided class does not subclass the Modifier\n            base class or is not a type\n        :param type_: The type of modifier to register\n        :param modifier_class: The class of the modifier to register, must subclass\n            the Modifier base class\n        \"\"\"\n        if not issubclass(modifier_class, Modifier):\n            raise ValueError(\n                \"The provided class does not subclass the Modifier base class.\"\n            )\n        if not isinstance(modifier_class, type):\n            raise ValueError(\"The provided class is not a type.\")\n\n        ModifierFactory._registered_registry[type_] = modifier_class\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierFactory.create","title":"<code>create(type_, allow_registered, allow_experimental, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Instantiate a modifier of the given type from registered modifiers.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>str</code> <p>The type of modifier to create</p> required <code>framework</code> <p>The framework the modifier is for</p> required <code>allow_registered</code> <code>bool</code> <p>Whether or not to allow registered modifiers</p> required <code>allow_experimental</code> <code>bool</code> <p>Whether or not to allow experimental modifiers</p> required <code>kwargs</code> <p>Additional keyword arguments to pass to the modifier during instantiation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Modifier</code> <p>The instantiated modifier</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no modifier of the given type is found</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef create(\n    type_: str,\n    allow_registered: bool,\n    allow_experimental: bool,\n    **kwargs,\n) -&gt; Modifier:\n    \"\"\"\n    Instantiate a modifier of the given type from registered modifiers.\n\n    :raises ValueError: If no modifier of the given type is found\n    :param type_: The type of modifier to create\n    :param framework: The framework the modifier is for\n    :param allow_registered: Whether or not to allow registered modifiers\n    :param allow_experimental: Whether or not to allow experimental modifiers\n    :param kwargs: Additional keyword arguments to pass to the modifier\n        during instantiation\n    :return: The instantiated modifier\n    \"\"\"\n    if type_ in ModifierFactory._errors:\n        raise ModifierFactory._errors[type_]\n\n    if type_ in ModifierFactory._registered_registry:\n        if allow_registered:\n            return ModifierFactory._registered_registry[type_](**kwargs)\n        else:\n            # TODO: log warning that modifier was skipped\n            pass\n\n    if type_ in ModifierFactory._experimental_registry:\n        if allow_experimental:\n            return ModifierFactory._experimental_registry[type_](**kwargs)\n        else:\n            # TODO: log warning that modifier was skipped\n            pass\n\n    if type_ in ModifierFactory._main_registry:\n        return ModifierFactory._main_registry[type_](**kwargs)\n\n    raise ValueError(f\"No modifier of type '{type_}' found.\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierFactory.load_from_package","title":"<code>load_from_package(package_path)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>package_path</code> <code>str</code> <p>The path to the package to load modifiers from</p> required <p>Returns:</p> Type Description <code>Dict[str, Type[Modifier]]</code> <p>The loaded modifiers, as a mapping of name to class</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef load_from_package(package_path: str) -&gt; Dict[str, Type[Modifier]]:\n    \"\"\"\n    :param package_path: The path to the package to load modifiers from\n    :return: The loaded modifiers, as a mapping of name to class\n    \"\"\"\n    loaded = {}\n    main_package = importlib.import_module(package_path)\n\n    for importer, modname, is_pkg in pkgutil.walk_packages(\n        main_package.__path__, package_path + \".\"\n    ):\n        try:\n            module = importlib.import_module(modname)\n\n            for attribute_name in dir(module):\n                if not attribute_name.endswith(\"Modifier\"):\n                    continue\n\n                try:\n                    if attribute_name in loaded:\n                        continue\n\n                    attr = getattr(module, attribute_name)\n\n                    if not isinstance(attr, type):\n                        raise ValueError(\n                            f\"Attribute {attribute_name} is not a type\"\n                        )\n\n                    if not issubclass(attr, Modifier):\n                        raise ValueError(\n                            f\"Attribute {attribute_name} is not a Modifier\"\n                        )\n\n                    loaded[attribute_name] = attr\n                except Exception as err:\n                    # TODO: log import error\n                    ModifierFactory._errors[attribute_name] = err\n        except Exception as module_err:\n            # TODO: log import error\n            print(module_err)\n\n    return loaded\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierFactory.refresh","title":"<code>refresh()</code>  <code>staticmethod</code>","text":"<p>A method to refresh the factory by reloading the modifiers Note: this will clear any previously registered modifiers</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef refresh():\n    \"\"\"\n    A method to refresh the factory by reloading the modifiers\n    Note: this will clear any previously registered modifiers\n    \"\"\"\n    ModifierFactory._main_registry = ModifierFactory.load_from_package(\n        ModifierFactory._MAIN_PACKAGE_PATH\n    )\n    ModifierFactory._experimental_registry = ModifierFactory.load_from_package(\n        ModifierFactory._EXPERIMENTAL_PACKAGE_PATH\n    )\n    ModifierFactory._loaded = True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierFactory.register","title":"<code>register(type_, modifier_class)</code>  <code>staticmethod</code>","text":"<p>Register a modifier class to be used by the factory.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>str</code> <p>The type of modifier to register</p> required <code>modifier_class</code> <code>Type[Modifier]</code> <p>The class of the modifier to register, must subclass the Modifier base class</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided class does not subclass the Modifier base class or is not a type</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef register(type_: str, modifier_class: Type[Modifier]):\n    \"\"\"\n    Register a modifier class to be used by the factory.\n\n    :raises ValueError: If the provided class does not subclass the Modifier\n        base class or is not a type\n    :param type_: The type of modifier to register\n    :param modifier_class: The class of the modifier to register, must subclass\n        the Modifier base class\n    \"\"\"\n    if not issubclass(modifier_class, Modifier):\n        raise ValueError(\n            \"The provided class does not subclass the Modifier base class.\"\n        )\n    if not isinstance(modifier_class, type):\n        raise ValueError(\"The provided class is not a type.\")\n\n    ModifierFactory._registered_registry[type_] = modifier_class\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierInterface","title":"<code>ModifierInterface</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the contract that all modifiers must implement</p> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>class ModifierInterface(ABC):\n    \"\"\"\n    Defines the contract that all modifiers must implement\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def initialized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been initialized\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def finalized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been finalized\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def initialize(self, state: State, **kwargs):\n        \"\"\"\n        Initialize the modifier\n\n        :param state: The current state of the model\n        :param kwargs: Additional keyword arguments\n            for modifier initialization\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def finalize(self, state: State, **kwargs):\n        \"\"\"\n        Finalize the modifier\n\n        :param state: The current state of the model\n        :param kwargs: Additional keyword arguments for\n            modifier finalization\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def update_event(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Update the modifier based on the event\n\n        :param state: The current state of the model\n        :param event: The event to update the modifier with\n        :param kwargs: Additional keyword arguments for\n            modifier update\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierInterface.finalized","title":"<code>finalized</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been finalized</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierInterface.initialized","title":"<code>initialized</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been initialized</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierInterface.finalize","title":"<code>finalize(state, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Finalize the modifier</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional keyword arguments for modifier finalization</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>@abstractmethod\ndef finalize(self, state: State, **kwargs):\n    \"\"\"\n    Finalize the modifier\n\n    :param state: The current state of the model\n    :param kwargs: Additional keyword arguments for\n        modifier finalization\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierInterface.initialize","title":"<code>initialize(state, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Initialize the modifier</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional keyword arguments for modifier initialization</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>@abstractmethod\ndef initialize(self, state: State, **kwargs):\n    \"\"\"\n    Initialize the modifier\n\n    :param state: The current state of the model\n    :param kwargs: Additional keyword arguments\n        for modifier initialization\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.ModifierInterface.update_event","title":"<code>update_event(state, event, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Update the modifier based on the event</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event to update the modifier with</p> required <code>kwargs</code> <p>Additional keyword arguments for modifier update</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>@abstractmethod\ndef update_event(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Update the modifier based on the event\n\n    :param state: The current state of the model\n    :param event: The event to update the modifier with\n    :param kwargs: Additional keyword arguments for\n        modifier update\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers","title":"<code>StageModifiers</code>","text":"<p>               Bases: <code>ModifierInterface</code>, <code>BaseModel</code></p> <p>Represents a collection of modifiers that are applied together as a stage.</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <p>The modifiers to apply as a stage</p> required <code>index</code> <p>The index of the stage, if applicable</p> required <code>group</code> <p>The group name of the stage, if applicable</p> required <code>applied</code> <p>Flag for indicating if this stage has has already been applied to the model through finalization</p> required Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>class StageModifiers(ModifierInterface, BaseModel):\n    \"\"\"\n    Represents a collection of modifiers that are applied together as a stage.\n\n    :param modifiers: The modifiers to apply as a stage\n    :param index: The index of the stage, if applicable\n    :param group: The group name of the stage, if applicable\n    :param applied: Flag for indicating if this stage has has already been\n    applied to the model through finalization\n    \"\"\"\n\n    modifiers: List[\"Modifier\"] = Field(default_factory=list)\n    index: Optional[int] = None\n    group: Optional[str] = None\n    applied: bool = False\n\n    @property\n    def initialized(self) -&gt; bool:\n        \"\"\"\n        :return: True if all of the stage modifiers have been initialized,\n            False otherwise\n        \"\"\"\n        return all(mod.initialized for mod in self.modifiers)\n\n    @property\n    def finalized(self) -&gt; bool:\n        \"\"\"\n        :return: True if all of the stage modifiers have been finalized,\n            False otherwise\n        \"\"\"\n        return all(mod.finalized for mod in self.modifiers)\n\n    @property\n    def unique_id(self) -&gt; str:\n        \"\"\"\n        :return: ID for stage containing the name and index\n        \"\"\"\n        return self.group + \"_\" + str(self.index)\n\n    def initialize(self, state: \"State\", **kwargs):\n        \"\"\"\n        Initialize all the stage modifiers\n\n        :param state: The state of current session\n        :param kwargs: Additional kwargs to pass to the modifier(s)\n            initialize method\n        \"\"\"\n\n        if self.applied:\n            return\n\n        accelerator = kwargs.get(\"accelerator\", None)\n        for modifier in self.modifiers:\n            if not modifier.initialized:\n                modifier.initialize(state, **kwargs)\n            if accelerator:\n                accelerator.wait_for_everyone()\n        state.loggers.system.info(tag=\"stage\", string=\"Modifiers initialized\")\n\n    def finalize(self, state: \"State\", **kwargs):\n        \"\"\"\n        Finalize all the stage modifiers and mark the stage as applied\n\n        :param state: The state of current session\n        :param kwargs: Additional kwargs to pass to the modifier(s)\n            finalize method\n        \"\"\"\n\n        if self.applied:\n            return\n\n        accelerator = kwargs.get(\"accelerator\", None)\n        for modifier in self.modifiers:\n            modifier.finalize(state, **kwargs)\n            if accelerator:\n                accelerator.wait_for_everyone()\n\n        self.applied = True\n        state.loggers.system.info(tag=\"stage\", string=\"Modifiers finalized\")\n\n    def update_event(self, state: \"State\", event: \"Event\", **kwargs):\n        \"\"\"\n        Propagate the event to all the stage modifiers\n\n        :param state: The state of current session\n        :param event: The event to propagate\n        :param kwargs: Additional kwargs to pass to the modifier(s)\n            update_event method\n        \"\"\"\n\n        if self.applied:\n            return\n\n        for modifier in self.modifiers:\n            modifier.update_event(state, event, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers.finalized","title":"<code>finalized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if all of the stage modifiers have been finalized, False otherwise</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers.initialized","title":"<code>initialized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if all of the stage modifiers have been initialized, False otherwise</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers.unique_id","title":"<code>unique_id</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>ID for stage containing the name and index</p>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers.finalize","title":"<code>finalize(state, **kwargs)</code>","text":"<p>Finalize all the stage modifiers and mark the stage as applied</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The state of current session</p> required <code>kwargs</code> <p>Additional kwargs to pass to the modifier(s) finalize method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>def finalize(self, state: \"State\", **kwargs):\n    \"\"\"\n    Finalize all the stage modifiers and mark the stage as applied\n\n    :param state: The state of current session\n    :param kwargs: Additional kwargs to pass to the modifier(s)\n        finalize method\n    \"\"\"\n\n    if self.applied:\n        return\n\n    accelerator = kwargs.get(\"accelerator\", None)\n    for modifier in self.modifiers:\n        modifier.finalize(state, **kwargs)\n        if accelerator:\n            accelerator.wait_for_everyone()\n\n    self.applied = True\n    state.loggers.system.info(tag=\"stage\", string=\"Modifiers finalized\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers.initialize","title":"<code>initialize(state, **kwargs)</code>","text":"<p>Initialize all the stage modifiers</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The state of current session</p> required <code>kwargs</code> <p>Additional kwargs to pass to the modifier(s) initialize method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>def initialize(self, state: \"State\", **kwargs):\n    \"\"\"\n    Initialize all the stage modifiers\n\n    :param state: The state of current session\n    :param kwargs: Additional kwargs to pass to the modifier(s)\n        initialize method\n    \"\"\"\n\n    if self.applied:\n        return\n\n    accelerator = kwargs.get(\"accelerator\", None)\n    for modifier in self.modifiers:\n        if not modifier.initialized:\n            modifier.initialize(state, **kwargs)\n        if accelerator:\n            accelerator.wait_for_everyone()\n    state.loggers.system.info(tag=\"stage\", string=\"Modifiers initialized\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/#llmcompressor.modifiers.StageModifiers.update_event","title":"<code>update_event(state, event, **kwargs)</code>","text":"<p>Propagate the event to all the stage modifiers</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The state of current session</p> required <code>event</code> <code>Event</code> <p>The event to propagate</p> required <code>kwargs</code> <p>Additional kwargs to pass to the modifier(s) update_event method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>def update_event(self, state: \"State\", event: \"Event\", **kwargs):\n    \"\"\"\n    Propagate the event to all the stage modifiers\n\n    :param state: The state of current session\n    :param event: The event to propagate\n    :param kwargs: Additional kwargs to pass to the modifier(s)\n        update_event method\n    \"\"\"\n\n    if self.applied:\n        return\n\n    for modifier in self.modifiers:\n        modifier.update_event(state, event, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/factory/","title":"llmcompressor.modifiers.factory","text":""},{"location":"reference/llmcompressor/modifiers/factory/#llmcompressor.modifiers.factory.ModifierFactory","title":"<code>ModifierFactory</code>","text":"<p>A factory for loading and registering modifiers</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>class ModifierFactory:\n    \"\"\"\n    A factory for loading and registering modifiers\n    \"\"\"\n\n    _MAIN_PACKAGE_PATH = \"llmcompressor.modifiers\"\n    _EXPERIMENTAL_PACKAGE_PATH = \"llmcompressor.modifiers.experimental\"\n\n    _loaded: bool = False\n    _main_registry: Dict[str, Type[Modifier]] = {}\n    _experimental_registry: Dict[str, Type[Modifier]] = {}\n    _registered_registry: Dict[str, Type[Modifier]] = {}\n    _errors: Dict[str, Exception] = {}\n\n    @staticmethod\n    def refresh():\n        \"\"\"\n        A method to refresh the factory by reloading the modifiers\n        Note: this will clear any previously registered modifiers\n        \"\"\"\n        ModifierFactory._main_registry = ModifierFactory.load_from_package(\n            ModifierFactory._MAIN_PACKAGE_PATH\n        )\n        ModifierFactory._experimental_registry = ModifierFactory.load_from_package(\n            ModifierFactory._EXPERIMENTAL_PACKAGE_PATH\n        )\n        ModifierFactory._loaded = True\n\n    @staticmethod\n    def load_from_package(package_path: str) -&gt; Dict[str, Type[Modifier]]:\n        \"\"\"\n        :param package_path: The path to the package to load modifiers from\n        :return: The loaded modifiers, as a mapping of name to class\n        \"\"\"\n        loaded = {}\n        main_package = importlib.import_module(package_path)\n\n        for importer, modname, is_pkg in pkgutil.walk_packages(\n            main_package.__path__, package_path + \".\"\n        ):\n            try:\n                module = importlib.import_module(modname)\n\n                for attribute_name in dir(module):\n                    if not attribute_name.endswith(\"Modifier\"):\n                        continue\n\n                    try:\n                        if attribute_name in loaded:\n                            continue\n\n                        attr = getattr(module, attribute_name)\n\n                        if not isinstance(attr, type):\n                            raise ValueError(\n                                f\"Attribute {attribute_name} is not a type\"\n                            )\n\n                        if not issubclass(attr, Modifier):\n                            raise ValueError(\n                                f\"Attribute {attribute_name} is not a Modifier\"\n                            )\n\n                        loaded[attribute_name] = attr\n                    except Exception as err:\n                        # TODO: log import error\n                        ModifierFactory._errors[attribute_name] = err\n            except Exception as module_err:\n                # TODO: log import error\n                print(module_err)\n\n        return loaded\n\n    @staticmethod\n    def create(\n        type_: str,\n        allow_registered: bool,\n        allow_experimental: bool,\n        **kwargs,\n    ) -&gt; Modifier:\n        \"\"\"\n        Instantiate a modifier of the given type from registered modifiers.\n\n        :raises ValueError: If no modifier of the given type is found\n        :param type_: The type of modifier to create\n        :param framework: The framework the modifier is for\n        :param allow_registered: Whether or not to allow registered modifiers\n        :param allow_experimental: Whether or not to allow experimental modifiers\n        :param kwargs: Additional keyword arguments to pass to the modifier\n            during instantiation\n        :return: The instantiated modifier\n        \"\"\"\n        if type_ in ModifierFactory._errors:\n            raise ModifierFactory._errors[type_]\n\n        if type_ in ModifierFactory._registered_registry:\n            if allow_registered:\n                return ModifierFactory._registered_registry[type_](**kwargs)\n            else:\n                # TODO: log warning that modifier was skipped\n                pass\n\n        if type_ in ModifierFactory._experimental_registry:\n            if allow_experimental:\n                return ModifierFactory._experimental_registry[type_](**kwargs)\n            else:\n                # TODO: log warning that modifier was skipped\n                pass\n\n        if type_ in ModifierFactory._main_registry:\n            return ModifierFactory._main_registry[type_](**kwargs)\n\n        raise ValueError(f\"No modifier of type '{type_}' found.\")\n\n    @staticmethod\n    def register(type_: str, modifier_class: Type[Modifier]):\n        \"\"\"\n        Register a modifier class to be used by the factory.\n\n        :raises ValueError: If the provided class does not subclass the Modifier\n            base class or is not a type\n        :param type_: The type of modifier to register\n        :param modifier_class: The class of the modifier to register, must subclass\n            the Modifier base class\n        \"\"\"\n        if not issubclass(modifier_class, Modifier):\n            raise ValueError(\n                \"The provided class does not subclass the Modifier base class.\"\n            )\n        if not isinstance(modifier_class, type):\n            raise ValueError(\"The provided class is not a type.\")\n\n        ModifierFactory._registered_registry[type_] = modifier_class\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/factory/#llmcompressor.modifiers.factory.ModifierFactory.create","title":"<code>create(type_, allow_registered, allow_experimental, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Instantiate a modifier of the given type from registered modifiers.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>str</code> <p>The type of modifier to create</p> required <code>framework</code> <p>The framework the modifier is for</p> required <code>allow_registered</code> <code>bool</code> <p>Whether or not to allow registered modifiers</p> required <code>allow_experimental</code> <code>bool</code> <p>Whether or not to allow experimental modifiers</p> required <code>kwargs</code> <p>Additional keyword arguments to pass to the modifier during instantiation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Modifier</code> <p>The instantiated modifier</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no modifier of the given type is found</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef create(\n    type_: str,\n    allow_registered: bool,\n    allow_experimental: bool,\n    **kwargs,\n) -&gt; Modifier:\n    \"\"\"\n    Instantiate a modifier of the given type from registered modifiers.\n\n    :raises ValueError: If no modifier of the given type is found\n    :param type_: The type of modifier to create\n    :param framework: The framework the modifier is for\n    :param allow_registered: Whether or not to allow registered modifiers\n    :param allow_experimental: Whether or not to allow experimental modifiers\n    :param kwargs: Additional keyword arguments to pass to the modifier\n        during instantiation\n    :return: The instantiated modifier\n    \"\"\"\n    if type_ in ModifierFactory._errors:\n        raise ModifierFactory._errors[type_]\n\n    if type_ in ModifierFactory._registered_registry:\n        if allow_registered:\n            return ModifierFactory._registered_registry[type_](**kwargs)\n        else:\n            # TODO: log warning that modifier was skipped\n            pass\n\n    if type_ in ModifierFactory._experimental_registry:\n        if allow_experimental:\n            return ModifierFactory._experimental_registry[type_](**kwargs)\n        else:\n            # TODO: log warning that modifier was skipped\n            pass\n\n    if type_ in ModifierFactory._main_registry:\n        return ModifierFactory._main_registry[type_](**kwargs)\n\n    raise ValueError(f\"No modifier of type '{type_}' found.\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/factory/#llmcompressor.modifiers.factory.ModifierFactory.load_from_package","title":"<code>load_from_package(package_path)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>package_path</code> <code>str</code> <p>The path to the package to load modifiers from</p> required <p>Returns:</p> Type Description <code>Dict[str, Type[Modifier]]</code> <p>The loaded modifiers, as a mapping of name to class</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef load_from_package(package_path: str) -&gt; Dict[str, Type[Modifier]]:\n    \"\"\"\n    :param package_path: The path to the package to load modifiers from\n    :return: The loaded modifiers, as a mapping of name to class\n    \"\"\"\n    loaded = {}\n    main_package = importlib.import_module(package_path)\n\n    for importer, modname, is_pkg in pkgutil.walk_packages(\n        main_package.__path__, package_path + \".\"\n    ):\n        try:\n            module = importlib.import_module(modname)\n\n            for attribute_name in dir(module):\n                if not attribute_name.endswith(\"Modifier\"):\n                    continue\n\n                try:\n                    if attribute_name in loaded:\n                        continue\n\n                    attr = getattr(module, attribute_name)\n\n                    if not isinstance(attr, type):\n                        raise ValueError(\n                            f\"Attribute {attribute_name} is not a type\"\n                        )\n\n                    if not issubclass(attr, Modifier):\n                        raise ValueError(\n                            f\"Attribute {attribute_name} is not a Modifier\"\n                        )\n\n                    loaded[attribute_name] = attr\n                except Exception as err:\n                    # TODO: log import error\n                    ModifierFactory._errors[attribute_name] = err\n        except Exception as module_err:\n            # TODO: log import error\n            print(module_err)\n\n    return loaded\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/factory/#llmcompressor.modifiers.factory.ModifierFactory.refresh","title":"<code>refresh()</code>  <code>staticmethod</code>","text":"<p>A method to refresh the factory by reloading the modifiers Note: this will clear any previously registered modifiers</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef refresh():\n    \"\"\"\n    A method to refresh the factory by reloading the modifiers\n    Note: this will clear any previously registered modifiers\n    \"\"\"\n    ModifierFactory._main_registry = ModifierFactory.load_from_package(\n        ModifierFactory._MAIN_PACKAGE_PATH\n    )\n    ModifierFactory._experimental_registry = ModifierFactory.load_from_package(\n        ModifierFactory._EXPERIMENTAL_PACKAGE_PATH\n    )\n    ModifierFactory._loaded = True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/factory/#llmcompressor.modifiers.factory.ModifierFactory.register","title":"<code>register(type_, modifier_class)</code>  <code>staticmethod</code>","text":"<p>Register a modifier class to be used by the factory.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>str</code> <p>The type of modifier to register</p> required <code>modifier_class</code> <code>Type[Modifier]</code> <p>The class of the modifier to register, must subclass the Modifier base class</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided class does not subclass the Modifier base class or is not a type</p> Source code in <code>src/llmcompressor/modifiers/factory.py</code> <pre><code>@staticmethod\ndef register(type_: str, modifier_class: Type[Modifier]):\n    \"\"\"\n    Register a modifier class to be used by the factory.\n\n    :raises ValueError: If the provided class does not subclass the Modifier\n        base class or is not a type\n    :param type_: The type of modifier to register\n    :param modifier_class: The class of the modifier to register, must subclass\n        the Modifier base class\n    \"\"\"\n    if not issubclass(modifier_class, Modifier):\n        raise ValueError(\n            \"The provided class does not subclass the Modifier base class.\"\n        )\n    if not isinstance(modifier_class, type):\n        raise ValueError(\"The provided class is not a type.\")\n\n    ModifierFactory._registered_registry[type_] = modifier_class\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/interface/","title":"llmcompressor.modifiers.interface","text":""},{"location":"reference/llmcompressor/modifiers/interface/#llmcompressor.modifiers.interface.ModifierInterface","title":"<code>ModifierInterface</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the contract that all modifiers must implement</p> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>class ModifierInterface(ABC):\n    \"\"\"\n    Defines the contract that all modifiers must implement\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def initialized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been initialized\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def finalized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been finalized\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def initialize(self, state: State, **kwargs):\n        \"\"\"\n        Initialize the modifier\n\n        :param state: The current state of the model\n        :param kwargs: Additional keyword arguments\n            for modifier initialization\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def finalize(self, state: State, **kwargs):\n        \"\"\"\n        Finalize the modifier\n\n        :param state: The current state of the model\n        :param kwargs: Additional keyword arguments for\n            modifier finalization\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def update_event(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Update the modifier based on the event\n\n        :param state: The current state of the model\n        :param event: The event to update the modifier with\n        :param kwargs: Additional keyword arguments for\n            modifier update\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/interface/#llmcompressor.modifiers.interface.ModifierInterface.finalized","title":"<code>finalized</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been finalized</p>"},{"location":"reference/llmcompressor/modifiers/interface/#llmcompressor.modifiers.interface.ModifierInterface.initialized","title":"<code>initialized</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been initialized</p>"},{"location":"reference/llmcompressor/modifiers/interface/#llmcompressor.modifiers.interface.ModifierInterface.finalize","title":"<code>finalize(state, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Finalize the modifier</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional keyword arguments for modifier finalization</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>@abstractmethod\ndef finalize(self, state: State, **kwargs):\n    \"\"\"\n    Finalize the modifier\n\n    :param state: The current state of the model\n    :param kwargs: Additional keyword arguments for\n        modifier finalization\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/interface/#llmcompressor.modifiers.interface.ModifierInterface.initialize","title":"<code>initialize(state, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Initialize the modifier</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional keyword arguments for modifier initialization</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>@abstractmethod\ndef initialize(self, state: State, **kwargs):\n    \"\"\"\n    Initialize the modifier\n\n    :param state: The current state of the model\n    :param kwargs: Additional keyword arguments\n        for modifier initialization\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/interface/#llmcompressor.modifiers.interface.ModifierInterface.update_event","title":"<code>update_event(state, event, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Update the modifier based on the event</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event to update the modifier with</p> required <code>kwargs</code> <p>Additional keyword arguments for modifier update</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/interface.py</code> <pre><code>@abstractmethod\ndef update_event(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Update the modifier based on the event\n\n    :param state: The current state of the model\n    :param event: The event to update the modifier with\n    :param kwargs: Additional keyword arguments for\n        modifier update\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/","title":"llmcompressor.modifiers.modifier","text":""},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier","title":"<code>Modifier</code>","text":"<p>               Bases: <code>ModifierInterface</code>, <code>HooksMixin</code></p> <p>A base class for all modifiers to inherit from. Modifiers are used to modify the training process for a model. Defines base attributes and methods available to all modifiers</p> <p>Lifecycle: 1. initialize 2. on_event -&gt;     * on_start if self.start &lt;= event.current_index     * on_end if self.end &gt;= event.current_index 5. finalize</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <p>The index of the modifier in the list of modifiers for the model</p> required <code>group</code> <p>The group name for the modifier</p> required <code>start</code> <p>The start step for the modifier</p> required <code>end</code> <p>The end step for the modifier</p> required <code>update</code> <p>The update step for the modifier</p> required Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>class Modifier(ModifierInterface, HooksMixin):\n    \"\"\"\n    A base class for all modifiers to inherit from.\n    Modifiers are used to modify the training process for a model.\n    Defines base attributes and methods available to all modifiers\n\n    Lifecycle:\n    1. initialize\n    2. on_event -&gt;\n        * on_start if self.start &lt;= event.current_index\n        * on_end if self.end &gt;= event.current_index\n    5. finalize\n\n    :param index: The index of the modifier in the list of modifiers\n        for the model\n    :param group: The group name for the modifier\n    :param start: The start step for the modifier\n    :param end: The end step for the modifier\n    :param update: The update step for the modifier\n    \"\"\"\n\n    index: Optional[int] = None\n    group: Optional[str] = None\n    start: Optional[float] = None\n    end: Optional[float] = None\n    update: Optional[float] = None\n\n    initialized_: bool = False\n    finalized_: bool = False\n    started_: bool = False\n    ended_: bool = False\n\n    @property\n    def initialized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been initialized\n        \"\"\"\n        return self.initialized_\n\n    @property\n    def finalized(self) -&gt; bool:\n        \"\"\"\n        :return: True if the modifier has been finalized\n        \"\"\"\n        return self.finalized_\n\n    def initialize(self, state: State, **kwargs):\n        \"\"\"\n        Initialize the modifier for the given model and state.\n\n        :raises RuntimeError: if the modifier has already been finalized\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for initializing the modifier\n        \"\"\"\n        if self.initialized_:\n            raise RuntimeError(\n                \"Cannot initialize a modifier that has already been initialized\"\n            )\n\n        if self.finalized_:\n            raise RuntimeError(\n                \"Cannot initialize a modifier that has already been finalized\"\n            )\n\n        self.initialized_ = self.on_initialize(state=state, **kwargs)\n\n        # trigger starts\n        fake_start_event = Event(type_=EventType.BATCH_START, global_step=0)\n        if self.should_start(fake_start_event):\n            self.on_start(state, fake_start_event, **kwargs)\n            self.started_ = True\n\n    def finalize(self, state: State, **kwargs):\n        \"\"\"\n        Finalize the modifier for the given model and state.\n\n        :raises RuntimeError: if the modifier has not been initialized\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for finalizing the modifier\n        \"\"\"\n        if self.finalized_:\n            raise RuntimeError(\"cannot finalize a modifier twice\")\n\n        if not self.initialized_:\n            raise RuntimeError(\"cannot finalize an uninitialized modifier\")\n\n        # TODO: all finalization should succeed\n        self.finalized_ = self.on_finalize(state=state, **kwargs)\n\n    def update_event(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Update modifier based on the given event. In turn calls\n        on_start, on_update, and on_end based on the event and\n        modifier settings. Returns immediately if the modifier is\n        not initialized\n\n        :raises RuntimeError: if the modifier has been finalized\n        :param state: The current state of sparsification\n        :param event: The event to update the modifier with\n        :param kwargs: Additional arguments for updating the modifier\n        \"\"\"\n        if not self.initialized_:\n            raise RuntimeError(\"Cannot update an uninitialized modifier\")\n\n        if self.finalized_:\n            raise RuntimeError(\"Cannot update a finalized modifier\")\n\n        self.on_event(state, event, **kwargs)\n\n        # handle starting the modifier if needed\n        if (\n            event.type_ == EventType.BATCH_START\n            and not self.started_\n            and self.should_start(event)\n        ):\n            self.on_start(state, event, **kwargs)\n            self.started_ = True\n            self.on_update(state, event, **kwargs)\n\n            return\n\n        # handle ending the modifier if needed\n        if (\n            event.type_ == EventType.BATCH_END\n            and not self.ended_\n            and self.should_end(event)\n        ):\n            self.on_end(state, event, **kwargs)\n            self.ended_ = True\n            self.on_update(state, event, **kwargs)\n\n            return\n\n        if self.started_ and not self.ended_:\n            self.on_update(state, event, **kwargs)\n\n    def should_start(self, event: Event) -&gt; bool:\n        \"\"\"\n        :param event: The event to check if the modifier should start\n        :return: True if the modifier should start based on the given event\n        \"\"\"\n        if self.start is None:\n            return False\n\n        current = event.current_index\n\n        return self.start &lt;= current and (self.end is None or current &lt; self.end)\n\n    def should_end(self, event: Event):\n        \"\"\"\n        :param event: The event to check if the modifier should end\n        :return: True if the modifier should end based on the given event\n        \"\"\"\n        current = event.current_index\n\n        return self.end is not None and current &gt;= self.end\n\n    @abstractmethod\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        on_initialize is called on modifier initialization and\n        must be implemented by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for initializing the modifier\n        :return: True if the modifier was initialized successfully,\n            False otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        on_finalize is called on modifier finalization and\n        must be implemented by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param kwargs: Additional arguments for finalizing the modifier\n        :return: True if the modifier was finalized successfully,\n            False otherwise\n        \"\"\"\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_start is called when the modifier starts and\n        must be implemented by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param event: The event that triggered the start\n        :param kwargs: Additional arguments for starting the modifier\n        \"\"\"\n        pass\n\n    def on_update(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_update is called when the model in question must be\n        updated based on passed in event. Must be implemented by the\n        inheriting modifier.\n\n        :param state: The current state of the model\n        :param event: The event that triggered the update\n        :param kwargs: Additional arguments for updating the model\n        \"\"\"\n        pass\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_end is called when the modifier ends and must be implemented\n        by the inheriting modifier.\n\n        :param state: The current state of the model\n        :param event: The event that triggered the end\n        :param kwargs: Additional arguments for ending the modifier\n        \"\"\"\n        pass\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        on_event is called whenever an event is triggered\n\n        :param state: The current state of the model\n        :param event: The event that triggered the update\n        :param kwargs: Additional arguments for updating the model\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.finalized","title":"<code>finalized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been finalized</p>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.initialized","title":"<code>initialized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier has been initialized</p>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.finalize","title":"<code>finalize(state, **kwargs)</code>","text":"<p>Finalize the modifier for the given model and state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for finalizing the modifier</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the modifier has not been initialized</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def finalize(self, state: State, **kwargs):\n    \"\"\"\n    Finalize the modifier for the given model and state.\n\n    :raises RuntimeError: if the modifier has not been initialized\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for finalizing the modifier\n    \"\"\"\n    if self.finalized_:\n        raise RuntimeError(\"cannot finalize a modifier twice\")\n\n    if not self.initialized_:\n        raise RuntimeError(\"cannot finalize an uninitialized modifier\")\n\n    # TODO: all finalization should succeed\n    self.finalized_ = self.on_finalize(state=state, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.initialize","title":"<code>initialize(state, **kwargs)</code>","text":"<p>Initialize the modifier for the given model and state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for initializing the modifier</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the modifier has already been finalized</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def initialize(self, state: State, **kwargs):\n    \"\"\"\n    Initialize the modifier for the given model and state.\n\n    :raises RuntimeError: if the modifier has already been finalized\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for initializing the modifier\n    \"\"\"\n    if self.initialized_:\n        raise RuntimeError(\n            \"Cannot initialize a modifier that has already been initialized\"\n        )\n\n    if self.finalized_:\n        raise RuntimeError(\n            \"Cannot initialize a modifier that has already been finalized\"\n        )\n\n    self.initialized_ = self.on_initialize(state=state, **kwargs)\n\n    # trigger starts\n    fake_start_event = Event(type_=EventType.BATCH_START, global_step=0)\n    if self.should_start(fake_start_event):\n        self.on_start(state, fake_start_event, **kwargs)\n        self.started_ = True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>on_end is called when the modifier ends and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the end</p> required <code>kwargs</code> <p>Additional arguments for ending the modifier</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_end is called when the modifier ends and must be implemented\n    by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param event: The event that triggered the end\n    :param kwargs: Additional arguments for ending the modifier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.on_event","title":"<code>on_event(state, event, **kwargs)</code>","text":"<p>on_event is called whenever an event is triggered</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the update</p> required <code>kwargs</code> <p>Additional arguments for updating the model</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_event(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_event is called whenever an event is triggered\n\n    :param state: The current state of the model\n    :param event: The event that triggered the update\n    :param kwargs: Additional arguments for updating the model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>on_finalize is called on modifier finalization and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for finalizing the modifier</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier was finalized successfully, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    on_finalize is called on modifier finalization and\n    must be implemented by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for finalizing the modifier\n    :return: True if the modifier was finalized successfully,\n        False otherwise\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>on_initialize is called on modifier initialization and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>kwargs</code> <p>Additional arguments for initializing the modifier</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier was initialized successfully, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>@abstractmethod\ndef on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    on_initialize is called on modifier initialization and\n    must be implemented by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param kwargs: Additional arguments for initializing the modifier\n    :return: True if the modifier was initialized successfully,\n        False otherwise\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.on_start","title":"<code>on_start(state, event, **kwargs)</code>","text":"<p>on_start is called when the modifier starts and must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the start</p> required <code>kwargs</code> <p>Additional arguments for starting the modifier</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_start(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_start is called when the modifier starts and\n    must be implemented by the inheriting modifier.\n\n    :param state: The current state of the model\n    :param event: The event that triggered the start\n    :param kwargs: Additional arguments for starting the modifier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.on_update","title":"<code>on_update(state, event, **kwargs)</code>","text":"<p>on_update is called when the model in question must be updated based on passed in event. Must be implemented by the inheriting modifier.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the model</p> required <code>event</code> <code>Event</code> <p>The event that triggered the update</p> required <code>kwargs</code> <p>Additional arguments for updating the model</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def on_update(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    on_update is called when the model in question must be\n    updated based on passed in event. Must be implemented by the\n    inheriting modifier.\n\n    :param state: The current state of the model\n    :param event: The event that triggered the update\n    :param kwargs: Additional arguments for updating the model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.should_end","title":"<code>should_end(event)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to check if the modifier should end</p> required <p>Returns:</p> Type Description <p>True if the modifier should end based on the given event</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def should_end(self, event: Event):\n    \"\"\"\n    :param event: The event to check if the modifier should end\n    :return: True if the modifier should end based on the given event\n    \"\"\"\n    current = event.current_index\n\n    return self.end is not None and current &gt;= self.end\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.should_start","title":"<code>should_start(event)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to check if the modifier should start</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the modifier should start based on the given event</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def should_start(self, event: Event) -&gt; bool:\n    \"\"\"\n    :param event: The event to check if the modifier should start\n    :return: True if the modifier should start based on the given event\n    \"\"\"\n    if self.start is None:\n        return False\n\n    current = event.current_index\n\n    return self.start &lt;= current and (self.end is None or current &lt; self.end)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/modifier/#llmcompressor.modifiers.modifier.Modifier.update_event","title":"<code>update_event(state, event, **kwargs)</code>","text":"<p>Update modifier based on the given event. In turn calls on_start, on_update, and on_end based on the event and modifier settings. Returns immediately if the modifier is not initialized</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of sparsification</p> required <code>event</code> <code>Event</code> <p>The event to update the modifier with</p> required <code>kwargs</code> <p>Additional arguments for updating the modifier</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the modifier has been finalized</p> Source code in <code>src/llmcompressor/modifiers/modifier.py</code> <pre><code>def update_event(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Update modifier based on the given event. In turn calls\n    on_start, on_update, and on_end based on the event and\n    modifier settings. Returns immediately if the modifier is\n    not initialized\n\n    :raises RuntimeError: if the modifier has been finalized\n    :param state: The current state of sparsification\n    :param event: The event to update the modifier with\n    :param kwargs: Additional arguments for updating the modifier\n    \"\"\"\n    if not self.initialized_:\n        raise RuntimeError(\"Cannot update an uninitialized modifier\")\n\n    if self.finalized_:\n        raise RuntimeError(\"Cannot update a finalized modifier\")\n\n    self.on_event(state, event, **kwargs)\n\n    # handle starting the modifier if needed\n    if (\n        event.type_ == EventType.BATCH_START\n        and not self.started_\n        and self.should_start(event)\n    ):\n        self.on_start(state, event, **kwargs)\n        self.started_ = True\n        self.on_update(state, event, **kwargs)\n\n        return\n\n    # handle ending the modifier if needed\n    if (\n        event.type_ == EventType.BATCH_END\n        and not self.ended_\n        and self.should_end(event)\n    ):\n        self.on_end(state, event, **kwargs)\n        self.ended_ = True\n        self.on_update(state, event, **kwargs)\n\n        return\n\n    if self.started_ and not self.ended_:\n        self.on_update(state, event, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/stage/","title":"llmcompressor.modifiers.stage","text":""},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers","title":"<code>StageModifiers</code>","text":"<p>               Bases: <code>ModifierInterface</code>, <code>BaseModel</code></p> <p>Represents a collection of modifiers that are applied together as a stage.</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <p>The modifiers to apply as a stage</p> required <code>index</code> <p>The index of the stage, if applicable</p> required <code>group</code> <p>The group name of the stage, if applicable</p> required <code>applied</code> <p>Flag for indicating if this stage has has already been applied to the model through finalization</p> required Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>class StageModifiers(ModifierInterface, BaseModel):\n    \"\"\"\n    Represents a collection of modifiers that are applied together as a stage.\n\n    :param modifiers: The modifiers to apply as a stage\n    :param index: The index of the stage, if applicable\n    :param group: The group name of the stage, if applicable\n    :param applied: Flag for indicating if this stage has has already been\n    applied to the model through finalization\n    \"\"\"\n\n    modifiers: List[\"Modifier\"] = Field(default_factory=list)\n    index: Optional[int] = None\n    group: Optional[str] = None\n    applied: bool = False\n\n    @property\n    def initialized(self) -&gt; bool:\n        \"\"\"\n        :return: True if all of the stage modifiers have been initialized,\n            False otherwise\n        \"\"\"\n        return all(mod.initialized for mod in self.modifiers)\n\n    @property\n    def finalized(self) -&gt; bool:\n        \"\"\"\n        :return: True if all of the stage modifiers have been finalized,\n            False otherwise\n        \"\"\"\n        return all(mod.finalized for mod in self.modifiers)\n\n    @property\n    def unique_id(self) -&gt; str:\n        \"\"\"\n        :return: ID for stage containing the name and index\n        \"\"\"\n        return self.group + \"_\" + str(self.index)\n\n    def initialize(self, state: \"State\", **kwargs):\n        \"\"\"\n        Initialize all the stage modifiers\n\n        :param state: The state of current session\n        :param kwargs: Additional kwargs to pass to the modifier(s)\n            initialize method\n        \"\"\"\n\n        if self.applied:\n            return\n\n        accelerator = kwargs.get(\"accelerator\", None)\n        for modifier in self.modifiers:\n            if not modifier.initialized:\n                modifier.initialize(state, **kwargs)\n            if accelerator:\n                accelerator.wait_for_everyone()\n        state.loggers.system.info(tag=\"stage\", string=\"Modifiers initialized\")\n\n    def finalize(self, state: \"State\", **kwargs):\n        \"\"\"\n        Finalize all the stage modifiers and mark the stage as applied\n\n        :param state: The state of current session\n        :param kwargs: Additional kwargs to pass to the modifier(s)\n            finalize method\n        \"\"\"\n\n        if self.applied:\n            return\n\n        accelerator = kwargs.get(\"accelerator\", None)\n        for modifier in self.modifiers:\n            modifier.finalize(state, **kwargs)\n            if accelerator:\n                accelerator.wait_for_everyone()\n\n        self.applied = True\n        state.loggers.system.info(tag=\"stage\", string=\"Modifiers finalized\")\n\n    def update_event(self, state: \"State\", event: \"Event\", **kwargs):\n        \"\"\"\n        Propagate the event to all the stage modifiers\n\n        :param state: The state of current session\n        :param event: The event to propagate\n        :param kwargs: Additional kwargs to pass to the modifier(s)\n            update_event method\n        \"\"\"\n\n        if self.applied:\n            return\n\n        for modifier in self.modifiers:\n            modifier.update_event(state, event, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers.finalized","title":"<code>finalized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if all of the stage modifiers have been finalized, False otherwise</p>"},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers.initialized","title":"<code>initialized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if all of the stage modifiers have been initialized, False otherwise</p>"},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers.unique_id","title":"<code>unique_id</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>ID for stage containing the name and index</p>"},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers.finalize","title":"<code>finalize(state, **kwargs)</code>","text":"<p>Finalize all the stage modifiers and mark the stage as applied</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The state of current session</p> required <code>kwargs</code> <p>Additional kwargs to pass to the modifier(s) finalize method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>def finalize(self, state: \"State\", **kwargs):\n    \"\"\"\n    Finalize all the stage modifiers and mark the stage as applied\n\n    :param state: The state of current session\n    :param kwargs: Additional kwargs to pass to the modifier(s)\n        finalize method\n    \"\"\"\n\n    if self.applied:\n        return\n\n    accelerator = kwargs.get(\"accelerator\", None)\n    for modifier in self.modifiers:\n        modifier.finalize(state, **kwargs)\n        if accelerator:\n            accelerator.wait_for_everyone()\n\n    self.applied = True\n    state.loggers.system.info(tag=\"stage\", string=\"Modifiers finalized\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers.initialize","title":"<code>initialize(state, **kwargs)</code>","text":"<p>Initialize all the stage modifiers</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The state of current session</p> required <code>kwargs</code> <p>Additional kwargs to pass to the modifier(s) initialize method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>def initialize(self, state: \"State\", **kwargs):\n    \"\"\"\n    Initialize all the stage modifiers\n\n    :param state: The state of current session\n    :param kwargs: Additional kwargs to pass to the modifier(s)\n        initialize method\n    \"\"\"\n\n    if self.applied:\n        return\n\n    accelerator = kwargs.get(\"accelerator\", None)\n    for modifier in self.modifiers:\n        if not modifier.initialized:\n            modifier.initialize(state, **kwargs)\n        if accelerator:\n            accelerator.wait_for_everyone()\n    state.loggers.system.info(tag=\"stage\", string=\"Modifiers initialized\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/stage/#llmcompressor.modifiers.stage.StageModifiers.update_event","title":"<code>update_event(state, event, **kwargs)</code>","text":"<p>Propagate the event to all the stage modifiers</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The state of current session</p> required <code>event</code> <code>Event</code> <p>The event to propagate</p> required <code>kwargs</code> <p>Additional kwargs to pass to the modifier(s) update_event method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/stage.py</code> <pre><code>def update_event(self, state: \"State\", event: \"Event\", **kwargs):\n    \"\"\"\n    Propagate the event to all the stage modifiers\n\n    :param state: The state of current session\n    :param event: The event to propagate\n    :param kwargs: Additional kwargs to pass to the modifier(s)\n        update_event method\n    \"\"\"\n\n    if self.applied:\n        return\n\n    for modifier in self.modifiers:\n        modifier.update_event(state, event, **kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/","title":"llmcompressor.modifiers.awq","text":""},{"location":"reference/llmcompressor/modifiers/awq/#llmcompressor.modifiers.awq.AWQMapping","title":"<code>AWQMapping</code>  <code>dataclass</code>","text":"<p>Dataclass storing config of activation mappings to smooth The output activations of smooth_layer are input activations into the balance_layers</p> <p><code>AWQMapping</code>s are resolved into <code>ResolvedMapping</code>s, which retain pointers to the actual <code>torch.nn.Module</code>s and additional metadata at runtime</p> Source code in <code>src/llmcompressor/modifiers/awq/mappings.py</code> <pre><code>@dataclass\nclass AWQMapping:\n    \"\"\"\n    Dataclass storing config of activation mappings to smooth\n    The output activations of smooth_layer are input activations\n    into the balance_layers\n\n    `AWQMapping`s are resolved into `ResolvedMapping`s, which\n    retain pointers to the actual `torch.nn.Module`s and additional\n    metadata at runtime\n    \"\"\"\n\n    smooth_layer: str\n    balance_layers: list[str]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/#llmcompressor.modifiers.awq.AWQModifier","title":"<code>AWQModifier</code>","text":"<p>               Bases: <code>Modifier</code></p> <p>Implements the AWQ (Activation-Weighted Quantization) algorithm, as described in https://arxiv.org/pdf/2306.00978. The algorithm significantly reduces quantization error by protecting only 1% of the most salient weight channels.</p> <p>Instead of relying on raw weight values, AWQ identifies important channels by analyzing activation patterns, focusing on the channels in the weight tensor that are most responsive to the input. To reduce quantization error, it scales these channels in a way that preserves the model's original behavior, using scaling factors computed offline from activation statistics.</p> <p>Because this modifier manipulates the weights of the model, it can only be used in in one-shot and not during training. Activation ranges are determined by running a small set of calibration data through the model.</p> <p>example recipe: <pre><code>AWQModifier:\n  bits: 4\n  mappings:\n    - smooth_layer: \"re:.*self_attn_layer_norm\"\n      balance_layers: [\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"]\n    - smooth_layer: \"re:.*final_layer_norm\"\n      balance_layers: [\"re:.*fc1\"]\n  ]\n  ignore: [\"model.decoder.final_layer_norm\"]\n</code></pre></p> <p>Lifecycle:     - on_initialize         - resolve mappings         - capture kwargs needed for forward passes into modules         - capture input activations to balance layers             - register hook to capture inputs and offload to cpu             - run calibration dataset through, to capture inputs             - clear hooks         - concatenate activations across all batches         - apply smooothing             - find best smoothing scale for each smoothing layer             - apply             - move to next smoothing layer     - on_finalize         - clear resolved mappings and captured activations</p> <p>Parameters:</p> Name Type Description Default <code>mappings</code> <p>list activation layers to smooth, and which layers to scale the output such that activations are smoothed. Each entry of the mapping list should be a list itself, in which the first entry is a list of layers who share the same input activation (the one to be to smoothed) and the second entry is the layer whose output is scaled to achieve the smoothing. If regex is used, it matches layers with the largest overlap in module name.</p> required <code>ignore</code> <p>list of layers to ignore, even if they match a regex in mappings. It should match the name of layers whose outputs are scaled to achieve smoothing (the second entry of the mappings list).</p> required <code>group_size</code> <p>number of weights to group together for scaling</p> required <code>max_chunk_memory</code> <p>maximum memory to use for each chunk of input activations</p> required <code>bits</code> <p>number of bits to quantize the weights to</p> required <code>symmetric</code> <p>whether to use symmetric quantization</p> required <code>duo_scaling</code> <p>whether to use duo scaling, which uses both input activations and weights to determine the scaling factor</p> required Source code in <code>src/llmcompressor/modifiers/awq/base.py</code> <pre><code>class AWQModifier(Modifier):\n    \"\"\"\n    Implements the AWQ (Activation-Weighted Quantization) algorithm,\n    as described in https://arxiv.org/pdf/2306.00978. The algorithm\n    significantly reduces quantization error by protecting only 1%\n    of the most salient weight channels.\n\n    Instead of relying on raw weight values, AWQ identifies important channels by\n    analyzing activation patterns, focusing on the channels in the weight tensor that\n    are most responsive to the input. To reduce quantization error, it scales these\n    channels in a way that preserves the model's original behavior, using scaling\n    factors computed offline from activation statistics.\n\n    Because this modifier manipulates the weights of the model, it can only be used in\n    in one-shot and not during training. Activation ranges are determined by running a\n    small set of calibration data through the model.\n\n    example recipe:\n    ```yaml\n    AWQModifier:\n      bits: 4\n      mappings:\n        - smooth_layer: \"re:.*self_attn_layer_norm\"\n          balance_layers: [\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"]\n        - smooth_layer: \"re:.*final_layer_norm\"\n          balance_layers: [\"re:.*fc1\"]\n      ]\n      ignore: [\"model.decoder.final_layer_norm\"]\n    ```\n\n    Lifecycle:\n        - on_initialize\n            - resolve mappings\n            - capture kwargs needed for forward passes into modules\n            - capture input activations to balance layers\n                - register hook to capture inputs and offload to cpu\n                - run calibration dataset through, to capture inputs\n                - clear hooks\n            - concatenate activations across all batches\n            - apply smooothing\n                - find best smoothing scale for each smoothing layer\n                - apply\n                - move to next smoothing layer\n        - on_finalize\n            - clear resolved mappings and captured activations\n\n    :param mappings: list activation layers to smooth, and which layers to\n        scale the output such that activations are smoothed.\n        Each entry of the mapping list should be a list itself, in which the first\n        entry is a list of layers who share the same input activation (the one to be\n        to smoothed) and the second entry is the layer whose output is scaled to\n        achieve the smoothing.\n        If regex is used, it matches layers with the largest overlap in module name.\n    :param ignore: list of layers to ignore, even if they match a regex in mappings.\n        It should match the name of layers whose outputs are scaled to achieve\n        smoothing (the second entry of the mappings list).\n    :param group_size: number of weights to group together for scaling\n    :param max_chunk_memory: maximum memory to use for each chunk of input activations\n    :param bits: number of bits to quantize the weights to\n    :param symmetric: whether to use symmetric quantization\n    :param duo_scaling: whether to use duo scaling, which uses both input activations\n        and weights to determine the scaling factor\n    \"\"\"\n\n    # Allow arbitrary types because AWQMapping has fields of type torch.nn.Module\n    model_config: ConfigDict = ConfigDict(arbitrary_types_allowed=True)\n\n    mappings: List[AWQMapping] = AWQ_MAPPING_REGISTRY[\"Llama\"]\n    ignore: List[str] = []\n    group_size: int = 128\n    max_chunk_memory: int = 1024 * 1024 * 1024\n    num_bits: int = 4\n    symmetric: bool = False\n    duo_scaling: bool = True\n\n    _resolved_mappings: List[ResolvedMapping] = []\n    _scales: Dict[str, Union[torch.Tensor, List[torch.Tensor]]] = {}\n    _module_kwargs: Dict = {}\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run AWQ on the given state\n\n        :param state: state to run AWQ on\n        :return: True on a successful run, False otherwise\n        \"\"\"\n\n        self._set_resolved_mappings(state.model)\n\n        with calibration_forward_context(state.model):\n            self._set_module_kwargs(state.model, state.data.calib)\n\n        self._setup_scale_hooks()\n        with calibration_forward_context(state.model):\n            self._calibrate(state.model, state.data.calib)\n        self.remove_hooks()\n        self._concat_collected_activations()\n\n        with calibration_forward_context(state.model):\n            self._apply_smoothing(state.model)\n\n        return True\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Clean up by clearing the scale and mapping data\n\n        :param state: unused\n        :return: True\n        \"\"\"\n        if self._scales is not None:\n            self._scales.clear()\n        if self._resolved_mappings is not None:\n            self._resolved_mappings.clear()\n\n        return True\n\n    def _set_resolved_mappings(self, model: Module) -&gt; None:\n        \"\"\"\n        Transforms the list of activations to smooth and their corresponding weights\n        into ResolvedMapping objects, resolving regular expressions.\n        Result is stored in _resolved_mappings.\n\n        For each activation in the mapping list, we find the corresponding weight to\n        balance by searching for the longest substring. For instance, if our balance\n        weight is \".*re:.*q_proj\" and the activation is \"re:.*self_attn_layer_norm\" we\n        would match model.layer.0.p_proj to model.layer.0.self_attn_layer_norm and\n        repeat for model.layer.1 and so on\n        \"\"\"\n        resolved_mappings: list[ResolvedMapping] = []\n        num_skipped_oproj_mappings = 0\n        for mapping in self.mappings:\n            to_smooth_layers = get_layers(mapping.smooth_layer, model)\n            for layer_name, smooth_layer in to_smooth_layers.items():\n                if layer_name not in self.ignore:\n                    balance_layers, balance_names = [], []\n                    for balance_suffix in mapping.balance_layers:\n                        # find the submodule that matches the activation layer\n                        balance_name, balance_layer = get_matching_layer(\n                            balance_suffix, layer_name, model\n                        )\n                        if not balance_layer:\n                            continue\n\n                        # exclude v_proj/o_proj mappings whose shapes are incompatible\n                        # https://github.com/mit-han-lab/llm-awq/pull/67#issuecomment-1681632777\n                        if (\n                            \".v_proj\" in layer_name\n                            and \".o_proj\" in balance_name\n                            and isinstance(smooth_layer, torch.nn.Linear)\n                            and isinstance(balance_layer, torch.nn.Linear)\n                            and smooth_layer.weight.shape != balance_layer.weight.shape\n                        ):\n                            num_skipped_oproj_mappings += 1\n                            continue\n\n                        balance_layers.append(balance_layer)\n                        balance_names.append(balance_name)\n\n                    if len(balance_layers) == 0:\n                        continue\n\n                    # each mapping can contain multiple layers to balance, but only\n                    # one layer to smooth\n                    if len(balance_layers) == 1:\n                        # for single balance layer, parent is the balance layer\n                        parent_name, parent = balance_name, balance_layer\n                    else:\n                        # for multiple balance layers,\n                        # parent of any balance layer is the parent\n                        parent_name, parent = get_parent_by_name(\n                            layer_name=balance_name, model=model\n                        )\n                    resolved_mappings.append(\n                        ResolvedMapping(\n                            layer_name,\n                            smooth_layer,\n                            balance_layers,\n                            balance_names=balance_names,\n                            parent=parent,\n                            parent_name=parent_name,\n                        )\n                    )\n        if num_skipped_oproj_mappings &gt; 0:\n            logger.info(\n                f\"Excluded {num_skipped_oproj_mappings} from resolved \"\n                \"mappings due to shape mismatch\"\n            )\n        self._resolved_mappings = resolved_mappings\n        return\n\n    def _setup_scale_hooks(self) -&gt; None:\n        \"\"\"\n        Attach a forward hook to each activation we want to smooth. This allows us to\n        calculate the dynamic range during calibration\n        \"\"\"\n\n        def create_hook_fn(layer_name):\n            def hook_fn(module, inp, out):\n                inp = inp[0].cpu().detach()\n\n                if layer_name in self._scales:\n                    self._scales[layer_name].append(inp)\n                else:\n                    self._scales[layer_name] = [inp]\n\n            return hook_fn\n\n        for mapping in self._resolved_mappings:\n            name = mapping.smooth_name\n            # storing inps to first balance layer\n            # is enough, as other balance layers\n            # get the same input\n            layer = mapping.balance_layers[0]\n            self.register_hook(layer, create_hook_fn(name), \"forward\")\n\n    @torch.no_grad()\n    def _calibrate(self, model: Module, calibration_dataloader: List) -&gt; None:\n        \"\"\"\n        Catch the output dynamic ranges of each layer that will be smoothed by running\n        forward passes with calibration_dataloader\n        \"\"\"\n        class_name = self.__class__.__name__.replace(\"PyTorch\", \"\")\n        logger.info(\n            f\"Running {class_name} calibration with \"\n            f\"{len(calibration_dataloader)} samples...\"\n        )\n        if not calibration_dataloader:\n            raise ValueError(\n                \"Calibration data loader not set, must populate the calib_data field of\"\n                \" CompressionSession to run the AWQ modifier\"\n            )\n\n        run_calibration_forward(\n            model,\n            calibration_dataloader,\n        )\n\n    def _concat_collected_activations(self) -&gt; None:\n        \"\"\"\n        Concatenate the collected activation values from each forward pass into a single\n        tensor for each layer\n\n        :postcondition: each layer in self._scales will have a single tensor containing\n            all the activation values seen during calibration\n        \"\"\"\n        for mapping in self._resolved_mappings:\n            name = mapping.smooth_name\n            self._scales[name] = torch.cat(self._scales[name], dim=0)\n\n    @torch.no_grad()\n    def _apply_smoothing(self, model: Module) -&gt; None:\n        \"\"\"\n        Calculate the best scaling factors for each layer to smooth activations and\n        apply the scaling factors to the weights of the next layer to offset the\n        smoothing\n\n        :param model: model to apply smoothing to\n        \"\"\"\n        logger.info(\"Smoothing activation scales...\")\n        for mapping in tqdm(self._resolved_mappings):\n            smooth_layer = mapping.smooth_layer\n            balance_layers = mapping.balance_layers\n\n            activations = self._scales[mapping.smooth_name]\n\n            module2inspect = mapping.parent\n\n            # [STEP 1]: Compute per-channel mean of normalised weights\n            # All layer weights are concatted together\n            weight = torch.cat([bl.weight for bl in balance_layers], dim=0)\n            org_shape = weight.shape\n            # The weights are reshaped to be organised by quantization group\n            weight = weight.view(-1, self.group_size)\n            # Calculates the relative magnitude of the weights within\n            # each of the quantization groups, and rescales each group\n            # individually so that each group has weights on a 0-1 scale.\n            w_scale = weight.abs() / (weight.abs().amax(dim=1, keepdim=True) + 1e-6)\n            # Resizes the rescaled weight matrix back up to its original dimensions\n            w_scale = w_scale.view(org_shape)\n            # Gets the average rescaled magnitude for each output channel\n            w_mean = w_scale.mean(0)\n\n            # [STEP 2]: Compute per-channel mean of the input activation with chunking\n            # move inp to cpu to avoid memory leak\n            inp = activations.to(weight.device)\n            inp_flat = activations.cpu().abs().view(-1, inp.shape[-1])\n            num_elements = inp_flat.size(0)\n            num_channels = inp_flat.size(1)\n            element_size_bytes = inp_flat.element_size() * 2  # multiplied by 2 for FP32\n\n            # Calculate chunk size dynamically based on max_chunk_memory\n            chunk_size = int(\n                self.max_chunk_memory // (element_size_bytes * num_channels)\n            )\n            chunk_size = min(chunk_size, num_elements)\n\n            # Use float32 for sum calculation\n            x_sum = torch.zeros(num_channels, dtype=torch.float32, device=inp.device)\n\n            for i in range(0, num_elements, chunk_size):\n                end = min(i + chunk_size, num_elements)\n                chunk_sum = inp_flat[i:end].to(torch.float32).sum(dim=0)\n                x_sum += chunk_sum.to(inp.device)\n\n            x_mean = (x_sum / num_elements).to(inp.dtype)\n\n            # [STEP 3]: Compute output of module\n            fp16_output = self._forward_input_with_kwargs(\n                module=module2inspect,\n                inputs=inp,\n                input_kwargs=_sanitize_kwargs(self._module_kwargs, module2inspect),\n            )\n            fp16_output = fp16_output.clip(\n                torch.finfo(fp16_output.dtype).min, torch.finfo(fp16_output.dtype).max\n            )\n\n            # [STEP 4]: Compute loss\n            best_scales = self._compute_best_scale(\n                inp, w_mean, x_mean, module2inspect, balance_layers, fp16_output\n            )\n\n            scales = best_scales\n\n            @torch.no_grad()\n            def smooth(module):\n                with align_module_device(module):\n                    if module in balance_layers:\n                        module.weight.mul_(scales.view(1, -1).to(module.weight.device))\n                    elif module == smooth_layer:\n                        if module.weight.ndim == 1:\n                            update_offload_parameter(\n                                module,\n                                \"weight\",\n                                module.weight.div(scales.to(module.weight.device)),\n                            )\n                        else:\n                            update_offload_parameter(\n                                module,\n                                \"weight\",\n                                module.weight.div(\n                                    scales.view(-1, 1).to(module.weight.device)\n                                ),\n                            )\n                        if hasattr(module, \"bias\") and module.bias is not None:\n                            update_offload_parameter(\n                                module,\n                                \"bias\",\n                                module.bias.div(scales.to(module.bias.device)),\n                            )\n\n            parent = get_fsdp_parent(mapping.smooth_name, model)\n            if parent is not None:\n                parent.apply(smooth)\n            else:\n                # if we're not running with FSDP we can apply smoothing directly\n                for layer in balance_layers:\n                    smooth(layer)\n                smooth(smooth_layer)\n\n    def _compute_best_scale(\n        self,\n        x: torch.Tensor,\n        w_mean: torch.Tensor,\n        x_mean: torch.Tensor,\n        module2inspect: torch.nn.Module,\n        linears2scale: List[torch.nn.Linear],\n        fp16_output: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute loss and select best scales\n\n        L(s) = || Q(W * s) (s^-1 * X) - W * X ||\n        Q: weight quantization function | _pseudo_quantize_tensor(W * s)\n        X: inputs from calib dataset    | X\n        W: original weights in FP16     | layer\n        s: per channel scaling factor   | s^-1 * X\n        \"\"\"\n        n_grid = 20\n        history = []\n        best_ratio = -1\n        best_scales = None\n        best_error = float(\"inf\")\n\n        org_sd = {k: v.cpu() for k, v in module2inspect.state_dict().items()}\n\n        device = x.device\n        x_mean = x_mean.view(-1).to(device)\n        w_mean = w_mean.view(-1).to(device)\n\n        for ratio in range(n_grid):\n            # create new scales\n            ratio = ratio / n_grid\n\n            # NOTE: s^-1 * x is fused here, according to paper\n            if self.duo_scaling:\n                scales = (x_mean.pow(ratio) / (w_mean.pow(1 - ratio) + 1e-4)).clamp(\n                    min=1e-4\n                )\n            else:\n                scales = x_mean.pow(ratio).clamp(min=1e-4).view(-1)\n            scales = scales / (scales.max() * scales.min()).sqrt()\n            _scalesview = scales.view(1, -1).to(device)\n\n            # avoid scaling values that overflow\n            scales[torch.isinf(scales)] = 1\n            scales[torch.isnan(scales)] = 1\n\n            # Q(W * s)\n            for fc in linears2scale:\n                with align_module_device(fc):\n                    fc.weight.mul_(_scalesview)\n                    update_offload_parameter(\n                        fc,\n                        \"weight\",\n                        _pseudo_quantize_tensor(\n                            w=fc.weight.data,\n                            symmetric=self.symmetric,\n                            bit_width=self.num_bits,\n                            group_size=self.group_size,\n                        )[0]\n                        / _scalesview,\n                    )\n\n            # W * X\n            int_w_output = self._forward_input_with_kwargs(\n                module=module2inspect, inputs=x, input_kwargs=self._module_kwargs\n            )\n            int_w_output = int_w_output.clip(\n                torch.finfo(int_w_output.dtype).min, torch.finfo(int_w_output.dtype).max\n            )\n\n            # compute mean squared error (L2 norm)\n            loss = self._compute_loss(fp16_output, int_w_output, device)\n\n            history.append(loss)\n            if loss &lt; best_error:\n                best_error = loss\n                best_ratio = ratio\n                best_scales = scales.clone()\n            module2inspect.load_state_dict(org_sd)\n\n        if best_ratio == -1:\n            logger.debug(history)\n            raise Exception\n\n        assert (\n            torch.isnan(best_scales).sum() == 0\n        ), f\"Nan found in scales: {best_scales}\"\n\n        return best_scales.detach().cpu()\n\n    @torch.no_grad()\n    def _compute_loss(\n        self,\n        fp16_output: torch.Tensor,\n        int_w_output: torch.Tensor,\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        loss = 0.0\n        fp16_output_flat = fp16_output.view(-1)\n        int_w_output_flat = int_w_output.view(-1)\n        num_elements = fp16_output_flat.size(0)\n        element_size_bytes = fp16_output.element_size()\n\n        # Calculate chunk size dynamically based on max_chunk_memory\n        # Divide the max_chunk_memory by twice the element size\n        chunk_size = self.max_chunk_memory // (element_size_bytes * 2)\n        chunk_size = min(chunk_size, num_elements)\n\n        # Split the computation into chunks\n        fp16_chunks = torch.split(fp16_output_flat, chunk_size)\n        int_w_chunks = torch.split(int_w_output_flat, chunk_size)\n\n        # Compute the loss for each chunk\n        for fp16_chunk, int_w_chunk in zip(fp16_chunks, int_w_chunks):\n            chunk_loss = (\n                (fp16_chunk.to(device) - int_w_chunk.to(device))\n                .float()\n                .pow(2)\n                .sum()\n                .item()\n            )\n            loss += chunk_loss\n\n        # Normalize the loss by the total number of elements\n        loss /= num_elements\n\n        return loss\n\n    def _set_module_kwargs(self, model, dataloader) -&gt; None:\n        _, modules = next(iter(get_layers(\"re:.*layers\", model).items()))\n\n        samples = [batch[\"input_ids\"] for batch in dataloader]\n\n        samples = torch.cat(samples, dim=0)\n\n        inps = []\n        layer_kwargs = {}\n\n        best_device = \"cuda\"\n        modules[0] = modules[0].to(best_device)\n\n        # get input and kwargs to layer 0\n        # with_kwargs is only supported in PyTorch 2.0\n        # use this Catcher hack for now\n        class Catcher(torch.nn.Module):\n            def __init__(self, module):\n                super().__init__()\n                self.module = module\n\n            def forward(self, *args, **kwargs):\n                # assume first input to forward is hidden states\n                if len(args) &gt; 0:\n                    hidden_states = args[0]\n                    del args\n                else:\n                    first_key = list(kwargs.keys())[0]\n                    hidden_states = kwargs.pop(first_key)\n\n                inps.append(hidden_states)\n                layer_kwargs.update(kwargs)\n                raise ValueError  # early exit to break later inference\n\n        # patch layer 0 to catch input and kwargs\n        modules[0] = Catcher(modules[0])\n        try:\n            model(samples.to(next(model.parameters()).device))\n        except ValueError:  # work with early exit\n            pass\n        modules[0] = modules[0].module  # restore\n\n        # Update the layer kwargs with `prepare_inputs_for_generation` method\n        # that takes care of everything to avoid unexpected errors.\n        layer_kwargs = model.prepare_inputs_for_generation(samples, **layer_kwargs)\n        # Pop the input_ids as they are not needed at all.\n        layer_kwargs.pop(\"input_ids\")\n\n        del samples\n        inps = inps[0]\n\n        if layer_kwargs.get(\"attention_mask\") is not None:\n            layer_kwargs[\"attention_mask\"] = layer_kwargs[\"attention_mask\"].to(\n                best_device\n            )\n\n        self._module_kwargs = layer_kwargs\n\n    def _forward_input_with_kwargs(\n        self,\n        module: Module,\n        inputs: torch.Tensor,\n        input_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass with input arguments\n\n        :param module: module to run forward pass on\n        :param inputs: input tensor to pass to the module\n        :param input_kwargs: additional arguments to pass to the module\n        :return: the first output tensor from the forward pass\n        \"\"\"\n        kwargs = input_kwargs or self._module_kwargs\n        kwargs = _sanitize_kwargs(kwargs, module)\n\n        inputs = inputs.to(get_execution_device(module))\n\n        return module(inputs, **kwargs)[0]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/#llmcompressor.modifiers.awq.AWQModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>Clean up by clearing the scale and mapping data</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>unused</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True</p> Source code in <code>src/llmcompressor/modifiers/awq/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Clean up by clearing the scale and mapping data\n\n    :param state: unused\n    :return: True\n    \"\"\"\n    if self._scales is not None:\n        self._scales.clear()\n    if self._resolved_mappings is not None:\n        self._resolved_mappings.clear()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/#llmcompressor.modifiers.awq.AWQModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run AWQ on the given state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>state to run AWQ on</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True on a successful run, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/awq/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run AWQ on the given state\n\n    :param state: state to run AWQ on\n    :return: True on a successful run, False otherwise\n    \"\"\"\n\n    self._set_resolved_mappings(state.model)\n\n    with calibration_forward_context(state.model):\n        self._set_module_kwargs(state.model, state.data.calib)\n\n    self._setup_scale_hooks()\n    with calibration_forward_context(state.model):\n        self._calibrate(state.model, state.data.calib)\n    self.remove_hooks()\n    self._concat_collected_activations()\n\n    with calibration_forward_context(state.model):\n        self._apply_smoothing(state.model)\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/base/","title":"llmcompressor.modifiers.awq.base","text":""},{"location":"reference/llmcompressor/modifiers/awq/base/#llmcompressor.modifiers.awq.base.AWQModifier","title":"<code>AWQModifier</code>","text":"<p>               Bases: <code>Modifier</code></p> <p>Implements the AWQ (Activation-Weighted Quantization) algorithm, as described in https://arxiv.org/pdf/2306.00978. The algorithm significantly reduces quantization error by protecting only 1% of the most salient weight channels.</p> <p>Instead of relying on raw weight values, AWQ identifies important channels by analyzing activation patterns, focusing on the channels in the weight tensor that are most responsive to the input. To reduce quantization error, it scales these channels in a way that preserves the model's original behavior, using scaling factors computed offline from activation statistics.</p> <p>Because this modifier manipulates the weights of the model, it can only be used in in one-shot and not during training. Activation ranges are determined by running a small set of calibration data through the model.</p> <p>example recipe: <pre><code>AWQModifier:\n  bits: 4\n  mappings:\n    - smooth_layer: \"re:.*self_attn_layer_norm\"\n      balance_layers: [\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"]\n    - smooth_layer: \"re:.*final_layer_norm\"\n      balance_layers: [\"re:.*fc1\"]\n  ]\n  ignore: [\"model.decoder.final_layer_norm\"]\n</code></pre></p> <p>Lifecycle:     - on_initialize         - resolve mappings         - capture kwargs needed for forward passes into modules         - capture input activations to balance layers             - register hook to capture inputs and offload to cpu             - run calibration dataset through, to capture inputs             - clear hooks         - concatenate activations across all batches         - apply smooothing             - find best smoothing scale for each smoothing layer             - apply             - move to next smoothing layer     - on_finalize         - clear resolved mappings and captured activations</p> <p>Parameters:</p> Name Type Description Default <code>mappings</code> <p>list activation layers to smooth, and which layers to scale the output such that activations are smoothed. Each entry of the mapping list should be a list itself, in which the first entry is a list of layers who share the same input activation (the one to be to smoothed) and the second entry is the layer whose output is scaled to achieve the smoothing. If regex is used, it matches layers with the largest overlap in module name.</p> required <code>ignore</code> <p>list of layers to ignore, even if they match a regex in mappings. It should match the name of layers whose outputs are scaled to achieve smoothing (the second entry of the mappings list).</p> required <code>group_size</code> <p>number of weights to group together for scaling</p> required <code>max_chunk_memory</code> <p>maximum memory to use for each chunk of input activations</p> required <code>bits</code> <p>number of bits to quantize the weights to</p> required <code>symmetric</code> <p>whether to use symmetric quantization</p> required <code>duo_scaling</code> <p>whether to use duo scaling, which uses both input activations and weights to determine the scaling factor</p> required Source code in <code>src/llmcompressor/modifiers/awq/base.py</code> <pre><code>class AWQModifier(Modifier):\n    \"\"\"\n    Implements the AWQ (Activation-Weighted Quantization) algorithm,\n    as described in https://arxiv.org/pdf/2306.00978. The algorithm\n    significantly reduces quantization error by protecting only 1%\n    of the most salient weight channels.\n\n    Instead of relying on raw weight values, AWQ identifies important channels by\n    analyzing activation patterns, focusing on the channels in the weight tensor that\n    are most responsive to the input. To reduce quantization error, it scales these\n    channels in a way that preserves the model's original behavior, using scaling\n    factors computed offline from activation statistics.\n\n    Because this modifier manipulates the weights of the model, it can only be used in\n    in one-shot and not during training. Activation ranges are determined by running a\n    small set of calibration data through the model.\n\n    example recipe:\n    ```yaml\n    AWQModifier:\n      bits: 4\n      mappings:\n        - smooth_layer: \"re:.*self_attn_layer_norm\"\n          balance_layers: [\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"]\n        - smooth_layer: \"re:.*final_layer_norm\"\n          balance_layers: [\"re:.*fc1\"]\n      ]\n      ignore: [\"model.decoder.final_layer_norm\"]\n    ```\n\n    Lifecycle:\n        - on_initialize\n            - resolve mappings\n            - capture kwargs needed for forward passes into modules\n            - capture input activations to balance layers\n                - register hook to capture inputs and offload to cpu\n                - run calibration dataset through, to capture inputs\n                - clear hooks\n            - concatenate activations across all batches\n            - apply smooothing\n                - find best smoothing scale for each smoothing layer\n                - apply\n                - move to next smoothing layer\n        - on_finalize\n            - clear resolved mappings and captured activations\n\n    :param mappings: list activation layers to smooth, and which layers to\n        scale the output such that activations are smoothed.\n        Each entry of the mapping list should be a list itself, in which the first\n        entry is a list of layers who share the same input activation (the one to be\n        to smoothed) and the second entry is the layer whose output is scaled to\n        achieve the smoothing.\n        If regex is used, it matches layers with the largest overlap in module name.\n    :param ignore: list of layers to ignore, even if they match a regex in mappings.\n        It should match the name of layers whose outputs are scaled to achieve\n        smoothing (the second entry of the mappings list).\n    :param group_size: number of weights to group together for scaling\n    :param max_chunk_memory: maximum memory to use for each chunk of input activations\n    :param bits: number of bits to quantize the weights to\n    :param symmetric: whether to use symmetric quantization\n    :param duo_scaling: whether to use duo scaling, which uses both input activations\n        and weights to determine the scaling factor\n    \"\"\"\n\n    # Allow arbitrary types because AWQMapping has fields of type torch.nn.Module\n    model_config: ConfigDict = ConfigDict(arbitrary_types_allowed=True)\n\n    mappings: List[AWQMapping] = AWQ_MAPPING_REGISTRY[\"Llama\"]\n    ignore: List[str] = []\n    group_size: int = 128\n    max_chunk_memory: int = 1024 * 1024 * 1024\n    num_bits: int = 4\n    symmetric: bool = False\n    duo_scaling: bool = True\n\n    _resolved_mappings: List[ResolvedMapping] = []\n    _scales: Dict[str, Union[torch.Tensor, List[torch.Tensor]]] = {}\n    _module_kwargs: Dict = {}\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run AWQ on the given state\n\n        :param state: state to run AWQ on\n        :return: True on a successful run, False otherwise\n        \"\"\"\n\n        self._set_resolved_mappings(state.model)\n\n        with calibration_forward_context(state.model):\n            self._set_module_kwargs(state.model, state.data.calib)\n\n        self._setup_scale_hooks()\n        with calibration_forward_context(state.model):\n            self._calibrate(state.model, state.data.calib)\n        self.remove_hooks()\n        self._concat_collected_activations()\n\n        with calibration_forward_context(state.model):\n            self._apply_smoothing(state.model)\n\n        return True\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Clean up by clearing the scale and mapping data\n\n        :param state: unused\n        :return: True\n        \"\"\"\n        if self._scales is not None:\n            self._scales.clear()\n        if self._resolved_mappings is not None:\n            self._resolved_mappings.clear()\n\n        return True\n\n    def _set_resolved_mappings(self, model: Module) -&gt; None:\n        \"\"\"\n        Transforms the list of activations to smooth and their corresponding weights\n        into ResolvedMapping objects, resolving regular expressions.\n        Result is stored in _resolved_mappings.\n\n        For each activation in the mapping list, we find the corresponding weight to\n        balance by searching for the longest substring. For instance, if our balance\n        weight is \".*re:.*q_proj\" and the activation is \"re:.*self_attn_layer_norm\" we\n        would match model.layer.0.p_proj to model.layer.0.self_attn_layer_norm and\n        repeat for model.layer.1 and so on\n        \"\"\"\n        resolved_mappings: list[ResolvedMapping] = []\n        num_skipped_oproj_mappings = 0\n        for mapping in self.mappings:\n            to_smooth_layers = get_layers(mapping.smooth_layer, model)\n            for layer_name, smooth_layer in to_smooth_layers.items():\n                if layer_name not in self.ignore:\n                    balance_layers, balance_names = [], []\n                    for balance_suffix in mapping.balance_layers:\n                        # find the submodule that matches the activation layer\n                        balance_name, balance_layer = get_matching_layer(\n                            balance_suffix, layer_name, model\n                        )\n                        if not balance_layer:\n                            continue\n\n                        # exclude v_proj/o_proj mappings whose shapes are incompatible\n                        # https://github.com/mit-han-lab/llm-awq/pull/67#issuecomment-1681632777\n                        if (\n                            \".v_proj\" in layer_name\n                            and \".o_proj\" in balance_name\n                            and isinstance(smooth_layer, torch.nn.Linear)\n                            and isinstance(balance_layer, torch.nn.Linear)\n                            and smooth_layer.weight.shape != balance_layer.weight.shape\n                        ):\n                            num_skipped_oproj_mappings += 1\n                            continue\n\n                        balance_layers.append(balance_layer)\n                        balance_names.append(balance_name)\n\n                    if len(balance_layers) == 0:\n                        continue\n\n                    # each mapping can contain multiple layers to balance, but only\n                    # one layer to smooth\n                    if len(balance_layers) == 1:\n                        # for single balance layer, parent is the balance layer\n                        parent_name, parent = balance_name, balance_layer\n                    else:\n                        # for multiple balance layers,\n                        # parent of any balance layer is the parent\n                        parent_name, parent = get_parent_by_name(\n                            layer_name=balance_name, model=model\n                        )\n                    resolved_mappings.append(\n                        ResolvedMapping(\n                            layer_name,\n                            smooth_layer,\n                            balance_layers,\n                            balance_names=balance_names,\n                            parent=parent,\n                            parent_name=parent_name,\n                        )\n                    )\n        if num_skipped_oproj_mappings &gt; 0:\n            logger.info(\n                f\"Excluded {num_skipped_oproj_mappings} from resolved \"\n                \"mappings due to shape mismatch\"\n            )\n        self._resolved_mappings = resolved_mappings\n        return\n\n    def _setup_scale_hooks(self) -&gt; None:\n        \"\"\"\n        Attach a forward hook to each activation we want to smooth. This allows us to\n        calculate the dynamic range during calibration\n        \"\"\"\n\n        def create_hook_fn(layer_name):\n            def hook_fn(module, inp, out):\n                inp = inp[0].cpu().detach()\n\n                if layer_name in self._scales:\n                    self._scales[layer_name].append(inp)\n                else:\n                    self._scales[layer_name] = [inp]\n\n            return hook_fn\n\n        for mapping in self._resolved_mappings:\n            name = mapping.smooth_name\n            # storing inps to first balance layer\n            # is enough, as other balance layers\n            # get the same input\n            layer = mapping.balance_layers[0]\n            self.register_hook(layer, create_hook_fn(name), \"forward\")\n\n    @torch.no_grad()\n    def _calibrate(self, model: Module, calibration_dataloader: List) -&gt; None:\n        \"\"\"\n        Catch the output dynamic ranges of each layer that will be smoothed by running\n        forward passes with calibration_dataloader\n        \"\"\"\n        class_name = self.__class__.__name__.replace(\"PyTorch\", \"\")\n        logger.info(\n            f\"Running {class_name} calibration with \"\n            f\"{len(calibration_dataloader)} samples...\"\n        )\n        if not calibration_dataloader:\n            raise ValueError(\n                \"Calibration data loader not set, must populate the calib_data field of\"\n                \" CompressionSession to run the AWQ modifier\"\n            )\n\n        run_calibration_forward(\n            model,\n            calibration_dataloader,\n        )\n\n    def _concat_collected_activations(self) -&gt; None:\n        \"\"\"\n        Concatenate the collected activation values from each forward pass into a single\n        tensor for each layer\n\n        :postcondition: each layer in self._scales will have a single tensor containing\n            all the activation values seen during calibration\n        \"\"\"\n        for mapping in self._resolved_mappings:\n            name = mapping.smooth_name\n            self._scales[name] = torch.cat(self._scales[name], dim=0)\n\n    @torch.no_grad()\n    def _apply_smoothing(self, model: Module) -&gt; None:\n        \"\"\"\n        Calculate the best scaling factors for each layer to smooth activations and\n        apply the scaling factors to the weights of the next layer to offset the\n        smoothing\n\n        :param model: model to apply smoothing to\n        \"\"\"\n        logger.info(\"Smoothing activation scales...\")\n        for mapping in tqdm(self._resolved_mappings):\n            smooth_layer = mapping.smooth_layer\n            balance_layers = mapping.balance_layers\n\n            activations = self._scales[mapping.smooth_name]\n\n            module2inspect = mapping.parent\n\n            # [STEP 1]: Compute per-channel mean of normalised weights\n            # All layer weights are concatted together\n            weight = torch.cat([bl.weight for bl in balance_layers], dim=0)\n            org_shape = weight.shape\n            # The weights are reshaped to be organised by quantization group\n            weight = weight.view(-1, self.group_size)\n            # Calculates the relative magnitude of the weights within\n            # each of the quantization groups, and rescales each group\n            # individually so that each group has weights on a 0-1 scale.\n            w_scale = weight.abs() / (weight.abs().amax(dim=1, keepdim=True) + 1e-6)\n            # Resizes the rescaled weight matrix back up to its original dimensions\n            w_scale = w_scale.view(org_shape)\n            # Gets the average rescaled magnitude for each output channel\n            w_mean = w_scale.mean(0)\n\n            # [STEP 2]: Compute per-channel mean of the input activation with chunking\n            # move inp to cpu to avoid memory leak\n            inp = activations.to(weight.device)\n            inp_flat = activations.cpu().abs().view(-1, inp.shape[-1])\n            num_elements = inp_flat.size(0)\n            num_channels = inp_flat.size(1)\n            element_size_bytes = inp_flat.element_size() * 2  # multiplied by 2 for FP32\n\n            # Calculate chunk size dynamically based on max_chunk_memory\n            chunk_size = int(\n                self.max_chunk_memory // (element_size_bytes * num_channels)\n            )\n            chunk_size = min(chunk_size, num_elements)\n\n            # Use float32 for sum calculation\n            x_sum = torch.zeros(num_channels, dtype=torch.float32, device=inp.device)\n\n            for i in range(0, num_elements, chunk_size):\n                end = min(i + chunk_size, num_elements)\n                chunk_sum = inp_flat[i:end].to(torch.float32).sum(dim=0)\n                x_sum += chunk_sum.to(inp.device)\n\n            x_mean = (x_sum / num_elements).to(inp.dtype)\n\n            # [STEP 3]: Compute output of module\n            fp16_output = self._forward_input_with_kwargs(\n                module=module2inspect,\n                inputs=inp,\n                input_kwargs=_sanitize_kwargs(self._module_kwargs, module2inspect),\n            )\n            fp16_output = fp16_output.clip(\n                torch.finfo(fp16_output.dtype).min, torch.finfo(fp16_output.dtype).max\n            )\n\n            # [STEP 4]: Compute loss\n            best_scales = self._compute_best_scale(\n                inp, w_mean, x_mean, module2inspect, balance_layers, fp16_output\n            )\n\n            scales = best_scales\n\n            @torch.no_grad()\n            def smooth(module):\n                with align_module_device(module):\n                    if module in balance_layers:\n                        module.weight.mul_(scales.view(1, -1).to(module.weight.device))\n                    elif module == smooth_layer:\n                        if module.weight.ndim == 1:\n                            update_offload_parameter(\n                                module,\n                                \"weight\",\n                                module.weight.div(scales.to(module.weight.device)),\n                            )\n                        else:\n                            update_offload_parameter(\n                                module,\n                                \"weight\",\n                                module.weight.div(\n                                    scales.view(-1, 1).to(module.weight.device)\n                                ),\n                            )\n                        if hasattr(module, \"bias\") and module.bias is not None:\n                            update_offload_parameter(\n                                module,\n                                \"bias\",\n                                module.bias.div(scales.to(module.bias.device)),\n                            )\n\n            parent = get_fsdp_parent(mapping.smooth_name, model)\n            if parent is not None:\n                parent.apply(smooth)\n            else:\n                # if we're not running with FSDP we can apply smoothing directly\n                for layer in balance_layers:\n                    smooth(layer)\n                smooth(smooth_layer)\n\n    def _compute_best_scale(\n        self,\n        x: torch.Tensor,\n        w_mean: torch.Tensor,\n        x_mean: torch.Tensor,\n        module2inspect: torch.nn.Module,\n        linears2scale: List[torch.nn.Linear],\n        fp16_output: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute loss and select best scales\n\n        L(s) = || Q(W * s) (s^-1 * X) - W * X ||\n        Q: weight quantization function | _pseudo_quantize_tensor(W * s)\n        X: inputs from calib dataset    | X\n        W: original weights in FP16     | layer\n        s: per channel scaling factor   | s^-1 * X\n        \"\"\"\n        n_grid = 20\n        history = []\n        best_ratio = -1\n        best_scales = None\n        best_error = float(\"inf\")\n\n        org_sd = {k: v.cpu() for k, v in module2inspect.state_dict().items()}\n\n        device = x.device\n        x_mean = x_mean.view(-1).to(device)\n        w_mean = w_mean.view(-1).to(device)\n\n        for ratio in range(n_grid):\n            # create new scales\n            ratio = ratio / n_grid\n\n            # NOTE: s^-1 * x is fused here, according to paper\n            if self.duo_scaling:\n                scales = (x_mean.pow(ratio) / (w_mean.pow(1 - ratio) + 1e-4)).clamp(\n                    min=1e-4\n                )\n            else:\n                scales = x_mean.pow(ratio).clamp(min=1e-4).view(-1)\n            scales = scales / (scales.max() * scales.min()).sqrt()\n            _scalesview = scales.view(1, -1).to(device)\n\n            # avoid scaling values that overflow\n            scales[torch.isinf(scales)] = 1\n            scales[torch.isnan(scales)] = 1\n\n            # Q(W * s)\n            for fc in linears2scale:\n                with align_module_device(fc):\n                    fc.weight.mul_(_scalesview)\n                    update_offload_parameter(\n                        fc,\n                        \"weight\",\n                        _pseudo_quantize_tensor(\n                            w=fc.weight.data,\n                            symmetric=self.symmetric,\n                            bit_width=self.num_bits,\n                            group_size=self.group_size,\n                        )[0]\n                        / _scalesview,\n                    )\n\n            # W * X\n            int_w_output = self._forward_input_with_kwargs(\n                module=module2inspect, inputs=x, input_kwargs=self._module_kwargs\n            )\n            int_w_output = int_w_output.clip(\n                torch.finfo(int_w_output.dtype).min, torch.finfo(int_w_output.dtype).max\n            )\n\n            # compute mean squared error (L2 norm)\n            loss = self._compute_loss(fp16_output, int_w_output, device)\n\n            history.append(loss)\n            if loss &lt; best_error:\n                best_error = loss\n                best_ratio = ratio\n                best_scales = scales.clone()\n            module2inspect.load_state_dict(org_sd)\n\n        if best_ratio == -1:\n            logger.debug(history)\n            raise Exception\n\n        assert (\n            torch.isnan(best_scales).sum() == 0\n        ), f\"Nan found in scales: {best_scales}\"\n\n        return best_scales.detach().cpu()\n\n    @torch.no_grad()\n    def _compute_loss(\n        self,\n        fp16_output: torch.Tensor,\n        int_w_output: torch.Tensor,\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        loss = 0.0\n        fp16_output_flat = fp16_output.view(-1)\n        int_w_output_flat = int_w_output.view(-1)\n        num_elements = fp16_output_flat.size(0)\n        element_size_bytes = fp16_output.element_size()\n\n        # Calculate chunk size dynamically based on max_chunk_memory\n        # Divide the max_chunk_memory by twice the element size\n        chunk_size = self.max_chunk_memory // (element_size_bytes * 2)\n        chunk_size = min(chunk_size, num_elements)\n\n        # Split the computation into chunks\n        fp16_chunks = torch.split(fp16_output_flat, chunk_size)\n        int_w_chunks = torch.split(int_w_output_flat, chunk_size)\n\n        # Compute the loss for each chunk\n        for fp16_chunk, int_w_chunk in zip(fp16_chunks, int_w_chunks):\n            chunk_loss = (\n                (fp16_chunk.to(device) - int_w_chunk.to(device))\n                .float()\n                .pow(2)\n                .sum()\n                .item()\n            )\n            loss += chunk_loss\n\n        # Normalize the loss by the total number of elements\n        loss /= num_elements\n\n        return loss\n\n    def _set_module_kwargs(self, model, dataloader) -&gt; None:\n        _, modules = next(iter(get_layers(\"re:.*layers\", model).items()))\n\n        samples = [batch[\"input_ids\"] for batch in dataloader]\n\n        samples = torch.cat(samples, dim=0)\n\n        inps = []\n        layer_kwargs = {}\n\n        best_device = \"cuda\"\n        modules[0] = modules[0].to(best_device)\n\n        # get input and kwargs to layer 0\n        # with_kwargs is only supported in PyTorch 2.0\n        # use this Catcher hack for now\n        class Catcher(torch.nn.Module):\n            def __init__(self, module):\n                super().__init__()\n                self.module = module\n\n            def forward(self, *args, **kwargs):\n                # assume first input to forward is hidden states\n                if len(args) &gt; 0:\n                    hidden_states = args[0]\n                    del args\n                else:\n                    first_key = list(kwargs.keys())[0]\n                    hidden_states = kwargs.pop(first_key)\n\n                inps.append(hidden_states)\n                layer_kwargs.update(kwargs)\n                raise ValueError  # early exit to break later inference\n\n        # patch layer 0 to catch input and kwargs\n        modules[0] = Catcher(modules[0])\n        try:\n            model(samples.to(next(model.parameters()).device))\n        except ValueError:  # work with early exit\n            pass\n        modules[0] = modules[0].module  # restore\n\n        # Update the layer kwargs with `prepare_inputs_for_generation` method\n        # that takes care of everything to avoid unexpected errors.\n        layer_kwargs = model.prepare_inputs_for_generation(samples, **layer_kwargs)\n        # Pop the input_ids as they are not needed at all.\n        layer_kwargs.pop(\"input_ids\")\n\n        del samples\n        inps = inps[0]\n\n        if layer_kwargs.get(\"attention_mask\") is not None:\n            layer_kwargs[\"attention_mask\"] = layer_kwargs[\"attention_mask\"].to(\n                best_device\n            )\n\n        self._module_kwargs = layer_kwargs\n\n    def _forward_input_with_kwargs(\n        self,\n        module: Module,\n        inputs: torch.Tensor,\n        input_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass with input arguments\n\n        :param module: module to run forward pass on\n        :param inputs: input tensor to pass to the module\n        :param input_kwargs: additional arguments to pass to the module\n        :return: the first output tensor from the forward pass\n        \"\"\"\n        kwargs = input_kwargs or self._module_kwargs\n        kwargs = _sanitize_kwargs(kwargs, module)\n\n        inputs = inputs.to(get_execution_device(module))\n\n        return module(inputs, **kwargs)[0]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/base/#llmcompressor.modifiers.awq.base.AWQModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>Clean up by clearing the scale and mapping data</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>unused</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True</p> Source code in <code>src/llmcompressor/modifiers/awq/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Clean up by clearing the scale and mapping data\n\n    :param state: unused\n    :return: True\n    \"\"\"\n    if self._scales is not None:\n        self._scales.clear()\n    if self._resolved_mappings is not None:\n        self._resolved_mappings.clear()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/base/#llmcompressor.modifiers.awq.base.AWQModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run AWQ on the given state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>state to run AWQ on</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True on a successful run, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/awq/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run AWQ on the given state\n\n    :param state: state to run AWQ on\n    :return: True on a successful run, False otherwise\n    \"\"\"\n\n    self._set_resolved_mappings(state.model)\n\n    with calibration_forward_context(state.model):\n        self._set_module_kwargs(state.model, state.data.calib)\n\n    self._setup_scale_hooks()\n    with calibration_forward_context(state.model):\n        self._calibrate(state.model, state.data.calib)\n    self.remove_hooks()\n    self._concat_collected_activations()\n\n    with calibration_forward_context(state.model):\n        self._apply_smoothing(state.model)\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/mappings/","title":"llmcompressor.modifiers.awq.mappings","text":""},{"location":"reference/llmcompressor/modifiers/awq/mappings/#llmcompressor.modifiers.awq.mappings.AWQMapping","title":"<code>AWQMapping</code>  <code>dataclass</code>","text":"<p>Dataclass storing config of activation mappings to smooth The output activations of smooth_layer are input activations into the balance_layers</p> <p><code>AWQMapping</code>s are resolved into <code>ResolvedMapping</code>s, which retain pointers to the actual <code>torch.nn.Module</code>s and additional metadata at runtime</p> Source code in <code>src/llmcompressor/modifiers/awq/mappings.py</code> <pre><code>@dataclass\nclass AWQMapping:\n    \"\"\"\n    Dataclass storing config of activation mappings to smooth\n    The output activations of smooth_layer are input activations\n    into the balance_layers\n\n    `AWQMapping`s are resolved into `ResolvedMapping`s, which\n    retain pointers to the actual `torch.nn.Module`s and additional\n    metadata at runtime\n    \"\"\"\n\n    smooth_layer: str\n    balance_layers: list[str]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/awq/mappings/#llmcompressor.modifiers.awq.mappings.ResolvedMapping","title":"<code>ResolvedMapping</code>  <code>dataclass</code>","text":"<p>Dataclass for storing the resolved mappings between an activation layer and the following weights that must be balanced during smoothing</p> <p>Parameters:</p> Name Type Description Default <code>smooth_name</code> <code>str</code> <p>name of the activation layer</p> required <code>smooth_layer</code> <code>Module</code> <p>PyTorch module storing the activation layer</p> required <code>balance_layers</code> <code>List[Module]</code> <p>list of PyTorch modules that smooth_layer feeds into, must be balanced to offset the smoothing of smooth_layer</p> required <code>balance_names</code> <code>Optional[List[str]]</code> <p>optional list of names of the balance_layers</p> <code>None</code> <code>parent</code> <code>Optional[Module]</code> <p>parent module of the balance_layers</p> <code>None</code> <code>parent_name</code> <code>Optional[str]</code> <p>name of the parent module</p> <code>None</code> Source code in <code>src/llmcompressor/modifiers/awq/mappings.py</code> <pre><code>@dataclass\nclass ResolvedMapping:\n    \"\"\"\n    Dataclass for storing the resolved mappings between an activation layer\n    and the following weights that must be balanced during smoothing\n\n    :param smooth_name: name of the activation layer\n    :param smooth_layer: PyTorch module storing the activation layer\n    :param balance_layers: list of PyTorch modules that smooth_layer feeds into, must be\n        balanced to offset the smoothing of smooth_layer\n    :param balance_names: optional list of names of the balance_layers\n    :param parent: parent module of the balance_layers\n    :param parent_name: name of the parent module\n    \"\"\"\n\n    smooth_name: str\n    smooth_layer: Module\n    balance_layers: List[Module]\n    balance_names: Optional[List[str]] = None\n    parent: Optional[Module] = None\n    parent_name: Optional[str] = None\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/","title":"llmcompressor.modifiers.distillation","text":""},{"location":"reference/llmcompressor/modifiers/distillation/output/","title":"llmcompressor.modifiers.distillation.output","text":""},{"location":"reference/llmcompressor/modifiers/distillation/output/base/","title":"llmcompressor.modifiers.distillation.output.base","text":""},{"location":"reference/llmcompressor/modifiers/distillation/utils/","title":"llmcompressor.modifiers.distillation.utils","text":""},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/","title":"llmcompressor.modifiers.distillation.utils.pytorch","text":""},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/#llmcompressor.modifiers.distillation.utils.pytorch.KDModelWrapper","title":"<code>KDModelWrapper</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper.py</code> <pre><code>class KDModelWrapper(Module):\n    KD_LAST_COMPARISON = \"kd_last_comparison\"\n\n    def __init__(\n        self,\n        student_model: Module,\n        teacher_model: Module,\n        wrappers: Dict[str, Any],\n        comparison,\n        fsdp_active: bool,\n    ):\n        super(KDModelWrapper, self).__init__()\n\n        self.student_model = student_model\n        self.teacher_model = teacher_model\n        self.wrappers = wrappers\n        self.kd_comparison = comparison\n        self._save_active = False\n        self._fsdp_active = fsdp_active\n        self.kd_enabled = False\n        self.register_buffer(self.KD_LAST_COMPARISON, torch.zeros(1, device=\"cpu\"))\n        self._init_called = True  # make sure this is last property to be set\n\n        def _clear_missing_keys(module, incompatible_keys):\n            incompatible_keys.missing_keys.clear()\n\n        self.register_load_state_dict_post_hook(_clear_missing_keys)\n\n    def forward(self, *args, **kwargs):\n        if not self.kd_enabled:\n            return self.student_model(*args, **kwargs)\n\n        org_output = self.student_model(*args, **kwargs)\n        with torch.no_grad():\n            self.teacher_model(*args, **kwargs)\n\n        layerwise_comps = []\n        nonpad_tokens = kwargs[\"attention_mask\"] == 1\n        device = nonpad_tokens.device\n        for key, (student_wrapper, teacher_wrapper) in self.wrappers.items():\n            student_out = student_wrapper.kd_last_transformed.to(device)[nonpad_tokens]\n            teacher_out = teacher_wrapper.kd_last_transformed.to(device)[nonpad_tokens]\n            comp = self.kd_comparison(student_out, teacher_out)\n            layerwise_comps.append(comp)\n\n        setattr(self, self.KD_LAST_COMPARISON, torch.stack(layerwise_comps).mean())\n\n        return org_output\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False, **kwargs):\n        return self.student_model.state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars, **kwargs\n        )\n\n    def load_state_dict(self, state_dict, strict=True):\n        return self.student_model.load_state_dict(state_dict, strict=strict)\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        self.student_model._load_from_state_dict(\n            state_dict=state_dict,\n            prefix=prefix,\n            local_metadata=local_metadata,\n            strict=strict,\n            missing_keys=missing_keys,\n            unexpected_keys=unexpected_keys,\n            error_msgs=error_msgs,\n        )\n\n    def named_modules(\n        self,\n        memo: Optional[Set[\"Module\"]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ):\n        # outside of saving, we want the full names of modules in two cases:\n        # 1. trainer initialization, so teacher is moved to the correct device. This is\n        # caught by the kd_enabled flag, which is set when the modifier is started\n        # 2. running in DataParallel (non-FSDP) mode so the replicate function can pick\n        # up the teacher.\n        if self._save_active or (self.kd_enabled and self._fsdp_active):\n            return self.student_model.named_modules(\n                memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n            )\n\n        return super().named_modules(\n            memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n        )\n\n    def named_children(self):\n        return self.student_model.named_children()\n\n    def train(self, mode: bool = True):\n        self.student_model.train(mode)\n        return self\n\n    def prepare_for_save(self):\n        \"\"\"\n        Prepare model structure to be saved, specifically `self.named_modules`\n        \"\"\"\n        self._save_active = True\n        for student_wrapper, teacher_wrapper in self.wrappers.values():\n            student_wrapper.prepare_for_save()\n            teacher_wrapper.prepare_for_save()\n\n    def finish_save(self):\n        \"\"\"\n        Finish saving model\n        \"\"\"\n        self._save_active = False\n        for student_wrapper, teacher_wrapper in self.wrappers.values():\n            student_wrapper.finish_save()\n            teacher_wrapper.finish_save()\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.student_model, name)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/#llmcompressor.modifiers.distillation.utils.pytorch.KDModelWrapper.finish_save","title":"<code>finish_save()</code>","text":"<p>Finish saving model</p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper.py</code> <pre><code>def finish_save(self):\n    \"\"\"\n    Finish saving model\n    \"\"\"\n    self._save_active = False\n    for student_wrapper, teacher_wrapper in self.wrappers.values():\n        student_wrapper.finish_save()\n        teacher_wrapper.finish_save()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/#llmcompressor.modifiers.distillation.utils.pytorch.KDModelWrapper.prepare_for_save","title":"<code>prepare_for_save()</code>","text":"<p>Prepare model structure to be saved, specifically <code>self.named_modules</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper.py</code> <pre><code>def prepare_for_save(self):\n    \"\"\"\n    Prepare model structure to be saved, specifically `self.named_modules`\n    \"\"\"\n    self._save_active = True\n    for student_wrapper, teacher_wrapper in self.wrappers.values():\n        student_wrapper.prepare_for_save()\n        teacher_wrapper.prepare_for_save()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/#llmcompressor.modifiers.distillation.utils.pytorch.KDModuleWrapper","title":"<code>KDModuleWrapper</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper.py</code> <pre><code>class KDModuleWrapper(Module):\n    KD_TRANSFORMED_BUFFER = \"kd_last_transformed\"\n\n    def __init__(\n        self,\n        layer: Module,\n        hidden_size: Tuple,\n        transforms: Optional[List[TransformFuncType]],\n        fsdp_active: bool,\n        offload_output: bool,\n    ):\n        super(KDModuleWrapper, self).__init__()\n\n        self.layer = layer\n        self._save_active = False\n        self._fsdp_active = fsdp_active\n        self.offload_output = offload_output\n        self.kd_transforms = transforms\n        self.kd_enabled = False\n        self.register_buffer(\n            self.KD_TRANSFORMED_BUFFER, torch.zeros(hidden_size, device=\"cpu\")\n        )\n        self._init_called = True  # make sure this is last property to be set\n\n        def _clear_missing_keys(module, incompatible_keys):\n            incompatible_keys.missing_keys.clear()\n\n        self.register_load_state_dict_post_hook(_clear_missing_keys)\n\n    def forward(self, *args, **kwargs):\n        if not self.kd_enabled:\n            return self.layer(*args, **kwargs)\n\n        org_output = self.layer(*args, **kwargs)\n        output = org_output if isinstance(org_output, torch.Tensor) else org_output[0]\n\n        if self.kd_transforms is not None:\n            for transform in self.kd_transforms:\n                output = transform(output)\n\n        if self.offload_output:\n            output = output.to(\"cpu\")\n        setattr(self, self.KD_TRANSFORMED_BUFFER, output)\n        return org_output\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False, **kwargs):\n        return self.layer.state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars, **kwargs\n        )\n\n    def load_state_dict(self, state_dict, strict=True):\n        return self.layer.load_state_dict(state_dict, strict=strict)\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        self.layer._load_from_state_dict(\n            state_dict=state_dict,\n            prefix=prefix,\n            local_metadata=local_metadata,\n            strict=strict,\n            missing_keys=missing_keys,\n            unexpected_keys=unexpected_keys,\n            error_msgs=error_msgs,\n        )\n\n    def named_modules(\n        self,\n        memo: Optional[Set[\"Module\"]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ):\n        # outside of saving, we want the full names of modules in two cases:\n        # 1. trainer initialization, so teacher is moved to the correct device. This is\n        # caught by the kd_enabled flag, which is set when the modifier is started\n        # 2. running in DataParallel (non-FSDP) mode so the replicate function can pick\n        # up the teacher.\n        if self._save_active or (self.kd_enabled and self._fsdp_active):\n            return self.layer.named_modules(\n                memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n            )\n\n        return super().named_modules(\n            memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n        )\n\n    def prepare_for_save(self):\n        \"\"\"\n        Prepare model structure to be saved, specifically `self.named_modules`\n        \"\"\"\n        self._save_active = True\n\n    def finish_save(self):\n        \"\"\"\n        Finish saving model\n        \"\"\"\n        self._save_active = False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/#llmcompressor.modifiers.distillation.utils.pytorch.KDModuleWrapper.finish_save","title":"<code>finish_save()</code>","text":"<p>Finish saving model</p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper.py</code> <pre><code>def finish_save(self):\n    \"\"\"\n    Finish saving model\n    \"\"\"\n    self._save_active = False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/#llmcompressor.modifiers.distillation.utils.pytorch.KDModuleWrapper.prepare_for_save","title":"<code>prepare_for_save()</code>","text":"<p>Prepare model structure to be saved, specifically <code>self.named_modules</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper.py</code> <pre><code>def prepare_for_save(self):\n    \"\"\"\n    Prepare model structure to be saved, specifically `self.named_modules`\n    \"\"\"\n    self._save_active = True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/kd_factory/","title":"llmcompressor.modifiers.distillation.utils.pytorch.kd_factory","text":""},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper/","title":"llmcompressor.modifiers.distillation.utils.pytorch.kd_wrapper","text":""},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper/#llmcompressor.modifiers.distillation.utils.pytorch.kd_wrapper.KDModuleWrapper","title":"<code>KDModuleWrapper</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper.py</code> <pre><code>class KDModuleWrapper(Module):\n    KD_TRANSFORMED_BUFFER = \"kd_last_transformed\"\n\n    def __init__(\n        self,\n        layer: Module,\n        hidden_size: Tuple,\n        transforms: Optional[List[TransformFuncType]],\n        fsdp_active: bool,\n        offload_output: bool,\n    ):\n        super(KDModuleWrapper, self).__init__()\n\n        self.layer = layer\n        self._save_active = False\n        self._fsdp_active = fsdp_active\n        self.offload_output = offload_output\n        self.kd_transforms = transforms\n        self.kd_enabled = False\n        self.register_buffer(\n            self.KD_TRANSFORMED_BUFFER, torch.zeros(hidden_size, device=\"cpu\")\n        )\n        self._init_called = True  # make sure this is last property to be set\n\n        def _clear_missing_keys(module, incompatible_keys):\n            incompatible_keys.missing_keys.clear()\n\n        self.register_load_state_dict_post_hook(_clear_missing_keys)\n\n    def forward(self, *args, **kwargs):\n        if not self.kd_enabled:\n            return self.layer(*args, **kwargs)\n\n        org_output = self.layer(*args, **kwargs)\n        output = org_output if isinstance(org_output, torch.Tensor) else org_output[0]\n\n        if self.kd_transforms is not None:\n            for transform in self.kd_transforms:\n                output = transform(output)\n\n        if self.offload_output:\n            output = output.to(\"cpu\")\n        setattr(self, self.KD_TRANSFORMED_BUFFER, output)\n        return org_output\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False, **kwargs):\n        return self.layer.state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars, **kwargs\n        )\n\n    def load_state_dict(self, state_dict, strict=True):\n        return self.layer.load_state_dict(state_dict, strict=strict)\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        self.layer._load_from_state_dict(\n            state_dict=state_dict,\n            prefix=prefix,\n            local_metadata=local_metadata,\n            strict=strict,\n            missing_keys=missing_keys,\n            unexpected_keys=unexpected_keys,\n            error_msgs=error_msgs,\n        )\n\n    def named_modules(\n        self,\n        memo: Optional[Set[\"Module\"]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ):\n        # outside of saving, we want the full names of modules in two cases:\n        # 1. trainer initialization, so teacher is moved to the correct device. This is\n        # caught by the kd_enabled flag, which is set when the modifier is started\n        # 2. running in DataParallel (non-FSDP) mode so the replicate function can pick\n        # up the teacher.\n        if self._save_active or (self.kd_enabled and self._fsdp_active):\n            return self.layer.named_modules(\n                memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n            )\n\n        return super().named_modules(\n            memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n        )\n\n    def prepare_for_save(self):\n        \"\"\"\n        Prepare model structure to be saved, specifically `self.named_modules`\n        \"\"\"\n        self._save_active = True\n\n    def finish_save(self):\n        \"\"\"\n        Finish saving model\n        \"\"\"\n        self._save_active = False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper/#llmcompressor.modifiers.distillation.utils.pytorch.kd_wrapper.KDModuleWrapper.finish_save","title":"<code>finish_save()</code>","text":"<p>Finish saving model</p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper.py</code> <pre><code>def finish_save(self):\n    \"\"\"\n    Finish saving model\n    \"\"\"\n    self._save_active = False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper/#llmcompressor.modifiers.distillation.utils.pytorch.kd_wrapper.KDModuleWrapper.prepare_for_save","title":"<code>prepare_for_save()</code>","text":"<p>Prepare model structure to be saved, specifically <code>self.named_modules</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/kd_wrapper.py</code> <pre><code>def prepare_for_save(self):\n    \"\"\"\n    Prepare model structure to be saved, specifically `self.named_modules`\n    \"\"\"\n    self._save_active = True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper/","title":"llmcompressor.modifiers.distillation.utils.pytorch.model_wrapper","text":""},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper/#llmcompressor.modifiers.distillation.utils.pytorch.model_wrapper.KDModelWrapper","title":"<code>KDModelWrapper</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper.py</code> <pre><code>class KDModelWrapper(Module):\n    KD_LAST_COMPARISON = \"kd_last_comparison\"\n\n    def __init__(\n        self,\n        student_model: Module,\n        teacher_model: Module,\n        wrappers: Dict[str, Any],\n        comparison,\n        fsdp_active: bool,\n    ):\n        super(KDModelWrapper, self).__init__()\n\n        self.student_model = student_model\n        self.teacher_model = teacher_model\n        self.wrappers = wrappers\n        self.kd_comparison = comparison\n        self._save_active = False\n        self._fsdp_active = fsdp_active\n        self.kd_enabled = False\n        self.register_buffer(self.KD_LAST_COMPARISON, torch.zeros(1, device=\"cpu\"))\n        self._init_called = True  # make sure this is last property to be set\n\n        def _clear_missing_keys(module, incompatible_keys):\n            incompatible_keys.missing_keys.clear()\n\n        self.register_load_state_dict_post_hook(_clear_missing_keys)\n\n    def forward(self, *args, **kwargs):\n        if not self.kd_enabled:\n            return self.student_model(*args, **kwargs)\n\n        org_output = self.student_model(*args, **kwargs)\n        with torch.no_grad():\n            self.teacher_model(*args, **kwargs)\n\n        layerwise_comps = []\n        nonpad_tokens = kwargs[\"attention_mask\"] == 1\n        device = nonpad_tokens.device\n        for key, (student_wrapper, teacher_wrapper) in self.wrappers.items():\n            student_out = student_wrapper.kd_last_transformed.to(device)[nonpad_tokens]\n            teacher_out = teacher_wrapper.kd_last_transformed.to(device)[nonpad_tokens]\n            comp = self.kd_comparison(student_out, teacher_out)\n            layerwise_comps.append(comp)\n\n        setattr(self, self.KD_LAST_COMPARISON, torch.stack(layerwise_comps).mean())\n\n        return org_output\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False, **kwargs):\n        return self.student_model.state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars, **kwargs\n        )\n\n    def load_state_dict(self, state_dict, strict=True):\n        return self.student_model.load_state_dict(state_dict, strict=strict)\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        self.student_model._load_from_state_dict(\n            state_dict=state_dict,\n            prefix=prefix,\n            local_metadata=local_metadata,\n            strict=strict,\n            missing_keys=missing_keys,\n            unexpected_keys=unexpected_keys,\n            error_msgs=error_msgs,\n        )\n\n    def named_modules(\n        self,\n        memo: Optional[Set[\"Module\"]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ):\n        # outside of saving, we want the full names of modules in two cases:\n        # 1. trainer initialization, so teacher is moved to the correct device. This is\n        # caught by the kd_enabled flag, which is set when the modifier is started\n        # 2. running in DataParallel (non-FSDP) mode so the replicate function can pick\n        # up the teacher.\n        if self._save_active or (self.kd_enabled and self._fsdp_active):\n            return self.student_model.named_modules(\n                memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n            )\n\n        return super().named_modules(\n            memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n        )\n\n    def named_children(self):\n        return self.student_model.named_children()\n\n    def train(self, mode: bool = True):\n        self.student_model.train(mode)\n        return self\n\n    def prepare_for_save(self):\n        \"\"\"\n        Prepare model structure to be saved, specifically `self.named_modules`\n        \"\"\"\n        self._save_active = True\n        for student_wrapper, teacher_wrapper in self.wrappers.values():\n            student_wrapper.prepare_for_save()\n            teacher_wrapper.prepare_for_save()\n\n    def finish_save(self):\n        \"\"\"\n        Finish saving model\n        \"\"\"\n        self._save_active = False\n        for student_wrapper, teacher_wrapper in self.wrappers.values():\n            student_wrapper.finish_save()\n            teacher_wrapper.finish_save()\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.student_model, name)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper/#llmcompressor.modifiers.distillation.utils.pytorch.model_wrapper.KDModelWrapper.finish_save","title":"<code>finish_save()</code>","text":"<p>Finish saving model</p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper.py</code> <pre><code>def finish_save(self):\n    \"\"\"\n    Finish saving model\n    \"\"\"\n    self._save_active = False\n    for student_wrapper, teacher_wrapper in self.wrappers.values():\n        student_wrapper.finish_save()\n        teacher_wrapper.finish_save()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper/#llmcompressor.modifiers.distillation.utils.pytorch.model_wrapper.KDModelWrapper.prepare_for_save","title":"<code>prepare_for_save()</code>","text":"<p>Prepare model structure to be saved, specifically <code>self.named_modules</code></p> Source code in <code>src/llmcompressor/modifiers/distillation/utils/pytorch/model_wrapper.py</code> <pre><code>def prepare_for_save(self):\n    \"\"\"\n    Prepare model structure to be saved, specifically `self.named_modules`\n    \"\"\"\n    self._save_active = True\n    for student_wrapper, teacher_wrapper in self.wrappers.values():\n        student_wrapper.prepare_for_save()\n        teacher_wrapper.prepare_for_save()\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/experimental/","title":"llmcompressor.modifiers.experimental","text":""},{"location":"reference/llmcompressor/modifiers/logarithmic_equalization/","title":"llmcompressor.modifiers.logarithmic_equalization","text":""},{"location":"reference/llmcompressor/modifiers/logarithmic_equalization/#llmcompressor.modifiers.logarithmic_equalization.LogarithmicEqualizationModifier","title":"<code>LogarithmicEqualizationModifier</code>","text":"<p>               Bases: <code>SmoothQuantModifier</code></p> <p>Implements the Logarithmic Equalization Algorithm from  https://arxiv.org/abs/2308.15987.  This modifier performs a channel-wise smoothing of outliers in activations,  making them easier to quantize by reducing the dynamic range. The smoothing is  offset by applying the inverse operation to the next layer of weights, making  the weights slightly more difficult to quantize.</p> <p>Because this modifier manipulates the weights of the model, it should only be  used in one-shot and not during training. Activation ranges are determined by  running a small set of calibration data through the model.</p> <p>This algorithm is very similar to SmoothQuant, changing only how the smoothing  scales are computed. This modifier inherits most functionality from the  SmoothQuantModifier.</p> <p>example recipe:  <pre><code>LogarithmicEqualizationModifier:\n  mappings: [\n    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n    [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n  ]\n  ignore: [\"model.decoder.final_layer_norm\"]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>mappings</code> <p>list activation layers to smooth, and which layers to scale the output such that activations are smoothed. Each entry of the mapping list should be a list itself, in which the first entry is a list of layers who share the same input activation (the one to be to smoothed) and the second entry is the layer whose output is scaled to achieve the smoothing. If regex is used, it matches layers with the largest overlap in module name.</p> required <code>ignore</code> <p>list of layers to ignore, even if they match a regex in mappings. It should match the name of layers whose outputs are scaled to achieve smoothing (the second entry of the mappings list).</p> required <code>num_calibration_steps</code> <p>number of samples to use for calibration, or None to use the whole dataset</p> required <code>calibration_function</code> <p>optional function to use for the forward pass, or None to use the default tensor_module_forward</p> required Source code in <code>src/llmcompressor/modifiers/logarithmic_equalization/base.py</code> <pre><code>class LogarithmicEqualizationModifier(SmoothQuantModifier):\n    \"\"\"\n     Implements the Logarithmic Equalization Algorithm from\n     https://arxiv.org/abs/2308.15987.\n     This modifier performs a channel-wise smoothing of outliers in activations,\n     making them easier to quantize by reducing the dynamic range. The smoothing is\n     offset by applying the inverse operation to the next layer of weights, making\n     the weights slightly more difficult to quantize.\n\n     Because this modifier manipulates the weights of the model, it should only be\n     used in one-shot and not during training. Activation ranges are determined by\n     running a small set of calibration data through the model.\n\n     This algorithm is very similar to SmoothQuant, changing only how the smoothing\n     scales are computed. This modifier inherits most functionality from the\n     SmoothQuantModifier.\n\n    example recipe:\n     ```yaml\n     LogarithmicEqualizationModifier:\n       mappings: [\n         [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n         [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n       ]\n       ignore: [\"model.decoder.final_layer_norm\"]\n     ```\n\n    :param mappings: list activation layers to smooth, and which layers to\n      scale the output such that activations are smoothed.\n      Each entry of the mapping list should be a list itself, in which the first\n      entry is a list of layers who share the same input activation (the one to be\n      to smoothed) and the second entry is the layer whose output is scaled to\n      achieve the smoothing.\n      If regex is used, it matches layers with the largest overlap in module name.\n    :param ignore: list of layers to ignore, even if they match a regex in mappings.\n      It should match the name of layers whose outputs are scaled to achieve\n      smoothing (the second entry of the mappings list).\n    :param num_calibration_steps: number of samples to use for calibration, or None to\n      use the whole dataset\n    :param calibration_function: optional function to use for the forward pass, or None\n      to use the default tensor_module_forward\n    \"\"\"\n\n    def _calculate_smoothing_scales(\n        self, balance_layers: List[Module], activation_scales: torch.Tensor\n    ) -&gt; List[float]:\n        \"\"\"\n        Calculate how much smoothing to apply to each channel based on the dynamic\n        range of the activations and the following weights.\n\n        :param balance_layers: layers to offset activation smoothing to\n        :param activation_scales: channel-wise dynamic range of activations to smooth\n        :return: channel-wise scales to use for smoothing activations\n        \"\"\"\n        # calculate the amount of smoothing to apply\n        # s_j = max(|X_j|) / log2( 2 + max(|X_j|) )\n        # where j is the input channel\n        scales = activation_scales / torch.log2(2 + activation_scales)\n        return scales\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/logarithmic_equalization/base/","title":"llmcompressor.modifiers.logarithmic_equalization.base","text":""},{"location":"reference/llmcompressor/modifiers/logarithmic_equalization/base/#llmcompressor.modifiers.logarithmic_equalization.base.LogarithmicEqualizationModifier","title":"<code>LogarithmicEqualizationModifier</code>","text":"<p>               Bases: <code>SmoothQuantModifier</code></p> <p>Implements the Logarithmic Equalization Algorithm from  https://arxiv.org/abs/2308.15987.  This modifier performs a channel-wise smoothing of outliers in activations,  making them easier to quantize by reducing the dynamic range. The smoothing is  offset by applying the inverse operation to the next layer of weights, making  the weights slightly more difficult to quantize.</p> <p>Because this modifier manipulates the weights of the model, it should only be  used in one-shot and not during training. Activation ranges are determined by  running a small set of calibration data through the model.</p> <p>This algorithm is very similar to SmoothQuant, changing only how the smoothing  scales are computed. This modifier inherits most functionality from the  SmoothQuantModifier.</p> <p>example recipe:  <pre><code>LogarithmicEqualizationModifier:\n  mappings: [\n    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n    [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n  ]\n  ignore: [\"model.decoder.final_layer_norm\"]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>mappings</code> <p>list activation layers to smooth, and which layers to scale the output such that activations are smoothed. Each entry of the mapping list should be a list itself, in which the first entry is a list of layers who share the same input activation (the one to be to smoothed) and the second entry is the layer whose output is scaled to achieve the smoothing. If regex is used, it matches layers with the largest overlap in module name.</p> required <code>ignore</code> <p>list of layers to ignore, even if they match a regex in mappings. It should match the name of layers whose outputs are scaled to achieve smoothing (the second entry of the mappings list).</p> required <code>num_calibration_steps</code> <p>number of samples to use for calibration, or None to use the whole dataset</p> required <code>calibration_function</code> <p>optional function to use for the forward pass, or None to use the default tensor_module_forward</p> required Source code in <code>src/llmcompressor/modifiers/logarithmic_equalization/base.py</code> <pre><code>class LogarithmicEqualizationModifier(SmoothQuantModifier):\n    \"\"\"\n     Implements the Logarithmic Equalization Algorithm from\n     https://arxiv.org/abs/2308.15987.\n     This modifier performs a channel-wise smoothing of outliers in activations,\n     making them easier to quantize by reducing the dynamic range. The smoothing is\n     offset by applying the inverse operation to the next layer of weights, making\n     the weights slightly more difficult to quantize.\n\n     Because this modifier manipulates the weights of the model, it should only be\n     used in one-shot and not during training. Activation ranges are determined by\n     running a small set of calibration data through the model.\n\n     This algorithm is very similar to SmoothQuant, changing only how the smoothing\n     scales are computed. This modifier inherits most functionality from the\n     SmoothQuantModifier.\n\n    example recipe:\n     ```yaml\n     LogarithmicEqualizationModifier:\n       mappings: [\n         [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n         [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n       ]\n       ignore: [\"model.decoder.final_layer_norm\"]\n     ```\n\n    :param mappings: list activation layers to smooth, and which layers to\n      scale the output such that activations are smoothed.\n      Each entry of the mapping list should be a list itself, in which the first\n      entry is a list of layers who share the same input activation (the one to be\n      to smoothed) and the second entry is the layer whose output is scaled to\n      achieve the smoothing.\n      If regex is used, it matches layers with the largest overlap in module name.\n    :param ignore: list of layers to ignore, even if they match a regex in mappings.\n      It should match the name of layers whose outputs are scaled to achieve\n      smoothing (the second entry of the mappings list).\n    :param num_calibration_steps: number of samples to use for calibration, or None to\n      use the whole dataset\n    :param calibration_function: optional function to use for the forward pass, or None\n      to use the default tensor_module_forward\n    \"\"\"\n\n    def _calculate_smoothing_scales(\n        self, balance_layers: List[Module], activation_scales: torch.Tensor\n    ) -&gt; List[float]:\n        \"\"\"\n        Calculate how much smoothing to apply to each channel based on the dynamic\n        range of the activations and the following weights.\n\n        :param balance_layers: layers to offset activation smoothing to\n        :param activation_scales: channel-wise dynamic range of activations to smooth\n        :return: channel-wise scales to use for smoothing activations\n        \"\"\"\n        # calculate the amount of smoothing to apply\n        # s_j = max(|X_j|) / log2( 2 + max(|X_j|) )\n        # where j is the input channel\n        scales = activation_scales / torch.log2(2 + activation_scales)\n        return scales\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/","title":"llmcompressor.modifiers.obcq","text":""},{"location":"reference/llmcompressor/modifiers/obcq/#llmcompressor.modifiers.obcq.SparseGPTModifier","title":"<code>SparseGPTModifier</code>","text":"<p>               Bases: <code>SparsityModifierBase</code></p> <p>Modifier for applying the one-shot SparseGPT algorithm to a model</p> <p>| Sample yaml: |   test_stage: |       obcq_modifiers: |           SparseGPTModifier: |               sparsity: 0.5 |               mask_structure: \"2:4\" |               dampening_frac: 0.001 |               block_size: 128 |               targets: ['Linear'] |               ignore: ['re:.*lm_head']</p> <p>Lifecycle:     - on_initialize         - register_hook(module, calibrate_module, \"forward\")     - on_sequential_batch_end         - sparsify_weight     - on_finalize         - remove_hooks()</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <p>Sparsity to compress model to</p> required <code>sparsity_profile</code> <p>Can be set to 'owl' to use Outlier Weighed Layerwise Sparsity (OWL), more information can be found in the paper https://arxiv.org/pdf/2310.05175</p> required <code>mask_structure</code> <p>String to define the structure of the mask to apply. Must be of the form N:M where N, M are integers that define a custom block shape. Defaults to 0:0 which represents an unstructured mask.</p> required <code>owl_m</code> <p>Number of outliers to use for OWL</p> required <code>owl_lmbda</code> <p>Lambda value to use for OWL</p> required <code>block_size</code> <p>Used to determine number of columns to compress in one pass</p> required <code>dampening_frac</code> <p>Amount of dampening to apply to H, as a fraction of the diagonal norm</p> required <code>preserve_sparsity_mask</code> <p>Whether or not to preserve the sparsity mask during when applying sparsegpt, this becomes useful when starting from a previously pruned model, defaults to False.</p> required <code>offload_hessians</code> <p>Set to True for decreased memory usage but increased runtime.</p> required <code>sequential_targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>targets</code></p> required <code>targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>sequential_targets</code></p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target. Defaults to empty list.</p> required Source code in <code>src/llmcompressor/modifiers/obcq/base.py</code> <pre><code>class SparseGPTModifier(SparsityModifierBase):\n    \"\"\"\n    Modifier for applying the one-shot SparseGPT algorithm to a model\n\n    | Sample yaml:\n    |   test_stage:\n    |       obcq_modifiers:\n    |           SparseGPTModifier:\n    |               sparsity: 0.5\n    |               mask_structure: \"2:4\"\n    |               dampening_frac: 0.001\n    |               block_size: 128\n    |               targets: ['Linear']\n    |               ignore: ['re:.*lm_head']\n\n    Lifecycle:\n        - on_initialize\n            - register_hook(module, calibrate_module, \"forward\")\n        - on_sequential_batch_end\n            - sparsify_weight\n        - on_finalize\n            - remove_hooks()\n\n    :param sparsity: Sparsity to compress model to\n    :param sparsity_profile: Can be set to 'owl' to use Outlier Weighed\n        Layerwise Sparsity (OWL), more information can be found\n        in the paper https://arxiv.org/pdf/2310.05175\n    :param mask_structure: String to define the structure of the mask to apply.\n        Must be of the form N:M where N, M are integers that define a custom block\n        shape. Defaults to 0:0 which represents an unstructured mask.\n    :param owl_m: Number of outliers to use for OWL\n    :param owl_lmbda: Lambda value to use for OWL\n    :param block_size: Used to determine number of columns to compress in one pass\n    :param dampening_frac: Amount of dampening to apply to H, as a fraction of the\n        diagonal norm\n    :param preserve_sparsity_mask: Whether or not to preserve the sparsity mask\n        during when applying sparsegpt, this becomes useful when starting from a\n        previously pruned model, defaults to False.\n    :param offload_hessians: Set to True for decreased memory usage but increased\n        runtime.\n    :param sequential_targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `targets`\n    :param targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `sequential_targets`\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target. Defaults to empty list.\n    \"\"\"\n\n    # modifier arguments\n    block_size: int = 128\n    dampening_frac: Optional[float] = 0.01\n    preserve_sparsity_mask: bool = False\n    offload_hessians: bool = False\n\n    # private variables\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n    _hessians: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(default_factory=dict)\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the hessian of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that the first argument is the input\n        inp = args[0]\n\n        # Initialize hessian if not present\n        if module not in self._num_samples:\n            device = get_execution_device(module)\n            self._hessians[module] = make_empty_hessian(module, device=device)\n            self._num_samples[module] = 0\n\n        # Accumulate hessian with input with optional offloading\n        with self._maybe_onload_hessian(module):\n            self._hessians[module], self._num_samples[module] = accumulate_hessian(\n                inp,\n                module,\n                self._hessians[module],\n                self._num_samples[module],\n            )\n\n    def compress_modules(self):\n        \"\"\"\n        Sparsify modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            sparsity = self._module_sparsities[module]\n            num_samples = self._num_samples[module]\n\n            logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(module), CompressionLogger(\n                module\n            ) as comp_logger:\n                loss, sparsified_weight = sparsify_weight(\n                    module=module,\n                    hessians_dict=self._hessians,\n                    sparsity=sparsity,\n                    prune_n=self._prune_n,\n                    prune_m=self._prune_m,\n                    block_size=self.block_size,\n                    dampening_frac=self.dampening_frac,\n                    preserve_sparsity_mask=self.preserve_sparsity_mask,\n                )\n                comp_logger.set_loss(loss)\n\n            update_offload_parameter(module, \"weight\", sparsified_weight)\n\n            # self._hessians[module] already deleted by sparsify_weight\n            del self._num_samples[module]\n\n    @contextlib.contextmanager\n    def _maybe_onload_hessian(self, module: torch.nn.Module):\n        if self.offload_hessians:\n            device = get_execution_device(module)\n            self._hessians[module] = self._hessians[module].to(device=device)\n\n        yield\n\n        if self.offload_hessians:\n            if module in self._hessians:  # may have been deleted in context\n                self._hessians[module] = self._hessians[module].to(device=\"cpu\")\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        # TODO: modify lifecycle to end on finalize\n        if not self.ended_:\n            self.on_end(state, None)  # remove hooks\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._hessians = dict()\n        self._num_samples = dict()\n        self._module_names = dict()\n        self._module_sparsities = dict()\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/#llmcompressor.modifiers.obcq.SparseGPTModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the hessian of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/obcq/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the hessian of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that the first argument is the input\n    inp = args[0]\n\n    # Initialize hessian if not present\n    if module not in self._num_samples:\n        device = get_execution_device(module)\n        self._hessians[module] = make_empty_hessian(module, device=device)\n        self._num_samples[module] = 0\n\n    # Accumulate hessian with input with optional offloading\n    with self._maybe_onload_hessian(module):\n        self._hessians[module], self._num_samples[module] = accumulate_hessian(\n            inp,\n            module,\n            self._hessians[module],\n            self._num_samples[module],\n        )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/#llmcompressor.modifiers.obcq.SparseGPTModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Sparsify modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/obcq/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Sparsify modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        sparsity = self._module_sparsities[module]\n        num_samples = self._num_samples[module]\n\n        logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(module), CompressionLogger(\n            module\n        ) as comp_logger:\n            loss, sparsified_weight = sparsify_weight(\n                module=module,\n                hessians_dict=self._hessians,\n                sparsity=sparsity,\n                prune_n=self._prune_n,\n                prune_m=self._prune_m,\n                block_size=self.block_size,\n                dampening_frac=self.dampening_frac,\n                preserve_sparsity_mask=self.preserve_sparsity_mask,\n            )\n            comp_logger.set_loss(loss)\n\n        update_offload_parameter(module, \"weight\", sparsified_weight)\n\n        # self._hessians[module] already deleted by sparsify_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/base/","title":"llmcompressor.modifiers.obcq.base","text":""},{"location":"reference/llmcompressor/modifiers/obcq/base/#llmcompressor.modifiers.obcq.base.SparseGPTModifier","title":"<code>SparseGPTModifier</code>","text":"<p>               Bases: <code>SparsityModifierBase</code></p> <p>Modifier for applying the one-shot SparseGPT algorithm to a model</p> <p>| Sample yaml: |   test_stage: |       obcq_modifiers: |           SparseGPTModifier: |               sparsity: 0.5 |               mask_structure: \"2:4\" |               dampening_frac: 0.001 |               block_size: 128 |               targets: ['Linear'] |               ignore: ['re:.*lm_head']</p> <p>Lifecycle:     - on_initialize         - register_hook(module, calibrate_module, \"forward\")     - on_sequential_batch_end         - sparsify_weight     - on_finalize         - remove_hooks()</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <p>Sparsity to compress model to</p> required <code>sparsity_profile</code> <p>Can be set to 'owl' to use Outlier Weighed Layerwise Sparsity (OWL), more information can be found in the paper https://arxiv.org/pdf/2310.05175</p> required <code>mask_structure</code> <p>String to define the structure of the mask to apply. Must be of the form N:M where N, M are integers that define a custom block shape. Defaults to 0:0 which represents an unstructured mask.</p> required <code>owl_m</code> <p>Number of outliers to use for OWL</p> required <code>owl_lmbda</code> <p>Lambda value to use for OWL</p> required <code>block_size</code> <p>Used to determine number of columns to compress in one pass</p> required <code>dampening_frac</code> <p>Amount of dampening to apply to H, as a fraction of the diagonal norm</p> required <code>preserve_sparsity_mask</code> <p>Whether or not to preserve the sparsity mask during when applying sparsegpt, this becomes useful when starting from a previously pruned model, defaults to False.</p> required <code>offload_hessians</code> <p>Set to True for decreased memory usage but increased runtime.</p> required <code>sequential_targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>targets</code></p> required <code>targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>sequential_targets</code></p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target. Defaults to empty list.</p> required Source code in <code>src/llmcompressor/modifiers/obcq/base.py</code> <pre><code>class SparseGPTModifier(SparsityModifierBase):\n    \"\"\"\n    Modifier for applying the one-shot SparseGPT algorithm to a model\n\n    | Sample yaml:\n    |   test_stage:\n    |       obcq_modifiers:\n    |           SparseGPTModifier:\n    |               sparsity: 0.5\n    |               mask_structure: \"2:4\"\n    |               dampening_frac: 0.001\n    |               block_size: 128\n    |               targets: ['Linear']\n    |               ignore: ['re:.*lm_head']\n\n    Lifecycle:\n        - on_initialize\n            - register_hook(module, calibrate_module, \"forward\")\n        - on_sequential_batch_end\n            - sparsify_weight\n        - on_finalize\n            - remove_hooks()\n\n    :param sparsity: Sparsity to compress model to\n    :param sparsity_profile: Can be set to 'owl' to use Outlier Weighed\n        Layerwise Sparsity (OWL), more information can be found\n        in the paper https://arxiv.org/pdf/2310.05175\n    :param mask_structure: String to define the structure of the mask to apply.\n        Must be of the form N:M where N, M are integers that define a custom block\n        shape. Defaults to 0:0 which represents an unstructured mask.\n    :param owl_m: Number of outliers to use for OWL\n    :param owl_lmbda: Lambda value to use for OWL\n    :param block_size: Used to determine number of columns to compress in one pass\n    :param dampening_frac: Amount of dampening to apply to H, as a fraction of the\n        diagonal norm\n    :param preserve_sparsity_mask: Whether or not to preserve the sparsity mask\n        during when applying sparsegpt, this becomes useful when starting from a\n        previously pruned model, defaults to False.\n    :param offload_hessians: Set to True for decreased memory usage but increased\n        runtime.\n    :param sequential_targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `targets`\n    :param targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `sequential_targets`\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target. Defaults to empty list.\n    \"\"\"\n\n    # modifier arguments\n    block_size: int = 128\n    dampening_frac: Optional[float] = 0.01\n    preserve_sparsity_mask: bool = False\n    offload_hessians: bool = False\n\n    # private variables\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n    _hessians: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(default_factory=dict)\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the hessian of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that the first argument is the input\n        inp = args[0]\n\n        # Initialize hessian if not present\n        if module not in self._num_samples:\n            device = get_execution_device(module)\n            self._hessians[module] = make_empty_hessian(module, device=device)\n            self._num_samples[module] = 0\n\n        # Accumulate hessian with input with optional offloading\n        with self._maybe_onload_hessian(module):\n            self._hessians[module], self._num_samples[module] = accumulate_hessian(\n                inp,\n                module,\n                self._hessians[module],\n                self._num_samples[module],\n            )\n\n    def compress_modules(self):\n        \"\"\"\n        Sparsify modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            sparsity = self._module_sparsities[module]\n            num_samples = self._num_samples[module]\n\n            logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(module), CompressionLogger(\n                module\n            ) as comp_logger:\n                loss, sparsified_weight = sparsify_weight(\n                    module=module,\n                    hessians_dict=self._hessians,\n                    sparsity=sparsity,\n                    prune_n=self._prune_n,\n                    prune_m=self._prune_m,\n                    block_size=self.block_size,\n                    dampening_frac=self.dampening_frac,\n                    preserve_sparsity_mask=self.preserve_sparsity_mask,\n                )\n                comp_logger.set_loss(loss)\n\n            update_offload_parameter(module, \"weight\", sparsified_weight)\n\n            # self._hessians[module] already deleted by sparsify_weight\n            del self._num_samples[module]\n\n    @contextlib.contextmanager\n    def _maybe_onload_hessian(self, module: torch.nn.Module):\n        if self.offload_hessians:\n            device = get_execution_device(module)\n            self._hessians[module] = self._hessians[module].to(device=device)\n\n        yield\n\n        if self.offload_hessians:\n            if module in self._hessians:  # may have been deleted in context\n                self._hessians[module] = self._hessians[module].to(device=\"cpu\")\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        # TODO: modify lifecycle to end on finalize\n        if not self.ended_:\n            self.on_end(state, None)  # remove hooks\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._hessians = dict()\n        self._num_samples = dict()\n        self._module_names = dict()\n        self._module_sparsities = dict()\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/base/#llmcompressor.modifiers.obcq.base.SparseGPTModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the hessian of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/obcq/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the hessian of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that the first argument is the input\n    inp = args[0]\n\n    # Initialize hessian if not present\n    if module not in self._num_samples:\n        device = get_execution_device(module)\n        self._hessians[module] = make_empty_hessian(module, device=device)\n        self._num_samples[module] = 0\n\n    # Accumulate hessian with input with optional offloading\n    with self._maybe_onload_hessian(module):\n        self._hessians[module], self._num_samples[module] = accumulate_hessian(\n            inp,\n            module,\n            self._hessians[module],\n            self._num_samples[module],\n        )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/base/#llmcompressor.modifiers.obcq.base.SparseGPTModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Sparsify modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/obcq/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Sparsify modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        sparsity = self._module_sparsities[module]\n        num_samples = self._num_samples[module]\n\n        logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(module), CompressionLogger(\n            module\n        ) as comp_logger:\n            loss, sparsified_weight = sparsify_weight(\n                module=module,\n                hessians_dict=self._hessians,\n                sparsity=sparsity,\n                prune_n=self._prune_n,\n                prune_m=self._prune_m,\n                block_size=self.block_size,\n                dampening_frac=self.dampening_frac,\n                preserve_sparsity_mask=self.preserve_sparsity_mask,\n            )\n            comp_logger.set_loss(loss)\n\n        update_offload_parameter(module, \"weight\", sparsified_weight)\n\n        # self._hessians[module] already deleted by sparsify_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/sgpt_base/","title":"llmcompressor.modifiers.obcq.sgpt_base","text":""},{"location":"reference/llmcompressor/modifiers/obcq/sgpt_base/#llmcompressor.modifiers.obcq.sgpt_base.SparsityModifierBase","title":"<code>SparsityModifierBase</code>","text":"<p>               Bases: <code>Modifier</code></p> <p>Abstract base class which implements functionality related to oneshot sparsity. Inheriters must implement <code>calibrate_module</code> and <code>compress_modules</code></p> Source code in <code>src/llmcompressor/modifiers/obcq/sgpt_base.py</code> <pre><code>class SparsityModifierBase(Modifier):\n    \"\"\"\n    Abstract base class which implements functionality related to oneshot sparsity.\n    Inheriters must implement `calibrate_module` and `compress_modules`\n    \"\"\"\n\n    # modifier arguments\n    sparsity: Optional[Union[float, List[float]]]\n    sparsity_profile: Optional[str] = None\n    mask_structure: str = \"0:0\"\n    owl_m: Optional[int] = None\n    owl_lmbda: Optional[float] = None\n\n    # data pipeline arguments\n    sequential_update: Optional[bool] = False  # deprecated\n    sequential_targets: Union[str, List[str], None] = None\n    targets: Union[str, List[str]] = [\"Linear\"]\n    ignore: List[str] = Field(default_factory=list)\n\n    # private variables\n    _prune_n: Optional[int] = PrivateAttr(default=None)\n    _prune_m: Optional[int] = PrivateAttr(default=None)\n    _module_names: Dict[torch.nn.Module, str] = PrivateAttr(default_factory=dict)\n    _target_layers: Dict[str, torch.nn.Module] = PrivateAttr(default_factory=dict)\n    _module_sparsities: Dict[torch.nn.Module, str] = PrivateAttr(default_factory=dict)\n\n    @field_validator(\"sequential_update\", mode=\"before\")\n    def validate_sequential_update(cls, value: bool) -&gt; bool:\n        if not value:\n            warnings.warn(\n                \"`sequential_update=False` is no longer supported, setting \"\n                \"sequential_update=True\",\n                DeprecationWarning,\n            )\n\n        return True\n\n    @field_validator(\"sparsity_profile\", mode=\"before\")\n    def validate_sparsity_profile(cls, value: Optional[str]) -&gt; bool:\n        if value is None:\n            return value\n\n        value = value.lower()\n\n        profile_options = [\"owl\"]\n        if value not in profile_options:\n            raise ValueError(f\"Please choose profile from {profile_options}\")\n\n        return value\n\n    @model_validator(mode=\"after\")\n    def validate_model_after(model: \"SparsityModifierBase\") -&gt; \"SparsityModifierBase\":\n        profile = model.sparsity_profile\n        owl_m = model.owl_m\n        owl_lmbda = model.owl_lmbda\n        mask_structure = model.mask_structure\n\n        has_owl_m = owl_m is not None\n        has_owl_lmbda = owl_lmbda is not None\n        has_owl = profile == \"owl\"\n        owl_args = (has_owl_m, has_owl_lmbda, has_owl)\n        if any(owl_args) and not all(owl_args):\n            raise ValueError(\n                'Must provide all of `profile=\"owl\"`, `owl_m` and `owl_lmbda` or none'\n            )\n\n        model._prune_n, model._prune_m = model._split_mask_structure(mask_structure)\n\n        return model\n\n    @abstractmethod\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def compress_modules(self):\n        raise NotImplementedError()\n\n    def on_initialize(self, state: \"State\", **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run the OBCQ algorithm on the current state\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        model: torch.nn.Module = state.model\n        dataloader: torch.utils.data.DataLoader = state.data.calib\n\n        # infer module and sequential targets\n        self.sequential_targets = self._infer_sequential_targets(model)\n        layers = get_layers(self.sequential_targets, model)\n        self._target_layers = get_layers(\n            self.targets, model\n        )  # layers containing targets\n\n        # infer layer sparsities\n        if self.sparsity_profile == \"owl\":\n            logger.info(\n                \"Using OWL to infer target layer-wise sparsities from \"\n                f\"{len(dataloader) if dataloader else 0} calibration samples...\"\n            )\n            self.sparsity = self._infer_owl_layer_sparsity(model, layers, dataloader)\n\n        # get layers and validate sparsity\n        if isinstance(self.sparsity, (list, dict)) and len(self._target_layers) != len(\n            self.sparsity\n        ):\n            raise ValueError(\n                f\"{self.__repr_name__} was initialized with {len(self.sparsity)} \"\n                f\"sparsities values, but model has {len(layers)} target layers\"\n            )\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        self.started_ = True\n\n        # register hooks\n        for index, (layer_name, layer) in enumerate(self._target_layers.items()):\n            if isinstance(self.sparsity, dict):\n                layer_sparsity = self.sparsity[layer_name]\n            elif isinstance(self.sparsity, list):\n                layer_sparsity = self.sparsity[index]\n            else:\n                layer_sparsity = self.sparsity\n\n            for name, module in get_prunable_layers(layer).items():\n                name = f\"{layer_name}.{name}\"\n\n                if match_targets(name, self.ignore)[0]:\n                    continue\n\n                # HACK: previously, embeddings were not quantized because they were not\n                # accessible by the layer compressor. For now, we manually ignore it,\n                # but in the FUTURE this should be ignored by the user\n                if isinstance(module, torch.nn.Embedding):\n                    continue\n\n                if name.endswith(\"lm_head\"):\n                    logger.warning(\n                        \"`lm_head` was previously auto-ignored by SparseGPT and Wanda \"\n                        \"modifiers and is not advised. Please add `re:.*lm_head` to \"\n                        \"your ignore list if this was unintentional\"\n                    )\n\n                self._module_names[module] = name\n                self._module_sparsities[module] = layer_sparsity\n                self.register_hook(module, self.calibrate_module, \"forward\")\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n            self.compress_modules()\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            self.compress_modules()\n\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        self.ended_ = True\n        self.remove_hooks()\n\n    def _infer_sequential_targets(\n        self, model: torch.nn.Module\n    ) -&gt; Union[str, List[str]]:\n        if self.sequential_targets is None:\n            return get_no_split_params(model)\n        if isinstance(self.sequential_targets, str):\n            return [self.sequential_targets]\n        return self.sequential_targets\n\n    def _infer_owl_layer_sparsity(\n        self,\n        model: torch.nn.Module,\n        layers: Dict[str, torch.nn.Module],\n        dataloader: torch.utils.data.DataLoader,\n    ) -&gt; Dict[str, float]:\n        activations = self._get_activations(model, dataloader)\n\n        groups = {}\n        for name, layer in layers.items():\n            prunable_layers = get_prunable_layers(layer)\n            z = [\n                m.weight.abs() * activations[f\"{name}.{n}\"].unsqueeze(0)\n                for n, m in prunable_layers.items()\n            ]\n            groups[name] = torch.cat([item.flatten().cpu() for item in z])\n\n        del activations\n\n        outlier_ratios = {}\n        for group in groups:\n            threshold = torch.mean(groups[group]) * self.owl_m\n            outlier_ratios[group] = (\n                100 * (groups[group] &gt; threshold).sum().item() / groups[group].numel()\n            )\n        outlier_ratios_arr = numpy.array([outlier_ratios[k] for k in outlier_ratios])\n        for k in outlier_ratios:\n            outlier_ratios[k] = (outlier_ratios[k] - outlier_ratios_arr.min()) * (\n                1\n                / (outlier_ratios_arr.max() - outlier_ratios_arr.min())\n                * self.owl_lmbda\n                * 2\n            )\n        outlier_ratios_arr = numpy.array([outlier_ratios[k] for k in outlier_ratios])\n        sparsities = {\n            k: 1\n            - (\n                outlier_ratios[k]\n                - numpy.mean(outlier_ratios_arr)\n                + (1 - float(self.sparsity))\n            )\n            for k in outlier_ratios\n        }\n        logger.info(f\"OWL sparsities for sp={self.sparsity} are:\")\n        for k in sparsities:\n            logger.info(f\"Sparsity for {k}: {sparsities[k]}\")\n        return sparsities\n\n    def _get_activations(self, model, dataloader, nsamples=128) -&gt; Dict[str, int]:\n        from llmcompressor.pipelines.basic import run_calibration\n\n        acts = defaultdict(int)\n\n        def save_acts(_module, input: Union[Tuple[Any, ...], torch.Tensor], name: str):\n            nonlocal acts\n            if isinstance(input, tuple):\n                input = input[0]\n            acts[name] += 1.0 / nsamples * input.pow(2).sum(dim=(0, 1)).sqrt()\n\n        hooks = set(\n            self.register_hook(mod, partial(save_acts, name=name), \"forward_pre\")\n            for name, mod in model.named_modules()\n            if isinstance(mod, torch.nn.Linear) and \"lm_head\" not in name\n        )\n        with HooksMixin.disable_hooks(keep=hooks):\n            run_calibration(model, dataloader)\n        self.remove_hooks(hooks)\n\n        return acts\n\n    def _split_mask_structure(self, mask_structure: str) -&gt; Tuple[int, int]:\n        n, m = mask_structure.split(\":\")\n        return int(n), int(m)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/sgpt_base/#llmcompressor.modifiers.obcq.sgpt_base.SparsityModifierBase.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run the OBCQ algorithm on the current state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/obcq/sgpt_base.py</code> <pre><code>def on_initialize(self, state: \"State\", **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run the OBCQ algorithm on the current state\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    model: torch.nn.Module = state.model\n    dataloader: torch.utils.data.DataLoader = state.data.calib\n\n    # infer module and sequential targets\n    self.sequential_targets = self._infer_sequential_targets(model)\n    layers = get_layers(self.sequential_targets, model)\n    self._target_layers = get_layers(\n        self.targets, model\n    )  # layers containing targets\n\n    # infer layer sparsities\n    if self.sparsity_profile == \"owl\":\n        logger.info(\n            \"Using OWL to infer target layer-wise sparsities from \"\n            f\"{len(dataloader) if dataloader else 0} calibration samples...\"\n        )\n        self.sparsity = self._infer_owl_layer_sparsity(model, layers, dataloader)\n\n    # get layers and validate sparsity\n    if isinstance(self.sparsity, (list, dict)) and len(self._target_layers) != len(\n        self.sparsity\n    ):\n        raise ValueError(\n            f\"{self.__repr_name__} was initialized with {len(self.sparsity)} \"\n            f\"sparsities values, but model has {len(layers)} target layers\"\n        )\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/obcq/sgpt_sparsify/","title":"llmcompressor.modifiers.obcq.sgpt_sparsify","text":""},{"location":"reference/llmcompressor/modifiers/obcq/sgpt_sparsify/#llmcompressor.modifiers.obcq.sgpt_sparsify.sparsify_weight","title":"<code>sparsify_weight(module, hessians_dict, sparsity, prune_n, prune_m, block_size, dampening_frac, preserve_sparsity_mask)</code>","text":"<p>Run pruning on the layer up to the target sparsity value.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module with weight being sparsified</p> required <code>hessian_dict</code> <p>dictionary containing preaccumulated hessian for sparsification</p> required <code>sparsity</code> <code>float</code> <p>target sparsity to reach for layer</p> required <code>prune_n</code> <code>int</code> <p>N for N:M pruning</p> required <code>prune_m</code> <code>int</code> <p>M for N:M pruning</p> required <code>block_size</code> <code>int</code> <p>Number of columns to compress in one pass</p> required <code>dampening_frac</code> <code>float</code> <p>Amount of dampening to apply to H, as a fraction of the diagonal norm</p> required <code>preserve_sparsity_mask</code> <code>bool</code> <p>Extend or ignore the base sparsity mask</p> required Source code in <code>src/llmcompressor/modifiers/obcq/sgpt_sparsify.py</code> <pre><code>def sparsify_weight(\n    module: torch.nn.Module,\n    hessians_dict: Dict[torch.nn.Module, torch.Tensor],\n    sparsity: float,\n    prune_n: int,\n    prune_m: int,\n    block_size: int,\n    dampening_frac: float,\n    preserve_sparsity_mask: bool,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Run pruning on the layer up to the target sparsity value.\n\n    :param module: module with weight being sparsified\n    :param hessian_dict: dictionary containing preaccumulated hessian for sparsification\n    :param sparsity: target sparsity to reach for layer\n    :param prune_n: N for N:M pruning\n    :param prune_m: M for N:M pruning\n    :param block_size: Number of columns to compress in one pass\n    :param dampening_frac: Amount of dampening to apply to H, as a fraction of the\n        diagonal norm\n    :param preserve_sparsity_mask: Extend or ignore the base sparsity mask\n    \"\"\"\n    final_shape = module.weight.shape\n    final_dtype = module.weight.dtype\n    W = module.weight.clone()\n    H = hessians_dict[module]  # unfortunately python does not have a `move` keyword\n    del hessians_dict[module]  # so we have to delete the original reference manually\n\n    # standardize shape and dtype\n    if isinstance(module, torch.nn.Conv2d):\n        W = W.flatten(1)\n    elif isinstance(module, transformers.Conv1D):\n        W.transpose_(0, 1)\n    W = W.to(dtype=SGPT_PRECISION)\n    num_rows = W.shape[0]\n    num_columns = W.shape[1]\n\n    # mask dead hessian values\n    dead = torch.diag(H) == 0\n    H[dead, dead] = 1\n    W[:, dead] = 0\n\n    # compute inverse hessian in place to save memory\n    try:\n        damp = dampening_frac * torch.mean(torch.diag(H))\n        diag = torch.arange(H.shape[0], device=H.device)\n        H[diag, diag] += damp\n        H = torch.linalg.cholesky(H)\n        H = torch.cholesky_inverse(H)\n        H = torch.linalg.cholesky(H, upper=True)\n        Hinv = H\n    except torch._C._LinAlgError:\n        raise torch._C._LinAlgError(\n            \"Failed to invert hessian due to numerical instability. Consider \"\n            \"increasing SparseGPTModifier.dampening_frac, increasing the number \"\n            \"of calibration samples, or shuffling the calibration dataset\"\n        )\n\n    # sparsity mask\n    # TODO: consider computing sparsity mask in the same way and place as gptq\n    mask = None\n    if preserve_sparsity_mask:\n        # compute existing sparsity mask\n        mask = torch.where(\n            W == 0,\n            torch.tensor(1, dtype=torch.bool),\n            torch.tensor(0, dtype=torch.bool),\n        )\n        current_sparsity = mask.sum() / W.numel()\n        if current_sparsity &gt; sparsity:\n            raise ValueError(\n                \"The target sparsity is lower than the sparsity \"\n                \"of the base model. Please retry \"\n                \"after turning preserve_sparsity_mask=False\"\n            )\n\n    losses = torch.zeros(num_rows, device=module.weight.device)\n\n    # See section 3.4 of https://arxiv.org/abs/2203.07259\n    for i1 in range(0, num_columns, block_size):\n        i2 = min(i1 + block_size, num_columns)\n        count = i2 - i1\n\n        W1 = W[:, i1:i2].clone()\n        Q1 = torch.zeros_like(W1)\n        Err1 = torch.zeros_like(W1)\n        Losses1 = torch.zeros_like(W1)\n        Hinv1 = Hinv[i1:i2, i1:i2]\n\n        if prune_n == 0:\n            if mask is not None:\n                mask1 = mask[:, i1:i2]\n                if int(W1.numel() * sparsity) &gt; mask1.sum():\n                    # target sparsity is higher than base sparsity, extend mask1\n                    tmp = (\n                        (~mask[:, i1:i2])\n                        * W1**2\n                        / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n                    )\n                    thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n                    mask1 = tmp &lt;= thresh\n            else:\n                tmp = W1**2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n                thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n                mask1 = tmp &lt;= thresh\n        else:\n            if mask is not None:\n                mask1 = mask[:, i1:i2]\n            else:\n                mask1 = torch.zeros_like(W1) == 1\n\n        for i in range(count):\n            w = W1[:, i]\n            d = Hinv1[i, i]\n\n            if prune_n != 0 and i % prune_m == 0:\n                tmp = (\n                    W1[:, i : (i + prune_m)] ** 2\n                    / (torch.diag(Hinv1)[i : (i + prune_m)].reshape((1, -1))) ** 2\n                )\n                if mask is not None:\n                    tmp = tmp * (~mask[:, i : (i + prune_m)])\n\n                mask1.scatter_(\n                    1, i + torch.topk(tmp, prune_n, dim=1, largest=False)[1], True\n                )\n\n            q = w.clone()\n            q[mask1[:, i]] = 0\n\n            Q1[:, i] = q\n            Losses1[:, i] = (w - q) ** 2 / d**2\n\n            err1 = (w - q) / d\n            W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n            Err1[:, i] = err1\n\n        W[:, i1:i2] = Q1\n        losses += torch.sum(Losses1, 1) / 2\n\n        if preserve_sparsity_mask:\n            # respect the sparsity of other groups\n            # really not needed, but kept for explicitness\n            W[:, i2:] -= (~mask[:, i2:]) * Err1.matmul(Hinv[i1:i2, i2:])\n        else:\n            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n\n    if isinstance(module, transformers.Conv1D):\n        W.transpose_(0, 1)\n    W = W.reshape(final_shape).to(final_dtype)\n\n    loss = torch.sum(losses).item()\n    return loss, W\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/","title":"llmcompressor.modifiers.pruning","text":""},{"location":"reference/llmcompressor/modifiers/pruning/#llmcompressor.modifiers.pruning.WandaPruningModifier","title":"<code>WandaPruningModifier</code>","text":"<p>               Bases: <code>SparsityModifierBase</code></p> <p>Modifier for applying the one-shot WANDA algorithm to a model from the paper: https://arxiv.org/abs/2306.11695</p> <p>| Sample yaml: |   test_stage: |       sparsity_modifiers: |           WandaPruningModifier: |               sparsity: 0.5 |               mask_structure: \"2:4\"</p> <p>Lifecycle:     - on_initialize         - register_hook(module, calibrate_module, \"forward\")         - run_sequential / run_layer_sequential / run_basic             - make_empty_row_scalars             - accumulate_row_scalars     - on_sequential_batch_end         - sparsify_weight     - on_finalize         - remove_hooks()</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <p>Sparsity to compress model to</p> required <code>sparsity_profile</code> <p>Can be set to 'owl' to use Outlier Weighed Layerwise Sparsity (OWL), more information can be found in the paper https://arxiv.org/pdf/2310.05175</p> required <code>mask_structure</code> <p>String to define the structure of the mask to apply. Must be of the form N:M where N, M are integers that define a custom block shape. Defaults to 0:0 which represents an unstructured mask.</p> required <code>owl_m</code> <p>Number of outliers to use for OWL</p> required <code>owl_lmbda</code> <p>Lambda value to use for OWL</p> required <code>sequential_targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>targets</code></p> required <code>targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>sequential_targets</code></p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target. Defaults to empty list.</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>class WandaPruningModifier(SparsityModifierBase):\n    \"\"\"\n    Modifier for applying the one-shot WANDA algorithm to a model\n    from the paper: https://arxiv.org/abs/2306.11695\n\n    | Sample yaml:\n    |   test_stage:\n    |       sparsity_modifiers:\n    |           WandaPruningModifier:\n    |               sparsity: 0.5\n    |               mask_structure: \"2:4\"\n\n    Lifecycle:\n        - on_initialize\n            - register_hook(module, calibrate_module, \"forward\")\n            - run_sequential / run_layer_sequential / run_basic\n                - make_empty_row_scalars\n                - accumulate_row_scalars\n        - on_sequential_batch_end\n            - sparsify_weight\n        - on_finalize\n            - remove_hooks()\n\n    :param sparsity: Sparsity to compress model to\n    :param sparsity_profile: Can be set to 'owl' to use Outlier Weighed\n        Layerwise Sparsity (OWL), more information can be found\n        in the paper https://arxiv.org/pdf/2310.05175\n    :param mask_structure: String to define the structure of the mask to apply.\n        Must be of the form N:M where N, M are integers that define a custom block\n        shape. Defaults to 0:0 which represents an unstructured mask.\n    :param owl_m: Number of outliers to use for OWL\n    :param owl_lmbda: Lambda value to use for OWL\n    :param sequential_targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `targets`\n    :param targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `sequential_targets`\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target. Defaults to empty list.\n    \"\"\"\n\n    # private variables\n    _row_scalars: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(\n        default_factory=dict\n    )\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the row scalars of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that the first argument is the input\n        inp = args[0]\n\n        # Initialize row scalars if not present\n        if module not in self._num_samples:\n            device = get_execution_device(module)\n            self._row_scalars[module] = make_empty_row_scalars(module, device=device)\n            self._num_samples[module] = 0\n\n        # Accumulate scalars using data\n        self._row_scalars[module], self._num_samples[module] = accumulate_row_scalars(\n            inp,\n            module,\n            self._row_scalars[module],\n            self._num_samples[module],\n        )\n\n    def compress_modules(self):\n        \"\"\"\n        Sparsify modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            sparsity = self._module_sparsities[module]\n            num_samples = self._num_samples[module]\n\n            logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(module), CompressionLogger(\n                module\n            ):\n                sparsified_weight = sparsify_weight(\n                    module=module,\n                    row_scalars_dict=self._row_scalars,\n                    sparsity=sparsity,\n                    prune_n=self._prune_n,\n                    prune_m=self._prune_m,\n                )\n\n            update_offload_parameter(module, \"weight\", sparsified_weight)\n\n            # self._row_scalars[module] already deleted by sparsify_weight\n            del self._num_samples[module]\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        # TODO: modify lifecycle to end on finalize\n        if not self.ended_:\n            self.on_end(state, None)  # remove hooks\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._row_scalars = dict()\n        self._num_samples = dict()\n        self._module_names = dict()\n        self._module_sparsities = dict()\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/#llmcompressor.modifiers.pruning.WandaPruningModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the row scalars of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the row scalars of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that the first argument is the input\n    inp = args[0]\n\n    # Initialize row scalars if not present\n    if module not in self._num_samples:\n        device = get_execution_device(module)\n        self._row_scalars[module] = make_empty_row_scalars(module, device=device)\n        self._num_samples[module] = 0\n\n    # Accumulate scalars using data\n    self._row_scalars[module], self._num_samples[module] = accumulate_row_scalars(\n        inp,\n        module,\n        self._row_scalars[module],\n        self._num_samples[module],\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/#llmcompressor.modifiers.pruning.WandaPruningModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Sparsify modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Sparsify modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        sparsity = self._module_sparsities[module]\n        num_samples = self._num_samples[module]\n\n        logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(module), CompressionLogger(\n            module\n        ):\n            sparsified_weight = sparsify_weight(\n                module=module,\n                row_scalars_dict=self._row_scalars,\n                sparsity=sparsity,\n                prune_n=self._prune_n,\n                prune_m=self._prune_m,\n            )\n\n        update_offload_parameter(module, \"weight\", sparsified_weight)\n\n        # self._row_scalars[module] already deleted by sparsify_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/helpers/","title":"llmcompressor.modifiers.pruning.helpers","text":""},{"location":"reference/llmcompressor/modifiers/pruning/constant/","title":"llmcompressor.modifiers.pruning.constant","text":""},{"location":"reference/llmcompressor/modifiers/pruning/constant/base/","title":"llmcompressor.modifiers.pruning.constant.base","text":""},{"location":"reference/llmcompressor/modifiers/pruning/magnitude/","title":"llmcompressor.modifiers.pruning.magnitude","text":""},{"location":"reference/llmcompressor/modifiers/pruning/magnitude/base/","title":"llmcompressor.modifiers.pruning.magnitude.base","text":""},{"location":"reference/llmcompressor/modifiers/pruning/utils/","title":"llmcompressor.modifiers.pruning.utils","text":""},{"location":"reference/llmcompressor/modifiers/pruning/utils/pytorch/","title":"llmcompressor.modifiers.pruning.utils.pytorch","text":""},{"location":"reference/llmcompressor/modifiers/pruning/utils/pytorch/#llmcompressor.modifiers.pruning.utils.pytorch.param_mask_name","title":"<code>param_mask_name()</code>","text":"<p>Name to use for mask buffer on a sparse layer</p> Source code in <code>src/llmcompressor/modifiers/pruning/utils/pytorch/layer_mask.py</code> <pre><code>def param_mask_name() -&gt; str:\n    \"\"\"\n    Name to use for mask buffer on a sparse layer\n    \"\"\"\n    return \"mask\"\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/utils/pytorch/layer_mask/","title":"llmcompressor.modifiers.pruning.utils.pytorch.layer_mask","text":""},{"location":"reference/llmcompressor/modifiers/pruning/utils/pytorch/layer_mask/#llmcompressor.modifiers.pruning.utils.pytorch.layer_mask.param_mask_name","title":"<code>param_mask_name()</code>","text":"<p>Name to use for mask buffer on a sparse layer</p> Source code in <code>src/llmcompressor/modifiers/pruning/utils/pytorch/layer_mask.py</code> <pre><code>def param_mask_name() -&gt; str:\n    \"\"\"\n    Name to use for mask buffer on a sparse layer\n    \"\"\"\n    return \"mask\"\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/utils/pytorch/mask_factory/","title":"llmcompressor.modifiers.pruning.utils.pytorch.mask_factory","text":""},{"location":"reference/llmcompressor/modifiers/pruning/wanda/","title":"llmcompressor.modifiers.pruning.wanda","text":""},{"location":"reference/llmcompressor/modifiers/pruning/wanda/#llmcompressor.modifiers.pruning.wanda.WandaPruningModifier","title":"<code>WandaPruningModifier</code>","text":"<p>               Bases: <code>SparsityModifierBase</code></p> <p>Modifier for applying the one-shot WANDA algorithm to a model from the paper: https://arxiv.org/abs/2306.11695</p> <p>| Sample yaml: |   test_stage: |       sparsity_modifiers: |           WandaPruningModifier: |               sparsity: 0.5 |               mask_structure: \"2:4\"</p> <p>Lifecycle:     - on_initialize         - register_hook(module, calibrate_module, \"forward\")         - run_sequential / run_layer_sequential / run_basic             - make_empty_row_scalars             - accumulate_row_scalars     - on_sequential_batch_end         - sparsify_weight     - on_finalize         - remove_hooks()</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <p>Sparsity to compress model to</p> required <code>sparsity_profile</code> <p>Can be set to 'owl' to use Outlier Weighed Layerwise Sparsity (OWL), more information can be found in the paper https://arxiv.org/pdf/2310.05175</p> required <code>mask_structure</code> <p>String to define the structure of the mask to apply. Must be of the form N:M where N, M are integers that define a custom block shape. Defaults to 0:0 which represents an unstructured mask.</p> required <code>owl_m</code> <p>Number of outliers to use for OWL</p> required <code>owl_lmbda</code> <p>Lambda value to use for OWL</p> required <code>sequential_targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>targets</code></p> required <code>targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>sequential_targets</code></p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target. Defaults to empty list.</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>class WandaPruningModifier(SparsityModifierBase):\n    \"\"\"\n    Modifier for applying the one-shot WANDA algorithm to a model\n    from the paper: https://arxiv.org/abs/2306.11695\n\n    | Sample yaml:\n    |   test_stage:\n    |       sparsity_modifiers:\n    |           WandaPruningModifier:\n    |               sparsity: 0.5\n    |               mask_structure: \"2:4\"\n\n    Lifecycle:\n        - on_initialize\n            - register_hook(module, calibrate_module, \"forward\")\n            - run_sequential / run_layer_sequential / run_basic\n                - make_empty_row_scalars\n                - accumulate_row_scalars\n        - on_sequential_batch_end\n            - sparsify_weight\n        - on_finalize\n            - remove_hooks()\n\n    :param sparsity: Sparsity to compress model to\n    :param sparsity_profile: Can be set to 'owl' to use Outlier Weighed\n        Layerwise Sparsity (OWL), more information can be found\n        in the paper https://arxiv.org/pdf/2310.05175\n    :param mask_structure: String to define the structure of the mask to apply.\n        Must be of the form N:M where N, M are integers that define a custom block\n        shape. Defaults to 0:0 which represents an unstructured mask.\n    :param owl_m: Number of outliers to use for OWL\n    :param owl_lmbda: Lambda value to use for OWL\n    :param sequential_targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `targets`\n    :param targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `sequential_targets`\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target. Defaults to empty list.\n    \"\"\"\n\n    # private variables\n    _row_scalars: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(\n        default_factory=dict\n    )\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the row scalars of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that the first argument is the input\n        inp = args[0]\n\n        # Initialize row scalars if not present\n        if module not in self._num_samples:\n            device = get_execution_device(module)\n            self._row_scalars[module] = make_empty_row_scalars(module, device=device)\n            self._num_samples[module] = 0\n\n        # Accumulate scalars using data\n        self._row_scalars[module], self._num_samples[module] = accumulate_row_scalars(\n            inp,\n            module,\n            self._row_scalars[module],\n            self._num_samples[module],\n        )\n\n    def compress_modules(self):\n        \"\"\"\n        Sparsify modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            sparsity = self._module_sparsities[module]\n            num_samples = self._num_samples[module]\n\n            logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(module), CompressionLogger(\n                module\n            ):\n                sparsified_weight = sparsify_weight(\n                    module=module,\n                    row_scalars_dict=self._row_scalars,\n                    sparsity=sparsity,\n                    prune_n=self._prune_n,\n                    prune_m=self._prune_m,\n                )\n\n            update_offload_parameter(module, \"weight\", sparsified_weight)\n\n            # self._row_scalars[module] already deleted by sparsify_weight\n            del self._num_samples[module]\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        # TODO: modify lifecycle to end on finalize\n        if not self.ended_:\n            self.on_end(state, None)  # remove hooks\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._row_scalars = dict()\n        self._num_samples = dict()\n        self._module_names = dict()\n        self._module_sparsities = dict()\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/wanda/#llmcompressor.modifiers.pruning.wanda.WandaPruningModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the row scalars of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the row scalars of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that the first argument is the input\n    inp = args[0]\n\n    # Initialize row scalars if not present\n    if module not in self._num_samples:\n        device = get_execution_device(module)\n        self._row_scalars[module] = make_empty_row_scalars(module, device=device)\n        self._num_samples[module] = 0\n\n    # Accumulate scalars using data\n    self._row_scalars[module], self._num_samples[module] = accumulate_row_scalars(\n        inp,\n        module,\n        self._row_scalars[module],\n        self._num_samples[module],\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/wanda/#llmcompressor.modifiers.pruning.wanda.WandaPruningModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Sparsify modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Sparsify modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        sparsity = self._module_sparsities[module]\n        num_samples = self._num_samples[module]\n\n        logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(module), CompressionLogger(\n            module\n        ):\n            sparsified_weight = sparsify_weight(\n                module=module,\n                row_scalars_dict=self._row_scalars,\n                sparsity=sparsity,\n                prune_n=self._prune_n,\n                prune_m=self._prune_m,\n            )\n\n        update_offload_parameter(module, \"weight\", sparsified_weight)\n\n        # self._row_scalars[module] already deleted by sparsify_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/wanda/base/","title":"llmcompressor.modifiers.pruning.wanda.base","text":""},{"location":"reference/llmcompressor/modifiers/pruning/wanda/base/#llmcompressor.modifiers.pruning.wanda.base.WandaPruningModifier","title":"<code>WandaPruningModifier</code>","text":"<p>               Bases: <code>SparsityModifierBase</code></p> <p>Modifier for applying the one-shot WANDA algorithm to a model from the paper: https://arxiv.org/abs/2306.11695</p> <p>| Sample yaml: |   test_stage: |       sparsity_modifiers: |           WandaPruningModifier: |               sparsity: 0.5 |               mask_structure: \"2:4\"</p> <p>Lifecycle:     - on_initialize         - register_hook(module, calibrate_module, \"forward\")         - run_sequential / run_layer_sequential / run_basic             - make_empty_row_scalars             - accumulate_row_scalars     - on_sequential_batch_end         - sparsify_weight     - on_finalize         - remove_hooks()</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <p>Sparsity to compress model to</p> required <code>sparsity_profile</code> <p>Can be set to 'owl' to use Outlier Weighed Layerwise Sparsity (OWL), more information can be found in the paper https://arxiv.org/pdf/2310.05175</p> required <code>mask_structure</code> <p>String to define the structure of the mask to apply. Must be of the form N:M where N, M are integers that define a custom block shape. Defaults to 0:0 which represents an unstructured mask.</p> required <code>owl_m</code> <p>Number of outliers to use for OWL</p> required <code>owl_lmbda</code> <p>Lambda value to use for OWL</p> required <code>sequential_targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>targets</code></p> required <code>targets</code> <p>list of layer names to compress during OBCQ, or 'ALL' to compress every layer in the model. Alias for <code>sequential_targets</code></p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target. Defaults to empty list.</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>class WandaPruningModifier(SparsityModifierBase):\n    \"\"\"\n    Modifier for applying the one-shot WANDA algorithm to a model\n    from the paper: https://arxiv.org/abs/2306.11695\n\n    | Sample yaml:\n    |   test_stage:\n    |       sparsity_modifiers:\n    |           WandaPruningModifier:\n    |               sparsity: 0.5\n    |               mask_structure: \"2:4\"\n\n    Lifecycle:\n        - on_initialize\n            - register_hook(module, calibrate_module, \"forward\")\n            - run_sequential / run_layer_sequential / run_basic\n                - make_empty_row_scalars\n                - accumulate_row_scalars\n        - on_sequential_batch_end\n            - sparsify_weight\n        - on_finalize\n            - remove_hooks()\n\n    :param sparsity: Sparsity to compress model to\n    :param sparsity_profile: Can be set to 'owl' to use Outlier Weighed\n        Layerwise Sparsity (OWL), more information can be found\n        in the paper https://arxiv.org/pdf/2310.05175\n    :param mask_structure: String to define the structure of the mask to apply.\n        Must be of the form N:M where N, M are integers that define a custom block\n        shape. Defaults to 0:0 which represents an unstructured mask.\n    :param owl_m: Number of outliers to use for OWL\n    :param owl_lmbda: Lambda value to use for OWL\n    :param sequential_targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `targets`\n    :param targets: list of layer names to compress during OBCQ, or '__ALL__'\n        to compress every layer in the model. Alias for `sequential_targets`\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target. Defaults to empty list.\n    \"\"\"\n\n    # private variables\n    _row_scalars: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(\n        default_factory=dict\n    )\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the row scalars of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that the first argument is the input\n        inp = args[0]\n\n        # Initialize row scalars if not present\n        if module not in self._num_samples:\n            device = get_execution_device(module)\n            self._row_scalars[module] = make_empty_row_scalars(module, device=device)\n            self._num_samples[module] = 0\n\n        # Accumulate scalars using data\n        self._row_scalars[module], self._num_samples[module] = accumulate_row_scalars(\n            inp,\n            module,\n            self._row_scalars[module],\n            self._num_samples[module],\n        )\n\n    def compress_modules(self):\n        \"\"\"\n        Sparsify modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            sparsity = self._module_sparsities[module]\n            num_samples = self._num_samples[module]\n\n            logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(module), CompressionLogger(\n                module\n            ):\n                sparsified_weight = sparsify_weight(\n                    module=module,\n                    row_scalars_dict=self._row_scalars,\n                    sparsity=sparsity,\n                    prune_n=self._prune_n,\n                    prune_m=self._prune_m,\n                )\n\n            update_offload_parameter(module, \"weight\", sparsified_weight)\n\n            # self._row_scalars[module] already deleted by sparsify_weight\n            del self._num_samples[module]\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        # TODO: modify lifecycle to end on finalize\n        if not self.ended_:\n            self.on_end(state, None)  # remove hooks\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._row_scalars = dict()\n        self._num_samples = dict()\n        self._module_names = dict()\n        self._module_sparsities = dict()\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/wanda/base/#llmcompressor.modifiers.pruning.wanda.base.WandaPruningModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the row scalars of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the row scalars of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that the first argument is the input\n    inp = args[0]\n\n    # Initialize row scalars if not present\n    if module not in self._num_samples:\n        device = get_execution_device(module)\n        self._row_scalars[module] = make_empty_row_scalars(module, device=device)\n        self._num_samples[module] = 0\n\n    # Accumulate scalars using data\n    self._row_scalars[module], self._num_samples[module] = accumulate_row_scalars(\n        inp,\n        module,\n        self._row_scalars[module],\n        self._num_samples[module],\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/wanda/base/#llmcompressor.modifiers.pruning.wanda.base.WandaPruningModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Sparsify modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/pruning/wanda/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Sparsify modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        sparsity = self._module_sparsities[module]\n        num_samples = self._num_samples[module]\n\n        logger.info(f\"Sparsifying {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(module), CompressionLogger(\n            module\n        ):\n            sparsified_weight = sparsify_weight(\n                module=module,\n                row_scalars_dict=self._row_scalars,\n                sparsity=sparsity,\n                prune_n=self._prune_n,\n                prune_m=self._prune_m,\n            )\n\n        update_offload_parameter(module, \"weight\", sparsified_weight)\n\n        # self._row_scalars[module] already deleted by sparsify_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/pruning/wanda/wanda_sparsify/","title":"llmcompressor.modifiers.pruning.wanda.wanda_sparsify","text":""},{"location":"reference/llmcompressor/modifiers/pruning/wanda/wanda_sparsify/#llmcompressor.modifiers.pruning.wanda.wanda_sparsify.sparsify_weight","title":"<code>sparsify_weight(module, row_scalars_dict, sparsity, prune_n, prune_m)</code>","text":"<p>Run pruning on the layer up to the target sparsity value.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>float</code> <p>target sparsity to reach for layer</p> required <code>prunen</code> <p>N for N:M pruning</p> required <code>prunem</code> <p>M for N:M pruning</p> required Source code in <code>src/llmcompressor/modifiers/pruning/wanda/wanda_sparsify.py</code> <pre><code>def sparsify_weight(\n    module: torch.nn.Module,\n    row_scalars_dict: Dict[torch.nn.Module, torch.Tensor],\n    sparsity: float,\n    prune_n: int,\n    prune_m: int,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Run pruning on the layer up to the target sparsity value.\n\n    :param sparsity: target sparsity to reach for layer\n    :param prunen: N for N:M pruning\n    :param prunem: M for N:M pruning\n    \"\"\"\n    final_shape = module.weight.shape\n    final_dtype = module.weight.dtype\n    W = module.weight.data.clone()\n    if isinstance(module, torch.nn.Conv2d):\n        W = W.flatten(1)\n    if isinstance(module, transformers.Conv1D):\n        W = W.t()\n    W = W.to(dtype=WANDA_PRECISION)\n    S = row_scalars_dict[module]  # unfortunately python does not have a `move` keyword\n    del row_scalars_dict[module]  # so we have to delete the original reference manually\n\n    W_metric = torch.abs(W) * torch.sqrt(S.reshape((1, -1)))\n\n    # initialize a mask to be all False\n    W_mask = torch.zeros_like(W_metric) == 1\n    if prune_n != 0:\n        # structured n:m sparsity\n        for ii in range(W_metric.shape[1]):\n            if ii % prune_m == 0:\n                tmp = W_metric[:, ii : (ii + prune_m)].float()\n                W_mask.scatter_(\n                    1,\n                    ii + torch.topk(tmp, prune_n, dim=1, largest=False)[1],\n                    True,\n                )\n    else:\n        sort_res = torch.sort(W_metric, dim=-1, stable=True)\n        indices = sort_res[1][:, : int(W_metric.shape[1] * sparsity)]\n        W_mask.scatter_(1, indices, True)\n\n    W[W_mask] = 0.0  # set weights to zero\n\n    if isinstance(module, transformers.Conv1D):\n        W = W.t()\n\n    W = W.reshape(final_shape).to(final_dtype)\n\n    return W\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/","title":"llmcompressor.modifiers.quantization","text":""},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.GPTQModifier","title":"<code>GPTQModifier</code>","text":"<p>               Bases: <code>Modifier</code>, <code>QuantizationMixin</code></p> <p>Implements the GPTQ algorithm from https://arxiv.org/abs/2210.17323. This modifier uses activations to calibrate a hessian matrix, which is then used to determine optimal quantizion values and orderings for the model weights.</p> <p>| Sample yaml: | test_stage: |    obcq_modifiers: |      GPTQModifier: |          block_size: 128 |          dampening_frac: 0.001 |          offload_hessians: False |          config_groups: |            group_0: |                targets: |                  - \"Linear\" |                input_activations: null |                output_activations: null |                weights: |                    num_bits: 8 |                    type: \"int\" |                    symmetric: true |                    strategy: \"tensor\" |                    group_size: 128 |                    actorder: False</p> <p>Lifecycle:     - on_initialize         - apply config to model     - on_start         - add activation calibration hooks         - add gptq weight calibration hooks     - on_sequential_epoch_end         - quantize_weight     - on_finalize         - remove_hooks()         - model.apply(freeze_module_quantization)</p> <p>Parameters:</p> Name Type Description Default <code>sequential_targets</code> <p>list of layer names to compress during GPTQ, or 'ALL' to compress every layer in the model</p> required <code>block_size</code> <p>Used to determine number of columns to compress in one pass</p> required <code>dampening_frac</code> <p>Amount of dampening to apply to H, as a fraction of the diagonal norm</p> required <code>offload_hessians</code> <p>Set to True for decreased memory usage but increased runtime.</p> required <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>class GPTQModifier(Modifier, QuantizationMixin):\n    \"\"\"\n    Implements the GPTQ algorithm from https://arxiv.org/abs/2210.17323. This modifier\n    uses activations to calibrate a hessian matrix, which is then used to determine\n    optimal quantizion values and orderings for the model weights.\n\n    | Sample yaml:\n    | test_stage:\n    |    obcq_modifiers:\n    |      GPTQModifier:\n    |          block_size: 128\n    |          dampening_frac: 0.001\n    |          offload_hessians: False\n    |          config_groups:\n    |            group_0:\n    |                targets:\n    |                  - \"Linear\"\n    |                input_activations: null\n    |                output_activations: null\n    |                weights:\n    |                    num_bits: 8\n    |                    type: \"int\"\n    |                    symmetric: true\n    |                    strategy: \"tensor\"\n    |                    group_size: 128\n    |                    actorder: False\n\n    Lifecycle:\n        - on_initialize\n            - apply config to model\n        - on_start\n            - add activation calibration hooks\n            - add gptq weight calibration hooks\n        - on_sequential_epoch_end\n            - quantize_weight\n        - on_finalize\n            - remove_hooks()\n            - model.apply(freeze_module_quantization)\n\n    :param sequential_targets: list of layer names to compress during GPTQ, or\n        '__ALL__' to compress every layer in the model\n    :param block_size: Used to determine number of columns to compress in one pass\n    :param dampening_frac: Amount of dampening to apply to H, as a fraction of the\n        diagonal norm\n    :param offload_hessians: Set to True for decreased memory usage but increased\n        runtime.\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    # gptq modifier arguments\n    sequential_update: bool = True  # DEPRECIATED\n    sequential_targets: Union[str, List[str], None] = None\n    block_size: int = 128\n    dampening_frac: Optional[float] = 0.01\n    offload_hessians: bool = False\n\n    # private variables\n    _module_names: Dict[torch.nn.Module, str] = PrivateAttr(default_factory=dict)\n    _hessians: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(default_factory=dict)\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n\n    @field_validator(\"sequential_update\", mode=\"before\")\n    def validate_sequential_update(cls, value: bool) -&gt; bool:\n        if not value:\n            warnings.warn(\n                \"`sequential_update=False` is no longer supported, setting \"\n                \"sequential_update=True\",\n                DeprecationWarning,\n            )\n\n        return True\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run the GPTQ algorithm on the current state\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        # apply config to model and prepare calibration hooks\n        if QuantizationMixin.has_config(self):\n            QuantizationMixin.initialize_quantization(self, state.model)\n\n        # prepare module names\n        self._module_names = {m: name for name, m in state.model.named_modules()}\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        self.started_ = True\n\n        # register quantization calibration hooks\n        # assume quantization has been initialized by this modifier or one before it\n        QuantizationMixin.start_calibration(self, state.model)\n        # Unlike qmod, do not quantize as we calibrate\n        # This choice does not seem to have a meaningful impact on accuracy\n        state.model.apply(disable_quantization)\n\n        # register gptq hooks\n        added_hook = False\n        for module in state.model.modules():\n            if getattr_chain(module, \"quantization_scheme.weights\", None) is not None:\n                # HACK: previously, embeddings were not quantized because they were not\n                # accessible by the layer compressor. For now, we manually ignore it,\n                # but in the FUTURE this should be ignored by the user\n                if not isinstance(module, torch.nn.Embedding):\n                    self.register_hook(module, self.calibrate_module, \"forward\")\n                    added_hook = True\n\n        if not added_hook:\n            raise ValueError(\n                \"GPTQModifier requires a weight quantization config be specified by \"\n                \"this modifier or a modifier preceding it\"\n            )\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n            self.compress_modules()\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            self.compress_modules()\n\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Finish calibrating by removing observers and calibration hooks\n        \"\"\"\n        self.ended_ = True\n        QuantizationMixin.end_calibration(self, state.model)\n        self.remove_hooks()  # remove gptq hooks\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        disable the quantization observers used by the OBCQ algorithm\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        if not self.ended_:\n            self.on_end(state, None)\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._hessians = dict()\n        self._num_samples = dict()\n\n        return True\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the hessian of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that first argument is the input\n        inp = args[0]\n\n        # Initialize hessian if not present\n        if module not in self._num_samples:\n            init_device = (\n                \"cpu\" if self.offload_hessians else get_execution_device(module)\n            )\n            self._hessians[module] = make_empty_hessian(module, device=init_device)\n            self._num_samples[module] = 0\n\n        # Accumulate hessian with input with optional offloading\n        with self._maybe_onload_hessian(module):\n            self._hessians[module], self._num_samples[module] = accumulate_hessian(\n                inp,\n                module,\n                self._hessians[module],\n                self._num_samples[module],\n            )\n\n    def compress_modules(self):\n        \"\"\"\n        Quantize modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            num_samples = self._num_samples[module]\n            quant_args = getattr_chain(module, \"quantization_scheme.weights\")\n\n            logger.info(f\"Quantizing {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(\n                module\n            ), self._maybe_onload_hessian(module), CompressionLogger(\n                module\n            ) as comp_logger:\n                loss, quantized_weight, scale, zero_point, g_idx = quantize_weight(\n                    module=module,\n                    quant_args=quant_args,\n                    hessians_dict=self._hessians,\n                    blocksize=self.block_size,\n                    percdamp=self.dampening_frac,\n                )\n                comp_logger.set_loss(loss)\n\n            update_offload_parameter(module, \"weight\", quantized_weight)\n            update_offload_parameter(module, \"weight_scale\", scale)\n            update_offload_parameter(module, \"weight_zero_point\", zero_point)\n            if g_idx is not None:\n                update_offload_parameter(module, \"weight_g_idx\", g_idx)\n\n            # self._hessians[module] already deleted by quantize_weight\n            del self._num_samples[module]\n\n    @contextlib.contextmanager\n    def _maybe_onload_hessian(self, module: torch.nn.Module):\n        if self.offload_hessians:\n            device = get_execution_device(module)\n            self._hessians[module] = self._hessians[module].to(device=device)\n\n        yield\n\n        if self.offload_hessians:\n            if module in self._hessians:  # may have been deleted in context\n                self._hessians[module] = self._hessians[module].to(device=\"cpu\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.GPTQModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the hessian of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the hessian of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that first argument is the input\n    inp = args[0]\n\n    # Initialize hessian if not present\n    if module not in self._num_samples:\n        init_device = (\n            \"cpu\" if self.offload_hessians else get_execution_device(module)\n        )\n        self._hessians[module] = make_empty_hessian(module, device=init_device)\n        self._num_samples[module] = 0\n\n    # Accumulate hessian with input with optional offloading\n    with self._maybe_onload_hessian(module):\n        self._hessians[module], self._num_samples[module] = accumulate_hessian(\n            inp,\n            module,\n            self._hessians[module],\n            self._num_samples[module],\n        )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.GPTQModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Quantize modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Quantize modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        num_samples = self._num_samples[module]\n        quant_args = getattr_chain(module, \"quantization_scheme.weights\")\n\n        logger.info(f\"Quantizing {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(\n            module\n        ), self._maybe_onload_hessian(module), CompressionLogger(\n            module\n        ) as comp_logger:\n            loss, quantized_weight, scale, zero_point, g_idx = quantize_weight(\n                module=module,\n                quant_args=quant_args,\n                hessians_dict=self._hessians,\n                blocksize=self.block_size,\n                percdamp=self.dampening_frac,\n            )\n            comp_logger.set_loss(loss)\n\n        update_offload_parameter(module, \"weight\", quantized_weight)\n        update_offload_parameter(module, \"weight_scale\", scale)\n        update_offload_parameter(module, \"weight_zero_point\", zero_point)\n        if g_idx is not None:\n            update_offload_parameter(module, \"weight_g_idx\", g_idx)\n\n        # self._hessians[module] already deleted by quantize_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.GPTQModifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>Finish calibrating by removing observers and calibration hooks</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Finish calibrating by removing observers and calibration hooks\n    \"\"\"\n    self.ended_ = True\n    QuantizationMixin.end_calibration(self, state.model)\n    self.remove_hooks()  # remove gptq hooks\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.GPTQModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>disable the quantization observers used by the OBCQ algorithm</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    disable the quantization observers used by the OBCQ algorithm\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    if not self.ended_:\n        self.on_end(state, None)\n\n    if len(self._num_samples) &gt; 0:\n        raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n    self._hessians = dict()\n    self._num_samples = dict()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.GPTQModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run the GPTQ algorithm on the current state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run the GPTQ algorithm on the current state\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    # apply config to model and prepare calibration hooks\n    if QuantizationMixin.has_config(self):\n        QuantizationMixin.initialize_quantization(self, state.model)\n\n    # prepare module names\n    self._module_names = {m: name for name, m in state.model.named_modules()}\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer","title":"<code>Observer</code>","text":"<p>               Bases: <code>Module</code>, <code>RegistryMixin</code></p> <p>Base Observer class to be subclassed for specific implementation. Subclasses should override <code>calculate_qparams</code> to return a scale, zero_point pair</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>class Observer(Module, RegistryMixin):\n    \"\"\"\n    Base Observer class to be subclassed for specific implementation.\n    Subclasses should override `calculate_qparams` to return a scale, zero_point\n    pair\n    \"\"\"\n\n    def __init__(self, quantization_args: QuantizationArgs):\n        self.quantization_args: QuantizationArgs = quantization_args\n        super().__init__()\n        self._scale = None\n        self._zero_point = None\n        self._num_observed_tokens = None\n\n    @torch.no_grad()\n    def forward(\n        self, observed: Tensor, g_idx: Optional[Tensor] = None\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        maps directly to get_qparams\n        :param observed: optional observed tensor from which to calculate\n            quantization parameters\n        :param g_idx: optional mapping from column index to group index\n        :return: tuple of scale and zero point based on last observed value\n        \"\"\"\n        self.record_observed_tokens(observed)\n        return self.get_qparams(observed=observed, g_idx=g_idx)\n\n    def calculate_qparams(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__} must implement calculate_qparams\")\n\n    def post_calculate_qparams(self) -&gt; None:\n        \"\"\"\n        Run any logic specific to its observers after running calculate_qparams\n        \"\"\"\n\n    def get_qparams(\n        self,\n        observed: Optional[Tensor] = None,\n        g_idx: Optional[Tensor] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        Convenience function to wrap overwritten calculate_qparams\n        adds support to make observed tensor optional and support for tracking latest\n        calculated scale and zero point\n\n        :param observed: optional observed tensor to calculate quantization parameters\n            from\n        :param g_idx: optional mapping from column index to group index\n        :return: tuple of scale and zero point based on last observed value\n        \"\"\"\n        if observed is not None:\n            group_size = self.quantization_args.group_size\n\n            if self.quantization_args.strategy == QuantizationStrategy.TENSOR:\n                # re-calculate scale and zero point, update the stored value\n                self._scale, self._zero_point = self.calculate_qparams(observed)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.GROUP:\n                rows = observed.shape[0]\n                columns = observed.shape[1]\n                num_groups = int(ceil(columns / group_size))\n                self._scale = torch.empty(\n                    (rows, num_groups), dtype=observed.dtype, device=observed.device\n                )\n                zp_dtype = self.quantization_args.pytorch_dtype()\n                self._zero_point = torch.empty(\n                    (rows, num_groups), dtype=zp_dtype, device=observed.device\n                )\n\n                # support column-order (default) quantization as well as other orderings\n                # such as activation ordering. Below checks if g_idx has initialized\n                is_column_order = g_idx is None or -1 in g_idx\n                if is_column_order:\n                    group_sizes = torch.full((num_groups,), group_size, dtype=torch.int)\n                else:\n                    group_indices, group_sizes = torch.unique(g_idx, return_counts=True)\n                    group_sizes = group_sizes[torch.argsort(group_indices)]\n\n                    perm = torch.argsort(g_idx)\n                    observed = safe_permute(observed, perm, dim=1)\n\n                # TODO: experiment with vectorizing for loop for performance\n                end = 0\n                for group_index, group_count in enumerate(group_sizes):\n                    start = end\n                    end = start + group_count\n                    scale, zero_point = self.get_qparams_along_dim(\n                        observed[:, start:end],\n                        0,\n                        tensor_id=group_index,\n                    )\n\n                    self._scale[:, group_index] = scale.squeeze(1)\n                    self._zero_point[:, group_index] = zero_point.squeeze(1)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:\n                # assume observed is transposed, because its the output, hence use dim 0\n                self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:\n                # use dim 1, assume the obsersed.shape = [batch, token, hidden]\n                # should be batch, token\n                self._scale, self._zero_point = self.get_qparams_along_dim(\n                    observed,\n                    dim={0, 1},\n                )\n\n        return self._scale, self._zero_point\n\n    def get_qparams_along_dim(\n        self,\n        observed,\n        dim: Union[int, Iterable[int]],\n        tensor_id: Optional[Any] = None,\n    ):\n        if isinstance(dim, int):\n            dim = [dim]\n        dim = set(dim)\n\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx not in dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def record_observed_tokens(self, batch_tensor: Tensor):\n        \"\"\"\n        Counts the number of tokens observed during the\n        forward passes. The count is aggregated in the\n        _num_observed_tokens attribute of the class.\n\n        Note: The batch_tensor is expected to have two dimensions\n            (batch_size * sequence_length, num_features). This is the\n            general shape expected by the forward pass of the expert\n            layers in a MOE model. If the input tensor does not have\n            two dimensions, the _num_observed_tokens attribute will be set\n            to None.\n        \"\"\"\n        if not isinstance(batch_tensor, Tensor):\n            raise ValueError(f\"Expected value to be a tensor, got {type(batch_tensor)}\")\n\n        if batch_tensor.ndim != 2:\n            logger.debug(\n                \"The input tensor is expected to have two dimensions \"\n                \"(batch_size * sequence_length, num_features). \"\n                f\"The input tensor has {batch_tensor.ndim} dimensions.\"\n            )\n            return\n\n        if self._num_observed_tokens is None:\n            # initialize the count\n            self._num_observed_tokens = 0\n\n        # batch_tensor (batch_size * sequence_length, num_features)\n        # observed_tokens (batch_size * sequence_length)\n        observed_tokens, _ = batch_tensor.shape\n        self._num_observed_tokens += observed_tokens\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer\n        \"\"\"\n        self._num_observed_tokens = None\n        self._scale = None\n        self._zero_point = None\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__} must implement calculate_qparams\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer.forward","title":"<code>forward(observed, g_idx=None)</code>","text":"<p>maps directly to get_qparams</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>optional observed tensor from which to calculate quantization parameters</p> required <code>g_idx</code> <code>Optional[Tensor]</code> <p>optional mapping from column index to group index</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point based on last observed value</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self, observed: Tensor, g_idx: Optional[Tensor] = None\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    maps directly to get_qparams\n    :param observed: optional observed tensor from which to calculate\n        quantization parameters\n    :param g_idx: optional mapping from column index to group index\n    :return: tuple of scale and zero point based on last observed value\n    \"\"\"\n    self.record_observed_tokens(observed)\n    return self.get_qparams(observed=observed, g_idx=g_idx)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer.get_qparams","title":"<code>get_qparams(observed=None, g_idx=None)</code>","text":"<p>Convenience function to wrap overwritten calculate_qparams adds support to make observed tensor optional and support for tracking latest calculated scale and zero point</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Optional[Tensor]</code> <p>optional observed tensor to calculate quantization parameters from</p> <code>None</code> <code>g_idx</code> <code>Optional[Tensor]</code> <p>optional mapping from column index to group index</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point based on last observed value</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def get_qparams(\n    self,\n    observed: Optional[Tensor] = None,\n    g_idx: Optional[Tensor] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    Convenience function to wrap overwritten calculate_qparams\n    adds support to make observed tensor optional and support for tracking latest\n    calculated scale and zero point\n\n    :param observed: optional observed tensor to calculate quantization parameters\n        from\n    :param g_idx: optional mapping from column index to group index\n    :return: tuple of scale and zero point based on last observed value\n    \"\"\"\n    if observed is not None:\n        group_size = self.quantization_args.group_size\n\n        if self.quantization_args.strategy == QuantizationStrategy.TENSOR:\n            # re-calculate scale and zero point, update the stored value\n            self._scale, self._zero_point = self.calculate_qparams(observed)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.GROUP:\n            rows = observed.shape[0]\n            columns = observed.shape[1]\n            num_groups = int(ceil(columns / group_size))\n            self._scale = torch.empty(\n                (rows, num_groups), dtype=observed.dtype, device=observed.device\n            )\n            zp_dtype = self.quantization_args.pytorch_dtype()\n            self._zero_point = torch.empty(\n                (rows, num_groups), dtype=zp_dtype, device=observed.device\n            )\n\n            # support column-order (default) quantization as well as other orderings\n            # such as activation ordering. Below checks if g_idx has initialized\n            is_column_order = g_idx is None or -1 in g_idx\n            if is_column_order:\n                group_sizes = torch.full((num_groups,), group_size, dtype=torch.int)\n            else:\n                group_indices, group_sizes = torch.unique(g_idx, return_counts=True)\n                group_sizes = group_sizes[torch.argsort(group_indices)]\n\n                perm = torch.argsort(g_idx)\n                observed = safe_permute(observed, perm, dim=1)\n\n            # TODO: experiment with vectorizing for loop for performance\n            end = 0\n            for group_index, group_count in enumerate(group_sizes):\n                start = end\n                end = start + group_count\n                scale, zero_point = self.get_qparams_along_dim(\n                    observed[:, start:end],\n                    0,\n                    tensor_id=group_index,\n                )\n\n                self._scale[:, group_index] = scale.squeeze(1)\n                self._zero_point[:, group_index] = zero_point.squeeze(1)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:\n            # assume observed is transposed, because its the output, hence use dim 0\n            self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:\n            # use dim 1, assume the obsersed.shape = [batch, token, hidden]\n            # should be batch, token\n            self._scale, self._zero_point = self.get_qparams_along_dim(\n                observed,\n                dim={0, 1},\n            )\n\n    return self._scale, self._zero_point\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer.post_calculate_qparams","title":"<code>post_calculate_qparams()</code>","text":"<p>Run any logic specific to its observers after running calculate_qparams</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def post_calculate_qparams(self) -&gt; None:\n    \"\"\"\n    Run any logic specific to its observers after running calculate_qparams\n    \"\"\"\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer.record_observed_tokens","title":"<code>record_observed_tokens(batch_tensor)</code>","text":"<p>Counts the number of tokens observed during the forward passes. The count is aggregated in the _num_observed_tokens attribute of the class.</p> <p>Note: The batch_tensor is expected to have two dimensions     (batch_size * sequence_length, num_features). This is the     general shape expected by the forward pass of the expert     layers in a MOE model. If the input tensor does not have     two dimensions, the _num_observed_tokens attribute will be set     to None.</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def record_observed_tokens(self, batch_tensor: Tensor):\n    \"\"\"\n    Counts the number of tokens observed during the\n    forward passes. The count is aggregated in the\n    _num_observed_tokens attribute of the class.\n\n    Note: The batch_tensor is expected to have two dimensions\n        (batch_size * sequence_length, num_features). This is the\n        general shape expected by the forward pass of the expert\n        layers in a MOE model. If the input tensor does not have\n        two dimensions, the _num_observed_tokens attribute will be set\n        to None.\n    \"\"\"\n    if not isinstance(batch_tensor, Tensor):\n        raise ValueError(f\"Expected value to be a tensor, got {type(batch_tensor)}\")\n\n    if batch_tensor.ndim != 2:\n        logger.debug(\n            \"The input tensor is expected to have two dimensions \"\n            \"(batch_size * sequence_length, num_features). \"\n            f\"The input tensor has {batch_tensor.ndim} dimensions.\"\n        )\n        return\n\n    if self._num_observed_tokens is None:\n        # initialize the count\n        self._num_observed_tokens = 0\n\n    # batch_tensor (batch_size * sequence_length, num_features)\n    # observed_tokens (batch_size * sequence_length)\n    observed_tokens, _ = batch_tensor.shape\n    self._num_observed_tokens += observed_tokens\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.Observer.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer\n    \"\"\"\n    self._num_observed_tokens = None\n    self._scale = None\n    self._zero_point = None\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationMixin","title":"<code>QuantizationMixin</code>","text":"<p>               Bases: <code>HooksMixin</code></p> <p>Mixin which enables a Modifier to act as a quantization config, attching observers, calibration hooks, and compression wrappers to modifiers</p> <p>Lifecycle:     - on_initialize: QuantizationMixin.initialize_quantization         - Attach schemes to modules         - Attach observers to modules         - Disable quantization until calibration starts/finishes     - on_start: QuantizationMixin.start_calibration         - Attach calibration hooks         - Apply calibration status         - Enable quantization during calibration     - on_end: QuantizationMixin.end_calibration         - Remove calibration hooks         - Apply freeze status         - Keep quantization enabled for future steps</p> <p>Parameters:</p> Name Type Description Default <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>class QuantizationMixin(HooksMixin):\n    \"\"\"\n    Mixin which enables a Modifier to act as a quantization config, attching observers,\n    calibration hooks, and compression wrappers to modifiers\n\n    Lifecycle:\n        - on_initialize: QuantizationMixin.initialize_quantization\n            - Attach schemes to modules\n            - Attach observers to modules\n            - Disable quantization until calibration starts/finishes\n        - on_start: QuantizationMixin.start_calibration\n            - Attach calibration hooks\n            - Apply calibration status\n            - Enable quantization during calibration\n        - on_end: QuantizationMixin.end_calibration\n            - Remove calibration hooks\n            - Apply freeze status\n            - Keep quantization enabled for future steps\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    config_groups: Optional[Dict[str, QuantizationScheme]] = None\n    targets: Union[str, List[str]] = Field(default_factory=lambda: [\"Linear\"])\n    ignore: List[str] = Field(default_factory=list)\n    scheme: Optional[Union[str, Dict[str, Any]]] = None\n    kv_cache_scheme: Optional[QuantizationArgs] = None\n\n    _calibration_hooks: Set[RemovableHandle] = PrivateAttr(default_factory=set)\n\n    @field_validator(\"targets\", mode=\"before\")\n    def validate_targets(cls, value: Union[str, List[str]]) -&gt; List[str]:\n        if isinstance(value, str):\n            return [value]\n\n        return value\n\n    @field_validator(\"scheme\", mode=\"before\")\n    def validate_scheme(\n        cls, value: Optional[Union[str, Dict[str, Any]]]\n    ) -&gt; Optional[Union[str, Dict[str, Any]]]:\n        if isinstance(value, str) and not is_preset_scheme(value):\n            raise ValueError(\n                \"`scheme` must either be a preset scheme name or a dictionary \"\n                \"of preset scheme names\"\n            )\n\n        if isinstance(value, dict):\n            for scheme_name in value.keys():\n                cls.validate_scheme(scheme_name)\n\n            for key, target in value.items():\n                value[key] = cls.validate_targets(target)\n\n        return value\n\n    def initialize_quantization(self, model: torch.nn.Module):\n        \"\"\"\n        Attach quantization schemes and observers to modules in the model according to\n        the quantization config specified on this modifier\n\n        :param model: model to attach schemes and observers to\n        \"\"\"\n        reset_quantization_status(model)  # reset any previously applied qconfigs\n\n        # apply scheme and status to model\n        config = self.resolve_quantization_config()\n        apply_quantization_config(model, config)\n\n        # apply observers, disable quantization until calibration\n        model.apply(self._initialize_observers)\n        model.apply(disable_quantization)\n\n    def start_calibration(self, model: torch.nn.Module):\n        \"\"\"\n        Register activation calibration hooks (including kv_cache quantization) and\n        enable quantization as we calibrate\n\n        :param model: model to prepare for calibration\n        \"\"\"\n        self._calibration_hooks = self._initialize_hooks(model)\n        model.apply(apply_calibration_status)\n        model.apply(enable_quantization)  # quantize at the same time as calibrate\n\n    def end_calibration(self, model: torch.nn.Module):\n        \"\"\"\n        Remove calibration hooks and set the model status to frozen. Keep quantization\n        enabled for future operations\n\n        :param model: model to end calibration for\n        \"\"\"\n        self.remove_hooks(self._calibration_hooks)\n        model.apply(freeze_module_quantization)  # remove observers\n        model.apply(enable_quantization)  # keep quantization enabled\n\n    def has_config(self) -&gt; bool:\n        \"\"\"\n        Determine if the user has specified a quantization config on this modifier\n        \"\"\"\n        return not (\n            self.config_groups is None\n            and self.targets == [\"Linear\"]\n            and self.ignore == []\n            and self.scheme is None\n            and self.kv_cache_scheme is None\n        )\n\n    def resolve_quantization_config(self) -&gt; QuantizationConfig:\n        \"\"\"\n        Returns the quantization config specified by this modifier\n        \"\"\"\n        scheme = self.scheme\n        targets = self.targets\n        config_groups = self.config_groups\n        kv_cache_scheme = self.kv_cache_scheme\n        ignore = self.ignore\n\n        if scheme is not None and config_groups is not None:\n            raise ValueError(\"Please specify either `scheme` or `config_groups`\")\n\n        if scheme is not None:\n            # takes precedence over config_groups\n\n            if isinstance(scheme, str) and is_preset_scheme(scheme):\n                # attach targets to scheme\n                scheme = {scheme: targets}\n\n            config_groups = {}\n            for idx, key in enumerate(scheme.keys()):\n                if is_preset_scheme(key):\n                    scheme = preset_name_to_scheme(key, scheme[key])\n                else:\n                    scheme = QuantizationScheme.model_validate(\n                        {\"targets\": scheme[key], **scheme}\n                    )\n\n                group_name = f\"group_{idx}\"\n                config_groups[group_name] = scheme\n\n        if config_groups is None or len(config_groups) == 0:\n            default_quant_scheme = QuantizationScheme(targets=targets)\n            config_groups = {\"group_0\": default_quant_scheme}\n\n        return QuantizationConfig(\n            config_groups=config_groups,\n            kv_cache_scheme=kv_cache_scheme,\n            quantization_status=QuantizationStatus.INITIALIZED,\n            ignore=ignore,\n        )\n\n    def _initialize_observers(self, module: torch.nn.Module):\n        if not hasattr(module, \"quantization_scheme\"):\n            return\n\n        scheme: QuantizationScheme = module.quantization_scheme\n        input = scheme.input_activations and not scheme.input_activations.dynamic\n        weight = scheme.weights is not None\n        output = scheme.output_activations and not scheme.output_activations.dynamic\n        is_attention = is_attention_module(module)\n\n        # input activations\n        if input:\n            initialize_observer(module, base_name=\"input\")\n\n        # weight observers (used by `update_weight_zp_scale` or child modifier)\n        if weight:\n            initialize_observer(module, base_name=\"weight\")\n\n        # kv_cache activations. Within `apply_quantization_config`, the config is\n        # modified to use attention output quantization if a kv_cache_scheme exists\n        if is_attention and output:\n            initialize_quantized_kv_cache(module)\n\n        # output activations\n        elif output:\n            initialize_observer(module, base_name=\"output\")\n\n    def _initialize_hooks(self, model: torch.nn.Module) -&gt; Set[RemovableHandle]:\n        hooks = set()\n        for module in model.modules():\n            if not hasattr(module, \"quantization_scheme\"):\n                continue\n\n            scheme: QuantizationScheme = module.quantization_scheme\n            input = scheme.input_activations and not scheme.input_activations.dynamic\n            output = scheme.output_activations and not scheme.output_activations.dynamic\n            is_attention = is_attention_module(module)\n\n            # input activations\n            if input:\n                hooks.add(\n                    self.register_hook(module, calibrate_input_hook, \"forward_pre\")\n                )\n\n            # kv_cache activations. Within `apply_quantization_config`, the config is\n            # modified to use attention output quantization if a kv_cache_scheme exists\n            if is_attention and output:\n                hooks.add(\n                    self.register_hook(\n                        module,\n                        calibrate_kv_cache_input_hook,\n                        \"forward_pre\",\n                        with_kwargs=True,\n                    )\n                )\n                hooks.add(\n                    self.register_hook(\n                        module, calibrate_kv_cache_output_hook, \"forward\"\n                    )\n                )\n\n            # output activations\n            elif output:\n                hooks.add(self.register_hook(module, calibrate_output_hook, \"forward\"))\n\n        return hooks\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationMixin.end_calibration","title":"<code>end_calibration(model)</code>","text":"<p>Remove calibration hooks and set the model status to frozen. Keep quantization enabled for future operations</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to end calibration for</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def end_calibration(self, model: torch.nn.Module):\n    \"\"\"\n    Remove calibration hooks and set the model status to frozen. Keep quantization\n    enabled for future operations\n\n    :param model: model to end calibration for\n    \"\"\"\n    self.remove_hooks(self._calibration_hooks)\n    model.apply(freeze_module_quantization)  # remove observers\n    model.apply(enable_quantization)  # keep quantization enabled\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationMixin.has_config","title":"<code>has_config()</code>","text":"<p>Determine if the user has specified a quantization config on this modifier</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def has_config(self) -&gt; bool:\n    \"\"\"\n    Determine if the user has specified a quantization config on this modifier\n    \"\"\"\n    return not (\n        self.config_groups is None\n        and self.targets == [\"Linear\"]\n        and self.ignore == []\n        and self.scheme is None\n        and self.kv_cache_scheme is None\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationMixin.initialize_quantization","title":"<code>initialize_quantization(model)</code>","text":"<p>Attach quantization schemes and observers to modules in the model according to the quantization config specified on this modifier</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to attach schemes and observers to</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def initialize_quantization(self, model: torch.nn.Module):\n    \"\"\"\n    Attach quantization schemes and observers to modules in the model according to\n    the quantization config specified on this modifier\n\n    :param model: model to attach schemes and observers to\n    \"\"\"\n    reset_quantization_status(model)  # reset any previously applied qconfigs\n\n    # apply scheme and status to model\n    config = self.resolve_quantization_config()\n    apply_quantization_config(model, config)\n\n    # apply observers, disable quantization until calibration\n    model.apply(self._initialize_observers)\n    model.apply(disable_quantization)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationMixin.resolve_quantization_config","title":"<code>resolve_quantization_config()</code>","text":"<p>Returns the quantization config specified by this modifier</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def resolve_quantization_config(self) -&gt; QuantizationConfig:\n    \"\"\"\n    Returns the quantization config specified by this modifier\n    \"\"\"\n    scheme = self.scheme\n    targets = self.targets\n    config_groups = self.config_groups\n    kv_cache_scheme = self.kv_cache_scheme\n    ignore = self.ignore\n\n    if scheme is not None and config_groups is not None:\n        raise ValueError(\"Please specify either `scheme` or `config_groups`\")\n\n    if scheme is not None:\n        # takes precedence over config_groups\n\n        if isinstance(scheme, str) and is_preset_scheme(scheme):\n            # attach targets to scheme\n            scheme = {scheme: targets}\n\n        config_groups = {}\n        for idx, key in enumerate(scheme.keys()):\n            if is_preset_scheme(key):\n                scheme = preset_name_to_scheme(key, scheme[key])\n            else:\n                scheme = QuantizationScheme.model_validate(\n                    {\"targets\": scheme[key], **scheme}\n                )\n\n            group_name = f\"group_{idx}\"\n            config_groups[group_name] = scheme\n\n    if config_groups is None or len(config_groups) == 0:\n        default_quant_scheme = QuantizationScheme(targets=targets)\n        config_groups = {\"group_0\": default_quant_scheme}\n\n    return QuantizationConfig(\n        config_groups=config_groups,\n        kv_cache_scheme=kv_cache_scheme,\n        quantization_status=QuantizationStatus.INITIALIZED,\n        ignore=ignore,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationMixin.start_calibration","title":"<code>start_calibration(model)</code>","text":"<p>Register activation calibration hooks (including kv_cache quantization) and enable quantization as we calibrate</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to prepare for calibration</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def start_calibration(self, model: torch.nn.Module):\n    \"\"\"\n    Register activation calibration hooks (including kv_cache quantization) and\n    enable quantization as we calibrate\n\n    :param model: model to prepare for calibration\n    \"\"\"\n    self._calibration_hooks = self._initialize_hooks(model)\n    model.apply(apply_calibration_status)\n    model.apply(enable_quantization)  # quantize at the same time as calibrate\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationModifier","title":"<code>QuantizationModifier</code>","text":"<p>               Bases: <code>Modifier</code>, <code>QuantizationMixin</code></p> <p>Enables post training quantization (PTQ) and quantization aware training (QAT) for a given module or its submodules. After calibration (PTQ) or the start epoch (QAT), the specified module(s) forward pass will emulate quantized execution and the modifier will be enabled until training is completed.</p> <p>Parameters:</p> Name Type Description Default <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>class QuantizationModifier(Modifier, QuantizationMixin):\n    \"\"\"\n    Enables post training quantization (PTQ) and quantization aware training (QAT) for a\n    given module or its submodules. After calibration (PTQ) or the start epoch (QAT),\n    the specified module(s) forward pass will emulate quantized execution and the\n    modifier will be enabled until training is completed.\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Prepare to calibrate activations and weights\n\n        According to the quantization config, a quantization scheme is attached to each\n        targeted module. The module's forward call is also overwritten to perform\n        quantization to inputs, weights, and outputs.\n\n        Then, according to the module's quantization scheme, observers and calibration\n        hooks are added. These hooks are disabled until the modifier starts.\n        \"\"\"\n        if not QuantizationMixin.has_config(self):\n            raise ValueError(\n                \"QuantizationModifier requires that quantization fields be specified\"\n            )\n        QuantizationMixin.initialize_quantization(self, state.model)\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Begin calibrating activations and weights. Calibrate weights only once on start\n        \"\"\"\n        self.started_ = True\n        QuantizationMixin.start_calibration(self, state.model)\n\n        modules = list(state.model.modules())\n        for module in tqdm.tqdm(modules, desc=\"Calibrating weights\"):\n            update_weight_zp_scale(module)\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Finish calibrating by removing observers and calibration hooks\n        \"\"\"\n        self.ended_ = True\n        QuantizationMixin.end_calibration(\n            self, state.model\n        )  # keep quantization enabled\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        if not self.ended_:\n            self.on_end(state, None)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationModifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>Finish calibrating by removing observers and calibration hooks</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Finish calibrating by removing observers and calibration hooks\n    \"\"\"\n    self.ended_ = True\n    QuantizationMixin.end_calibration(\n        self, state.model\n    )  # keep quantization enabled\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Prepare to calibrate activations and weights</p> <p>According to the quantization config, a quantization scheme is attached to each targeted module. The module's forward call is also overwritten to perform quantization to inputs, weights, and outputs.</p> <p>Then, according to the module's quantization scheme, observers and calibration hooks are added. These hooks are disabled until the modifier starts.</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Prepare to calibrate activations and weights\n\n    According to the quantization config, a quantization scheme is attached to each\n    targeted module. The module's forward call is also overwritten to perform\n    quantization to inputs, weights, and outputs.\n\n    Then, according to the module's quantization scheme, observers and calibration\n    hooks are added. These hooks are disabled until the modifier starts.\n    \"\"\"\n    if not QuantizationMixin.has_config(self):\n        raise ValueError(\n            \"QuantizationModifier requires that quantization fields be specified\"\n        )\n    QuantizationMixin.initialize_quantization(self, state.model)\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizationModifier.on_start","title":"<code>on_start(state, event, **kwargs)</code>","text":"<p>Begin calibrating activations and weights. Calibrate weights only once on start</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_start(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Begin calibrating activations and weights. Calibrate weights only once on start\n    \"\"\"\n    self.started_ = True\n    QuantizationMixin.start_calibration(self, state.model)\n\n    modules = list(state.model.modules())\n    for module in tqdm.tqdm(modules, desc=\"Calibrating weights\"):\n        update_weight_zp_scale(module)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizedKVParameterCache","title":"<code>QuantizedKVParameterCache</code>","text":"<p>               Bases: <code>DynamicCache</code></p> <p>Quantized KV cache used in the forward call based on HF's dynamic cache. Quantization strategy (tensor, group, channel) set from Quantization arg's strategy Singleton, so that the same cache gets reused in all forward call of self_attn. Each time forward is called, .update() is called, and ._quantize(), ._dequantize()  gets called appropriately. The size of tensor is  <code>[batch_size, num_heads, seq_len - residual_length, head_dim]</code>.</p> <p>Triggered by adding kv_cache_scheme in the recipe.</p> <p>Example:</p> <p>```python3 recipe = ''' quant_stage:     quant_modifiers:         QuantizationModifier:             kv_cache_scheme:                 num_bits: 8                 type: float                 strategy: tensor                 dynamic: false                 symmetric: true '''</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>class QuantizedKVParameterCache(DynamicCache):\n    \"\"\"\n    Quantized KV cache used in the forward call based on HF's dynamic cache.\n    Quantization strategy (tensor, group, channel) set from Quantization arg's strategy\n    Singleton, so that the same cache gets reused in all forward call of self_attn.\n    Each time forward is called, .update() is called, and ._quantize(), ._dequantize()\n     gets called appropriately.\n    The size of tensor is\n     `[batch_size, num_heads, seq_len - residual_length, head_dim]`.\n\n\n    Triggered by adding kv_cache_scheme in the recipe.\n\n    Example:\n\n    ```python3\n    recipe = '''\n    quant_stage:\n        quant_modifiers:\n            QuantizationModifier:\n                kv_cache_scheme:\n                    num_bits: 8\n                    type: float\n                    strategy: tensor\n                    dynamic: false\n                    symmetric: true\n    '''\n\n    \"\"\"\n\n    _instance = None\n    _initialized = False\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Singleton\"\"\"\n        if cls._instance is None:\n            cls._instance = super(QuantizedKVParameterCache, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self, quantization_args: QuantizationArgs):\n        if not self._initialized:\n            super().__init__()\n\n            self.quantization_args = quantization_args\n\n            self.k_observers: List[Observer] = []\n            self.v_observers: List[Observer] = []\n\n            # each index corresponds to layer_idx of the attention layer\n            self.k_scales: List[Tensor] = []\n            self.v_scales: List[Tensor] = []\n\n            self.k_zps: List[Tensor] = []\n            self.v_zps: List[Tensor] = []\n\n            self._initialized = True\n\n    def update(\n        self,\n        key_states: Tensor,\n        value_states: Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"\n        Get the k_scale and v_scale and output the\n         fakequant-ed key_states and value_states\n        \"\"\"\n\n        if len(self.k_observers) &lt;= layer_idx:\n            k_observer_name = self.quantization_args.observer\n            k_observer = Observer.load_from_registry(\n                k_observer_name, quantization_args=self.quantization_args\n            )\n            v_observer_name = self.quantization_args.observer\n            v_observer = Observer.load_from_registry(\n                v_observer_name, quantization_args=self.quantization_args\n            )\n\n            # NOTE: User may ignore some layers in configuration,\n            # meaning len(self.k_observers) &lt;= layer_idx-1\n            # Must account for that case by padding list so that\n            # index of lists corresponds to layer_idx\n            _pad_and_append_at_idx_(self.k_observers, layer_idx, k_observer)\n            _pad_and_append_at_idx_(self.v_observers, layer_idx, v_observer)\n\n        q_key_states = self._quantize(\n            key_states.contiguous(), KVCacheScaleType.KEY, layer_idx\n        )\n        q_value_states = self._quantize(\n            value_states.contiguous(), KVCacheScaleType.VALUE, layer_idx\n        )\n\n        qdq_key_states = self._dequantize(q_key_states, KVCacheScaleType.KEY, layer_idx)\n        qdq_value_states = self._dequantize(\n            q_value_states, KVCacheScaleType.VALUE, layer_idx\n        )\n\n        keys_to_return, values_to_return = qdq_key_states, qdq_value_states\n\n        return keys_to_return, values_to_return\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -&gt; int:\n        \"\"\"\n        Returns the sequence length of the cached states.\n        A layer index can be optionally passed.\n        \"\"\"\n        if len(self.key_cache) &lt;= layer_idx:\n            return 0\n        # since we cannot get the seq_length of each layer directly and\n        # rely on `_seen_tokens` which is updated every \"layer_idx\" == 0,\n        # this is a hack to get the actual seq_length for the given layer_idx\n        # this part of code otherwise fails when used to\n        # verify attn_weight shape in some models\n        return self._seen_tokens if layer_idx == 0 else self._seen_tokens - 1\n\n    def reset_states(self):\n        \"\"\"reset the kv states (used in calibration)\"\"\"\n        self.key_cache: List[Tensor] = []\n        self.value_cache: List[Tensor] = []\n        # Used in `generate` to keep tally of how many tokens the cache has seen\n        self._seen_tokens = 0\n        self._quantized_key_cache: List[Tensor] = []\n        self._quantized_value_cache: List[Tensor] = []\n\n    def reset(self):\n        \"\"\"\n        Reset the instantiation, create new instance on init\n        \"\"\"\n        QuantizedKVParameterCache._instance = None\n        QuantizedKVParameterCache._initialized = False\n\n    def _quantize(self, tensor, kv_type, layer_idx):\n        \"\"\"Quantizes a key/value using a defined quantization method.\"\"\"\n        from compressed_tensors.quantization.lifecycle.forward import quantize\n\n        if kv_type == KVCacheScaleType.KEY:  # key type\n            observer = self.k_observers[layer_idx]\n            scales = self.k_scales\n            zps = self.k_zps\n        else:\n            assert kv_type == KVCacheScaleType.VALUE\n            observer = self.v_observers[layer_idx]\n            scales = self.v_scales\n            zps = self.v_zps\n\n        scale, zp = observer(tensor)\n        _pad_and_append_at_idx_(scales, layer_idx, scale)\n        _pad_and_append_at_idx_(zps, layer_idx, zp)\n\n        q_tensor = quantize(\n            x=tensor,\n            scale=scale,\n            zero_point=zp,\n            args=self.quantization_args,\n        )\n        return q_tensor\n\n    def _dequantize(self, qtensor, kv_type, layer_idx):\n        \"\"\"Dequantizes back the tensor that was quantized by `self._quantize()`\"\"\"\n        from compressed_tensors.quantization.lifecycle.forward import dequantize\n\n        if kv_type == KVCacheScaleType.KEY:\n            scale = self.k_scales[layer_idx]\n            zp = self.k_zps[layer_idx]\n        else:\n            assert kv_type == KVCacheScaleType.VALUE\n            scale = self.v_scales[layer_idx]\n            zp = self.v_zps[layer_idx]\n\n        qdq_tensor = dequantize(\n            x_q=qtensor,\n            scale=scale,\n            zero_point=zp,\n            args=self.quantization_args,\n        )\n        return qdq_tensor\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizedKVParameterCache.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Singleton</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def __new__(cls, *args, **kwargs):\n    \"\"\"Singleton\"\"\"\n    if cls._instance is None:\n        cls._instance = super(QuantizedKVParameterCache, cls).__new__(cls)\n    return cls._instance\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizedKVParameterCache.get_seq_length","title":"<code>get_seq_length(layer_idx=0)</code>","text":"<p>Returns the sequence length of the cached states. A layer index can be optionally passed.</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def get_seq_length(self, layer_idx: Optional[int] = 0) -&gt; int:\n    \"\"\"\n    Returns the sequence length of the cached states.\n    A layer index can be optionally passed.\n    \"\"\"\n    if len(self.key_cache) &lt;= layer_idx:\n        return 0\n    # since we cannot get the seq_length of each layer directly and\n    # rely on `_seen_tokens` which is updated every \"layer_idx\" == 0,\n    # this is a hack to get the actual seq_length for the given layer_idx\n    # this part of code otherwise fails when used to\n    # verify attn_weight shape in some models\n    return self._seen_tokens if layer_idx == 0 else self._seen_tokens - 1\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizedKVParameterCache.reset","title":"<code>reset()</code>","text":"<p>Reset the instantiation, create new instance on init</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the instantiation, create new instance on init\n    \"\"\"\n    QuantizedKVParameterCache._instance = None\n    QuantizedKVParameterCache._initialized = False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizedKVParameterCache.reset_states","title":"<code>reset_states()</code>","text":"<p>reset the kv states (used in calibration)</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def reset_states(self):\n    \"\"\"reset the kv states (used in calibration)\"\"\"\n    self.key_cache: List[Tensor] = []\n    self.value_cache: List[Tensor] = []\n    # Used in `generate` to keep tally of how many tokens the cache has seen\n    self._seen_tokens = 0\n    self._quantized_key_cache: List[Tensor] = []\n    self._quantized_value_cache: List[Tensor] = []\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/#llmcompressor.modifiers.quantization.QuantizedKVParameterCache.update","title":"<code>update(key_states, value_states, layer_idx, cache_kwargs=None)</code>","text":"<p>Get the k_scale and v_scale and output the  fakequant-ed key_states and value_states</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def update(\n    self,\n    key_states: Tensor,\n    value_states: Tensor,\n    layer_idx: int,\n    cache_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"\n    Get the k_scale and v_scale and output the\n     fakequant-ed key_states and value_states\n    \"\"\"\n\n    if len(self.k_observers) &lt;= layer_idx:\n        k_observer_name = self.quantization_args.observer\n        k_observer = Observer.load_from_registry(\n            k_observer_name, quantization_args=self.quantization_args\n        )\n        v_observer_name = self.quantization_args.observer\n        v_observer = Observer.load_from_registry(\n            v_observer_name, quantization_args=self.quantization_args\n        )\n\n        # NOTE: User may ignore some layers in configuration,\n        # meaning len(self.k_observers) &lt;= layer_idx-1\n        # Must account for that case by padding list so that\n        # index of lists corresponds to layer_idx\n        _pad_and_append_at_idx_(self.k_observers, layer_idx, k_observer)\n        _pad_and_append_at_idx_(self.v_observers, layer_idx, v_observer)\n\n    q_key_states = self._quantize(\n        key_states.contiguous(), KVCacheScaleType.KEY, layer_idx\n    )\n    q_value_states = self._quantize(\n        value_states.contiguous(), KVCacheScaleType.VALUE, layer_idx\n    )\n\n    qdq_key_states = self._dequantize(q_key_states, KVCacheScaleType.KEY, layer_idx)\n    qdq_value_states = self._dequantize(\n        q_value_states, KVCacheScaleType.VALUE, layer_idx\n    )\n\n    keys_to_return, values_to_return = qdq_key_states, qdq_value_states\n\n    return keys_to_return, values_to_return\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/cache/","title":"llmcompressor.modifiers.quantization.cache","text":""},{"location":"reference/llmcompressor/modifiers/quantization/cache/#llmcompressor.modifiers.quantization.cache.QuantizedKVParameterCache","title":"<code>QuantizedKVParameterCache</code>","text":"<p>               Bases: <code>DynamicCache</code></p> <p>Quantized KV cache used in the forward call based on HF's dynamic cache. Quantization strategy (tensor, group, channel) set from Quantization arg's strategy Singleton, so that the same cache gets reused in all forward call of self_attn. Each time forward is called, .update() is called, and ._quantize(), ._dequantize()  gets called appropriately. The size of tensor is  <code>[batch_size, num_heads, seq_len - residual_length, head_dim]</code>.</p> <p>Triggered by adding kv_cache_scheme in the recipe.</p> <p>Example:</p> <p>```python3 recipe = ''' quant_stage:     quant_modifiers:         QuantizationModifier:             kv_cache_scheme:                 num_bits: 8                 type: float                 strategy: tensor                 dynamic: false                 symmetric: true '''</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>class QuantizedKVParameterCache(DynamicCache):\n    \"\"\"\n    Quantized KV cache used in the forward call based on HF's dynamic cache.\n    Quantization strategy (tensor, group, channel) set from Quantization arg's strategy\n    Singleton, so that the same cache gets reused in all forward call of self_attn.\n    Each time forward is called, .update() is called, and ._quantize(), ._dequantize()\n     gets called appropriately.\n    The size of tensor is\n     `[batch_size, num_heads, seq_len - residual_length, head_dim]`.\n\n\n    Triggered by adding kv_cache_scheme in the recipe.\n\n    Example:\n\n    ```python3\n    recipe = '''\n    quant_stage:\n        quant_modifiers:\n            QuantizationModifier:\n                kv_cache_scheme:\n                    num_bits: 8\n                    type: float\n                    strategy: tensor\n                    dynamic: false\n                    symmetric: true\n    '''\n\n    \"\"\"\n\n    _instance = None\n    _initialized = False\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Singleton\"\"\"\n        if cls._instance is None:\n            cls._instance = super(QuantizedKVParameterCache, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self, quantization_args: QuantizationArgs):\n        if not self._initialized:\n            super().__init__()\n\n            self.quantization_args = quantization_args\n\n            self.k_observers: List[Observer] = []\n            self.v_observers: List[Observer] = []\n\n            # each index corresponds to layer_idx of the attention layer\n            self.k_scales: List[Tensor] = []\n            self.v_scales: List[Tensor] = []\n\n            self.k_zps: List[Tensor] = []\n            self.v_zps: List[Tensor] = []\n\n            self._initialized = True\n\n    def update(\n        self,\n        key_states: Tensor,\n        value_states: Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"\n        Get the k_scale and v_scale and output the\n         fakequant-ed key_states and value_states\n        \"\"\"\n\n        if len(self.k_observers) &lt;= layer_idx:\n            k_observer_name = self.quantization_args.observer\n            k_observer = Observer.load_from_registry(\n                k_observer_name, quantization_args=self.quantization_args\n            )\n            v_observer_name = self.quantization_args.observer\n            v_observer = Observer.load_from_registry(\n                v_observer_name, quantization_args=self.quantization_args\n            )\n\n            # NOTE: User may ignore some layers in configuration,\n            # meaning len(self.k_observers) &lt;= layer_idx-1\n            # Must account for that case by padding list so that\n            # index of lists corresponds to layer_idx\n            _pad_and_append_at_idx_(self.k_observers, layer_idx, k_observer)\n            _pad_and_append_at_idx_(self.v_observers, layer_idx, v_observer)\n\n        q_key_states = self._quantize(\n            key_states.contiguous(), KVCacheScaleType.KEY, layer_idx\n        )\n        q_value_states = self._quantize(\n            value_states.contiguous(), KVCacheScaleType.VALUE, layer_idx\n        )\n\n        qdq_key_states = self._dequantize(q_key_states, KVCacheScaleType.KEY, layer_idx)\n        qdq_value_states = self._dequantize(\n            q_value_states, KVCacheScaleType.VALUE, layer_idx\n        )\n\n        keys_to_return, values_to_return = qdq_key_states, qdq_value_states\n\n        return keys_to_return, values_to_return\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -&gt; int:\n        \"\"\"\n        Returns the sequence length of the cached states.\n        A layer index can be optionally passed.\n        \"\"\"\n        if len(self.key_cache) &lt;= layer_idx:\n            return 0\n        # since we cannot get the seq_length of each layer directly and\n        # rely on `_seen_tokens` which is updated every \"layer_idx\" == 0,\n        # this is a hack to get the actual seq_length for the given layer_idx\n        # this part of code otherwise fails when used to\n        # verify attn_weight shape in some models\n        return self._seen_tokens if layer_idx == 0 else self._seen_tokens - 1\n\n    def reset_states(self):\n        \"\"\"reset the kv states (used in calibration)\"\"\"\n        self.key_cache: List[Tensor] = []\n        self.value_cache: List[Tensor] = []\n        # Used in `generate` to keep tally of how many tokens the cache has seen\n        self._seen_tokens = 0\n        self._quantized_key_cache: List[Tensor] = []\n        self._quantized_value_cache: List[Tensor] = []\n\n    def reset(self):\n        \"\"\"\n        Reset the instantiation, create new instance on init\n        \"\"\"\n        QuantizedKVParameterCache._instance = None\n        QuantizedKVParameterCache._initialized = False\n\n    def _quantize(self, tensor, kv_type, layer_idx):\n        \"\"\"Quantizes a key/value using a defined quantization method.\"\"\"\n        from compressed_tensors.quantization.lifecycle.forward import quantize\n\n        if kv_type == KVCacheScaleType.KEY:  # key type\n            observer = self.k_observers[layer_idx]\n            scales = self.k_scales\n            zps = self.k_zps\n        else:\n            assert kv_type == KVCacheScaleType.VALUE\n            observer = self.v_observers[layer_idx]\n            scales = self.v_scales\n            zps = self.v_zps\n\n        scale, zp = observer(tensor)\n        _pad_and_append_at_idx_(scales, layer_idx, scale)\n        _pad_and_append_at_idx_(zps, layer_idx, zp)\n\n        q_tensor = quantize(\n            x=tensor,\n            scale=scale,\n            zero_point=zp,\n            args=self.quantization_args,\n        )\n        return q_tensor\n\n    def _dequantize(self, qtensor, kv_type, layer_idx):\n        \"\"\"Dequantizes back the tensor that was quantized by `self._quantize()`\"\"\"\n        from compressed_tensors.quantization.lifecycle.forward import dequantize\n\n        if kv_type == KVCacheScaleType.KEY:\n            scale = self.k_scales[layer_idx]\n            zp = self.k_zps[layer_idx]\n        else:\n            assert kv_type == KVCacheScaleType.VALUE\n            scale = self.v_scales[layer_idx]\n            zp = self.v_zps[layer_idx]\n\n        qdq_tensor = dequantize(\n            x_q=qtensor,\n            scale=scale,\n            zero_point=zp,\n            args=self.quantization_args,\n        )\n        return qdq_tensor\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/cache/#llmcompressor.modifiers.quantization.cache.QuantizedKVParameterCache.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Singleton</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def __new__(cls, *args, **kwargs):\n    \"\"\"Singleton\"\"\"\n    if cls._instance is None:\n        cls._instance = super(QuantizedKVParameterCache, cls).__new__(cls)\n    return cls._instance\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/cache/#llmcompressor.modifiers.quantization.cache.QuantizedKVParameterCache.get_seq_length","title":"<code>get_seq_length(layer_idx=0)</code>","text":"<p>Returns the sequence length of the cached states. A layer index can be optionally passed.</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def get_seq_length(self, layer_idx: Optional[int] = 0) -&gt; int:\n    \"\"\"\n    Returns the sequence length of the cached states.\n    A layer index can be optionally passed.\n    \"\"\"\n    if len(self.key_cache) &lt;= layer_idx:\n        return 0\n    # since we cannot get the seq_length of each layer directly and\n    # rely on `_seen_tokens` which is updated every \"layer_idx\" == 0,\n    # this is a hack to get the actual seq_length for the given layer_idx\n    # this part of code otherwise fails when used to\n    # verify attn_weight shape in some models\n    return self._seen_tokens if layer_idx == 0 else self._seen_tokens - 1\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/cache/#llmcompressor.modifiers.quantization.cache.QuantizedKVParameterCache.reset","title":"<code>reset()</code>","text":"<p>Reset the instantiation, create new instance on init</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the instantiation, create new instance on init\n    \"\"\"\n    QuantizedKVParameterCache._instance = None\n    QuantizedKVParameterCache._initialized = False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/cache/#llmcompressor.modifiers.quantization.cache.QuantizedKVParameterCache.reset_states","title":"<code>reset_states()</code>","text":"<p>reset the kv states (used in calibration)</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def reset_states(self):\n    \"\"\"reset the kv states (used in calibration)\"\"\"\n    self.key_cache: List[Tensor] = []\n    self.value_cache: List[Tensor] = []\n    # Used in `generate` to keep tally of how many tokens the cache has seen\n    self._seen_tokens = 0\n    self._quantized_key_cache: List[Tensor] = []\n    self._quantized_value_cache: List[Tensor] = []\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/cache/#llmcompressor.modifiers.quantization.cache.QuantizedKVParameterCache.update","title":"<code>update(key_states, value_states, layer_idx, cache_kwargs=None)</code>","text":"<p>Get the k_scale and v_scale and output the  fakequant-ed key_states and value_states</p> Source code in <code>src/llmcompressor/modifiers/quantization/cache.py</code> <pre><code>def update(\n    self,\n    key_states: Tensor,\n    value_states: Tensor,\n    layer_idx: int,\n    cache_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"\n    Get the k_scale and v_scale and output the\n     fakequant-ed key_states and value_states\n    \"\"\"\n\n    if len(self.k_observers) &lt;= layer_idx:\n        k_observer_name = self.quantization_args.observer\n        k_observer = Observer.load_from_registry(\n            k_observer_name, quantization_args=self.quantization_args\n        )\n        v_observer_name = self.quantization_args.observer\n        v_observer = Observer.load_from_registry(\n            v_observer_name, quantization_args=self.quantization_args\n        )\n\n        # NOTE: User may ignore some layers in configuration,\n        # meaning len(self.k_observers) &lt;= layer_idx-1\n        # Must account for that case by padding list so that\n        # index of lists corresponds to layer_idx\n        _pad_and_append_at_idx_(self.k_observers, layer_idx, k_observer)\n        _pad_and_append_at_idx_(self.v_observers, layer_idx, v_observer)\n\n    q_key_states = self._quantize(\n        key_states.contiguous(), KVCacheScaleType.KEY, layer_idx\n    )\n    q_value_states = self._quantize(\n        value_states.contiguous(), KVCacheScaleType.VALUE, layer_idx\n    )\n\n    qdq_key_states = self._dequantize(q_key_states, KVCacheScaleType.KEY, layer_idx)\n    qdq_value_states = self._dequantize(\n        q_value_states, KVCacheScaleType.VALUE, layer_idx\n    )\n\n    keys_to_return, values_to_return = qdq_key_states, qdq_value_states\n\n    return keys_to_return, values_to_return\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/","title":"llmcompressor.modifiers.quantization.calibration","text":""},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.calibrate_activations","title":"<code>calibrate_activations(module, value, base_name)</code>","text":"<p>Calibrate input or output activations by calling the a module's attached observer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>torch.nn.Module</p> required <code>base_name</code> <code>str</code> <p>substring used to fetch the observer, scales, and zp</p> required <code>value</code> <code>Tensor</code> <p>torch.Tensor to be passed to the observer</p> required Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def calibrate_activations(module: Module, value: torch.Tensor, base_name: str):\n    \"\"\"\n    Calibrate input or output activations by calling the a module's attached\n    observer.\n\n    :param module: torch.nn.Module\n    :param base_name: substring used to fetch the observer, scales, and zp\n    :param value: torch.Tensor to be passed to the observer\n\n    \"\"\"\n    # If empty tensor, can't update zp/scale\n    # Case for MoEs\n    if value.numel() == 0:\n        return\n\n    call_observer(\n        module=module,\n        base_name=base_name,\n        value=value,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.calibrate_input_hook","title":"<code>calibrate_input_hook(module, args)</code>","text":"<p>Hook to calibrate input activations. Will call the observers to update the scales/zp before applying input QDQ in the module's forward pass.</p> Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def calibrate_input_hook(module: Module, args: Any):\n    \"\"\"\n    Hook to calibrate input activations.\n    Will call the observers to update the scales/zp before applying\n    input QDQ in the module's forward pass.\n    \"\"\"\n    args = args[0] if isinstance(args, tuple) else args\n    calibrate_activations(module, value=args, base_name=\"input\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.calibrate_kv_cache_input_hook","title":"<code>calibrate_kv_cache_input_hook(module, args, kwargs)</code>","text":"<p>Hook to update inputs to attention layers when running kv_cache quantization. Will update the passed in kv_cache to singleton QuantizedKVParameterCache.</p> Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def calibrate_kv_cache_input_hook(\n    module: Module, args: Any, kwargs: Dict[str, Any]\n) -&gt; Tuple[Tuple[Any, ...], Dict[str, Any]]:\n    \"\"\"\n    Hook to update inputs to attention layers when running\n    kv_cache quantization. Will update the passed in\n    kv_cache to singleton QuantizedKVParameterCache.\n    \"\"\"\n    kv_cache = getattr(module, \"kv_cache\")\n    kwargs[\"past_key_value\"] = kv_cache\n    kwargs[\"use_cache\"] = False\n    return args, kwargs\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.calibrate_kv_cache_output_hook","title":"<code>calibrate_kv_cache_output_hook(module, _args, _output)</code>","text":"<p>Hook to update k_scale and v_scale parameters when running kv_cache quantization.</p> Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def calibrate_kv_cache_output_hook(module: Module, _args: Any, _output: torch.Tensor):\n    \"\"\"\n    Hook to update k_scale and v_scale parameters when running kv_cache quantization.\n    \"\"\"\n    kv_cache = getattr(module, \"kv_cache\")\n    k_scale = kv_cache.k_scales[module.layer_idx]\n    v_scale = kv_cache.v_scales[module.layer_idx]\n    update_parameter_data(module, k_scale, KVCacheScaleType.KEY.value)\n    update_parameter_data(module, v_scale, KVCacheScaleType.VALUE.value)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.calibrate_output_hook","title":"<code>calibrate_output_hook(module, _args, output)</code>","text":"<p>Hook to calibrate output activations. Will call the observers to update the scales/zp before applying output QDQ.</p> Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def calibrate_output_hook(module: Module, _args: Any, output: torch.Tensor):\n    \"\"\"\n    Hook to calibrate output activations.\n    Will call the observers to update the scales/zp before applying\n    output QDQ.\n    \"\"\"\n    calibrate_activations(\n        module,\n        value=output,\n        base_name=\"output\",\n    )\n    output = forward_quantize(\n        module=module,\n        value=output,\n        base_name=\"output\",\n        args=module.quantization_scheme.output_activations,\n    )\n    return output\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.call_observer","title":"<code>call_observer(module, base_name, value=None)</code>","text":"<p>Call a module's attached input/weight/output observer using a provided value. Update the module's scale and zp using the observer's return values.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>torch.nn.Module</p> required <code>base_name</code> <code>str</code> <p>substring used to fetch the observer, scales, and zp</p> required <code>value</code> <code>Optional[Tensor]</code> <p>torch.Tensor to be passed to the observer for activations. If base_name is \"weight\", then the module's weight tensor will be used</p> <code>None</code> Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def call_observer(module: Module, base_name: str, value: Optional[torch.Tensor] = None):\n    \"\"\"\n    Call a module's attached input/weight/output observer using a provided value.\n    Update the module's scale and zp using the observer's return values.\n\n    :param module: torch.nn.Module\n    :param base_name: substring used to fetch the observer, scales, and zp\n    :param value: torch.Tensor to be passed to the observer for activations. If\n        base_name is \"weight\", then the module's weight tensor will be used\n    \"\"\"\n    with align_module_device(module):\n        if base_name == \"weight\":\n            value = module.weight\n            g_idx = getattr(module, \"weight_g_idx\", None)\n        elif value is not None:\n            g_idx = None\n        else:\n            raise ValueError(\n                \"Must provide a value to observe if not using weight observer\"\n            )\n\n        observer = getattr(module, f\"{base_name}_observer\")\n        updated_scale, updated_zero_point = observer(value, g_idx=g_idx)\n\n        # update scale and zero point\n        update_parameter_data(module, updated_scale, f\"{base_name}_scale\")\n        update_parameter_data(module, updated_zero_point, f\"{base_name}_zero_point\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.freeze_module_quantization","title":"<code>freeze_module_quantization(module)</code>","text":"<p>deletes observers when calibration is complete.</p> <p>apply to full model with <code>model.apply(freeze_module_quantization)</code></p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to freeze quantization for</p> required Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def freeze_module_quantization(module: Module):\n    \"\"\"\n    deletes observers when calibration is complete.\n\n    apply to full model with `model.apply(freeze_module_quantization)`\n\n    :param module: module to freeze quantization for\n    \"\"\"\n    scheme = getattr(module, \"quantization_scheme\", None)\n    if not scheme:\n        # no quantization scheme nothing to do\n        return\n\n    if module.quantization_status == QuantizationStatus.FROZEN:\n        # nothing to do, already frozen\n        return\n\n    # remove observers\n    for name in (\"input\", \"weight\", \"output\"):\n        obs_name = f\"{name}_observer\"\n        if hasattr(module, obs_name):\n            delattr(module, obs_name)\n\n    # remove quantized kv_cache\n    kv_cache = getattr(module, \"kv_cache\", None)\n    if isinstance(kv_cache, QuantizedKVParameterCache):\n        delattr(module, \"kv_cache\")\n\n    module.quantization_status = QuantizationStatus.FROZEN\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.initialize_observer","title":"<code>initialize_observer(module, base_name)</code>","text":"<p>Initialize observer module and attach as submodule. The name of the observer is fetched from the quantization_args. The name is then used to load the observer from the registry and attached to the module. The name of the observer uses the base_name provided.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>torch.nn.Module that the observer is being attached to</p> required <code>base_name</code> <code>str</code> <p>str used to name the observer attribute</p> required Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def initialize_observer(\n    module: Module,\n    base_name: str,\n):\n    \"\"\"\n    Initialize observer module and attach as submodule.\n    The name of the observer is fetched from the quantization_args.\n    The name is then used to load the observer from the registry and attached\n    to the module. The name of the observer uses the base_name provided.\n\n    :param module: torch.nn.Module that the observer is being attached to\n    :param base_name: str used to name the observer attribute\n\n    \"\"\"\n\n    arg_name = \"weights\" if base_name == \"weight\" else f\"{base_name}_activations\"\n    quantization_scheme = getattr(module, \"quantization_scheme\", None)\n    if not quantization_scheme:\n        # no quantization scheme nothing to do\n        return\n\n    quantization_args = getattr(quantization_scheme, arg_name, None)\n    # dont need observers for dynamic\n    if quantization_args is not None and not quantization_args.dynamic:\n        observer = Observer.load_from_registry(\n            quantization_args.observer, quantization_args=quantization_args\n        )\n        module.register_module(f\"{base_name}_observer\", observer)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.initialize_quantized_kv_cache","title":"<code>initialize_quantized_kv_cache(module)</code>","text":"<p>Initialize a quantized kv_cache on a module (analogous to initializing an observer) When a config specifying kv_cache quantization is applied to a model, the kv_cache args are redefined as the output_activations targeting attention modules.</p> <p>This function should be called on attention modules with output_activations</p> Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def initialize_quantized_kv_cache(module: Module):\n    \"\"\"\n    Initialize a quantized kv_cache on a module (analogous to initializing an observer)\n    When a config specifying kv_cache quantization is applied to a model, the kv_cache\n    args are redefined as the output_activations targeting attention modules.\n\n    This function should be called on attention modules with output_activations\n    \"\"\"\n    scheme: Optional[QuantizationScheme] = getattr(module, \"quantization_scheme\", None)\n    existing_kv_cache = getattr(module, \"kv_cache\", None)\n\n    if (\n        scheme is None\n        or not is_kv_cache_quant_scheme(scheme)\n        or isinstance(existing_kv_cache, QuantizedKVParameterCache)\n    ):\n        return\n\n    quantized_kv_cache = QuantizedKVParameterCache(scheme.output_activations)\n    setattr(module, \"kv_cache\", quantized_kv_cache)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/calibration/#llmcompressor.modifiers.quantization.calibration.update_weight_zp_scale","title":"<code>update_weight_zp_scale(module)</code>","text":"<p>marks a layer as ready for calibration which activates observers to update scales and zero points on each forward pass</p> <p>apply to full model with <code>model.apply(update_weight_zp_scale)</code></p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to set for calibration</p> required <code>quantize_weights_upfront</code> <p>whether to automatically run weight quantization at the start of calibration</p> required Source code in <code>src/llmcompressor/modifiers/quantization/calibration.py</code> <pre><code>def update_weight_zp_scale(module: Module):\n    \"\"\"\n    marks a layer as ready for calibration which activates observers\n    to update scales and zero points on each forward pass\n\n    apply to full model with `model.apply(update_weight_zp_scale)`\n\n    :param module: module to set for calibration\n    :param quantize_weights_upfront: whether to automatically\n       run weight quantization at the start of calibration\n    \"\"\"\n    if getattr_chain(module, \"quantization_scheme.weights\", None) is None:\n        return\n\n    if getattr(module, \"quantization_status\", None) != QuantizationStatus.CALIBRATION:\n        logger.warning(\n            \"Attempting to calibrate weights of a module not in calibration mode\"\n        )\n\n    call_observer(module=module, base_name=\"weight\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/","title":"llmcompressor.modifiers.quantization.gptq","text":""},{"location":"reference/llmcompressor/modifiers/quantization/gptq/#llmcompressor.modifiers.quantization.gptq.GPTQModifier","title":"<code>GPTQModifier</code>","text":"<p>               Bases: <code>Modifier</code>, <code>QuantizationMixin</code></p> <p>Implements the GPTQ algorithm from https://arxiv.org/abs/2210.17323. This modifier uses activations to calibrate a hessian matrix, which is then used to determine optimal quantizion values and orderings for the model weights.</p> <p>| Sample yaml: | test_stage: |    obcq_modifiers: |      GPTQModifier: |          block_size: 128 |          dampening_frac: 0.001 |          offload_hessians: False |          config_groups: |            group_0: |                targets: |                  - \"Linear\" |                input_activations: null |                output_activations: null |                weights: |                    num_bits: 8 |                    type: \"int\" |                    symmetric: true |                    strategy: \"tensor\" |                    group_size: 128 |                    actorder: False</p> <p>Lifecycle:     - on_initialize         - apply config to model     - on_start         - add activation calibration hooks         - add gptq weight calibration hooks     - on_sequential_epoch_end         - quantize_weight     - on_finalize         - remove_hooks()         - model.apply(freeze_module_quantization)</p> <p>Parameters:</p> Name Type Description Default <code>sequential_targets</code> <p>list of layer names to compress during GPTQ, or 'ALL' to compress every layer in the model</p> required <code>block_size</code> <p>Used to determine number of columns to compress in one pass</p> required <code>dampening_frac</code> <p>Amount of dampening to apply to H, as a fraction of the diagonal norm</p> required <code>offload_hessians</code> <p>Set to True for decreased memory usage but increased runtime.</p> required <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>class GPTQModifier(Modifier, QuantizationMixin):\n    \"\"\"\n    Implements the GPTQ algorithm from https://arxiv.org/abs/2210.17323. This modifier\n    uses activations to calibrate a hessian matrix, which is then used to determine\n    optimal quantizion values and orderings for the model weights.\n\n    | Sample yaml:\n    | test_stage:\n    |    obcq_modifiers:\n    |      GPTQModifier:\n    |          block_size: 128\n    |          dampening_frac: 0.001\n    |          offload_hessians: False\n    |          config_groups:\n    |            group_0:\n    |                targets:\n    |                  - \"Linear\"\n    |                input_activations: null\n    |                output_activations: null\n    |                weights:\n    |                    num_bits: 8\n    |                    type: \"int\"\n    |                    symmetric: true\n    |                    strategy: \"tensor\"\n    |                    group_size: 128\n    |                    actorder: False\n\n    Lifecycle:\n        - on_initialize\n            - apply config to model\n        - on_start\n            - add activation calibration hooks\n            - add gptq weight calibration hooks\n        - on_sequential_epoch_end\n            - quantize_weight\n        - on_finalize\n            - remove_hooks()\n            - model.apply(freeze_module_quantization)\n\n    :param sequential_targets: list of layer names to compress during GPTQ, or\n        '__ALL__' to compress every layer in the model\n    :param block_size: Used to determine number of columns to compress in one pass\n    :param dampening_frac: Amount of dampening to apply to H, as a fraction of the\n        diagonal norm\n    :param offload_hessians: Set to True for decreased memory usage but increased\n        runtime.\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    # gptq modifier arguments\n    sequential_update: bool = True  # DEPRECIATED\n    sequential_targets: Union[str, List[str], None] = None\n    block_size: int = 128\n    dampening_frac: Optional[float] = 0.01\n    offload_hessians: bool = False\n\n    # private variables\n    _module_names: Dict[torch.nn.Module, str] = PrivateAttr(default_factory=dict)\n    _hessians: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(default_factory=dict)\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n\n    @field_validator(\"sequential_update\", mode=\"before\")\n    def validate_sequential_update(cls, value: bool) -&gt; bool:\n        if not value:\n            warnings.warn(\n                \"`sequential_update=False` is no longer supported, setting \"\n                \"sequential_update=True\",\n                DeprecationWarning,\n            )\n\n        return True\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run the GPTQ algorithm on the current state\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        # apply config to model and prepare calibration hooks\n        if QuantizationMixin.has_config(self):\n            QuantizationMixin.initialize_quantization(self, state.model)\n\n        # prepare module names\n        self._module_names = {m: name for name, m in state.model.named_modules()}\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        self.started_ = True\n\n        # register quantization calibration hooks\n        # assume quantization has been initialized by this modifier or one before it\n        QuantizationMixin.start_calibration(self, state.model)\n        # Unlike qmod, do not quantize as we calibrate\n        # This choice does not seem to have a meaningful impact on accuracy\n        state.model.apply(disable_quantization)\n\n        # register gptq hooks\n        added_hook = False\n        for module in state.model.modules():\n            if getattr_chain(module, \"quantization_scheme.weights\", None) is not None:\n                # HACK: previously, embeddings were not quantized because they were not\n                # accessible by the layer compressor. For now, we manually ignore it,\n                # but in the FUTURE this should be ignored by the user\n                if not isinstance(module, torch.nn.Embedding):\n                    self.register_hook(module, self.calibrate_module, \"forward\")\n                    added_hook = True\n\n        if not added_hook:\n            raise ValueError(\n                \"GPTQModifier requires a weight quantization config be specified by \"\n                \"this modifier or a modifier preceding it\"\n            )\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n            self.compress_modules()\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            self.compress_modules()\n\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Finish calibrating by removing observers and calibration hooks\n        \"\"\"\n        self.ended_ = True\n        QuantizationMixin.end_calibration(self, state.model)\n        self.remove_hooks()  # remove gptq hooks\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        disable the quantization observers used by the OBCQ algorithm\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        if not self.ended_:\n            self.on_end(state, None)\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._hessians = dict()\n        self._num_samples = dict()\n\n        return True\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the hessian of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that first argument is the input\n        inp = args[0]\n\n        # Initialize hessian if not present\n        if module not in self._num_samples:\n            init_device = (\n                \"cpu\" if self.offload_hessians else get_execution_device(module)\n            )\n            self._hessians[module] = make_empty_hessian(module, device=init_device)\n            self._num_samples[module] = 0\n\n        # Accumulate hessian with input with optional offloading\n        with self._maybe_onload_hessian(module):\n            self._hessians[module], self._num_samples[module] = accumulate_hessian(\n                inp,\n                module,\n                self._hessians[module],\n                self._num_samples[module],\n            )\n\n    def compress_modules(self):\n        \"\"\"\n        Quantize modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            num_samples = self._num_samples[module]\n            quant_args = getattr_chain(module, \"quantization_scheme.weights\")\n\n            logger.info(f\"Quantizing {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(\n                module\n            ), self._maybe_onload_hessian(module), CompressionLogger(\n                module\n            ) as comp_logger:\n                loss, quantized_weight, scale, zero_point, g_idx = quantize_weight(\n                    module=module,\n                    quant_args=quant_args,\n                    hessians_dict=self._hessians,\n                    blocksize=self.block_size,\n                    percdamp=self.dampening_frac,\n                )\n                comp_logger.set_loss(loss)\n\n            update_offload_parameter(module, \"weight\", quantized_weight)\n            update_offload_parameter(module, \"weight_scale\", scale)\n            update_offload_parameter(module, \"weight_zero_point\", zero_point)\n            if g_idx is not None:\n                update_offload_parameter(module, \"weight_g_idx\", g_idx)\n\n            # self._hessians[module] already deleted by quantize_weight\n            del self._num_samples[module]\n\n    @contextlib.contextmanager\n    def _maybe_onload_hessian(self, module: torch.nn.Module):\n        if self.offload_hessians:\n            device = get_execution_device(module)\n            self._hessians[module] = self._hessians[module].to(device=device)\n\n        yield\n\n        if self.offload_hessians:\n            if module in self._hessians:  # may have been deleted in context\n                self._hessians[module] = self._hessians[module].to(device=\"cpu\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/#llmcompressor.modifiers.quantization.gptq.GPTQModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the hessian of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the hessian of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that first argument is the input\n    inp = args[0]\n\n    # Initialize hessian if not present\n    if module not in self._num_samples:\n        init_device = (\n            \"cpu\" if self.offload_hessians else get_execution_device(module)\n        )\n        self._hessians[module] = make_empty_hessian(module, device=init_device)\n        self._num_samples[module] = 0\n\n    # Accumulate hessian with input with optional offloading\n    with self._maybe_onload_hessian(module):\n        self._hessians[module], self._num_samples[module] = accumulate_hessian(\n            inp,\n            module,\n            self._hessians[module],\n            self._num_samples[module],\n        )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/#llmcompressor.modifiers.quantization.gptq.GPTQModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Quantize modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Quantize modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        num_samples = self._num_samples[module]\n        quant_args = getattr_chain(module, \"quantization_scheme.weights\")\n\n        logger.info(f\"Quantizing {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(\n            module\n        ), self._maybe_onload_hessian(module), CompressionLogger(\n            module\n        ) as comp_logger:\n            loss, quantized_weight, scale, zero_point, g_idx = quantize_weight(\n                module=module,\n                quant_args=quant_args,\n                hessians_dict=self._hessians,\n                blocksize=self.block_size,\n                percdamp=self.dampening_frac,\n            )\n            comp_logger.set_loss(loss)\n\n        update_offload_parameter(module, \"weight\", quantized_weight)\n        update_offload_parameter(module, \"weight_scale\", scale)\n        update_offload_parameter(module, \"weight_zero_point\", zero_point)\n        if g_idx is not None:\n            update_offload_parameter(module, \"weight_g_idx\", g_idx)\n\n        # self._hessians[module] already deleted by quantize_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/#llmcompressor.modifiers.quantization.gptq.GPTQModifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>Finish calibrating by removing observers and calibration hooks</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Finish calibrating by removing observers and calibration hooks\n    \"\"\"\n    self.ended_ = True\n    QuantizationMixin.end_calibration(self, state.model)\n    self.remove_hooks()  # remove gptq hooks\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/#llmcompressor.modifiers.quantization.gptq.GPTQModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>disable the quantization observers used by the OBCQ algorithm</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    disable the quantization observers used by the OBCQ algorithm\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    if not self.ended_:\n        self.on_end(state, None)\n\n    if len(self._num_samples) &gt; 0:\n        raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n    self._hessians = dict()\n    self._num_samples = dict()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/#llmcompressor.modifiers.quantization.gptq.GPTQModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run the GPTQ algorithm on the current state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run the GPTQ algorithm on the current state\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    # apply config to model and prepare calibration hooks\n    if QuantizationMixin.has_config(self):\n        QuantizationMixin.initialize_quantization(self, state.model)\n\n    # prepare module names\n    self._module_names = {m: name for name, m in state.model.named_modules()}\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/","title":"llmcompressor.modifiers.quantization.gptq.base","text":""},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/#llmcompressor.modifiers.quantization.gptq.base.GPTQModifier","title":"<code>GPTQModifier</code>","text":"<p>               Bases: <code>Modifier</code>, <code>QuantizationMixin</code></p> <p>Implements the GPTQ algorithm from https://arxiv.org/abs/2210.17323. This modifier uses activations to calibrate a hessian matrix, which is then used to determine optimal quantizion values and orderings for the model weights.</p> <p>| Sample yaml: | test_stage: |    obcq_modifiers: |      GPTQModifier: |          block_size: 128 |          dampening_frac: 0.001 |          offload_hessians: False |          config_groups: |            group_0: |                targets: |                  - \"Linear\" |                input_activations: null |                output_activations: null |                weights: |                    num_bits: 8 |                    type: \"int\" |                    symmetric: true |                    strategy: \"tensor\" |                    group_size: 128 |                    actorder: False</p> <p>Lifecycle:     - on_initialize         - apply config to model     - on_start         - add activation calibration hooks         - add gptq weight calibration hooks     - on_sequential_epoch_end         - quantize_weight     - on_finalize         - remove_hooks()         - model.apply(freeze_module_quantization)</p> <p>Parameters:</p> Name Type Description Default <code>sequential_targets</code> <p>list of layer names to compress during GPTQ, or 'ALL' to compress every layer in the model</p> required <code>block_size</code> <p>Used to determine number of columns to compress in one pass</p> required <code>dampening_frac</code> <p>Amount of dampening to apply to H, as a fraction of the diagonal norm</p> required <code>offload_hessians</code> <p>Set to True for decreased memory usage but increased runtime.</p> required <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>class GPTQModifier(Modifier, QuantizationMixin):\n    \"\"\"\n    Implements the GPTQ algorithm from https://arxiv.org/abs/2210.17323. This modifier\n    uses activations to calibrate a hessian matrix, which is then used to determine\n    optimal quantizion values and orderings for the model weights.\n\n    | Sample yaml:\n    | test_stage:\n    |    obcq_modifiers:\n    |      GPTQModifier:\n    |          block_size: 128\n    |          dampening_frac: 0.001\n    |          offload_hessians: False\n    |          config_groups:\n    |            group_0:\n    |                targets:\n    |                  - \"Linear\"\n    |                input_activations: null\n    |                output_activations: null\n    |                weights:\n    |                    num_bits: 8\n    |                    type: \"int\"\n    |                    symmetric: true\n    |                    strategy: \"tensor\"\n    |                    group_size: 128\n    |                    actorder: False\n\n    Lifecycle:\n        - on_initialize\n            - apply config to model\n        - on_start\n            - add activation calibration hooks\n            - add gptq weight calibration hooks\n        - on_sequential_epoch_end\n            - quantize_weight\n        - on_finalize\n            - remove_hooks()\n            - model.apply(freeze_module_quantization)\n\n    :param sequential_targets: list of layer names to compress during GPTQ, or\n        '__ALL__' to compress every layer in the model\n    :param block_size: Used to determine number of columns to compress in one pass\n    :param dampening_frac: Amount of dampening to apply to H, as a fraction of the\n        diagonal norm\n    :param offload_hessians: Set to True for decreased memory usage but increased\n        runtime.\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    # gptq modifier arguments\n    sequential_update: bool = True  # DEPRECIATED\n    sequential_targets: Union[str, List[str], None] = None\n    block_size: int = 128\n    dampening_frac: Optional[float] = 0.01\n    offload_hessians: bool = False\n\n    # private variables\n    _module_names: Dict[torch.nn.Module, str] = PrivateAttr(default_factory=dict)\n    _hessians: Dict[torch.nn.Module, torch.Tensor] = PrivateAttr(default_factory=dict)\n    _num_samples: Dict[torch.nn.Module, int] = PrivateAttr(default_factory=dict)\n\n    @field_validator(\"sequential_update\", mode=\"before\")\n    def validate_sequential_update(cls, value: bool) -&gt; bool:\n        if not value:\n            warnings.warn(\n                \"`sequential_update=False` is no longer supported, setting \"\n                \"sequential_update=True\",\n                DeprecationWarning,\n            )\n\n        return True\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run the GPTQ algorithm on the current state\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        # apply config to model and prepare calibration hooks\n        if QuantizationMixin.has_config(self):\n            QuantizationMixin.initialize_quantization(self, state.model)\n\n        # prepare module names\n        self._module_names = {m: name for name, m in state.model.named_modules()}\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        self.started_ = True\n\n        # register quantization calibration hooks\n        # assume quantization has been initialized by this modifier or one before it\n        QuantizationMixin.start_calibration(self, state.model)\n        # Unlike qmod, do not quantize as we calibrate\n        # This choice does not seem to have a meaningful impact on accuracy\n        state.model.apply(disable_quantization)\n\n        # register gptq hooks\n        added_hook = False\n        for module in state.model.modules():\n            if getattr_chain(module, \"quantization_scheme.weights\", None) is not None:\n                # HACK: previously, embeddings were not quantized because they were not\n                # accessible by the layer compressor. For now, we manually ignore it,\n                # but in the FUTURE this should be ignored by the user\n                if not isinstance(module, torch.nn.Embedding):\n                    self.register_hook(module, self.calibrate_module, \"forward\")\n                    added_hook = True\n\n        if not added_hook:\n            raise ValueError(\n                \"GPTQModifier requires a weight quantization config be specified by \"\n                \"this modifier or a modifier preceding it\"\n            )\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n            self.compress_modules()\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            self.compress_modules()\n\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Finish calibrating by removing observers and calibration hooks\n        \"\"\"\n        self.ended_ = True\n        QuantizationMixin.end_calibration(self, state.model)\n        self.remove_hooks()  # remove gptq hooks\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        disable the quantization observers used by the OBCQ algorithm\n\n        :param state: session state storing input model and calibration data\n        \"\"\"\n        if not self.ended_:\n            self.on_end(state, None)\n\n        if len(self._num_samples) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n        self._hessians = dict()\n        self._num_samples = dict()\n\n        return True\n\n    def calibrate_module(\n        self,\n        module: torch.nn.Module,\n        args: Tuple[torch.Tensor, ...],\n        _output: torch.Tensor,\n    ):\n        \"\"\"\n        Calibration hook used to accumulate the hessian of the input to the module\n\n        :param module: module being calibrated\n        :param args: inputs to the module, the first element of which is the\n            cannonical input\n        :param _output: uncompressed module output, unused\n        \"\"\"\n        # Assume that first argument is the input\n        inp = args[0]\n\n        # Initialize hessian if not present\n        if module not in self._num_samples:\n            init_device = (\n                \"cpu\" if self.offload_hessians else get_execution_device(module)\n            )\n            self._hessians[module] = make_empty_hessian(module, device=init_device)\n            self._num_samples[module] = 0\n\n        # Accumulate hessian with input with optional offloading\n        with self._maybe_onload_hessian(module):\n            self._hessians[module], self._num_samples[module] = accumulate_hessian(\n                inp,\n                module,\n                self._hessians[module],\n                self._num_samples[module],\n            )\n\n    def compress_modules(self):\n        \"\"\"\n        Quantize modules which have been calibrated\n        \"\"\"\n        for module in list(self._num_samples.keys()):\n            name = self._module_names[module]\n            num_samples = self._num_samples[module]\n            quant_args = getattr_chain(module, \"quantization_scheme.weights\")\n\n            logger.info(f\"Quantizing {name} using {num_samples} samples\")\n            with torch.no_grad(), align_module_device(\n                module\n            ), self._maybe_onload_hessian(module), CompressionLogger(\n                module\n            ) as comp_logger:\n                loss, quantized_weight, scale, zero_point, g_idx = quantize_weight(\n                    module=module,\n                    quant_args=quant_args,\n                    hessians_dict=self._hessians,\n                    blocksize=self.block_size,\n                    percdamp=self.dampening_frac,\n                )\n                comp_logger.set_loss(loss)\n\n            update_offload_parameter(module, \"weight\", quantized_weight)\n            update_offload_parameter(module, \"weight_scale\", scale)\n            update_offload_parameter(module, \"weight_zero_point\", zero_point)\n            if g_idx is not None:\n                update_offload_parameter(module, \"weight_g_idx\", g_idx)\n\n            # self._hessians[module] already deleted by quantize_weight\n            del self._num_samples[module]\n\n    @contextlib.contextmanager\n    def _maybe_onload_hessian(self, module: torch.nn.Module):\n        if self.offload_hessians:\n            device = get_execution_device(module)\n            self._hessians[module] = self._hessians[module].to(device=device)\n\n        yield\n\n        if self.offload_hessians:\n            if module in self._hessians:  # may have been deleted in context\n                self._hessians[module] = self._hessians[module].to(device=\"cpu\")\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/#llmcompressor.modifiers.quantization.gptq.base.GPTQModifier.calibrate_module","title":"<code>calibrate_module(module, args, _output)</code>","text":"<p>Calibration hook used to accumulate the hessian of the input to the module</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module being calibrated</p> required <code>args</code> <code>Tuple[Tensor, ...]</code> <p>inputs to the module, the first element of which is the cannonical input</p> required <code>_output</code> <code>Tensor</code> <p>uncompressed module output, unused</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def calibrate_module(\n    self,\n    module: torch.nn.Module,\n    args: Tuple[torch.Tensor, ...],\n    _output: torch.Tensor,\n):\n    \"\"\"\n    Calibration hook used to accumulate the hessian of the input to the module\n\n    :param module: module being calibrated\n    :param args: inputs to the module, the first element of which is the\n        cannonical input\n    :param _output: uncompressed module output, unused\n    \"\"\"\n    # Assume that first argument is the input\n    inp = args[0]\n\n    # Initialize hessian if not present\n    if module not in self._num_samples:\n        init_device = (\n            \"cpu\" if self.offload_hessians else get_execution_device(module)\n        )\n        self._hessians[module] = make_empty_hessian(module, device=init_device)\n        self._num_samples[module] = 0\n\n    # Accumulate hessian with input with optional offloading\n    with self._maybe_onload_hessian(module):\n        self._hessians[module], self._num_samples[module] = accumulate_hessian(\n            inp,\n            module,\n            self._hessians[module],\n            self._num_samples[module],\n        )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/#llmcompressor.modifiers.quantization.gptq.base.GPTQModifier.compress_modules","title":"<code>compress_modules()</code>","text":"<p>Quantize modules which have been calibrated</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def compress_modules(self):\n    \"\"\"\n    Quantize modules which have been calibrated\n    \"\"\"\n    for module in list(self._num_samples.keys()):\n        name = self._module_names[module]\n        num_samples = self._num_samples[module]\n        quant_args = getattr_chain(module, \"quantization_scheme.weights\")\n\n        logger.info(f\"Quantizing {name} using {num_samples} samples\")\n        with torch.no_grad(), align_module_device(\n            module\n        ), self._maybe_onload_hessian(module), CompressionLogger(\n            module\n        ) as comp_logger:\n            loss, quantized_weight, scale, zero_point, g_idx = quantize_weight(\n                module=module,\n                quant_args=quant_args,\n                hessians_dict=self._hessians,\n                blocksize=self.block_size,\n                percdamp=self.dampening_frac,\n            )\n            comp_logger.set_loss(loss)\n\n        update_offload_parameter(module, \"weight\", quantized_weight)\n        update_offload_parameter(module, \"weight_scale\", scale)\n        update_offload_parameter(module, \"weight_zero_point\", zero_point)\n        if g_idx is not None:\n            update_offload_parameter(module, \"weight_g_idx\", g_idx)\n\n        # self._hessians[module] already deleted by quantize_weight\n        del self._num_samples[module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/#llmcompressor.modifiers.quantization.gptq.base.GPTQModifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>Finish calibrating by removing observers and calibration hooks</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Finish calibrating by removing observers and calibration hooks\n    \"\"\"\n    self.ended_ = True\n    QuantizationMixin.end_calibration(self, state.model)\n    self.remove_hooks()  # remove gptq hooks\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/#llmcompressor.modifiers.quantization.gptq.base.GPTQModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>disable the quantization observers used by the OBCQ algorithm</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    disable the quantization observers used by the OBCQ algorithm\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    if not self.ended_:\n        self.on_end(state, None)\n\n    if len(self._num_samples) &gt; 0:\n        raise ValueError(f\"Failed to compress {len(self._num_samples)} modules\")\n\n    self._hessians = dict()\n    self._num_samples = dict()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/base/#llmcompressor.modifiers.quantization.gptq.base.GPTQModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run the GPTQ algorithm on the current state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>session state storing input model and calibration data</p> required Source code in <code>src/llmcompressor/modifiers/quantization/gptq/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run the GPTQ algorithm on the current state\n\n    :param state: session state storing input model and calibration data\n    \"\"\"\n    # apply config to model and prepare calibration hooks\n    if QuantizationMixin.has_config(self):\n        QuantizationMixin.initialize_quantization(self, state.model)\n\n    # prepare module names\n    self._module_names = {m: name for name, m in state.model.named_modules()}\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/gptq/gptq_quantize/","title":"llmcompressor.modifiers.quantization.gptq.gptq_quantize","text":""},{"location":"reference/llmcompressor/modifiers/quantization/gptq/gptq_quantize/#llmcompressor.modifiers.quantization.gptq.gptq_quantize.quantize_weight","title":"<code>quantize_weight(module, quant_args, hessians_dict, blocksize=128, percdamp=0.01)</code>","text":"<p>Quantize a module weight according to the GPTQ algorithm</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module with weight being quantized</p> required <code>quant_args</code> <code>QuantizationArgs</code> <p>quantization arguments used to find quantization parameters</p> required <code>hessian_dict</code> <p>dictionary containing preaccumulated hessian for quantization</p> required <code>blocksize</code> <code>int</code> <p>chunk size of quantization updates</p> <code>128</code> <code>percdamp</code> <code>float</code> <p>dampening factor on hessian diagonal</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tuple[float, Tensor, Tensor, Union[Tensor, None], Tensor]</code> <p>loss, quantized_weight, scale, zero_point, g_idx</p> Source code in <code>src/llmcompressor/modifiers/quantization/gptq/gptq_quantize.py</code> <pre><code>def quantize_weight(\n    module: torch.nn.Module,\n    quant_args: QuantizationArgs,\n    hessians_dict: Dict[torch.nn.Module, torch.Tensor],\n    blocksize: int = 128,\n    percdamp: float = 0.01,\n) -&gt; Tuple[float, torch.Tensor, torch.Tensor, Union[torch.Tensor, None], torch.Tensor]:\n    \"\"\"\n    Quantize a module weight according to the GPTQ algorithm\n\n    :param module: module with weight being quantized\n    :param quant_args: quantization arguments used to find quantization parameters\n    :param hessian_dict: dictionary containing preaccumulated hessian for quantization\n    :param blocksize: chunk size of quantization updates\n    :param percdamp: dampening factor on hessian diagonal\n    :return: loss, quantized_weight, scale, zero_point, g_idx\n    \"\"\"\n    strategy = quant_args.strategy\n    actorder = quant_args.actorder\n    final_shape = module.weight.shape\n    final_dtype = module.weight.dtype\n    W = module.weight.clone()\n    H = hessians_dict[module]  # unfortunately python does not have a `move` keyword\n    del hessians_dict[module]  # so we have to delete the original reference manually\n\n    # create observer for calculating quantization parameters\n    observer = Observer.load_from_registry(\n        quant_args.observer,\n        quantization_args=quant_args,\n        averaging_constant=1.0,  # ignore moving average\n    )\n\n    # standardize shape and dtype\n    if isinstance(module, torch.nn.Conv2d):\n        W = W.flatten(1)\n    elif isinstance(module, transformers.Conv1D):\n        W.transpose_(0, 1)\n    W = W.to(dtype=GPTQ_PRECISION)\n    num_rows = W.shape[0]\n    num_columns = W.shape[1]\n\n    if strategy == QuantizationStrategy.GROUP:\n        # mapping from column index to group index\n        g_idx = (\n            torch.arange(num_columns, device=W.device, dtype=torch.int)\n            // quant_args.group_size\n        )\n\n        if actorder == ActivationOrdering.GROUP:\n            # permute by activation order first, then update groups\n            W, H, perm = _apply_activation_ordering(W, H)\n            scale, zero_point = observer(W, g_idx=None)\n\n            # use identity g_idx (invert permutation later)\n\n        elif actorder == ActivationOrdering.WEIGHT:\n            # update groups first, then permute by activation order\n            scale, zero_point = observer(W, g_idx=None)\n            W, H, perm = _apply_activation_ordering(W, H)\n\n            # permute g_idx to maintain identity mapping after unpermutation\n            g_idx = g_idx[perm]\n\n        else:\n            scale, zero_point = observer(W, g_idx=None)\n    else:\n        scale, zero_point = observer(W, g_idx=None)\n\n    # sparsity mask\n    sparsity = tensor_sparsity(W)\n    preserve_zeros = sparsity &gt;= SPARSITY_THRESHOLD\n    W_nz_mask = (\n        (~torch.isclose(W, torch.zeros(1, device=W.device).float())).float()\n        if preserve_zeros\n        else None\n    )\n\n    losses = torch.zeros(num_rows, device=module.weight.device)\n\n    # mask dead hessian values\n    dead = torch.diag(H) == 0\n    H[dead, dead] = 1\n    W[:, dead] = 0\n\n    # compute inverse hessian in place to save memory\n    try:\n        damp = percdamp * torch.mean(torch.diag(H))\n        diag = torch.arange(H.shape[0], device=H.device)\n        H[diag, diag] += damp\n        H = torch.linalg.cholesky(H)\n        H = torch.cholesky_inverse(H)\n        H = torch.linalg.cholesky(H, upper=True)\n        Hinv = H\n    except torch._C._LinAlgError:\n        logger.warning(\n            \"Failed to invert hessian due to numerical instability. Consider \"\n            \"increasing GPTQModifier.dampening_frac, increasing the number \"\n            \"of calibration samples, or shuffling the calibration dataset. \"\n            \"Falling back to round-to-nearest for this module.\"\n        )\n        Hinv = H = torch.eye(num_columns, dtype=H.dtype, device=H.device)\n\n    # See section 3.4 of https://arxiv.org/abs/2203.07259\n    for i1 in range(0, num_columns, blocksize):\n        i2 = min(i1 + blocksize, num_columns)\n        count = i2 - i1\n\n        W1 = W[:, i1:i2].clone()\n        Q1 = torch.zeros_like(W1)\n        Err1 = torch.zeros_like(W1)\n        losses1 = torch.zeros_like(W1)\n        Hinv1 = Hinv[i1:i2, i1:i2]\n\n        if preserve_zeros:\n            W1_nz_mask = W_nz_mask[:, i1:i2]\n\n        for i in range(count):\n            w = W1[:, i]\n            d = Hinv1[i, i]\n            q = w.clone()\n\n            # quantize column\n            if strategy == QuantizationStrategy.TENSOR:\n                q = fake_quantize(\n                    q,\n                    scale,\n                    zero_point,\n                    quant_args,\n                )\n            elif strategy == QuantizationStrategy.CHANNEL:\n                q = fake_quantize(\n                    q,\n                    scale[:, 0],\n                    zero_point[:, 0],\n                    quant_args,\n                )\n            elif strategy == QuantizationStrategy.GROUP:\n                # get the group index for the current column\n                column_idx = i1 + i\n                group_index = g_idx[column_idx]\n\n                # Since we're only applying quantization to a slice, this\n                # ends up being a channelwise application\n                altered_qargs = copy(quant_args)\n                altered_qargs.strategy = QuantizationStrategy.CHANNEL\n                q = fake_quantize(\n                    q,\n                    scale[:, group_index],\n                    zero_point[:, group_index],\n                    altered_qargs,\n                )\n            else:\n                raise ValueError(\n                    f\"Quantization strategy is not supported for GPTQ: {strategy}\"\n                )\n\n            # propagate column error\n            Q1[:, i] = q\n            losses1[:, i] = (w - q) ** 2 / d**2\n\n            err1 = (w - q) / d\n            w1_err = err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n            if preserve_zeros:\n                W1[:, i:] -= w1_err * W1_nz_mask[:, i:]\n            else:\n                W1[:, i:] -= w1_err\n            Err1[:, i] = err1\n\n        # propagate block error\n        W[:, i1:i2] = Q1\n        losses += torch.sum(losses1, 1) / 2\n\n        w_err = Err1.matmul(Hinv[i1:i2, i2:])\n        if preserve_zeros:\n            W[:, i2:] -= w_err * W_nz_mask[:, i2:]\n        else:\n            W[:, i2:] -= w_err\n\n    has_gidx = False\n    if strategy == QuantizationStrategy.GROUP:\n        if actorder == ActivationOrdering.WEIGHT:\n            # restore original permutation\n            invperm = torch.argsort(perm)\n            W = W[:, invperm]\n\n        elif actorder == ActivationOrdering.GROUP:\n            # restore original permutation\n            invperm = torch.argsort(perm)\n            W = W[:, invperm]\n            g_idx = g_idx[invperm]\n\n            # only save g_idx if mapping is not identity\n            has_gidx = True\n\n    if not has_gidx:\n        g_idx = None\n\n    if isinstance(module, transformers.Conv1D):\n        W.transpose_(0, 1)\n    W = W.reshape(final_shape).to(final_dtype)\n\n    loss = torch.sum(losses).item()\n    return (\n        loss,\n        W,\n        scale.to(dtype=final_dtype),\n        zero_point.to(dtype=quant_args.pytorch_dtype()),\n        g_idx,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/","title":"llmcompressor.modifiers.quantization.quantization","text":""},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationMixin","title":"<code>QuantizationMixin</code>","text":"<p>               Bases: <code>HooksMixin</code></p> <p>Mixin which enables a Modifier to act as a quantization config, attching observers, calibration hooks, and compression wrappers to modifiers</p> <p>Lifecycle:     - on_initialize: QuantizationMixin.initialize_quantization         - Attach schemes to modules         - Attach observers to modules         - Disable quantization until calibration starts/finishes     - on_start: QuantizationMixin.start_calibration         - Attach calibration hooks         - Apply calibration status         - Enable quantization during calibration     - on_end: QuantizationMixin.end_calibration         - Remove calibration hooks         - Apply freeze status         - Keep quantization enabled for future steps</p> <p>Parameters:</p> Name Type Description Default <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>class QuantizationMixin(HooksMixin):\n    \"\"\"\n    Mixin which enables a Modifier to act as a quantization config, attching observers,\n    calibration hooks, and compression wrappers to modifiers\n\n    Lifecycle:\n        - on_initialize: QuantizationMixin.initialize_quantization\n            - Attach schemes to modules\n            - Attach observers to modules\n            - Disable quantization until calibration starts/finishes\n        - on_start: QuantizationMixin.start_calibration\n            - Attach calibration hooks\n            - Apply calibration status\n            - Enable quantization during calibration\n        - on_end: QuantizationMixin.end_calibration\n            - Remove calibration hooks\n            - Apply freeze status\n            - Keep quantization enabled for future steps\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    config_groups: Optional[Dict[str, QuantizationScheme]] = None\n    targets: Union[str, List[str]] = Field(default_factory=lambda: [\"Linear\"])\n    ignore: List[str] = Field(default_factory=list)\n    scheme: Optional[Union[str, Dict[str, Any]]] = None\n    kv_cache_scheme: Optional[QuantizationArgs] = None\n\n    _calibration_hooks: Set[RemovableHandle] = PrivateAttr(default_factory=set)\n\n    @field_validator(\"targets\", mode=\"before\")\n    def validate_targets(cls, value: Union[str, List[str]]) -&gt; List[str]:\n        if isinstance(value, str):\n            return [value]\n\n        return value\n\n    @field_validator(\"scheme\", mode=\"before\")\n    def validate_scheme(\n        cls, value: Optional[Union[str, Dict[str, Any]]]\n    ) -&gt; Optional[Union[str, Dict[str, Any]]]:\n        if isinstance(value, str) and not is_preset_scheme(value):\n            raise ValueError(\n                \"`scheme` must either be a preset scheme name or a dictionary \"\n                \"of preset scheme names\"\n            )\n\n        if isinstance(value, dict):\n            for scheme_name in value.keys():\n                cls.validate_scheme(scheme_name)\n\n            for key, target in value.items():\n                value[key] = cls.validate_targets(target)\n\n        return value\n\n    def initialize_quantization(self, model: torch.nn.Module):\n        \"\"\"\n        Attach quantization schemes and observers to modules in the model according to\n        the quantization config specified on this modifier\n\n        :param model: model to attach schemes and observers to\n        \"\"\"\n        reset_quantization_status(model)  # reset any previously applied qconfigs\n\n        # apply scheme and status to model\n        config = self.resolve_quantization_config()\n        apply_quantization_config(model, config)\n\n        # apply observers, disable quantization until calibration\n        model.apply(self._initialize_observers)\n        model.apply(disable_quantization)\n\n    def start_calibration(self, model: torch.nn.Module):\n        \"\"\"\n        Register activation calibration hooks (including kv_cache quantization) and\n        enable quantization as we calibrate\n\n        :param model: model to prepare for calibration\n        \"\"\"\n        self._calibration_hooks = self._initialize_hooks(model)\n        model.apply(apply_calibration_status)\n        model.apply(enable_quantization)  # quantize at the same time as calibrate\n\n    def end_calibration(self, model: torch.nn.Module):\n        \"\"\"\n        Remove calibration hooks and set the model status to frozen. Keep quantization\n        enabled for future operations\n\n        :param model: model to end calibration for\n        \"\"\"\n        self.remove_hooks(self._calibration_hooks)\n        model.apply(freeze_module_quantization)  # remove observers\n        model.apply(enable_quantization)  # keep quantization enabled\n\n    def has_config(self) -&gt; bool:\n        \"\"\"\n        Determine if the user has specified a quantization config on this modifier\n        \"\"\"\n        return not (\n            self.config_groups is None\n            and self.targets == [\"Linear\"]\n            and self.ignore == []\n            and self.scheme is None\n            and self.kv_cache_scheme is None\n        )\n\n    def resolve_quantization_config(self) -&gt; QuantizationConfig:\n        \"\"\"\n        Returns the quantization config specified by this modifier\n        \"\"\"\n        scheme = self.scheme\n        targets = self.targets\n        config_groups = self.config_groups\n        kv_cache_scheme = self.kv_cache_scheme\n        ignore = self.ignore\n\n        if scheme is not None and config_groups is not None:\n            raise ValueError(\"Please specify either `scheme` or `config_groups`\")\n\n        if scheme is not None:\n            # takes precedence over config_groups\n\n            if isinstance(scheme, str) and is_preset_scheme(scheme):\n                # attach targets to scheme\n                scheme = {scheme: targets}\n\n            config_groups = {}\n            for idx, key in enumerate(scheme.keys()):\n                if is_preset_scheme(key):\n                    scheme = preset_name_to_scheme(key, scheme[key])\n                else:\n                    scheme = QuantizationScheme.model_validate(\n                        {\"targets\": scheme[key], **scheme}\n                    )\n\n                group_name = f\"group_{idx}\"\n                config_groups[group_name] = scheme\n\n        if config_groups is None or len(config_groups) == 0:\n            default_quant_scheme = QuantizationScheme(targets=targets)\n            config_groups = {\"group_0\": default_quant_scheme}\n\n        return QuantizationConfig(\n            config_groups=config_groups,\n            kv_cache_scheme=kv_cache_scheme,\n            quantization_status=QuantizationStatus.INITIALIZED,\n            ignore=ignore,\n        )\n\n    def _initialize_observers(self, module: torch.nn.Module):\n        if not hasattr(module, \"quantization_scheme\"):\n            return\n\n        scheme: QuantizationScheme = module.quantization_scheme\n        input = scheme.input_activations and not scheme.input_activations.dynamic\n        weight = scheme.weights is not None\n        output = scheme.output_activations and not scheme.output_activations.dynamic\n        is_attention = is_attention_module(module)\n\n        # input activations\n        if input:\n            initialize_observer(module, base_name=\"input\")\n\n        # weight observers (used by `update_weight_zp_scale` or child modifier)\n        if weight:\n            initialize_observer(module, base_name=\"weight\")\n\n        # kv_cache activations. Within `apply_quantization_config`, the config is\n        # modified to use attention output quantization if a kv_cache_scheme exists\n        if is_attention and output:\n            initialize_quantized_kv_cache(module)\n\n        # output activations\n        elif output:\n            initialize_observer(module, base_name=\"output\")\n\n    def _initialize_hooks(self, model: torch.nn.Module) -&gt; Set[RemovableHandle]:\n        hooks = set()\n        for module in model.modules():\n            if not hasattr(module, \"quantization_scheme\"):\n                continue\n\n            scheme: QuantizationScheme = module.quantization_scheme\n            input = scheme.input_activations and not scheme.input_activations.dynamic\n            output = scheme.output_activations and not scheme.output_activations.dynamic\n            is_attention = is_attention_module(module)\n\n            # input activations\n            if input:\n                hooks.add(\n                    self.register_hook(module, calibrate_input_hook, \"forward_pre\")\n                )\n\n            # kv_cache activations. Within `apply_quantization_config`, the config is\n            # modified to use attention output quantization if a kv_cache_scheme exists\n            if is_attention and output:\n                hooks.add(\n                    self.register_hook(\n                        module,\n                        calibrate_kv_cache_input_hook,\n                        \"forward_pre\",\n                        with_kwargs=True,\n                    )\n                )\n                hooks.add(\n                    self.register_hook(\n                        module, calibrate_kv_cache_output_hook, \"forward\"\n                    )\n                )\n\n            # output activations\n            elif output:\n                hooks.add(self.register_hook(module, calibrate_output_hook, \"forward\"))\n\n        return hooks\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationMixin.end_calibration","title":"<code>end_calibration(model)</code>","text":"<p>Remove calibration hooks and set the model status to frozen. Keep quantization enabled for future operations</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to end calibration for</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def end_calibration(self, model: torch.nn.Module):\n    \"\"\"\n    Remove calibration hooks and set the model status to frozen. Keep quantization\n    enabled for future operations\n\n    :param model: model to end calibration for\n    \"\"\"\n    self.remove_hooks(self._calibration_hooks)\n    model.apply(freeze_module_quantization)  # remove observers\n    model.apply(enable_quantization)  # keep quantization enabled\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationMixin.has_config","title":"<code>has_config()</code>","text":"<p>Determine if the user has specified a quantization config on this modifier</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def has_config(self) -&gt; bool:\n    \"\"\"\n    Determine if the user has specified a quantization config on this modifier\n    \"\"\"\n    return not (\n        self.config_groups is None\n        and self.targets == [\"Linear\"]\n        and self.ignore == []\n        and self.scheme is None\n        and self.kv_cache_scheme is None\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationMixin.initialize_quantization","title":"<code>initialize_quantization(model)</code>","text":"<p>Attach quantization schemes and observers to modules in the model according to the quantization config specified on this modifier</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to attach schemes and observers to</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def initialize_quantization(self, model: torch.nn.Module):\n    \"\"\"\n    Attach quantization schemes and observers to modules in the model according to\n    the quantization config specified on this modifier\n\n    :param model: model to attach schemes and observers to\n    \"\"\"\n    reset_quantization_status(model)  # reset any previously applied qconfigs\n\n    # apply scheme and status to model\n    config = self.resolve_quantization_config()\n    apply_quantization_config(model, config)\n\n    # apply observers, disable quantization until calibration\n    model.apply(self._initialize_observers)\n    model.apply(disable_quantization)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationMixin.resolve_quantization_config","title":"<code>resolve_quantization_config()</code>","text":"<p>Returns the quantization config specified by this modifier</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def resolve_quantization_config(self) -&gt; QuantizationConfig:\n    \"\"\"\n    Returns the quantization config specified by this modifier\n    \"\"\"\n    scheme = self.scheme\n    targets = self.targets\n    config_groups = self.config_groups\n    kv_cache_scheme = self.kv_cache_scheme\n    ignore = self.ignore\n\n    if scheme is not None and config_groups is not None:\n        raise ValueError(\"Please specify either `scheme` or `config_groups`\")\n\n    if scheme is not None:\n        # takes precedence over config_groups\n\n        if isinstance(scheme, str) and is_preset_scheme(scheme):\n            # attach targets to scheme\n            scheme = {scheme: targets}\n\n        config_groups = {}\n        for idx, key in enumerate(scheme.keys()):\n            if is_preset_scheme(key):\n                scheme = preset_name_to_scheme(key, scheme[key])\n            else:\n                scheme = QuantizationScheme.model_validate(\n                    {\"targets\": scheme[key], **scheme}\n                )\n\n            group_name = f\"group_{idx}\"\n            config_groups[group_name] = scheme\n\n    if config_groups is None or len(config_groups) == 0:\n        default_quant_scheme = QuantizationScheme(targets=targets)\n        config_groups = {\"group_0\": default_quant_scheme}\n\n    return QuantizationConfig(\n        config_groups=config_groups,\n        kv_cache_scheme=kv_cache_scheme,\n        quantization_status=QuantizationStatus.INITIALIZED,\n        ignore=ignore,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationMixin.start_calibration","title":"<code>start_calibration(model)</code>","text":"<p>Register activation calibration hooks (including kv_cache quantization) and enable quantization as we calibrate</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to prepare for calibration</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def start_calibration(self, model: torch.nn.Module):\n    \"\"\"\n    Register activation calibration hooks (including kv_cache quantization) and\n    enable quantization as we calibrate\n\n    :param model: model to prepare for calibration\n    \"\"\"\n    self._calibration_hooks = self._initialize_hooks(model)\n    model.apply(apply_calibration_status)\n    model.apply(enable_quantization)  # quantize at the same time as calibrate\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationModifier","title":"<code>QuantizationModifier</code>","text":"<p>               Bases: <code>Modifier</code>, <code>QuantizationMixin</code></p> <p>Enables post training quantization (PTQ) and quantization aware training (QAT) for a given module or its submodules. After calibration (PTQ) or the start epoch (QAT), the specified module(s) forward pass will emulate quantized execution and the modifier will be enabled until training is completed.</p> <p>Parameters:</p> Name Type Description Default <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>class QuantizationModifier(Modifier, QuantizationMixin):\n    \"\"\"\n    Enables post training quantization (PTQ) and quantization aware training (QAT) for a\n    given module or its submodules. After calibration (PTQ) or the start epoch (QAT),\n    the specified module(s) forward pass will emulate quantized execution and the\n    modifier will be enabled until training is completed.\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Prepare to calibrate activations and weights\n\n        According to the quantization config, a quantization scheme is attached to each\n        targeted module. The module's forward call is also overwritten to perform\n        quantization to inputs, weights, and outputs.\n\n        Then, according to the module's quantization scheme, observers and calibration\n        hooks are added. These hooks are disabled until the modifier starts.\n        \"\"\"\n        if not QuantizationMixin.has_config(self):\n            raise ValueError(\n                \"QuantizationModifier requires that quantization fields be specified\"\n            )\n        QuantizationMixin.initialize_quantization(self, state.model)\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Begin calibrating activations and weights. Calibrate weights only once on start\n        \"\"\"\n        self.started_ = True\n        QuantizationMixin.start_calibration(self, state.model)\n\n        modules = list(state.model.modules())\n        for module in tqdm.tqdm(modules, desc=\"Calibrating weights\"):\n            update_weight_zp_scale(module)\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Finish calibrating by removing observers and calibration hooks\n        \"\"\"\n        self.ended_ = True\n        QuantizationMixin.end_calibration(\n            self, state.model\n        )  # keep quantization enabled\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        if not self.ended_:\n            self.on_end(state, None)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationModifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>Finish calibrating by removing observers and calibration hooks</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Finish calibrating by removing observers and calibration hooks\n    \"\"\"\n    self.ended_ = True\n    QuantizationMixin.end_calibration(\n        self, state.model\n    )  # keep quantization enabled\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Prepare to calibrate activations and weights</p> <p>According to the quantization config, a quantization scheme is attached to each targeted module. The module's forward call is also overwritten to perform quantization to inputs, weights, and outputs.</p> <p>Then, according to the module's quantization scheme, observers and calibration hooks are added. These hooks are disabled until the modifier starts.</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Prepare to calibrate activations and weights\n\n    According to the quantization config, a quantization scheme is attached to each\n    targeted module. The module's forward call is also overwritten to perform\n    quantization to inputs, weights, and outputs.\n\n    Then, according to the module's quantization scheme, observers and calibration\n    hooks are added. These hooks are disabled until the modifier starts.\n    \"\"\"\n    if not QuantizationMixin.has_config(self):\n        raise ValueError(\n            \"QuantizationModifier requires that quantization fields be specified\"\n        )\n    QuantizationMixin.initialize_quantization(self, state.model)\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/#llmcompressor.modifiers.quantization.quantization.QuantizationModifier.on_start","title":"<code>on_start(state, event, **kwargs)</code>","text":"<p>Begin calibrating activations and weights. Calibrate weights only once on start</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_start(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Begin calibrating activations and weights. Calibrate weights only once on start\n    \"\"\"\n    self.started_ = True\n    QuantizationMixin.start_calibration(self, state.model)\n\n    modules = list(state.model.modules())\n    for module in tqdm.tqdm(modules, desc=\"Calibrating weights\"):\n        update_weight_zp_scale(module)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/base/","title":"llmcompressor.modifiers.quantization.quantization.base","text":""},{"location":"reference/llmcompressor/modifiers/quantization/quantization/base/#llmcompressor.modifiers.quantization.quantization.base.QuantizationModifier","title":"<code>QuantizationModifier</code>","text":"<p>               Bases: <code>Modifier</code>, <code>QuantizationMixin</code></p> <p>Enables post training quantization (PTQ) and quantization aware training (QAT) for a given module or its submodules. After calibration (PTQ) or the start epoch (QAT), the specified module(s) forward pass will emulate quantized execution and the modifier will be enabled until training is completed.</p> <p>Parameters:</p> Name Type Description Default <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>class QuantizationModifier(Modifier, QuantizationMixin):\n    \"\"\"\n    Enables post training quantization (PTQ) and quantization aware training (QAT) for a\n    given module or its submodules. After calibration (PTQ) or the start epoch (QAT),\n    the specified module(s) forward pass will emulate quantized execution and the\n    modifier will be enabled until training is completed.\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Prepare to calibrate activations and weights\n\n        According to the quantization config, a quantization scheme is attached to each\n        targeted module. The module's forward call is also overwritten to perform\n        quantization to inputs, weights, and outputs.\n\n        Then, according to the module's quantization scheme, observers and calibration\n        hooks are added. These hooks are disabled until the modifier starts.\n        \"\"\"\n        if not QuantizationMixin.has_config(self):\n            raise ValueError(\n                \"QuantizationModifier requires that quantization fields be specified\"\n            )\n        QuantizationMixin.initialize_quantization(self, state.model)\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Begin calibrating activations and weights. Calibrate weights only once on start\n        \"\"\"\n        self.started_ = True\n        QuantizationMixin.start_calibration(self, state.model)\n\n        modules = list(state.model.modules())\n        for module in tqdm.tqdm(modules, desc=\"Calibrating weights\"):\n            update_weight_zp_scale(module)\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        \"\"\"\n        Finish calibrating by removing observers and calibration hooks\n        \"\"\"\n        self.ended_ = True\n        QuantizationMixin.end_calibration(\n            self, state.model\n        )  # keep quantization enabled\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        if not self.ended_:\n            self.on_end(state, None)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/base/#llmcompressor.modifiers.quantization.quantization.base.QuantizationModifier.on_end","title":"<code>on_end(state, event, **kwargs)</code>","text":"<p>Finish calibrating by removing observers and calibration hooks</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_end(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Finish calibrating by removing observers and calibration hooks\n    \"\"\"\n    self.ended_ = True\n    QuantizationMixin.end_calibration(\n        self, state.model\n    )  # keep quantization enabled\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/base/#llmcompressor.modifiers.quantization.quantization.base.QuantizationModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Prepare to calibrate activations and weights</p> <p>According to the quantization config, a quantization scheme is attached to each targeted module. The module's forward call is also overwritten to perform quantization to inputs, weights, and outputs.</p> <p>Then, according to the module's quantization scheme, observers and calibration hooks are added. These hooks are disabled until the modifier starts.</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Prepare to calibrate activations and weights\n\n    According to the quantization config, a quantization scheme is attached to each\n    targeted module. The module's forward call is also overwritten to perform\n    quantization to inputs, weights, and outputs.\n\n    Then, according to the module's quantization scheme, observers and calibration\n    hooks are added. These hooks are disabled until the modifier starts.\n    \"\"\"\n    if not QuantizationMixin.has_config(self):\n        raise ValueError(\n            \"QuantizationModifier requires that quantization fields be specified\"\n        )\n    QuantizationMixin.initialize_quantization(self, state.model)\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/base/#llmcompressor.modifiers.quantization.quantization.base.QuantizationModifier.on_start","title":"<code>on_start(state, event, **kwargs)</code>","text":"<p>Begin calibrating activations and weights. Calibrate weights only once on start</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/base.py</code> <pre><code>def on_start(self, state: State, event: Event, **kwargs):\n    \"\"\"\n    Begin calibrating activations and weights. Calibrate weights only once on start\n    \"\"\"\n    self.started_ = True\n    QuantizationMixin.start_calibration(self, state.model)\n\n    modules = list(state.model.modules())\n    for module in tqdm.tqdm(modules, desc=\"Calibrating weights\"):\n        update_weight_zp_scale(module)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/","title":"llmcompressor.modifiers.quantization.quantization.mixin","text":""},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/#llmcompressor.modifiers.quantization.quantization.mixin.QuantizationMixin","title":"<code>QuantizationMixin</code>","text":"<p>               Bases: <code>HooksMixin</code></p> <p>Mixin which enables a Modifier to act as a quantization config, attching observers, calibration hooks, and compression wrappers to modifiers</p> <p>Lifecycle:     - on_initialize: QuantizationMixin.initialize_quantization         - Attach schemes to modules         - Attach observers to modules         - Disable quantization until calibration starts/finishes     - on_start: QuantizationMixin.start_calibration         - Attach calibration hooks         - Apply calibration status         - Enable quantization during calibration     - on_end: QuantizationMixin.end_calibration         - Remove calibration hooks         - Apply freeze status         - Keep quantization enabled for future steps</p> <p>Parameters:</p> Name Type Description Default <code>config_groups</code> <p>dictionary specifying quantization schemes to apply to target modules. Modules not matching a scheme target will NOT be quantized.</p> required <code>targets</code> <p>list of layer names to quantize if a scheme is provided. Defaults to Linear layers</p> required <code>ignore</code> <p>optional list of module class names or submodule names to not quantize even if they match a target in config_groups. Defaults to empty list.</p> required <code>scheme</code> <p>a single quantization scheme to apply to the model. This is a dictionary that supports all keys from QuantizationScheme except targets, which will be set to the targets parameter set at the modifier level. Can also be set to a dictionary of the format <code>preset_scheme_name: targets</code> for example: <code>W8A8: ['Linear']</code> for weight and activation 8-bit.</p> required <code>kv_cache_scheme</code> <p>optional QuantizationArgs, that specify the quantization of the kv cache. If None, kv cache is not quantized. When applying kv cache quantization to transformer AutoModelForCausalLM, the kv_cache_scheme gets converted into a QuantizationScheme that: - targets the <code>q_proj</code> and <code>k_proj</code> modules of the model. The outputs of those modules are the keys and values that might be cached - quantizes the outputs of the aformentioned layers, so that keys and values are compressed before storing them in the cache There is an explicit assumption that the model contains modules with <code>k_proj</code> and <code>v_proj</code> in their names. If this is not the case and kv_cache_scheme != None, the quantization of kv cache will fail</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>class QuantizationMixin(HooksMixin):\n    \"\"\"\n    Mixin which enables a Modifier to act as a quantization config, attching observers,\n    calibration hooks, and compression wrappers to modifiers\n\n    Lifecycle:\n        - on_initialize: QuantizationMixin.initialize_quantization\n            - Attach schemes to modules\n            - Attach observers to modules\n            - Disable quantization until calibration starts/finishes\n        - on_start: QuantizationMixin.start_calibration\n            - Attach calibration hooks\n            - Apply calibration status\n            - Enable quantization during calibration\n        - on_end: QuantizationMixin.end_calibration\n            - Remove calibration hooks\n            - Apply freeze status\n            - Keep quantization enabled for future steps\n\n    :param config_groups: dictionary specifying quantization schemes to apply to target\n        modules. Modules not matching a scheme target will NOT be quantized.\n    :param targets: list of layer names to quantize if a scheme is provided. Defaults\n        to Linear layers\n    :param ignore: optional list of module class names or submodule names to not\n        quantize even if they match a target in config_groups. Defaults to empty list.\n    :param scheme: a single quantization scheme to apply to the model. This is a\n        dictionary that supports all keys from QuantizationScheme except targets, which\n        will be set to the targets parameter set at the modifier level. Can also be set\n        to a dictionary of the format `preset_scheme_name: targets` for example:\n        `W8A8: ['Linear']` for weight and activation 8-bit.\n    :param kv_cache_scheme: optional QuantizationArgs, that specify the\n        quantization of the kv cache. If None, kv cache is not quantized.\n        When applying kv cache quantization to transformer AutoModelForCausalLM,\n        the kv_cache_scheme gets converted into a QuantizationScheme that:\n            - targets the `q_proj` and `k_proj` modules of the model. The outputs\n              of those modules are the keys and values that might be cached\n            - quantizes the outputs of the aformentioned layers, so that\n              keys and values are compressed before storing them in the cache\n        There is an explicit assumption that the model contains modules with\n        `k_proj` and `v_proj` in their names. If this is not the case\n        and kv_cache_scheme != None, the quantization of kv cache will fail\n    \"\"\"\n\n    config_groups: Optional[Dict[str, QuantizationScheme]] = None\n    targets: Union[str, List[str]] = Field(default_factory=lambda: [\"Linear\"])\n    ignore: List[str] = Field(default_factory=list)\n    scheme: Optional[Union[str, Dict[str, Any]]] = None\n    kv_cache_scheme: Optional[QuantizationArgs] = None\n\n    _calibration_hooks: Set[RemovableHandle] = PrivateAttr(default_factory=set)\n\n    @field_validator(\"targets\", mode=\"before\")\n    def validate_targets(cls, value: Union[str, List[str]]) -&gt; List[str]:\n        if isinstance(value, str):\n            return [value]\n\n        return value\n\n    @field_validator(\"scheme\", mode=\"before\")\n    def validate_scheme(\n        cls, value: Optional[Union[str, Dict[str, Any]]]\n    ) -&gt; Optional[Union[str, Dict[str, Any]]]:\n        if isinstance(value, str) and not is_preset_scheme(value):\n            raise ValueError(\n                \"`scheme` must either be a preset scheme name or a dictionary \"\n                \"of preset scheme names\"\n            )\n\n        if isinstance(value, dict):\n            for scheme_name in value.keys():\n                cls.validate_scheme(scheme_name)\n\n            for key, target in value.items():\n                value[key] = cls.validate_targets(target)\n\n        return value\n\n    def initialize_quantization(self, model: torch.nn.Module):\n        \"\"\"\n        Attach quantization schemes and observers to modules in the model according to\n        the quantization config specified on this modifier\n\n        :param model: model to attach schemes and observers to\n        \"\"\"\n        reset_quantization_status(model)  # reset any previously applied qconfigs\n\n        # apply scheme and status to model\n        config = self.resolve_quantization_config()\n        apply_quantization_config(model, config)\n\n        # apply observers, disable quantization until calibration\n        model.apply(self._initialize_observers)\n        model.apply(disable_quantization)\n\n    def start_calibration(self, model: torch.nn.Module):\n        \"\"\"\n        Register activation calibration hooks (including kv_cache quantization) and\n        enable quantization as we calibrate\n\n        :param model: model to prepare for calibration\n        \"\"\"\n        self._calibration_hooks = self._initialize_hooks(model)\n        model.apply(apply_calibration_status)\n        model.apply(enable_quantization)  # quantize at the same time as calibrate\n\n    def end_calibration(self, model: torch.nn.Module):\n        \"\"\"\n        Remove calibration hooks and set the model status to frozen. Keep quantization\n        enabled for future operations\n\n        :param model: model to end calibration for\n        \"\"\"\n        self.remove_hooks(self._calibration_hooks)\n        model.apply(freeze_module_quantization)  # remove observers\n        model.apply(enable_quantization)  # keep quantization enabled\n\n    def has_config(self) -&gt; bool:\n        \"\"\"\n        Determine if the user has specified a quantization config on this modifier\n        \"\"\"\n        return not (\n            self.config_groups is None\n            and self.targets == [\"Linear\"]\n            and self.ignore == []\n            and self.scheme is None\n            and self.kv_cache_scheme is None\n        )\n\n    def resolve_quantization_config(self) -&gt; QuantizationConfig:\n        \"\"\"\n        Returns the quantization config specified by this modifier\n        \"\"\"\n        scheme = self.scheme\n        targets = self.targets\n        config_groups = self.config_groups\n        kv_cache_scheme = self.kv_cache_scheme\n        ignore = self.ignore\n\n        if scheme is not None and config_groups is not None:\n            raise ValueError(\"Please specify either `scheme` or `config_groups`\")\n\n        if scheme is not None:\n            # takes precedence over config_groups\n\n            if isinstance(scheme, str) and is_preset_scheme(scheme):\n                # attach targets to scheme\n                scheme = {scheme: targets}\n\n            config_groups = {}\n            for idx, key in enumerate(scheme.keys()):\n                if is_preset_scheme(key):\n                    scheme = preset_name_to_scheme(key, scheme[key])\n                else:\n                    scheme = QuantizationScheme.model_validate(\n                        {\"targets\": scheme[key], **scheme}\n                    )\n\n                group_name = f\"group_{idx}\"\n                config_groups[group_name] = scheme\n\n        if config_groups is None or len(config_groups) == 0:\n            default_quant_scheme = QuantizationScheme(targets=targets)\n            config_groups = {\"group_0\": default_quant_scheme}\n\n        return QuantizationConfig(\n            config_groups=config_groups,\n            kv_cache_scheme=kv_cache_scheme,\n            quantization_status=QuantizationStatus.INITIALIZED,\n            ignore=ignore,\n        )\n\n    def _initialize_observers(self, module: torch.nn.Module):\n        if not hasattr(module, \"quantization_scheme\"):\n            return\n\n        scheme: QuantizationScheme = module.quantization_scheme\n        input = scheme.input_activations and not scheme.input_activations.dynamic\n        weight = scheme.weights is not None\n        output = scheme.output_activations and not scheme.output_activations.dynamic\n        is_attention = is_attention_module(module)\n\n        # input activations\n        if input:\n            initialize_observer(module, base_name=\"input\")\n\n        # weight observers (used by `update_weight_zp_scale` or child modifier)\n        if weight:\n            initialize_observer(module, base_name=\"weight\")\n\n        # kv_cache activations. Within `apply_quantization_config`, the config is\n        # modified to use attention output quantization if a kv_cache_scheme exists\n        if is_attention and output:\n            initialize_quantized_kv_cache(module)\n\n        # output activations\n        elif output:\n            initialize_observer(module, base_name=\"output\")\n\n    def _initialize_hooks(self, model: torch.nn.Module) -&gt; Set[RemovableHandle]:\n        hooks = set()\n        for module in model.modules():\n            if not hasattr(module, \"quantization_scheme\"):\n                continue\n\n            scheme: QuantizationScheme = module.quantization_scheme\n            input = scheme.input_activations and not scheme.input_activations.dynamic\n            output = scheme.output_activations and not scheme.output_activations.dynamic\n            is_attention = is_attention_module(module)\n\n            # input activations\n            if input:\n                hooks.add(\n                    self.register_hook(module, calibrate_input_hook, \"forward_pre\")\n                )\n\n            # kv_cache activations. Within `apply_quantization_config`, the config is\n            # modified to use attention output quantization if a kv_cache_scheme exists\n            if is_attention and output:\n                hooks.add(\n                    self.register_hook(\n                        module,\n                        calibrate_kv_cache_input_hook,\n                        \"forward_pre\",\n                        with_kwargs=True,\n                    )\n                )\n                hooks.add(\n                    self.register_hook(\n                        module, calibrate_kv_cache_output_hook, \"forward\"\n                    )\n                )\n\n            # output activations\n            elif output:\n                hooks.add(self.register_hook(module, calibrate_output_hook, \"forward\"))\n\n        return hooks\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/#llmcompressor.modifiers.quantization.quantization.mixin.QuantizationMixin.end_calibration","title":"<code>end_calibration(model)</code>","text":"<p>Remove calibration hooks and set the model status to frozen. Keep quantization enabled for future operations</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to end calibration for</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def end_calibration(self, model: torch.nn.Module):\n    \"\"\"\n    Remove calibration hooks and set the model status to frozen. Keep quantization\n    enabled for future operations\n\n    :param model: model to end calibration for\n    \"\"\"\n    self.remove_hooks(self._calibration_hooks)\n    model.apply(freeze_module_quantization)  # remove observers\n    model.apply(enable_quantization)  # keep quantization enabled\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/#llmcompressor.modifiers.quantization.quantization.mixin.QuantizationMixin.has_config","title":"<code>has_config()</code>","text":"<p>Determine if the user has specified a quantization config on this modifier</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def has_config(self) -&gt; bool:\n    \"\"\"\n    Determine if the user has specified a quantization config on this modifier\n    \"\"\"\n    return not (\n        self.config_groups is None\n        and self.targets == [\"Linear\"]\n        and self.ignore == []\n        and self.scheme is None\n        and self.kv_cache_scheme is None\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/#llmcompressor.modifiers.quantization.quantization.mixin.QuantizationMixin.initialize_quantization","title":"<code>initialize_quantization(model)</code>","text":"<p>Attach quantization schemes and observers to modules in the model according to the quantization config specified on this modifier</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to attach schemes and observers to</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def initialize_quantization(self, model: torch.nn.Module):\n    \"\"\"\n    Attach quantization schemes and observers to modules in the model according to\n    the quantization config specified on this modifier\n\n    :param model: model to attach schemes and observers to\n    \"\"\"\n    reset_quantization_status(model)  # reset any previously applied qconfigs\n\n    # apply scheme and status to model\n    config = self.resolve_quantization_config()\n    apply_quantization_config(model, config)\n\n    # apply observers, disable quantization until calibration\n    model.apply(self._initialize_observers)\n    model.apply(disable_quantization)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/#llmcompressor.modifiers.quantization.quantization.mixin.QuantizationMixin.resolve_quantization_config","title":"<code>resolve_quantization_config()</code>","text":"<p>Returns the quantization config specified by this modifier</p> Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def resolve_quantization_config(self) -&gt; QuantizationConfig:\n    \"\"\"\n    Returns the quantization config specified by this modifier\n    \"\"\"\n    scheme = self.scheme\n    targets = self.targets\n    config_groups = self.config_groups\n    kv_cache_scheme = self.kv_cache_scheme\n    ignore = self.ignore\n\n    if scheme is not None and config_groups is not None:\n        raise ValueError(\"Please specify either `scheme` or `config_groups`\")\n\n    if scheme is not None:\n        # takes precedence over config_groups\n\n        if isinstance(scheme, str) and is_preset_scheme(scheme):\n            # attach targets to scheme\n            scheme = {scheme: targets}\n\n        config_groups = {}\n        for idx, key in enumerate(scheme.keys()):\n            if is_preset_scheme(key):\n                scheme = preset_name_to_scheme(key, scheme[key])\n            else:\n                scheme = QuantizationScheme.model_validate(\n                    {\"targets\": scheme[key], **scheme}\n                )\n\n            group_name = f\"group_{idx}\"\n            config_groups[group_name] = scheme\n\n    if config_groups is None or len(config_groups) == 0:\n        default_quant_scheme = QuantizationScheme(targets=targets)\n        config_groups = {\"group_0\": default_quant_scheme}\n\n    return QuantizationConfig(\n        config_groups=config_groups,\n        kv_cache_scheme=kv_cache_scheme,\n        quantization_status=QuantizationStatus.INITIALIZED,\n        ignore=ignore,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/quantization/quantization/mixin/#llmcompressor.modifiers.quantization.quantization.mixin.QuantizationMixin.start_calibration","title":"<code>start_calibration(model)</code>","text":"<p>Register activation calibration hooks (including kv_cache quantization) and enable quantization as we calibrate</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to prepare for calibration</p> required Source code in <code>src/llmcompressor/modifiers/quantization/quantization/mixin.py</code> <pre><code>def start_calibration(self, model: torch.nn.Module):\n    \"\"\"\n    Register activation calibration hooks (including kv_cache quantization) and\n    enable quantization as we calibrate\n\n    :param model: model to prepare for calibration\n    \"\"\"\n    self._calibration_hooks = self._initialize_hooks(model)\n    model.apply(apply_calibration_status)\n    model.apply(enable_quantization)  # quantize at the same time as calibrate\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/","title":"llmcompressor.modifiers.smoothquant","text":""},{"location":"reference/llmcompressor/modifiers/smoothquant/#llmcompressor.modifiers.smoothquant.SmoothQuantMapping","title":"<code>SmoothQuantMapping</code>  <code>dataclass</code>","text":"<p>Dataclass for storing the mapping between an activation layer and the following weights that must be balanced during smoothing</p> <p>Parameters:</p> Name Type Description Default <code>smooth_name</code> <code>str</code> <p>name of the activation layer</p> required <code>smooth_layer</code> <code>Module</code> <p>PyTorch module storing the activation layer</p> required <code>balance_layers</code> <code>List[Module]</code> <p>list of PyTorch modules that smooth_layer feeds into, must be balanced to offset the smoothing of smooth_layer</p> required Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>@dataclass\nclass SmoothQuantMapping:\n    \"\"\"\n    Dataclass for storing the mapping between an activation layer and the following\n    weights that must be balanced during smoothing\n\n    :param smooth_name: name of the activation layer\n    :param smooth_layer: PyTorch module storing the activation layer\n    :param balance_layers: list of PyTorch modules that smooth_layer feeds into, must be\n    balanced to offset the smoothing of smooth_layer\n    \"\"\"\n\n    smooth_name: str\n    smooth_layer: Module\n    balance_layers: List[Module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/#llmcompressor.modifiers.smoothquant.SmoothQuantModifier","title":"<code>SmoothQuantModifier</code>","text":"<p>               Bases: <code>Modifier</code></p> <p>Implements the SmoothQuant algorithm from https://arxiv.org/abs/2211.10438. This  modifier performs a channel-wise smoothing of outliers in activations, making them  easier to quantize by reducing the dynamic range. The smoothing is offset by  applying the inverse operation to the next layer of weights, making the weights  slightly more difficult to quantize.</p> <p>Because this modifier manipulates the weights of the model, it can only be used in  in one-shot and not during training. Activation ranges are determined by running a  small set of calibration data through the model.</p> <p>example recipe:  <pre><code>SmoothQuantModifier:\n  smoothing_strength: 0.5\n  mappings: [\n    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n    [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n  ]\n  ignore: [\"model.decoder.final_layer_norm\"]\n</code></pre></p> <p>:param smoothing_strength: alpha, intensity of smoothing to perform (0-1 range)  :param mappings: list activation layers to smooth, and which layers to     scale the output such that activations are smoothed.     Each entry of the mapping list should be a list itself, in which the first     entry is a list of layers who share the same input activation (the one to be     to smoothed) and the second entry is the layer whose output is scaled to     achieve the smoothing. If regex is used, it matches layers with the largest     overlap in module name.  If not supplied the argument will be inferred from the     model architecture.  :param ignore: list of layers to ignore, even if they match a regex in mappings.     It should match the name of layers whose outputs are scaled to achieve     smoothing (the second entry of the mappings list).  :param num_calibration_steps: number of samples to use for calibration, or None to  use the whole dataset</p> <p>Parameters:</p> Name Type Description Default <code>calibration_function</code> <p>optional function to use for the forward pass, or None to use the default tensor_module_forward</p> required Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>class SmoothQuantModifier(Modifier):\n    \"\"\"\n     Implements the SmoothQuant algorithm from https://arxiv.org/abs/2211.10438. This\n     modifier performs a channel-wise smoothing of outliers in activations, making them\n     easier to quantize by reducing the dynamic range. The smoothing is offset by\n     applying the inverse operation to the next layer of weights, making the weights\n     slightly more difficult to quantize.\n\n     Because this modifier manipulates the weights of the model, it can only be used in\n     in one-shot and not during training. Activation ranges are determined by running a\n     small set of calibration data through the model.\n\n    example recipe:\n     ```yaml\n     SmoothQuantModifier:\n       smoothing_strength: 0.5\n       mappings: [\n         [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n         [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n       ]\n       ignore: [\"model.decoder.final_layer_norm\"]\n     ```\n\n     :param smoothing_strength: alpha, intensity of smoothing to perform (0-1 range)\n     :param mappings: list activation layers to smooth, and which layers to\n        scale the output such that activations are smoothed.\n        Each entry of the mapping list should be a list itself, in which the first\n        entry is a list of layers who share the same input activation (the one to be\n        to smoothed) and the second entry is the layer whose output is scaled to\n        achieve the smoothing. If regex is used, it matches layers with the largest\n        overlap in module name.  If not supplied the argument will be inferred from the\n        model architecture.\n     :param ignore: list of layers to ignore, even if they match a regex in mappings.\n        It should match the name of layers whose outputs are scaled to achieve\n        smoothing (the second entry of the mappings list).\n     :param num_calibration_steps: number of samples to use for calibration, or None to\n     use the whole dataset\n    :param calibration_function: optional function to use for the forward pass, or None\n    to use the default tensor_module_forward\n    \"\"\"\n\n    smoothing_strength: float = 0.5\n    mappings: Optional[List[Union[Tuple, List]]] = None\n    ignore: Optional[List[str]] = None\n    num_calibration_steps: Optional[int] = None\n    calibration_function: Optional[Callable] = None\n\n    resolved_mappings_: Optional[List[SmoothQuantMapping]] = Field(\n        default=None, repr=False\n    )\n    scales_: Optional[Dict] = Field(default=None, repr=False)\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run SmoothQuant on the given state\n\n        :param state: state to run SmoothQuant on\n        :return: True on a successful run, False otherwise\n        \"\"\"\n        if self.end and self.end != -1:\n            raise ValueError(\n                f\"{self.__class__.__name__} can only be applied during one-shot. \"\n                f\" Expected end to be None or -1, got {self.end}\"\n            )\n        if self.start and self.start != -1:\n            raise ValueError(\n                f\"{self.__class__.__name__} can only be applied during one-shot. \"\n                f\"Expected start to be None or -1, got {self.end}\"\n            )\n\n        self.ignore = [] if not self.ignore else self.ignore\n        self.mappings = self._infer_mappings_from_model(state.model)\n        self.resolved_mappings_ = self._resolve_mappings(state.model)\n        self.scales_ = {}\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        self.started_ = True\n        self._setup_scale_hooks()\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n            self._apply_smoothing(state.model)\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            self._apply_smoothing(state.model)\n\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        self.ended_ = True\n        self.remove_hooks()  # remove hooks\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Clean up by clearing the scale and mapping data\n        \"\"\"\n        if not self.ended_:\n            self.on_end(state, None)\n\n        if len(self.scales_) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self.scales_)} modules\")\n\n        if self.scales_ is not None:\n            self.scales_.clear()\n        if self.resolved_mappings_ is not None:\n            self.resolved_mappings_.clear()\n\n        return True\n\n    def _infer_mappings_from_model(\n        self,\n        model: Module,\n    ) -&gt; List[Tuple]:\n        if self.mappings is not None:\n            return self.mappings\n\n        logger.info(\"No SmoothQuantModifier.mappings provided, inferring from model...\")\n        return get_layer_mappings_from_architecture(\n            architecture=model.__class__.__name__\n        )\n\n    @handle_mapping_resolution_errors\n    def _resolve_mappings(self, model: Module) -&gt; List[SmoothQuantMapping]:\n        \"\"\"\n        Transforms the list of activations to smooth and their corresponding weights\n        into SmoothQuantMapping objects, resolving regular expressions.\n\n        For each activation in the mapping list, we find the corresponding weight to\n        balance by searching for the longest substring. For instance, if our balance\n        weight is \".*re:.*q_proj\" and the activation is \"re:.*self_attn_layer_norm\" we\n        would match model.layer.0.p_proj to model.layer.0.self_attn_layer_norm and\n        repeat for model.layer.1 and so on\n        \"\"\"\n        resolved_mappings = []\n        for to_balance, to_smooth in self.mappings:\n            to_smooth_layers = get_layers(to_smooth, model)\n            for layer_name, smooth_layer in to_smooth_layers.items():\n                if not match_targets(layer_name, self.ignore)[0]:\n                    balance_layers = []\n                    for balance_suffix in to_balance:\n                        # find the submodule that matches the activation layer\n                        _, balance_layer = get_matching_layer(\n                            balance_suffix, layer_name, model\n                        )\n                        if balance_layer:\n                            balance_layers.append(balance_layer)\n                    # each mapping can contain multiple layers to balance, but only\n                    # one layer to smooth\n                    mapping = SmoothQuantMapping(\n                        layer_name, smooth_layer, balance_layers\n                    )\n                    resolved_mappings.append(mapping)\n        return resolved_mappings\n\n    def _setup_scale_hooks(self):\n        \"\"\"\n        Attach a forward hook to each activation we want to smooth. This allows us to\n        calculate the dynamic range during calibration\n        \"\"\"\n\n        def create_hook_fn(layer_name):\n            def hook_fn(module, inp, out):\n                # update the per-channel min/max output values seen during calibration\n                if isinstance(out, tuple):\n                    out = out[0]\n\n                hidden_dim = out.shape[-1]\n                out = out.view(-1, hidden_dim)\n                latest_mins = torch.min(out, dim=0)[0]\n                latest_maxes = torch.max(out, dim=0)[0]\n\n                if layer_name in self.scales_:\n                    self.scales_[layer_name].min_channel_vals = torch.minimum(\n                        self.scales_[layer_name].min_channel_vals, latest_mins\n                    )\n                    self.scales_[layer_name].max_channel_vals = torch.maximum(\n                        self.scales_[layer_name].max_channel_vals, latest_maxes\n                    )\n                else:\n                    self.scales_[layer_name] = SmoothQuantScale(\n                        min_channel_vals=latest_mins, max_channel_vals=latest_maxes\n                    )\n\n            return hook_fn\n\n        for mapping in self.resolved_mappings_:\n            name = mapping.smooth_name\n            layer = mapping.smooth_layer\n            self.register_hook(layer, create_hook_fn(name), \"forward\")\n\n    @torch.no_grad()\n    def _apply_smoothing(self, model: Module):\n        \"\"\"\n        After calibration, apply smoothing to the activations and push the transform\n        into the following weights by applying the inverse to each balance weight.\n\n        Y = (Xdiag(scales)^(-1) * diag(scales)W) where W is the to_balance weights and\n        X is the to_smooth weights\n\n        This modifies the weights of the model in-place.\n        \"\"\"\n        for mapping in self.resolved_mappings_:\n            if mapping.smooth_name not in self.scales_:\n                continue\n            logger.info(f\"Smoothing with {mapping.smooth_name}\")\n\n            activation_scales = (  # get dynamic range for each activation channel\n                self.scales_[mapping.smooth_name].max_channel_vals\n                - self.scales_[mapping.smooth_name].min_channel_vals\n            )\n            smooth_layer = mapping.smooth_layer\n            balance_layers = mapping.balance_layers\n\n            scales = self._calculate_smoothing_scales(balance_layers, activation_scales)\n            scales = torch.maximum(\n                scales, torch.Tensor([MINIMUM_SMOOTHING_SCALE]).to(scales.device)\n            )\n\n            @torch.no_grad()\n            def smooth(module):\n                with align_module_device(module):\n                    if module in balance_layers:\n                        module.weight.mul_(scales.view(1, -1))\n                    elif module == smooth_layer:\n                        if module.weight.ndim == 1:\n                            module.weight.div_(scales)\n                        else:\n                            module.weight.div_(scales.view(-1, 1))\n                        if hasattr(module, \"bias\") and module.bias is not None:\n                            module.bias.div_(scales)\n\n            parent = get_fsdp_parent(mapping.smooth_name, model)\n            if parent is not None:\n                parent.apply(smooth)\n            else:\n                # if we're not running with FSDP we can apply smoothing directly\n                for layer in balance_layers:\n                    smooth(layer)\n                smooth(smooth_layer)\n\n            # clear calibration data\n            del self.scales_[mapping.smooth_name]\n\n    def _calculate_smoothing_scales(\n        self, balance_layers: List[Module], activation_scales: torch.Tensor\n    ) -&gt; List[float]:\n        \"\"\"\n        Calculate how much smoothing to apply to each channel based on the dynamic\n        range of the activation and the following weights\n\n        :param balance_layers: layers to offset activation smoothing to\n        :param activation_scales: channel-wise dynamic range of activations to smooth\n        :return: channel-wise scales to use for smoothing activations\n        \"\"\"\n        # get the channel-wise dynamic range for each layer to be balanced\n        weight_scales = []\n        for layer in balance_layers:\n            with align_module_device(layer):\n                scale = layer.weight.abs().max(dim=0, keepdim=True)[0]\n                weight_scales.append(scale)\n\n        weight_scales = 2.0 * torch.cat(weight_scales, dim=0).max(dim=0)[0]\n\n        # calculate the amount of smoothing to apply\n        # s_j = max(|X_j|)^alpha / max(|W_j|)^(1-alpha)\n        # where j is the input channel, alpha is smoothing strength\n        scales = activation_scales.pow(self.smoothing_strength) / weight_scales.pow(\n            1 - self.smoothing_strength\n        )\n        scales = torch.where(weight_scales &gt; 0.0, scales, activation_scales)\n\n        return scales\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/#llmcompressor.modifiers.smoothquant.SmoothQuantModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>Clean up by clearing the scale and mapping data</p> Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Clean up by clearing the scale and mapping data\n    \"\"\"\n    if not self.ended_:\n        self.on_end(state, None)\n\n    if len(self.scales_) &gt; 0:\n        raise ValueError(f\"Failed to compress {len(self.scales_)} modules\")\n\n    if self.scales_ is not None:\n        self.scales_.clear()\n    if self.resolved_mappings_ is not None:\n        self.resolved_mappings_.clear()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/#llmcompressor.modifiers.smoothquant.SmoothQuantModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run SmoothQuant on the given state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>state to run SmoothQuant on</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True on a successful run, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run SmoothQuant on the given state\n\n    :param state: state to run SmoothQuant on\n    :return: True on a successful run, False otherwise\n    \"\"\"\n    if self.end and self.end != -1:\n        raise ValueError(\n            f\"{self.__class__.__name__} can only be applied during one-shot. \"\n            f\" Expected end to be None or -1, got {self.end}\"\n        )\n    if self.start and self.start != -1:\n        raise ValueError(\n            f\"{self.__class__.__name__} can only be applied during one-shot. \"\n            f\"Expected start to be None or -1, got {self.end}\"\n        )\n\n    self.ignore = [] if not self.ignore else self.ignore\n    self.mappings = self._infer_mappings_from_model(state.model)\n    self.resolved_mappings_ = self._resolve_mappings(state.model)\n    self.scales_ = {}\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/#llmcompressor.modifiers.smoothquant.SmoothQuantScale","title":"<code>SmoothQuantScale</code>  <code>dataclass</code>","text":"<p>Dataclass for storing the channel-wise minimum and maximum values for a layer. This is updated each forward pass during calibration</p> <p>Parameters:</p> Name Type Description Default <code>min_channel_vals</code> <code>Tensor</code> <p>minimum output value seen so far, per channel</p> required <code>max_channel_vals</code> <code>Tensor</code> <p>maximum output value seen so far, per channel</p> required Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>@dataclass\nclass SmoothQuantScale:\n    \"\"\"\n    Dataclass for storing the channel-wise minimum and maximum values for a layer. This\n    is updated each forward pass during calibration\n\n    :param min_channel_vals: minimum output value seen so far, per channel\n    :param max_channel_vals: maximum output value seen so far, per channel\n    \"\"\"\n\n    min_channel_vals: torch.Tensor\n    max_channel_vals: torch.Tensor\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/base/","title":"llmcompressor.modifiers.smoothquant.base","text":""},{"location":"reference/llmcompressor/modifiers/smoothquant/base/#llmcompressor.modifiers.smoothquant.base.SmoothQuantMapping","title":"<code>SmoothQuantMapping</code>  <code>dataclass</code>","text":"<p>Dataclass for storing the mapping between an activation layer and the following weights that must be balanced during smoothing</p> <p>Parameters:</p> Name Type Description Default <code>smooth_name</code> <code>str</code> <p>name of the activation layer</p> required <code>smooth_layer</code> <code>Module</code> <p>PyTorch module storing the activation layer</p> required <code>balance_layers</code> <code>List[Module]</code> <p>list of PyTorch modules that smooth_layer feeds into, must be balanced to offset the smoothing of smooth_layer</p> required Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>@dataclass\nclass SmoothQuantMapping:\n    \"\"\"\n    Dataclass for storing the mapping between an activation layer and the following\n    weights that must be balanced during smoothing\n\n    :param smooth_name: name of the activation layer\n    :param smooth_layer: PyTorch module storing the activation layer\n    :param balance_layers: list of PyTorch modules that smooth_layer feeds into, must be\n    balanced to offset the smoothing of smooth_layer\n    \"\"\"\n\n    smooth_name: str\n    smooth_layer: Module\n    balance_layers: List[Module]\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/base/#llmcompressor.modifiers.smoothquant.base.SmoothQuantModifier","title":"<code>SmoothQuantModifier</code>","text":"<p>               Bases: <code>Modifier</code></p> <p>Implements the SmoothQuant algorithm from https://arxiv.org/abs/2211.10438. This  modifier performs a channel-wise smoothing of outliers in activations, making them  easier to quantize by reducing the dynamic range. The smoothing is offset by  applying the inverse operation to the next layer of weights, making the weights  slightly more difficult to quantize.</p> <p>Because this modifier manipulates the weights of the model, it can only be used in  in one-shot and not during training. Activation ranges are determined by running a  small set of calibration data through the model.</p> <p>example recipe:  <pre><code>SmoothQuantModifier:\n  smoothing_strength: 0.5\n  mappings: [\n    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n    [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n  ]\n  ignore: [\"model.decoder.final_layer_norm\"]\n</code></pre></p> <p>:param smoothing_strength: alpha, intensity of smoothing to perform (0-1 range)  :param mappings: list activation layers to smooth, and which layers to     scale the output such that activations are smoothed.     Each entry of the mapping list should be a list itself, in which the first     entry is a list of layers who share the same input activation (the one to be     to smoothed) and the second entry is the layer whose output is scaled to     achieve the smoothing. If regex is used, it matches layers with the largest     overlap in module name.  If not supplied the argument will be inferred from the     model architecture.  :param ignore: list of layers to ignore, even if they match a regex in mappings.     It should match the name of layers whose outputs are scaled to achieve     smoothing (the second entry of the mappings list).  :param num_calibration_steps: number of samples to use for calibration, or None to  use the whole dataset</p> <p>Parameters:</p> Name Type Description Default <code>calibration_function</code> <p>optional function to use for the forward pass, or None to use the default tensor_module_forward</p> required Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>class SmoothQuantModifier(Modifier):\n    \"\"\"\n     Implements the SmoothQuant algorithm from https://arxiv.org/abs/2211.10438. This\n     modifier performs a channel-wise smoothing of outliers in activations, making them\n     easier to quantize by reducing the dynamic range. The smoothing is offset by\n     applying the inverse operation to the next layer of weights, making the weights\n     slightly more difficult to quantize.\n\n     Because this modifier manipulates the weights of the model, it can only be used in\n     in one-shot and not during training. Activation ranges are determined by running a\n     small set of calibration data through the model.\n\n    example recipe:\n     ```yaml\n     SmoothQuantModifier:\n       smoothing_strength: 0.5\n       mappings: [\n         [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*self_attn_layer_norm\"],\n         [[\"re:.*fc1\"], \"re:.*final_layer_norm\"]\n       ]\n       ignore: [\"model.decoder.final_layer_norm\"]\n     ```\n\n     :param smoothing_strength: alpha, intensity of smoothing to perform (0-1 range)\n     :param mappings: list activation layers to smooth, and which layers to\n        scale the output such that activations are smoothed.\n        Each entry of the mapping list should be a list itself, in which the first\n        entry is a list of layers who share the same input activation (the one to be\n        to smoothed) and the second entry is the layer whose output is scaled to\n        achieve the smoothing. If regex is used, it matches layers with the largest\n        overlap in module name.  If not supplied the argument will be inferred from the\n        model architecture.\n     :param ignore: list of layers to ignore, even if they match a regex in mappings.\n        It should match the name of layers whose outputs are scaled to achieve\n        smoothing (the second entry of the mappings list).\n     :param num_calibration_steps: number of samples to use for calibration, or None to\n     use the whole dataset\n    :param calibration_function: optional function to use for the forward pass, or None\n    to use the default tensor_module_forward\n    \"\"\"\n\n    smoothing_strength: float = 0.5\n    mappings: Optional[List[Union[Tuple, List]]] = None\n    ignore: Optional[List[str]] = None\n    num_calibration_steps: Optional[int] = None\n    calibration_function: Optional[Callable] = None\n\n    resolved_mappings_: Optional[List[SmoothQuantMapping]] = Field(\n        default=None, repr=False\n    )\n    scales_: Optional[Dict] = Field(default=None, repr=False)\n\n    def on_initialize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Initialize and run SmoothQuant on the given state\n\n        :param state: state to run SmoothQuant on\n        :return: True on a successful run, False otherwise\n        \"\"\"\n        if self.end and self.end != -1:\n            raise ValueError(\n                f\"{self.__class__.__name__} can only be applied during one-shot. \"\n                f\" Expected end to be None or -1, got {self.end}\"\n            )\n        if self.start and self.start != -1:\n            raise ValueError(\n                f\"{self.__class__.__name__} can only be applied during one-shot. \"\n                f\"Expected start to be None or -1, got {self.end}\"\n            )\n\n        self.ignore = [] if not self.ignore else self.ignore\n        self.mappings = self._infer_mappings_from_model(state.model)\n        self.resolved_mappings_ = self._resolve_mappings(state.model)\n        self.scales_ = {}\n\n        return True\n\n    def on_start(self, state: State, event: Event, **kwargs):\n        self.started_ = True\n        self._setup_scale_hooks()\n\n    def on_event(self, state: State, event: Event, **kwargs):\n        if event.type_ == EventType.CALIBRATION_EPOCH_START:\n            if not self.started_:\n                self.on_start(state, None)\n\n        if event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n            self._apply_smoothing(state.model)\n\n        if event.type_ == EventType.CALIBRATION_EPOCH_END:\n            self._apply_smoothing(state.model)\n\n            if not self.ended_:\n                self.on_end(state, None)\n\n    def on_end(self, state: State, event: Event, **kwargs):\n        self.ended_ = True\n        self.remove_hooks()  # remove hooks\n\n    def on_finalize(self, state: State, **kwargs) -&gt; bool:\n        \"\"\"\n        Clean up by clearing the scale and mapping data\n        \"\"\"\n        if not self.ended_:\n            self.on_end(state, None)\n\n        if len(self.scales_) &gt; 0:\n            raise ValueError(f\"Failed to compress {len(self.scales_)} modules\")\n\n        if self.scales_ is not None:\n            self.scales_.clear()\n        if self.resolved_mappings_ is not None:\n            self.resolved_mappings_.clear()\n\n        return True\n\n    def _infer_mappings_from_model(\n        self,\n        model: Module,\n    ) -&gt; List[Tuple]:\n        if self.mappings is not None:\n            return self.mappings\n\n        logger.info(\"No SmoothQuantModifier.mappings provided, inferring from model...\")\n        return get_layer_mappings_from_architecture(\n            architecture=model.__class__.__name__\n        )\n\n    @handle_mapping_resolution_errors\n    def _resolve_mappings(self, model: Module) -&gt; List[SmoothQuantMapping]:\n        \"\"\"\n        Transforms the list of activations to smooth and their corresponding weights\n        into SmoothQuantMapping objects, resolving regular expressions.\n\n        For each activation in the mapping list, we find the corresponding weight to\n        balance by searching for the longest substring. For instance, if our balance\n        weight is \".*re:.*q_proj\" and the activation is \"re:.*self_attn_layer_norm\" we\n        would match model.layer.0.p_proj to model.layer.0.self_attn_layer_norm and\n        repeat for model.layer.1 and so on\n        \"\"\"\n        resolved_mappings = []\n        for to_balance, to_smooth in self.mappings:\n            to_smooth_layers = get_layers(to_smooth, model)\n            for layer_name, smooth_layer in to_smooth_layers.items():\n                if not match_targets(layer_name, self.ignore)[0]:\n                    balance_layers = []\n                    for balance_suffix in to_balance:\n                        # find the submodule that matches the activation layer\n                        _, balance_layer = get_matching_layer(\n                            balance_suffix, layer_name, model\n                        )\n                        if balance_layer:\n                            balance_layers.append(balance_layer)\n                    # each mapping can contain multiple layers to balance, but only\n                    # one layer to smooth\n                    mapping = SmoothQuantMapping(\n                        layer_name, smooth_layer, balance_layers\n                    )\n                    resolved_mappings.append(mapping)\n        return resolved_mappings\n\n    def _setup_scale_hooks(self):\n        \"\"\"\n        Attach a forward hook to each activation we want to smooth. This allows us to\n        calculate the dynamic range during calibration\n        \"\"\"\n\n        def create_hook_fn(layer_name):\n            def hook_fn(module, inp, out):\n                # update the per-channel min/max output values seen during calibration\n                if isinstance(out, tuple):\n                    out = out[0]\n\n                hidden_dim = out.shape[-1]\n                out = out.view(-1, hidden_dim)\n                latest_mins = torch.min(out, dim=0)[0]\n                latest_maxes = torch.max(out, dim=0)[0]\n\n                if layer_name in self.scales_:\n                    self.scales_[layer_name].min_channel_vals = torch.minimum(\n                        self.scales_[layer_name].min_channel_vals, latest_mins\n                    )\n                    self.scales_[layer_name].max_channel_vals = torch.maximum(\n                        self.scales_[layer_name].max_channel_vals, latest_maxes\n                    )\n                else:\n                    self.scales_[layer_name] = SmoothQuantScale(\n                        min_channel_vals=latest_mins, max_channel_vals=latest_maxes\n                    )\n\n            return hook_fn\n\n        for mapping in self.resolved_mappings_:\n            name = mapping.smooth_name\n            layer = mapping.smooth_layer\n            self.register_hook(layer, create_hook_fn(name), \"forward\")\n\n    @torch.no_grad()\n    def _apply_smoothing(self, model: Module):\n        \"\"\"\n        After calibration, apply smoothing to the activations and push the transform\n        into the following weights by applying the inverse to each balance weight.\n\n        Y = (Xdiag(scales)^(-1) * diag(scales)W) where W is the to_balance weights and\n        X is the to_smooth weights\n\n        This modifies the weights of the model in-place.\n        \"\"\"\n        for mapping in self.resolved_mappings_:\n            if mapping.smooth_name not in self.scales_:\n                continue\n            logger.info(f\"Smoothing with {mapping.smooth_name}\")\n\n            activation_scales = (  # get dynamic range for each activation channel\n                self.scales_[mapping.smooth_name].max_channel_vals\n                - self.scales_[mapping.smooth_name].min_channel_vals\n            )\n            smooth_layer = mapping.smooth_layer\n            balance_layers = mapping.balance_layers\n\n            scales = self._calculate_smoothing_scales(balance_layers, activation_scales)\n            scales = torch.maximum(\n                scales, torch.Tensor([MINIMUM_SMOOTHING_SCALE]).to(scales.device)\n            )\n\n            @torch.no_grad()\n            def smooth(module):\n                with align_module_device(module):\n                    if module in balance_layers:\n                        module.weight.mul_(scales.view(1, -1))\n                    elif module == smooth_layer:\n                        if module.weight.ndim == 1:\n                            module.weight.div_(scales)\n                        else:\n                            module.weight.div_(scales.view(-1, 1))\n                        if hasattr(module, \"bias\") and module.bias is not None:\n                            module.bias.div_(scales)\n\n            parent = get_fsdp_parent(mapping.smooth_name, model)\n            if parent is not None:\n                parent.apply(smooth)\n            else:\n                # if we're not running with FSDP we can apply smoothing directly\n                for layer in balance_layers:\n                    smooth(layer)\n                smooth(smooth_layer)\n\n            # clear calibration data\n            del self.scales_[mapping.smooth_name]\n\n    def _calculate_smoothing_scales(\n        self, balance_layers: List[Module], activation_scales: torch.Tensor\n    ) -&gt; List[float]:\n        \"\"\"\n        Calculate how much smoothing to apply to each channel based on the dynamic\n        range of the activation and the following weights\n\n        :param balance_layers: layers to offset activation smoothing to\n        :param activation_scales: channel-wise dynamic range of activations to smooth\n        :return: channel-wise scales to use for smoothing activations\n        \"\"\"\n        # get the channel-wise dynamic range for each layer to be balanced\n        weight_scales = []\n        for layer in balance_layers:\n            with align_module_device(layer):\n                scale = layer.weight.abs().max(dim=0, keepdim=True)[0]\n                weight_scales.append(scale)\n\n        weight_scales = 2.0 * torch.cat(weight_scales, dim=0).max(dim=0)[0]\n\n        # calculate the amount of smoothing to apply\n        # s_j = max(|X_j|)^alpha / max(|W_j|)^(1-alpha)\n        # where j is the input channel, alpha is smoothing strength\n        scales = activation_scales.pow(self.smoothing_strength) / weight_scales.pow(\n            1 - self.smoothing_strength\n        )\n        scales = torch.where(weight_scales &gt; 0.0, scales, activation_scales)\n\n        return scales\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/base/#llmcompressor.modifiers.smoothquant.base.SmoothQuantModifier.on_finalize","title":"<code>on_finalize(state, **kwargs)</code>","text":"<p>Clean up by clearing the scale and mapping data</p> Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>def on_finalize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Clean up by clearing the scale and mapping data\n    \"\"\"\n    if not self.ended_:\n        self.on_end(state, None)\n\n    if len(self.scales_) &gt; 0:\n        raise ValueError(f\"Failed to compress {len(self.scales_)} modules\")\n\n    if self.scales_ is not None:\n        self.scales_.clear()\n    if self.resolved_mappings_ is not None:\n        self.resolved_mappings_.clear()\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/base/#llmcompressor.modifiers.smoothquant.base.SmoothQuantModifier.on_initialize","title":"<code>on_initialize(state, **kwargs)</code>","text":"<p>Initialize and run SmoothQuant on the given state</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>state to run SmoothQuant on</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True on a successful run, False otherwise</p> Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>def on_initialize(self, state: State, **kwargs) -&gt; bool:\n    \"\"\"\n    Initialize and run SmoothQuant on the given state\n\n    :param state: state to run SmoothQuant on\n    :return: True on a successful run, False otherwise\n    \"\"\"\n    if self.end and self.end != -1:\n        raise ValueError(\n            f\"{self.__class__.__name__} can only be applied during one-shot. \"\n            f\" Expected end to be None or -1, got {self.end}\"\n        )\n    if self.start and self.start != -1:\n        raise ValueError(\n            f\"{self.__class__.__name__} can only be applied during one-shot. \"\n            f\"Expected start to be None or -1, got {self.end}\"\n        )\n\n    self.ignore = [] if not self.ignore else self.ignore\n    self.mappings = self._infer_mappings_from_model(state.model)\n    self.resolved_mappings_ = self._resolve_mappings(state.model)\n    self.scales_ = {}\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/base/#llmcompressor.modifiers.smoothquant.base.SmoothQuantScale","title":"<code>SmoothQuantScale</code>  <code>dataclass</code>","text":"<p>Dataclass for storing the channel-wise minimum and maximum values for a layer. This is updated each forward pass during calibration</p> <p>Parameters:</p> Name Type Description Default <code>min_channel_vals</code> <code>Tensor</code> <p>minimum output value seen so far, per channel</p> required <code>max_channel_vals</code> <code>Tensor</code> <p>maximum output value seen so far, per channel</p> required Source code in <code>src/llmcompressor/modifiers/smoothquant/base.py</code> <pre><code>@dataclass\nclass SmoothQuantScale:\n    \"\"\"\n    Dataclass for storing the channel-wise minimum and maximum values for a layer. This\n    is updated each forward pass during calibration\n\n    :param min_channel_vals: minimum output value seen so far, per channel\n    :param max_channel_vals: maximum output value seen so far, per channel\n    \"\"\"\n\n    min_channel_vals: torch.Tensor\n    max_channel_vals: torch.Tensor\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/utils/","title":"llmcompressor.modifiers.smoothquant.utils","text":""},{"location":"reference/llmcompressor/modifiers/smoothquant/utils/#llmcompressor.modifiers.smoothquant.utils.get_layer_mappings_from_architecture","title":"<code>get_layer_mappings_from_architecture(architecture)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>str</code> <p>str: The architecture of the model</p> required <p>Returns:</p> Type Description <code>List[LayerMap]</code> <p>list: The layer mappings for the given architecture</p> Source code in <code>src/llmcompressor/modifiers/smoothquant/utils.py</code> <pre><code>def get_layer_mappings_from_architecture(architecture: str) -&gt; List[LayerMap]:\n    \"\"\"\n    :param architecture: str: The architecture of the model\n    :return: list: The layer mappings for the given architecture\n    \"\"\"\n\n    if architecture not in MAPPINGS_REGISTRY:\n        logger.info(\n            f\"Architecture {architecture} not found in mappings. \"\n            f\"Using default mappings: {DEFAULT_SMOOTHQUANT_MAPPINGS}\"\n        )\n\n    return MAPPINGS_REGISTRY.get(architecture, DEFAULT_SMOOTHQUANT_MAPPINGS)\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/smoothquant/utils/#llmcompressor.modifiers.smoothquant.utils.handle_mapping_resolution_errors","title":"<code>handle_mapping_resolution_errors(func)</code>","text":"<p>Decorator to catch any errors that occur when resolving mappings and provide a helpful error message to the user pointing them to the README</p> Source code in <code>src/llmcompressor/modifiers/smoothquant/utils.py</code> <pre><code>def handle_mapping_resolution_errors(func):\n    \"\"\"\n    Decorator to catch any errors that occur when resolving mappings and provide a\n    helpful error message to the user pointing them to the README\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as original_exception:\n            readme_location = (\n                \"https://github.com/vllm-project/llm-compressor/tree/main/\"\n                \"src/llmcompressor/modifiers/smoothquant\"\n            )\n            raise RuntimeError(\n                f\"Error resolving mappings for given architecture.\"\n                f\"Please refer to the README at {readme_location} for more information.\"\n            ) from original_exception\n\n    return wrapper\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/","title":"llmcompressor.modifiers.utils","text":""},{"location":"reference/llmcompressor/modifiers/utils/constants/","title":"llmcompressor.modifiers.utils.constants","text":""},{"location":"reference/llmcompressor/modifiers/utils/hooks/","title":"llmcompressor.modifiers.utils.hooks","text":""},{"location":"reference/llmcompressor/modifiers/utils/hooks/#llmcompressor.modifiers.utils.hooks.HooksMixin","title":"<code>HooksMixin</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mixin to manage hook registration, disabling, and removal. Modifiers should use <code>self.register_hook(module, hook, hook_type)</code> for hook registration and <code>self.remove_hooks()</code> for removal.</p> <p>Modifiers which implement hooks should register them using <code>self.register_..._hook(module, hook)</code> rather than the usual <code>module.register_..._hook(hook)</code>. Modifiers should remove hooks with <code>self.remove_hooks()</code>.</p> <p>Hooks can be applied to modules or parameters</p> <p>Typical example</p> <p>modifier.register_forward_hook(module, hook) with HooksMixin.disable_hooks():         model.forward(...) modifier.remove_hooks()</p> <p>Example of activating only a specific subset of hooks</p> <p>hooks = [modifier.register_forward_hook(module, hook) for module in ...] with HooksMixin.disable_hooks(keep=hooks):         model.forward(...) modifier.remove_hooks(hooks)</p> Source code in <code>src/llmcompressor/modifiers/utils/hooks.py</code> <pre><code>class HooksMixin(BaseModel):\n    \"\"\"\n    Mixin to manage hook registration, disabling, and removal.\n    Modifiers should use `self.register_hook(module, hook, hook_type)`\n    for hook registration and `self.remove_hooks()` for removal.\n\n    Modifiers which implement hooks should register them using\n    `self.register_..._hook(module, hook)` rather than the usual\n    `module.register_..._hook(hook)`. Modifiers should remove hooks with\n    `self.remove_hooks()`.\n\n    Hooks can be applied to modules or parameters\n\n    Typical example\n    &gt;&gt;&gt; modifier.register_forward_hook(module, hook)\n    &gt;&gt;&gt; with HooksMixin.disable_hooks():\n            model.forward(...)\n    &gt;&gt;&gt; modifier.remove_hooks()\n\n    Example of activating only a specific subset of hooks\n    &gt;&gt;&gt; hooks = [modifier.register_forward_hook(module, hook) for module in ...]\n    &gt;&gt;&gt; with HooksMixin.disable_hooks(keep=hooks):\n            model.forward(...)\n    &gt;&gt;&gt; modifier.remove_hooks(hooks)\n    \"\"\"\n\n    # attached to global HooksMixin class\n    _HOOKS_DISABLED: ClassVar[bool] = False\n    _HOOKS_KEEP_ENABLED: ClassVar[Set[RemovableHandle]] = set()\n\n    # attached to local subclasses\n    _hooks: Set[RemovableHandle] = set()\n\n    @classmethod\n    @contextlib.contextmanager\n    def disable_hooks(cls, keep: Set[RemovableHandle] = frozenset()):\n        \"\"\"\n        Disable all hooks across all modifiers. Composing multiple contexts is\n        equivalent to the union of `keep` arguments\n\n        :param keep: optional set of handles to keep enabled\n        \"\"\"\n        try:\n            cls._HOOKS_DISABLED = True\n            cls._HOOKS_KEEP_ENABLED |= keep\n            yield\n        finally:\n            cls._HOOKS_DISABLED = False\n            cls._HOOKS_KEEP_ENABLED -= keep\n\n    def register_hook(\n        self,\n        target: Union[torch.nn.Module, torch.nn.Parameter],\n        hook: Callable[[Any], Any],\n        hook_type: str,\n        **kwargs,\n    ) -&gt; RemovableHandle:\n        \"\"\"\n        Registers a hook on a specified module/parameter with the option to disable it\n        with HooksMixin.disable_hooks()\n\n        :param target: the module or parameter on which the hook should be registered\n        :param hook: the hook to register\n        :param hook_type: the type of hook to register corresponding to the\n            `register_{hook_type}_hook` attribute on torch.nn.Module.\n            Ex. \"forward\", \"forward_pre\", \"full_backward\", \"state_dict_post\", \"\"\n        :param kwargs: keyword arguments to pass to register hook method\n        \"\"\"\n        handle = None\n\n        @wraps(hook)\n        def wrapped_hook(*args, **kwargs):\n            nonlocal handle\n\n            if (\n                HooksMixin._HOOKS_DISABLED\n                and handle not in HooksMixin._HOOKS_KEEP_ENABLED\n            ):\n                return\n\n            return hook(*args, **kwargs)\n\n        register_function = getattr(target, f\"register_{hook_type}_hook\")\n        handle = register_function(wrapped_hook, **kwargs)\n        self._hooks.add(handle)\n        logger.debug(f\"{self} added {handle}\")\n\n        return handle\n\n    def remove_hooks(self, handles: Optional[Set[RemovableHandle]] = None):\n        \"\"\"\n        Removes hooks registered by this modifier\n\n        :param handles: optional list of handles to remove, defaults to all hooks\n            registerd by this modifier\n        \"\"\"\n        if handles is None:\n            handles = self._hooks\n\n        for hook in handles:\n            hook.remove()\n\n        self._hooks -= handles\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/hooks/#llmcompressor.modifiers.utils.hooks.HooksMixin.disable_hooks","title":"<code>disable_hooks(keep=frozenset())</code>  <code>classmethod</code>","text":"<p>Disable all hooks across all modifiers. Composing multiple contexts is equivalent to the union of <code>keep</code> arguments</p> <p>Parameters:</p> Name Type Description Default <code>keep</code> <code>Set[RemovableHandle]</code> <p>optional set of handles to keep enabled</p> <code>frozenset()</code> Source code in <code>src/llmcompressor/modifiers/utils/hooks.py</code> <pre><code>@classmethod\n@contextlib.contextmanager\ndef disable_hooks(cls, keep: Set[RemovableHandle] = frozenset()):\n    \"\"\"\n    Disable all hooks across all modifiers. Composing multiple contexts is\n    equivalent to the union of `keep` arguments\n\n    :param keep: optional set of handles to keep enabled\n    \"\"\"\n    try:\n        cls._HOOKS_DISABLED = True\n        cls._HOOKS_KEEP_ENABLED |= keep\n        yield\n    finally:\n        cls._HOOKS_DISABLED = False\n        cls._HOOKS_KEEP_ENABLED -= keep\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/hooks/#llmcompressor.modifiers.utils.hooks.HooksMixin.register_hook","title":"<code>register_hook(target, hook, hook_type, **kwargs)</code>","text":"<p>Registers a hook on a specified module/parameter with the option to disable it with HooksMixin.disable_hooks()</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Union[Module, Parameter]</code> <p>the module or parameter on which the hook should be registered</p> required <code>hook</code> <code>Callable[[Any], Any]</code> <p>the hook to register</p> required <code>hook_type</code> <code>str</code> <p>the type of hook to register corresponding to the <code>register_{hook_type}_hook</code> attribute on torch.nn.Module. Ex. \"forward\", \"forward_pre\", \"full_backward\", \"state_dict_post\", \"\"</p> required <code>kwargs</code> <p>keyword arguments to pass to register hook method</p> <code>{}</code> Source code in <code>src/llmcompressor/modifiers/utils/hooks.py</code> <pre><code>def register_hook(\n    self,\n    target: Union[torch.nn.Module, torch.nn.Parameter],\n    hook: Callable[[Any], Any],\n    hook_type: str,\n    **kwargs,\n) -&gt; RemovableHandle:\n    \"\"\"\n    Registers a hook on a specified module/parameter with the option to disable it\n    with HooksMixin.disable_hooks()\n\n    :param target: the module or parameter on which the hook should be registered\n    :param hook: the hook to register\n    :param hook_type: the type of hook to register corresponding to the\n        `register_{hook_type}_hook` attribute on torch.nn.Module.\n        Ex. \"forward\", \"forward_pre\", \"full_backward\", \"state_dict_post\", \"\"\n    :param kwargs: keyword arguments to pass to register hook method\n    \"\"\"\n    handle = None\n\n    @wraps(hook)\n    def wrapped_hook(*args, **kwargs):\n        nonlocal handle\n\n        if (\n            HooksMixin._HOOKS_DISABLED\n            and handle not in HooksMixin._HOOKS_KEEP_ENABLED\n        ):\n            return\n\n        return hook(*args, **kwargs)\n\n    register_function = getattr(target, f\"register_{hook_type}_hook\")\n    handle = register_function(wrapped_hook, **kwargs)\n    self._hooks.add(handle)\n    logger.debug(f\"{self} added {handle}\")\n\n    return handle\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/hooks/#llmcompressor.modifiers.utils.hooks.HooksMixin.remove_hooks","title":"<code>remove_hooks(handles=None)</code>","text":"<p>Removes hooks registered by this modifier</p> <p>Parameters:</p> Name Type Description Default <code>handles</code> <code>Optional[Set[RemovableHandle]]</code> <p>optional list of handles to remove, defaults to all hooks registerd by this modifier</p> <code>None</code> Source code in <code>src/llmcompressor/modifiers/utils/hooks.py</code> <pre><code>def remove_hooks(self, handles: Optional[Set[RemovableHandle]] = None):\n    \"\"\"\n    Removes hooks registered by this modifier\n\n    :param handles: optional list of handles to remove, defaults to all hooks\n        registerd by this modifier\n    \"\"\"\n    if handles is None:\n        handles = self._hooks\n\n    for hook in handles:\n        hook.remove()\n\n    self._hooks -= handles\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/pytorch_helpers/","title":"llmcompressor.modifiers.utils.pytorch_helpers","text":""},{"location":"reference/llmcompressor/modifiers/utils/pytorch_helpers/#llmcompressor.modifiers.utils.pytorch_helpers.apply_pad_mask_to_batch","title":"<code>apply_pad_mask_to_batch(batch)</code>","text":"<p>Apply a mask to the input ids of a batch. This is used to zero out padding tokens so they do not contribute to the hessian calculation in the GPTQ and SparseGPT algorithms</p> <p>Assumes that <code>attention_mask</code> only contains zeros and ones</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>batch to apply padding to if it exists</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>batch with padding zeroed out in the input_ids</p> Source code in <code>src/llmcompressor/modifiers/utils/pytorch_helpers.py</code> <pre><code>def apply_pad_mask_to_batch(batch: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Apply a mask to the input ids of a batch. This is used to zero out\n    padding tokens so they do not contribute to the hessian calculation in the\n    GPTQ and SparseGPT algorithms\n\n    Assumes that `attention_mask` only contains zeros and ones\n\n    :param batch: batch to apply padding to if it exists\n    :return: batch with padding zeroed out in the input_ids\n    \"\"\"\n    if \"attention_mask\" in batch:\n        for key in (\"input_ids\", \"decoder_input_ids\"):\n            if key in batch:\n                batch[key] = batch[key] * batch[\"attention_mask\"]\n\n    return batch\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/pytorch_helpers/#llmcompressor.modifiers.utils.pytorch_helpers.is_moe_model","title":"<code>is_moe_model(model)</code>","text":"<p>Check if the model is a mixture of experts model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the model is a mixture of experts model</p> Source code in <code>src/llmcompressor/modifiers/utils/pytorch_helpers.py</code> <pre><code>def is_moe_model(model: Module) -&gt; bool:\n    \"\"\"\n    Check if the model is a mixture of experts model\n\n    :param model: the model to check\n    :return: True if the model is a mixture of experts model\n    \"\"\"\n\n    # Check for MoE components\n    for _, module in model.named_modules():\n        module_name = module.__class__.__name__\n        if \"MoE\" in module_name or \"Expert\" in module_name:\n            return True\n\n    # Check config for MoE attributes\n    if hasattr(model, \"config\"):\n        if any(\n            \"moe\" in attr.lower() or \"expert\" in attr.lower()\n            for attr in dir(model.config)\n        ):\n            return True\n\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/modifiers/utils/pytorch_helpers/#llmcompressor.modifiers.utils.pytorch_helpers.run_calibration_forward","title":"<code>run_calibration_forward(model, calibration_dataloader, num_calibration_steps=None, calibration_function=None, device=None, mask_padding=False)</code>","text":"<p>Helper function used by one-shot modifiers, runs calibration data through a model to update modifier statistics and trigger hooks</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model to run</p> required <code>calibration_dataloader</code> <code>DataLoader</code> <p>data to use for calibration</p> required <code>num_calibration_steps</code> <code>Optional[int]</code> <p>number of items in calibration_dataloader to process, None or a negative number to process all available data</p> <code>None</code> <code>calibration_function</code> <code>Optional[Callable]</code> <p>option to pass a custom forward function for model</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>option to move the model to a specific device before calibration</p> <code>None</code> <code>mask_padding</code> <code>bool</code> <p>whether to zero out padding tokens during calibration</p> <code>False</code> Source code in <code>src/llmcompressor/modifiers/utils/pytorch_helpers.py</code> <pre><code>def run_calibration_forward(\n    model: Module,\n    calibration_dataloader: DataLoader,\n    num_calibration_steps: Optional[int] = None,\n    calibration_function: Optional[Callable] = None,\n    device: Optional[str] = None,\n    mask_padding: bool = False,\n):\n    \"\"\"\n    Helper function used by one-shot modifiers, runs calibration data through a model to\n    update modifier statistics and trigger hooks\n\n    :param model: PyTorch model to run\n    :param calibration_dataloader: data to use for calibration\n    :param num_calibration_steps: number of items in calibration_dataloader to process,\n        None or a negative number to process all available data\n    :param calibration_function: option to pass a custom forward function for model\n    :param device: option to move the model to a specific device before calibration\n    :param mask_padding: whether to zero out padding tokens during calibration\n    \"\"\"\n    model.eval()\n\n    forward_fn: Callable = (\n        calibration_function if calibration_function else tensors_module_forward\n    )\n\n    # move model to optional specified device if it is not already there\n    model_device = next(model.parameters()).device\n    if device is not None and model_device != device:\n        model.to(device)\n        model_device = next(model.parameters()).device\n    _dataloader = (\n        calibration_dataloader\n        if num_calibration_steps is None\n        else cycle(calibration_dataloader)\n    )\n\n    # run through the calibration data\n    for batch_idx, batch in enumerate(tqdm(_dataloader)):\n        if num_calibration_steps and batch_idx &gt;= num_calibration_steps:\n            break\n        if mask_padding:\n            batch = apply_pad_mask_to_batch(batch)\n        batch = tensors_to_device(batch, model_device)\n        with torch.no_grad():\n            forward_fn(batch, module=model)\n</code></pre>"},{"location":"reference/llmcompressor/observers/","title":"llmcompressor.observers","text":""},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MinMaxObserver","title":"<code>MinMaxObserver</code>","text":"<p>               Bases: <code>Observer</code></p> <p>Implements a quantization observer that calculates scale and zero point based on the minimum and maximum values of the tensor being observed. If averaging_constant is specified, then the scales are updated using a moving average</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>@Observer.register(\"minmax\")\nclass MinMaxObserver(Observer):\n    \"\"\"\n    Implements a quantization observer that calculates scale and zero point based on the\n    minimum and maximum values of the tensor being observed. If averaging_constant is\n    specified, then the scales are updated using a moving average\n    \"\"\"\n\n    def __init__(\n        self, quantization_args: QuantizationArgs, averaging_constant: float = 0.01\n    ):\n        super().__init__(quantization_args=quantization_args)\n\n        self.min_val = {}\n        self.max_val = {}\n        self.averaging_constant = averaging_constant\n\n    def calculate_qparams(\n        self,\n        observed: torch.Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n        tensor_id: Optional[Any] = None,\n    ) -&gt; Tuple[torch.FloatTensor, torch.IntTensor]:\n        \"\"\"\n        Updates the observed min and max using a moving average smoothed by the\n        averaging_constant. Set the averaging_constant to 1.0 to disable averaging.\n\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :param tensor_id: Optional id if different ranges of observed tensors are\n            passed, useful for sharding tensors by group_size\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        tensor_id = tensor_id or \"default\"\n\n        if not reduce_dims:\n            min_val, max_val = torch.aminmax(observed)\n        else:\n            min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n            max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n        # early stopping, save some computation and memory\n        if self.averaging_constant == 1.0:\n            return calculate_qparams(min_val, max_val, self.quantization_args)\n\n        running_min_val = self.min_val.get(tensor_id, None)\n        running_max_val = self.max_val.get(tensor_id, None)\n\n        if running_min_val is None or running_max_val is None:\n            updated_min_val = min_val\n            updated_max_val = max_val\n        else:\n            updated_min_val = running_min_val + self.averaging_constant * (\n                min_val - running_min_val\n            )\n            updated_max_val = running_max_val + self.averaging_constant * (\n                max_val - running_max_val\n            )\n\n        self.min_val[tensor_id] = updated_min_val\n        self.max_val[tensor_id] = updated_max_val\n\n        return calculate_qparams(\n            updated_min_val, updated_max_val, self.quantization_args\n        )\n\n    def get_qparams_along_dim(\n        self, observed: torch.Tensor, dim: int, tensor_id: Optional[Any] = None\n    ):\n        \"\"\"\n        Calculate quantization parameters along the specified dimension\n        \"\"\"\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer, including min and maximum values\n        \"\"\"\n        super().reset()\n        self.min_val = {}\n        self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MinMaxObserver.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None, tensor_id=None)</code>","text":"<p>Updates the observed min and max using a moving average smoothed by the averaging_constant. Set the averaging_constant to 1.0 to disable averaging.</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <code>tensor_id</code> <code>Optional[Any]</code> <p>Optional id if different ranges of observed tensors are passed, useful for sharding tensors by group_size</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: torch.Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n    tensor_id: Optional[Any] = None,\n) -&gt; Tuple[torch.FloatTensor, torch.IntTensor]:\n    \"\"\"\n    Updates the observed min and max using a moving average smoothed by the\n    averaging_constant. Set the averaging_constant to 1.0 to disable averaging.\n\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :param tensor_id: Optional id if different ranges of observed tensors are\n        passed, useful for sharding tensors by group_size\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    tensor_id = tensor_id or \"default\"\n\n    if not reduce_dims:\n        min_val, max_val = torch.aminmax(observed)\n    else:\n        min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n        max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n    # early stopping, save some computation and memory\n    if self.averaging_constant == 1.0:\n        return calculate_qparams(min_val, max_val, self.quantization_args)\n\n    running_min_val = self.min_val.get(tensor_id, None)\n    running_max_val = self.max_val.get(tensor_id, None)\n\n    if running_min_val is None or running_max_val is None:\n        updated_min_val = min_val\n        updated_max_val = max_val\n    else:\n        updated_min_val = running_min_val + self.averaging_constant * (\n            min_val - running_min_val\n        )\n        updated_max_val = running_max_val + self.averaging_constant * (\n            max_val - running_max_val\n        )\n\n    self.min_val[tensor_id] = updated_min_val\n    self.max_val[tensor_id] = updated_max_val\n\n    return calculate_qparams(\n        updated_min_val, updated_max_val, self.quantization_args\n    )\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MinMaxObserver.get_qparams_along_dim","title":"<code>get_qparams_along_dim(observed, dim, tensor_id=None)</code>","text":"<p>Calculate quantization parameters along the specified dimension</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>def get_qparams_along_dim(\n    self, observed: torch.Tensor, dim: int, tensor_id: Optional[Any] = None\n):\n    \"\"\"\n    Calculate quantization parameters along the specified dimension\n    \"\"\"\n    reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)\n    return self.calculate_qparams(\n        observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n    )\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MinMaxObserver.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer, including min and maximum values</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer, including min and maximum values\n    \"\"\"\n    super().reset()\n    self.min_val = {}\n    self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MovingAverageMSEObserver","title":"<code>MovingAverageMSEObserver</code>","text":"<p>               Bases: <code>Observer</code></p> <p>Implements a dynamic quantization observer that sets the scale and zero point based on a moving average of the mse-clipped min and max observed values</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>@Observer.register(\"mse\")\nclass MovingAverageMSEObserver(Observer):\n    \"\"\"\n    Implements a dynamic quantization observer that sets the scale and\n    zero point based on a moving average of the mse-clipped min and max observed values\n    \"\"\"\n\n    def __init__(\n        self,\n        quantization_args: QuantizationArgs,\n        averaging_constant: float = 0.01,\n        grid: float = 100.0,\n        maxshrink: float = 0.80,\n        norm: float = 2.4,\n    ):\n        super().__init__(quantization_args=quantization_args)\n\n        self.min_val = {}\n        self.max_val = {}\n        self.averaging_constant = averaging_constant\n        self.grid = grid\n        self.maxshrink = maxshrink\n        self.norm = norm\n\n    def calculate_mse_min_max(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n    ):\n        \"\"\"\n        Computes the mse-clipped min and max values of the observed tensor by\n        optimizing for quantization error\n\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned values will be shaped (1,) along the reduced dimensions\n        :return: tuple of min and max values derived from the observed tensor\n        \"\"\"\n        from compressed_tensors.quantization.lifecycle import fake_quantize\n\n        if not reduce_dims:\n            absolute_min_val, absolute_max_val = torch.aminmax(observed)\n        else:\n            absolute_min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n            absolute_max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n        best = torch.full_like(\n            absolute_min_val, torch.finfo(absolute_min_val.dtype).max\n        )\n        min_val = torch.ones_like(absolute_min_val)\n        max_val = torch.zeros_like(absolute_max_val)\n        for i in range(int(self.maxshrink * self.grid)):\n            p = 1 - i / self.grid\n            shrinked_min_val = p * absolute_min_val\n            shrinked_max_val = p * absolute_max_val\n\n            candidate_scales, candidate_zero_points = calculate_qparams(\n                shrinked_min_val, shrinked_max_val, self.quantization_args\n            )\n            q = fake_quantize(\n                observed,\n                candidate_scales,\n                candidate_zero_points,\n                self.quantization_args,\n            )\n\n            q -= observed\n            q.abs_()\n            q.pow_(self.norm)\n            if not reduce_dims:\n                err = torch.sum(q)\n            else:\n                err = torch.sum(q, reduce_dims, keepdims=True)\n\n            tmp = err &lt; best\n            if torch.any(tmp):\n                best[tmp] = err[tmp]\n                min_val[tmp] = shrinked_min_val[tmp]\n                max_val[tmp] = shrinked_max_val[tmp]\n        return min_val, max_val\n\n    def calculate_qparams(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n        tensor_id: Optional[Any] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        Updates the mse-clipped min and max values of the observed tensor using\n        a moving average smoothed by the averaging_constant\n\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :param tensor_id: Optional id if different ranges of observed tensors are\n            passed, useful for sharding tensors by group_size\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        min_val, max_val = self.calculate_mse_min_max(observed, reduce_dims)\n\n        running_min_val = self.min_val.get(tensor_id, None)\n        running_max_val = self.max_val.get(tensor_id, None)\n\n        if running_min_val is None or running_max_val is None:\n            updated_min_val = min_val\n            updated_max_val = max_val\n        else:\n            updated_min_val = running_min_val + self.averaging_constant * (\n                min_val - running_min_val\n            )\n            updated_max_val = running_max_val + self.averaging_constant * (\n                max_val - running_max_val\n            )\n\n        tensor_id = tensor_id or \"default\"\n        self.min_val[tensor_id] = updated_min_val\n        self.max_val[tensor_id] = updated_max_val\n\n        return calculate_qparams(\n            updated_min_val, updated_max_val, self.quantization_args\n        )\n\n    def get_qparams_along_dim(\n        self, observed, dim: int, tensor_id: Optional[Any] = None\n    ):\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer, including min and maximum values\n        \"\"\"\n        super().reset()\n        self.min_val = {}\n        self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MovingAverageMSEObserver.calculate_mse_min_max","title":"<code>calculate_mse_min_max(observed, reduce_dims=None)</code>","text":"<p>Computes the mse-clipped min and max values of the observed tensor by optimizing for quantization error</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned values will be shaped (1,) along the reduced dimensions</p> <code>None</code> <p>Returns:</p> Type Description <p>tuple of min and max values derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>def calculate_mse_min_max(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n):\n    \"\"\"\n    Computes the mse-clipped min and max values of the observed tensor by\n    optimizing for quantization error\n\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned values will be shaped (1,) along the reduced dimensions\n    :return: tuple of min and max values derived from the observed tensor\n    \"\"\"\n    from compressed_tensors.quantization.lifecycle import fake_quantize\n\n    if not reduce_dims:\n        absolute_min_val, absolute_max_val = torch.aminmax(observed)\n    else:\n        absolute_min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n        absolute_max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n    best = torch.full_like(\n        absolute_min_val, torch.finfo(absolute_min_val.dtype).max\n    )\n    min_val = torch.ones_like(absolute_min_val)\n    max_val = torch.zeros_like(absolute_max_val)\n    for i in range(int(self.maxshrink * self.grid)):\n        p = 1 - i / self.grid\n        shrinked_min_val = p * absolute_min_val\n        shrinked_max_val = p * absolute_max_val\n\n        candidate_scales, candidate_zero_points = calculate_qparams(\n            shrinked_min_val, shrinked_max_val, self.quantization_args\n        )\n        q = fake_quantize(\n            observed,\n            candidate_scales,\n            candidate_zero_points,\n            self.quantization_args,\n        )\n\n        q -= observed\n        q.abs_()\n        q.pow_(self.norm)\n        if not reduce_dims:\n            err = torch.sum(q)\n        else:\n            err = torch.sum(q, reduce_dims, keepdims=True)\n\n        tmp = err &lt; best\n        if torch.any(tmp):\n            best[tmp] = err[tmp]\n            min_val[tmp] = shrinked_min_val[tmp]\n            max_val[tmp] = shrinked_max_val[tmp]\n    return min_val, max_val\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MovingAverageMSEObserver.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None, tensor_id=None)</code>","text":"<p>Updates the mse-clipped min and max values of the observed tensor using a moving average smoothed by the averaging_constant</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <code>tensor_id</code> <code>Optional[Any]</code> <p>Optional id if different ranges of observed tensors are passed, useful for sharding tensors by group_size</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n    tensor_id: Optional[Any] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    Updates the mse-clipped min and max values of the observed tensor using\n    a moving average smoothed by the averaging_constant\n\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :param tensor_id: Optional id if different ranges of observed tensors are\n        passed, useful for sharding tensors by group_size\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    min_val, max_val = self.calculate_mse_min_max(observed, reduce_dims)\n\n    running_min_val = self.min_val.get(tensor_id, None)\n    running_max_val = self.max_val.get(tensor_id, None)\n\n    if running_min_val is None or running_max_val is None:\n        updated_min_val = min_val\n        updated_max_val = max_val\n    else:\n        updated_min_val = running_min_val + self.averaging_constant * (\n            min_val - running_min_val\n        )\n        updated_max_val = running_max_val + self.averaging_constant * (\n            max_val - running_max_val\n        )\n\n    tensor_id = tensor_id or \"default\"\n    self.min_val[tensor_id] = updated_min_val\n    self.max_val[tensor_id] = updated_max_val\n\n    return calculate_qparams(\n        updated_min_val, updated_max_val, self.quantization_args\n    )\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.MovingAverageMSEObserver.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer, including min and maximum values</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer, including min and maximum values\n    \"\"\"\n    super().reset()\n    self.min_val = {}\n    self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer","title":"<code>Observer</code>","text":"<p>               Bases: <code>Module</code>, <code>RegistryMixin</code></p> <p>Base Observer class to be subclassed for specific implementation. Subclasses should override <code>calculate_qparams</code> to return a scale, zero_point pair</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>class Observer(Module, RegistryMixin):\n    \"\"\"\n    Base Observer class to be subclassed for specific implementation.\n    Subclasses should override `calculate_qparams` to return a scale, zero_point\n    pair\n    \"\"\"\n\n    def __init__(self, quantization_args: QuantizationArgs):\n        self.quantization_args: QuantizationArgs = quantization_args\n        super().__init__()\n        self._scale = None\n        self._zero_point = None\n        self._num_observed_tokens = None\n\n    @torch.no_grad()\n    def forward(\n        self, observed: Tensor, g_idx: Optional[Tensor] = None\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        maps directly to get_qparams\n        :param observed: optional observed tensor from which to calculate\n            quantization parameters\n        :param g_idx: optional mapping from column index to group index\n        :return: tuple of scale and zero point based on last observed value\n        \"\"\"\n        self.record_observed_tokens(observed)\n        return self.get_qparams(observed=observed, g_idx=g_idx)\n\n    def calculate_qparams(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__} must implement calculate_qparams\")\n\n    def post_calculate_qparams(self) -&gt; None:\n        \"\"\"\n        Run any logic specific to its observers after running calculate_qparams\n        \"\"\"\n\n    def get_qparams(\n        self,\n        observed: Optional[Tensor] = None,\n        g_idx: Optional[Tensor] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        Convenience function to wrap overwritten calculate_qparams\n        adds support to make observed tensor optional and support for tracking latest\n        calculated scale and zero point\n\n        :param observed: optional observed tensor to calculate quantization parameters\n            from\n        :param g_idx: optional mapping from column index to group index\n        :return: tuple of scale and zero point based on last observed value\n        \"\"\"\n        if observed is not None:\n            group_size = self.quantization_args.group_size\n\n            if self.quantization_args.strategy == QuantizationStrategy.TENSOR:\n                # re-calculate scale and zero point, update the stored value\n                self._scale, self._zero_point = self.calculate_qparams(observed)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.GROUP:\n                rows = observed.shape[0]\n                columns = observed.shape[1]\n                num_groups = int(ceil(columns / group_size))\n                self._scale = torch.empty(\n                    (rows, num_groups), dtype=observed.dtype, device=observed.device\n                )\n                zp_dtype = self.quantization_args.pytorch_dtype()\n                self._zero_point = torch.empty(\n                    (rows, num_groups), dtype=zp_dtype, device=observed.device\n                )\n\n                # support column-order (default) quantization as well as other orderings\n                # such as activation ordering. Below checks if g_idx has initialized\n                is_column_order = g_idx is None or -1 in g_idx\n                if is_column_order:\n                    group_sizes = torch.full((num_groups,), group_size, dtype=torch.int)\n                else:\n                    group_indices, group_sizes = torch.unique(g_idx, return_counts=True)\n                    group_sizes = group_sizes[torch.argsort(group_indices)]\n\n                    perm = torch.argsort(g_idx)\n                    observed = safe_permute(observed, perm, dim=1)\n\n                # TODO: experiment with vectorizing for loop for performance\n                end = 0\n                for group_index, group_count in enumerate(group_sizes):\n                    start = end\n                    end = start + group_count\n                    scale, zero_point = self.get_qparams_along_dim(\n                        observed[:, start:end],\n                        0,\n                        tensor_id=group_index,\n                    )\n\n                    self._scale[:, group_index] = scale.squeeze(1)\n                    self._zero_point[:, group_index] = zero_point.squeeze(1)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:\n                # assume observed is transposed, because its the output, hence use dim 0\n                self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:\n                # use dim 1, assume the obsersed.shape = [batch, token, hidden]\n                # should be batch, token\n                self._scale, self._zero_point = self.get_qparams_along_dim(\n                    observed,\n                    dim={0, 1},\n                )\n\n        return self._scale, self._zero_point\n\n    def get_qparams_along_dim(\n        self,\n        observed,\n        dim: Union[int, Iterable[int]],\n        tensor_id: Optional[Any] = None,\n    ):\n        if isinstance(dim, int):\n            dim = [dim]\n        dim = set(dim)\n\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx not in dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def record_observed_tokens(self, batch_tensor: Tensor):\n        \"\"\"\n        Counts the number of tokens observed during the\n        forward passes. The count is aggregated in the\n        _num_observed_tokens attribute of the class.\n\n        Note: The batch_tensor is expected to have two dimensions\n            (batch_size * sequence_length, num_features). This is the\n            general shape expected by the forward pass of the expert\n            layers in a MOE model. If the input tensor does not have\n            two dimensions, the _num_observed_tokens attribute will be set\n            to None.\n        \"\"\"\n        if not isinstance(batch_tensor, Tensor):\n            raise ValueError(f\"Expected value to be a tensor, got {type(batch_tensor)}\")\n\n        if batch_tensor.ndim != 2:\n            logger.debug(\n                \"The input tensor is expected to have two dimensions \"\n                \"(batch_size * sequence_length, num_features). \"\n                f\"The input tensor has {batch_tensor.ndim} dimensions.\"\n            )\n            return\n\n        if self._num_observed_tokens is None:\n            # initialize the count\n            self._num_observed_tokens = 0\n\n        # batch_tensor (batch_size * sequence_length, num_features)\n        # observed_tokens (batch_size * sequence_length)\n        observed_tokens, _ = batch_tensor.shape\n        self._num_observed_tokens += observed_tokens\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer\n        \"\"\"\n        self._num_observed_tokens = None\n        self._scale = None\n        self._zero_point = None\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__} must implement calculate_qparams\")\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer.forward","title":"<code>forward(observed, g_idx=None)</code>","text":"<p>maps directly to get_qparams</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>optional observed tensor from which to calculate quantization parameters</p> required <code>g_idx</code> <code>Optional[Tensor]</code> <p>optional mapping from column index to group index</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point based on last observed value</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self, observed: Tensor, g_idx: Optional[Tensor] = None\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    maps directly to get_qparams\n    :param observed: optional observed tensor from which to calculate\n        quantization parameters\n    :param g_idx: optional mapping from column index to group index\n    :return: tuple of scale and zero point based on last observed value\n    \"\"\"\n    self.record_observed_tokens(observed)\n    return self.get_qparams(observed=observed, g_idx=g_idx)\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer.get_qparams","title":"<code>get_qparams(observed=None, g_idx=None)</code>","text":"<p>Convenience function to wrap overwritten calculate_qparams adds support to make observed tensor optional and support for tracking latest calculated scale and zero point</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Optional[Tensor]</code> <p>optional observed tensor to calculate quantization parameters from</p> <code>None</code> <code>g_idx</code> <code>Optional[Tensor]</code> <p>optional mapping from column index to group index</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point based on last observed value</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def get_qparams(\n    self,\n    observed: Optional[Tensor] = None,\n    g_idx: Optional[Tensor] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    Convenience function to wrap overwritten calculate_qparams\n    adds support to make observed tensor optional and support for tracking latest\n    calculated scale and zero point\n\n    :param observed: optional observed tensor to calculate quantization parameters\n        from\n    :param g_idx: optional mapping from column index to group index\n    :return: tuple of scale and zero point based on last observed value\n    \"\"\"\n    if observed is not None:\n        group_size = self.quantization_args.group_size\n\n        if self.quantization_args.strategy == QuantizationStrategy.TENSOR:\n            # re-calculate scale and zero point, update the stored value\n            self._scale, self._zero_point = self.calculate_qparams(observed)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.GROUP:\n            rows = observed.shape[0]\n            columns = observed.shape[1]\n            num_groups = int(ceil(columns / group_size))\n            self._scale = torch.empty(\n                (rows, num_groups), dtype=observed.dtype, device=observed.device\n            )\n            zp_dtype = self.quantization_args.pytorch_dtype()\n            self._zero_point = torch.empty(\n                (rows, num_groups), dtype=zp_dtype, device=observed.device\n            )\n\n            # support column-order (default) quantization as well as other orderings\n            # such as activation ordering. Below checks if g_idx has initialized\n            is_column_order = g_idx is None or -1 in g_idx\n            if is_column_order:\n                group_sizes = torch.full((num_groups,), group_size, dtype=torch.int)\n            else:\n                group_indices, group_sizes = torch.unique(g_idx, return_counts=True)\n                group_sizes = group_sizes[torch.argsort(group_indices)]\n\n                perm = torch.argsort(g_idx)\n                observed = safe_permute(observed, perm, dim=1)\n\n            # TODO: experiment with vectorizing for loop for performance\n            end = 0\n            for group_index, group_count in enumerate(group_sizes):\n                start = end\n                end = start + group_count\n                scale, zero_point = self.get_qparams_along_dim(\n                    observed[:, start:end],\n                    0,\n                    tensor_id=group_index,\n                )\n\n                self._scale[:, group_index] = scale.squeeze(1)\n                self._zero_point[:, group_index] = zero_point.squeeze(1)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:\n            # assume observed is transposed, because its the output, hence use dim 0\n            self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:\n            # use dim 1, assume the obsersed.shape = [batch, token, hidden]\n            # should be batch, token\n            self._scale, self._zero_point = self.get_qparams_along_dim(\n                observed,\n                dim={0, 1},\n            )\n\n    return self._scale, self._zero_point\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer.post_calculate_qparams","title":"<code>post_calculate_qparams()</code>","text":"<p>Run any logic specific to its observers after running calculate_qparams</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def post_calculate_qparams(self) -&gt; None:\n    \"\"\"\n    Run any logic specific to its observers after running calculate_qparams\n    \"\"\"\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer.record_observed_tokens","title":"<code>record_observed_tokens(batch_tensor)</code>","text":"<p>Counts the number of tokens observed during the forward passes. The count is aggregated in the _num_observed_tokens attribute of the class.</p> <p>Note: The batch_tensor is expected to have two dimensions     (batch_size * sequence_length, num_features). This is the     general shape expected by the forward pass of the expert     layers in a MOE model. If the input tensor does not have     two dimensions, the _num_observed_tokens attribute will be set     to None.</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def record_observed_tokens(self, batch_tensor: Tensor):\n    \"\"\"\n    Counts the number of tokens observed during the\n    forward passes. The count is aggregated in the\n    _num_observed_tokens attribute of the class.\n\n    Note: The batch_tensor is expected to have two dimensions\n        (batch_size * sequence_length, num_features). This is the\n        general shape expected by the forward pass of the expert\n        layers in a MOE model. If the input tensor does not have\n        two dimensions, the _num_observed_tokens attribute will be set\n        to None.\n    \"\"\"\n    if not isinstance(batch_tensor, Tensor):\n        raise ValueError(f\"Expected value to be a tensor, got {type(batch_tensor)}\")\n\n    if batch_tensor.ndim != 2:\n        logger.debug(\n            \"The input tensor is expected to have two dimensions \"\n            \"(batch_size * sequence_length, num_features). \"\n            f\"The input tensor has {batch_tensor.ndim} dimensions.\"\n        )\n        return\n\n    if self._num_observed_tokens is None:\n        # initialize the count\n        self._num_observed_tokens = 0\n\n    # batch_tensor (batch_size * sequence_length, num_features)\n    # observed_tokens (batch_size * sequence_length)\n    observed_tokens, _ = batch_tensor.shape\n    self._num_observed_tokens += observed_tokens\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.Observer.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer\n    \"\"\"\n    self._num_observed_tokens = None\n    self._scale = None\n    self._zero_point = None\n</code></pre>"},{"location":"reference/llmcompressor/observers/#llmcompressor.observers.get_observer_token_count","title":"<code>get_observer_token_count(module)</code>","text":"<p>Parse the module and return the number of tokens observed by each module's observer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to parse</p> required <p>Returns:</p> Type Description <code>Counter</code> <p>counter with the number of tokens observed by each observer</p> Source code in <code>src/llmcompressor/observers/helpers.py</code> <pre><code>def get_observer_token_count(module: torch.nn.Module) -&gt; Counter:\n    \"\"\"\n    Parse the module and return the number of tokens observed by\n    each module's observer.\n\n    :param module: module to parse\n    :return: counter with the number of tokens observed by each observer\n    \"\"\"\n    token_counts = Counter()\n    for name, module in module.named_modules():\n        if name.endswith(\".input_observer\"):\n            token_counts[name.replace(\".input_observer\", \"\")] = (\n                module._num_observed_tokens\n            )\n    return token_counts\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/","title":"llmcompressor.observers.base","text":""},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer","title":"<code>Observer</code>","text":"<p>               Bases: <code>Module</code>, <code>RegistryMixin</code></p> <p>Base Observer class to be subclassed for specific implementation. Subclasses should override <code>calculate_qparams</code> to return a scale, zero_point pair</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>class Observer(Module, RegistryMixin):\n    \"\"\"\n    Base Observer class to be subclassed for specific implementation.\n    Subclasses should override `calculate_qparams` to return a scale, zero_point\n    pair\n    \"\"\"\n\n    def __init__(self, quantization_args: QuantizationArgs):\n        self.quantization_args: QuantizationArgs = quantization_args\n        super().__init__()\n        self._scale = None\n        self._zero_point = None\n        self._num_observed_tokens = None\n\n    @torch.no_grad()\n    def forward(\n        self, observed: Tensor, g_idx: Optional[Tensor] = None\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        maps directly to get_qparams\n        :param observed: optional observed tensor from which to calculate\n            quantization parameters\n        :param g_idx: optional mapping from column index to group index\n        :return: tuple of scale and zero point based on last observed value\n        \"\"\"\n        self.record_observed_tokens(observed)\n        return self.get_qparams(observed=observed, g_idx=g_idx)\n\n    def calculate_qparams(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__} must implement calculate_qparams\")\n\n    def post_calculate_qparams(self) -&gt; None:\n        \"\"\"\n        Run any logic specific to its observers after running calculate_qparams\n        \"\"\"\n\n    def get_qparams(\n        self,\n        observed: Optional[Tensor] = None,\n        g_idx: Optional[Tensor] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        Convenience function to wrap overwritten calculate_qparams\n        adds support to make observed tensor optional and support for tracking latest\n        calculated scale and zero point\n\n        :param observed: optional observed tensor to calculate quantization parameters\n            from\n        :param g_idx: optional mapping from column index to group index\n        :return: tuple of scale and zero point based on last observed value\n        \"\"\"\n        if observed is not None:\n            group_size = self.quantization_args.group_size\n\n            if self.quantization_args.strategy == QuantizationStrategy.TENSOR:\n                # re-calculate scale and zero point, update the stored value\n                self._scale, self._zero_point = self.calculate_qparams(observed)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.GROUP:\n                rows = observed.shape[0]\n                columns = observed.shape[1]\n                num_groups = int(ceil(columns / group_size))\n                self._scale = torch.empty(\n                    (rows, num_groups), dtype=observed.dtype, device=observed.device\n                )\n                zp_dtype = self.quantization_args.pytorch_dtype()\n                self._zero_point = torch.empty(\n                    (rows, num_groups), dtype=zp_dtype, device=observed.device\n                )\n\n                # support column-order (default) quantization as well as other orderings\n                # such as activation ordering. Below checks if g_idx has initialized\n                is_column_order = g_idx is None or -1 in g_idx\n                if is_column_order:\n                    group_sizes = torch.full((num_groups,), group_size, dtype=torch.int)\n                else:\n                    group_indices, group_sizes = torch.unique(g_idx, return_counts=True)\n                    group_sizes = group_sizes[torch.argsort(group_indices)]\n\n                    perm = torch.argsort(g_idx)\n                    observed = safe_permute(observed, perm, dim=1)\n\n                # TODO: experiment with vectorizing for loop for performance\n                end = 0\n                for group_index, group_count in enumerate(group_sizes):\n                    start = end\n                    end = start + group_count\n                    scale, zero_point = self.get_qparams_along_dim(\n                        observed[:, start:end],\n                        0,\n                        tensor_id=group_index,\n                    )\n\n                    self._scale[:, group_index] = scale.squeeze(1)\n                    self._zero_point[:, group_index] = zero_point.squeeze(1)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:\n                # assume observed is transposed, because its the output, hence use dim 0\n                self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)\n\n            elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:\n                # use dim 1, assume the obsersed.shape = [batch, token, hidden]\n                # should be batch, token\n                self._scale, self._zero_point = self.get_qparams_along_dim(\n                    observed,\n                    dim={0, 1},\n                )\n\n        return self._scale, self._zero_point\n\n    def get_qparams_along_dim(\n        self,\n        observed,\n        dim: Union[int, Iterable[int]],\n        tensor_id: Optional[Any] = None,\n    ):\n        if isinstance(dim, int):\n            dim = [dim]\n        dim = set(dim)\n\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx not in dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def record_observed_tokens(self, batch_tensor: Tensor):\n        \"\"\"\n        Counts the number of tokens observed during the\n        forward passes. The count is aggregated in the\n        _num_observed_tokens attribute of the class.\n\n        Note: The batch_tensor is expected to have two dimensions\n            (batch_size * sequence_length, num_features). This is the\n            general shape expected by the forward pass of the expert\n            layers in a MOE model. If the input tensor does not have\n            two dimensions, the _num_observed_tokens attribute will be set\n            to None.\n        \"\"\"\n        if not isinstance(batch_tensor, Tensor):\n            raise ValueError(f\"Expected value to be a tensor, got {type(batch_tensor)}\")\n\n        if batch_tensor.ndim != 2:\n            logger.debug(\n                \"The input tensor is expected to have two dimensions \"\n                \"(batch_size * sequence_length, num_features). \"\n                f\"The input tensor has {batch_tensor.ndim} dimensions.\"\n            )\n            return\n\n        if self._num_observed_tokens is None:\n            # initialize the count\n            self._num_observed_tokens = 0\n\n        # batch_tensor (batch_size * sequence_length, num_features)\n        # observed_tokens (batch_size * sequence_length)\n        observed_tokens, _ = batch_tensor.shape\n        self._num_observed_tokens += observed_tokens\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer\n        \"\"\"\n        self._num_observed_tokens = None\n        self._scale = None\n        self._zero_point = None\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__} must implement calculate_qparams\")\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer.forward","title":"<code>forward(observed, g_idx=None)</code>","text":"<p>maps directly to get_qparams</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>optional observed tensor from which to calculate quantization parameters</p> required <code>g_idx</code> <code>Optional[Tensor]</code> <p>optional mapping from column index to group index</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point based on last observed value</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self, observed: Tensor, g_idx: Optional[Tensor] = None\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    maps directly to get_qparams\n    :param observed: optional observed tensor from which to calculate\n        quantization parameters\n    :param g_idx: optional mapping from column index to group index\n    :return: tuple of scale and zero point based on last observed value\n    \"\"\"\n    self.record_observed_tokens(observed)\n    return self.get_qparams(observed=observed, g_idx=g_idx)\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer.get_qparams","title":"<code>get_qparams(observed=None, g_idx=None)</code>","text":"<p>Convenience function to wrap overwritten calculate_qparams adds support to make observed tensor optional and support for tracking latest calculated scale and zero point</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Optional[Tensor]</code> <p>optional observed tensor to calculate quantization parameters from</p> <code>None</code> <code>g_idx</code> <code>Optional[Tensor]</code> <p>optional mapping from column index to group index</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point based on last observed value</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def get_qparams(\n    self,\n    observed: Optional[Tensor] = None,\n    g_idx: Optional[Tensor] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    Convenience function to wrap overwritten calculate_qparams\n    adds support to make observed tensor optional and support for tracking latest\n    calculated scale and zero point\n\n    :param observed: optional observed tensor to calculate quantization parameters\n        from\n    :param g_idx: optional mapping from column index to group index\n    :return: tuple of scale and zero point based on last observed value\n    \"\"\"\n    if observed is not None:\n        group_size = self.quantization_args.group_size\n\n        if self.quantization_args.strategy == QuantizationStrategy.TENSOR:\n            # re-calculate scale and zero point, update the stored value\n            self._scale, self._zero_point = self.calculate_qparams(observed)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.GROUP:\n            rows = observed.shape[0]\n            columns = observed.shape[1]\n            num_groups = int(ceil(columns / group_size))\n            self._scale = torch.empty(\n                (rows, num_groups), dtype=observed.dtype, device=observed.device\n            )\n            zp_dtype = self.quantization_args.pytorch_dtype()\n            self._zero_point = torch.empty(\n                (rows, num_groups), dtype=zp_dtype, device=observed.device\n            )\n\n            # support column-order (default) quantization as well as other orderings\n            # such as activation ordering. Below checks if g_idx has initialized\n            is_column_order = g_idx is None or -1 in g_idx\n            if is_column_order:\n                group_sizes = torch.full((num_groups,), group_size, dtype=torch.int)\n            else:\n                group_indices, group_sizes = torch.unique(g_idx, return_counts=True)\n                group_sizes = group_sizes[torch.argsort(group_indices)]\n\n                perm = torch.argsort(g_idx)\n                observed = safe_permute(observed, perm, dim=1)\n\n            # TODO: experiment with vectorizing for loop for performance\n            end = 0\n            for group_index, group_count in enumerate(group_sizes):\n                start = end\n                end = start + group_count\n                scale, zero_point = self.get_qparams_along_dim(\n                    observed[:, start:end],\n                    0,\n                    tensor_id=group_index,\n                )\n\n                self._scale[:, group_index] = scale.squeeze(1)\n                self._zero_point[:, group_index] = zero_point.squeeze(1)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:\n            # assume observed is transposed, because its the output, hence use dim 0\n            self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)\n\n        elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:\n            # use dim 1, assume the obsersed.shape = [batch, token, hidden]\n            # should be batch, token\n            self._scale, self._zero_point = self.get_qparams_along_dim(\n                observed,\n                dim={0, 1},\n            )\n\n    return self._scale, self._zero_point\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer.post_calculate_qparams","title":"<code>post_calculate_qparams()</code>","text":"<p>Run any logic specific to its observers after running calculate_qparams</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def post_calculate_qparams(self) -&gt; None:\n    \"\"\"\n    Run any logic specific to its observers after running calculate_qparams\n    \"\"\"\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer.record_observed_tokens","title":"<code>record_observed_tokens(batch_tensor)</code>","text":"<p>Counts the number of tokens observed during the forward passes. The count is aggregated in the _num_observed_tokens attribute of the class.</p> <p>Note: The batch_tensor is expected to have two dimensions     (batch_size * sequence_length, num_features). This is the     general shape expected by the forward pass of the expert     layers in a MOE model. If the input tensor does not have     two dimensions, the _num_observed_tokens attribute will be set     to None.</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def record_observed_tokens(self, batch_tensor: Tensor):\n    \"\"\"\n    Counts the number of tokens observed during the\n    forward passes. The count is aggregated in the\n    _num_observed_tokens attribute of the class.\n\n    Note: The batch_tensor is expected to have two dimensions\n        (batch_size * sequence_length, num_features). This is the\n        general shape expected by the forward pass of the expert\n        layers in a MOE model. If the input tensor does not have\n        two dimensions, the _num_observed_tokens attribute will be set\n        to None.\n    \"\"\"\n    if not isinstance(batch_tensor, Tensor):\n        raise ValueError(f\"Expected value to be a tensor, got {type(batch_tensor)}\")\n\n    if batch_tensor.ndim != 2:\n        logger.debug(\n            \"The input tensor is expected to have two dimensions \"\n            \"(batch_size * sequence_length, num_features). \"\n            f\"The input tensor has {batch_tensor.ndim} dimensions.\"\n        )\n        return\n\n    if self._num_observed_tokens is None:\n        # initialize the count\n        self._num_observed_tokens = 0\n\n    # batch_tensor (batch_size * sequence_length, num_features)\n    # observed_tokens (batch_size * sequence_length)\n    observed_tokens, _ = batch_tensor.shape\n    self._num_observed_tokens += observed_tokens\n</code></pre>"},{"location":"reference/llmcompressor/observers/base/#llmcompressor.observers.base.Observer.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer</p> Source code in <code>src/llmcompressor/observers/base.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer\n    \"\"\"\n    self._num_observed_tokens = None\n    self._scale = None\n    self._zero_point = None\n</code></pre>"},{"location":"reference/llmcompressor/observers/helpers/","title":"llmcompressor.observers.helpers","text":""},{"location":"reference/llmcompressor/observers/helpers/#llmcompressor.observers.helpers.get_observer_token_count","title":"<code>get_observer_token_count(module)</code>","text":"<p>Parse the module and return the number of tokens observed by each module's observer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to parse</p> required <p>Returns:</p> Type Description <code>Counter</code> <p>counter with the number of tokens observed by each observer</p> Source code in <code>src/llmcompressor/observers/helpers.py</code> <pre><code>def get_observer_token_count(module: torch.nn.Module) -&gt; Counter:\n    \"\"\"\n    Parse the module and return the number of tokens observed by\n    each module's observer.\n\n    :param module: module to parse\n    :return: counter with the number of tokens observed by each observer\n    \"\"\"\n    token_counts = Counter()\n    for name, module in module.named_modules():\n        if name.endswith(\".input_observer\"):\n            token_counts[name.replace(\".input_observer\", \"\")] = (\n                module._num_observed_tokens\n            )\n    return token_counts\n</code></pre>"},{"location":"reference/llmcompressor/observers/min_max/","title":"llmcompressor.observers.min_max","text":""},{"location":"reference/llmcompressor/observers/min_max/#llmcompressor.observers.min_max.MinMaxObserver","title":"<code>MinMaxObserver</code>","text":"<p>               Bases: <code>Observer</code></p> <p>Implements a quantization observer that calculates scale and zero point based on the minimum and maximum values of the tensor being observed. If averaging_constant is specified, then the scales are updated using a moving average</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>@Observer.register(\"minmax\")\nclass MinMaxObserver(Observer):\n    \"\"\"\n    Implements a quantization observer that calculates scale and zero point based on the\n    minimum and maximum values of the tensor being observed. If averaging_constant is\n    specified, then the scales are updated using a moving average\n    \"\"\"\n\n    def __init__(\n        self, quantization_args: QuantizationArgs, averaging_constant: float = 0.01\n    ):\n        super().__init__(quantization_args=quantization_args)\n\n        self.min_val = {}\n        self.max_val = {}\n        self.averaging_constant = averaging_constant\n\n    def calculate_qparams(\n        self,\n        observed: torch.Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n        tensor_id: Optional[Any] = None,\n    ) -&gt; Tuple[torch.FloatTensor, torch.IntTensor]:\n        \"\"\"\n        Updates the observed min and max using a moving average smoothed by the\n        averaging_constant. Set the averaging_constant to 1.0 to disable averaging.\n\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :param tensor_id: Optional id if different ranges of observed tensors are\n            passed, useful for sharding tensors by group_size\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        tensor_id = tensor_id or \"default\"\n\n        if not reduce_dims:\n            min_val, max_val = torch.aminmax(observed)\n        else:\n            min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n            max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n        # early stopping, save some computation and memory\n        if self.averaging_constant == 1.0:\n            return calculate_qparams(min_val, max_val, self.quantization_args)\n\n        running_min_val = self.min_val.get(tensor_id, None)\n        running_max_val = self.max_val.get(tensor_id, None)\n\n        if running_min_val is None or running_max_val is None:\n            updated_min_val = min_val\n            updated_max_val = max_val\n        else:\n            updated_min_val = running_min_val + self.averaging_constant * (\n                min_val - running_min_val\n            )\n            updated_max_val = running_max_val + self.averaging_constant * (\n                max_val - running_max_val\n            )\n\n        self.min_val[tensor_id] = updated_min_val\n        self.max_val[tensor_id] = updated_max_val\n\n        return calculate_qparams(\n            updated_min_val, updated_max_val, self.quantization_args\n        )\n\n    def get_qparams_along_dim(\n        self, observed: torch.Tensor, dim: int, tensor_id: Optional[Any] = None\n    ):\n        \"\"\"\n        Calculate quantization parameters along the specified dimension\n        \"\"\"\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer, including min and maximum values\n        \"\"\"\n        super().reset()\n        self.min_val = {}\n        self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/min_max/#llmcompressor.observers.min_max.MinMaxObserver.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None, tensor_id=None)</code>","text":"<p>Updates the observed min and max using a moving average smoothed by the averaging_constant. Set the averaging_constant to 1.0 to disable averaging.</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <code>tensor_id</code> <code>Optional[Any]</code> <p>Optional id if different ranges of observed tensors are passed, useful for sharding tensors by group_size</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: torch.Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n    tensor_id: Optional[Any] = None,\n) -&gt; Tuple[torch.FloatTensor, torch.IntTensor]:\n    \"\"\"\n    Updates the observed min and max using a moving average smoothed by the\n    averaging_constant. Set the averaging_constant to 1.0 to disable averaging.\n\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :param tensor_id: Optional id if different ranges of observed tensors are\n        passed, useful for sharding tensors by group_size\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    tensor_id = tensor_id or \"default\"\n\n    if not reduce_dims:\n        min_val, max_val = torch.aminmax(observed)\n    else:\n        min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n        max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n    # early stopping, save some computation and memory\n    if self.averaging_constant == 1.0:\n        return calculate_qparams(min_val, max_val, self.quantization_args)\n\n    running_min_val = self.min_val.get(tensor_id, None)\n    running_max_val = self.max_val.get(tensor_id, None)\n\n    if running_min_val is None or running_max_val is None:\n        updated_min_val = min_val\n        updated_max_val = max_val\n    else:\n        updated_min_val = running_min_val + self.averaging_constant * (\n            min_val - running_min_val\n        )\n        updated_max_val = running_max_val + self.averaging_constant * (\n            max_val - running_max_val\n        )\n\n    self.min_val[tensor_id] = updated_min_val\n    self.max_val[tensor_id] = updated_max_val\n\n    return calculate_qparams(\n        updated_min_val, updated_max_val, self.quantization_args\n    )\n</code></pre>"},{"location":"reference/llmcompressor/observers/min_max/#llmcompressor.observers.min_max.MinMaxObserver.get_qparams_along_dim","title":"<code>get_qparams_along_dim(observed, dim, tensor_id=None)</code>","text":"<p>Calculate quantization parameters along the specified dimension</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>def get_qparams_along_dim(\n    self, observed: torch.Tensor, dim: int, tensor_id: Optional[Any] = None\n):\n    \"\"\"\n    Calculate quantization parameters along the specified dimension\n    \"\"\"\n    reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)\n    return self.calculate_qparams(\n        observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n    )\n</code></pre>"},{"location":"reference/llmcompressor/observers/min_max/#llmcompressor.observers.min_max.MinMaxObserver.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer, including min and maximum values</p> Source code in <code>src/llmcompressor/observers/min_max.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer, including min and maximum values\n    \"\"\"\n    super().reset()\n    self.min_val = {}\n    self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/mse/","title":"llmcompressor.observers.mse","text":""},{"location":"reference/llmcompressor/observers/mse/#llmcompressor.observers.mse.MovingAverageMSEObserver","title":"<code>MovingAverageMSEObserver</code>","text":"<p>               Bases: <code>Observer</code></p> <p>Implements a dynamic quantization observer that sets the scale and zero point based on a moving average of the mse-clipped min and max observed values</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>@Observer.register(\"mse\")\nclass MovingAverageMSEObserver(Observer):\n    \"\"\"\n    Implements a dynamic quantization observer that sets the scale and\n    zero point based on a moving average of the mse-clipped min and max observed values\n    \"\"\"\n\n    def __init__(\n        self,\n        quantization_args: QuantizationArgs,\n        averaging_constant: float = 0.01,\n        grid: float = 100.0,\n        maxshrink: float = 0.80,\n        norm: float = 2.4,\n    ):\n        super().__init__(quantization_args=quantization_args)\n\n        self.min_val = {}\n        self.max_val = {}\n        self.averaging_constant = averaging_constant\n        self.grid = grid\n        self.maxshrink = maxshrink\n        self.norm = norm\n\n    def calculate_mse_min_max(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n    ):\n        \"\"\"\n        Computes the mse-clipped min and max values of the observed tensor by\n        optimizing for quantization error\n\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned values will be shaped (1,) along the reduced dimensions\n        :return: tuple of min and max values derived from the observed tensor\n        \"\"\"\n        from compressed_tensors.quantization.lifecycle import fake_quantize\n\n        if not reduce_dims:\n            absolute_min_val, absolute_max_val = torch.aminmax(observed)\n        else:\n            absolute_min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n            absolute_max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n        best = torch.full_like(\n            absolute_min_val, torch.finfo(absolute_min_val.dtype).max\n        )\n        min_val = torch.ones_like(absolute_min_val)\n        max_val = torch.zeros_like(absolute_max_val)\n        for i in range(int(self.maxshrink * self.grid)):\n            p = 1 - i / self.grid\n            shrinked_min_val = p * absolute_min_val\n            shrinked_max_val = p * absolute_max_val\n\n            candidate_scales, candidate_zero_points = calculate_qparams(\n                shrinked_min_val, shrinked_max_val, self.quantization_args\n            )\n            q = fake_quantize(\n                observed,\n                candidate_scales,\n                candidate_zero_points,\n                self.quantization_args,\n            )\n\n            q -= observed\n            q.abs_()\n            q.pow_(self.norm)\n            if not reduce_dims:\n                err = torch.sum(q)\n            else:\n                err = torch.sum(q, reduce_dims, keepdims=True)\n\n            tmp = err &lt; best\n            if torch.any(tmp):\n                best[tmp] = err[tmp]\n                min_val[tmp] = shrinked_min_val[tmp]\n                max_val[tmp] = shrinked_max_val[tmp]\n        return min_val, max_val\n\n    def calculate_qparams(\n        self,\n        observed: Tensor,\n        reduce_dims: Optional[Tuple[int]] = None,\n        tensor_id: Optional[Any] = None,\n    ) -&gt; Tuple[FloatTensor, IntTensor]:\n        \"\"\"\n        Updates the mse-clipped min and max values of the observed tensor using\n        a moving average smoothed by the averaging_constant\n\n        :param observed: observed tensor to calculate quantization parameters for\n        :param reduce_dims: optional tuple of dimensions to reduce along,\n            returned scale and zero point will be shaped (1,) along the\n            reduced dimensions\n        :param tensor_id: Optional id if different ranges of observed tensors are\n            passed, useful for sharding tensors by group_size\n        :return: tuple of scale and zero point derived from the observed tensor\n        \"\"\"\n        min_val, max_val = self.calculate_mse_min_max(observed, reduce_dims)\n\n        running_min_val = self.min_val.get(tensor_id, None)\n        running_max_val = self.max_val.get(tensor_id, None)\n\n        if running_min_val is None or running_max_val is None:\n            updated_min_val = min_val\n            updated_max_val = max_val\n        else:\n            updated_min_val = running_min_val + self.averaging_constant * (\n                min_val - running_min_val\n            )\n            updated_max_val = running_max_val + self.averaging_constant * (\n                max_val - running_max_val\n            )\n\n        tensor_id = tensor_id or \"default\"\n        self.min_val[tensor_id] = updated_min_val\n        self.max_val[tensor_id] = updated_max_val\n\n        return calculate_qparams(\n            updated_min_val, updated_max_val, self.quantization_args\n        )\n\n    def get_qparams_along_dim(\n        self, observed, dim: int, tensor_id: Optional[Any] = None\n    ):\n        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)\n        return self.calculate_qparams(\n            observed, reduce_dims=reduce_dims, tensor_id=tensor_id\n        )\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the observer, including min and maximum values\n        \"\"\"\n        super().reset()\n        self.min_val = {}\n        self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/observers/mse/#llmcompressor.observers.mse.MovingAverageMSEObserver.calculate_mse_min_max","title":"<code>calculate_mse_min_max(observed, reduce_dims=None)</code>","text":"<p>Computes the mse-clipped min and max values of the observed tensor by optimizing for quantization error</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned values will be shaped (1,) along the reduced dimensions</p> <code>None</code> <p>Returns:</p> Type Description <p>tuple of min and max values derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>def calculate_mse_min_max(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n):\n    \"\"\"\n    Computes the mse-clipped min and max values of the observed tensor by\n    optimizing for quantization error\n\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned values will be shaped (1,) along the reduced dimensions\n    :return: tuple of min and max values derived from the observed tensor\n    \"\"\"\n    from compressed_tensors.quantization.lifecycle import fake_quantize\n\n    if not reduce_dims:\n        absolute_min_val, absolute_max_val = torch.aminmax(observed)\n    else:\n        absolute_min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)\n        absolute_max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)\n\n    best = torch.full_like(\n        absolute_min_val, torch.finfo(absolute_min_val.dtype).max\n    )\n    min_val = torch.ones_like(absolute_min_val)\n    max_val = torch.zeros_like(absolute_max_val)\n    for i in range(int(self.maxshrink * self.grid)):\n        p = 1 - i / self.grid\n        shrinked_min_val = p * absolute_min_val\n        shrinked_max_val = p * absolute_max_val\n\n        candidate_scales, candidate_zero_points = calculate_qparams(\n            shrinked_min_val, shrinked_max_val, self.quantization_args\n        )\n        q = fake_quantize(\n            observed,\n            candidate_scales,\n            candidate_zero_points,\n            self.quantization_args,\n        )\n\n        q -= observed\n        q.abs_()\n        q.pow_(self.norm)\n        if not reduce_dims:\n            err = torch.sum(q)\n        else:\n            err = torch.sum(q, reduce_dims, keepdims=True)\n\n        tmp = err &lt; best\n        if torch.any(tmp):\n            best[tmp] = err[tmp]\n            min_val[tmp] = shrinked_min_val[tmp]\n            max_val[tmp] = shrinked_max_val[tmp]\n    return min_val, max_val\n</code></pre>"},{"location":"reference/llmcompressor/observers/mse/#llmcompressor.observers.mse.MovingAverageMSEObserver.calculate_qparams","title":"<code>calculate_qparams(observed, reduce_dims=None, tensor_id=None)</code>","text":"<p>Updates the mse-clipped min and max values of the observed tensor using a moving average smoothed by the averaging_constant</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>Tensor</code> <p>observed tensor to calculate quantization parameters for</p> required <code>reduce_dims</code> <code>Optional[Tuple[int]]</code> <p>optional tuple of dimensions to reduce along, returned scale and zero point will be shaped (1,) along the reduced dimensions</p> <code>None</code> <code>tensor_id</code> <code>Optional[Any]</code> <p>Optional id if different ranges of observed tensors are passed, useful for sharding tensors by group_size</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[FloatTensor, IntTensor]</code> <p>tuple of scale and zero point derived from the observed tensor</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>def calculate_qparams(\n    self,\n    observed: Tensor,\n    reduce_dims: Optional[Tuple[int]] = None,\n    tensor_id: Optional[Any] = None,\n) -&gt; Tuple[FloatTensor, IntTensor]:\n    \"\"\"\n    Updates the mse-clipped min and max values of the observed tensor using\n    a moving average smoothed by the averaging_constant\n\n    :param observed: observed tensor to calculate quantization parameters for\n    :param reduce_dims: optional tuple of dimensions to reduce along,\n        returned scale and zero point will be shaped (1,) along the\n        reduced dimensions\n    :param tensor_id: Optional id if different ranges of observed tensors are\n        passed, useful for sharding tensors by group_size\n    :return: tuple of scale and zero point derived from the observed tensor\n    \"\"\"\n    min_val, max_val = self.calculate_mse_min_max(observed, reduce_dims)\n\n    running_min_val = self.min_val.get(tensor_id, None)\n    running_max_val = self.max_val.get(tensor_id, None)\n\n    if running_min_val is None or running_max_val is None:\n        updated_min_val = min_val\n        updated_max_val = max_val\n    else:\n        updated_min_val = running_min_val + self.averaging_constant * (\n            min_val - running_min_val\n        )\n        updated_max_val = running_max_val + self.averaging_constant * (\n            max_val - running_max_val\n        )\n\n    tensor_id = tensor_id or \"default\"\n    self.min_val[tensor_id] = updated_min_val\n    self.max_val[tensor_id] = updated_max_val\n\n    return calculate_qparams(\n        updated_min_val, updated_max_val, self.quantization_args\n    )\n</code></pre>"},{"location":"reference/llmcompressor/observers/mse/#llmcompressor.observers.mse.MovingAverageMSEObserver.reset","title":"<code>reset()</code>","text":"<p>Reset the state of the observer, including min and maximum values</p> Source code in <code>src/llmcompressor/observers/mse.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the state of the observer, including min and maximum values\n    \"\"\"\n    super().reset()\n    self.min_val = {}\n    self.max_val = {}\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/","title":"llmcompressor.pipelines","text":""},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.BasicPipeline","title":"<code>BasicPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/basic/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"basic\")\nclass BasicPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: Union[\"DatasetArguments\", None],\n    ):\n        \"\"\"\n        Run a basic data pipeline.\n\n        Batches are fetched from the data loader and are used to perform forward passes\n        through the model. This pipeline is typically used for basic model calibration\n        and, unlike the sequential pipelines, does not propagate compression error when\n        used to calibrate model compression\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        model_device = get_execution_device(model)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model):\n            for batch in tqdm.tqdm(dataloader, desc=\"Calibrating\"):\n                batch = apply_pad_mask_to_batch(batch)\n                batch = tensors_to_device(batch, model_device)\n                model(**batch)\n\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.BasicPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a basic data pipeline.</p> <p>Batches are fetched from the data loader and are used to perform forward passes through the model. This pipeline is typically used for basic model calibration and, unlike the sequential pipelines, does not propagate compression error when used to calibrate model compression</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>Union[DatasetArguments, None]</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/basic/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    dataset_args: Union[\"DatasetArguments\", None],\n):\n    \"\"\"\n    Run a basic data pipeline.\n\n    Batches are fetched from the data loader and are used to perform forward passes\n    through the model. This pipeline is typically used for basic model calibration\n    and, unlike the sequential pipelines, does not propagate compression error when\n    used to calibrate model compression\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    model_device = get_execution_device(model)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model):\n        for batch in tqdm.tqdm(dataloader, desc=\"Calibrating\"):\n            batch = apply_pad_mask_to_batch(batch)\n            batch = tensors_to_device(batch, model_device)\n            model(**batch)\n\n    LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.CalibrationPipeline","title":"<code>CalibrationPipeline</code>","text":"<p>               Bases: <code>ABC</code>, <code>RegistryMixin</code></p> Source code in <code>src/llmcompressor/pipelines/registry.py</code> <pre><code>class CalibrationPipeline(ABC, RegistryMixin):\n    @staticmethod\n    @abstractmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: \"DatasetArguments\",\n    ):\n        raise NotImplementedError()\n\n    @classmethod\n    def from_modifiers(\n        cls, modifiers: List[Modifier], user: Optional[str] = None\n    ) -&gt; \"CalibrationPipeline\":\n        \"\"\"\n        Infer which calibration pipeline to use based on the available modifiers and\n        any user specifications\n\n        :param modifiers: modifiers to apply to model\n        :param user: pipeline name passed by user\n        :return: CalibrationPipeline instance to be called with data (if not datafree)\n        \"\"\"\n        user = standardize_lookup_name(user) if user else None\n        inferred = standardize_lookup_name(cls._validate_infer_pipeline(modifiers))\n        independent = standardize_lookup_name(\"independent\")\n\n        if user == independent:\n            inferred = independent\n\n        if user is not None and user != inferred:\n            logger.warning(\n                f\"Calibration pipeline is set to `{user}`, but it is recommended to \"\n                f\"use `{inferred}`\"\n            )\n\n        pipeline = user or inferred\n        return cls.load_from_registry(pipeline)\n\n    @staticmethod\n    def _validate_infer_pipeline(modifiers: List[Modifier]) -&gt; str:\n        if any(isinstance(modifier, AWQModifier) for modifier in modifiers):\n            if len(modifiers) &gt; 1:\n                logger.warning(\n                    \"AWQ does not currently support sharing a data pipeline with other \"\n                    \"modifiers. Inferring `independent` calibration pipeline\"\n                )\n                return \"independent\"\n            return \"datafree\"\n\n        if any(isinstance(modifier, SEQUENTIAL_MODIFIERS) for modifier in modifiers):\n            return \"sequential\"\n\n        active_qmods = _get_active_quant_modifiers(modifiers)\n        if len(active_qmods) &gt; 1:\n            raise ValueError(\n                f\"Recipe contains more than one active quantization config \"\n                f\"({active_qmods}). These configs may be conflicting, Please modify \"\n                \"your recipe to use at most one quantization config\"\n            )\n\n        if len(active_qmods) == 1:\n            quant_modifier = active_qmods[0]\n            config = quant_modifier.resolve_quantization_config()\n            if config.requires_calibration_data():\n                return \"basic\"\n            else:\n                return \"datafree\"\n\n        if any(isinstance(modifier, SmoothQuantModifier) for modifier in modifiers):\n            return \"basic\"\n\n        return \"datafree\"\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.CalibrationPipeline.from_modifiers","title":"<code>from_modifiers(modifiers, user=None)</code>  <code>classmethod</code>","text":"<p>Infer which calibration pipeline to use based on the available modifiers and any user specifications</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <code>List[Modifier]</code> <p>modifiers to apply to model</p> required <code>user</code> <code>Optional[str]</code> <p>pipeline name passed by user</p> <code>None</code> <p>Returns:</p> Type Description <code>CalibrationPipeline</code> <p>CalibrationPipeline instance to be called with data (if not datafree)</p> Source code in <code>src/llmcompressor/pipelines/registry.py</code> <pre><code>@classmethod\ndef from_modifiers(\n    cls, modifiers: List[Modifier], user: Optional[str] = None\n) -&gt; \"CalibrationPipeline\":\n    \"\"\"\n    Infer which calibration pipeline to use based on the available modifiers and\n    any user specifications\n\n    :param modifiers: modifiers to apply to model\n    :param user: pipeline name passed by user\n    :return: CalibrationPipeline instance to be called with data (if not datafree)\n    \"\"\"\n    user = standardize_lookup_name(user) if user else None\n    inferred = standardize_lookup_name(cls._validate_infer_pipeline(modifiers))\n    independent = standardize_lookup_name(\"independent\")\n\n    if user == independent:\n        inferred = independent\n\n    if user is not None and user != inferred:\n        logger.warning(\n            f\"Calibration pipeline is set to `{user}`, but it is recommended to \"\n            f\"use `{inferred}`\"\n        )\n\n    pipeline = user or inferred\n    return cls.load_from_registry(pipeline)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.DataFreePipeline","title":"<code>DataFreePipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/data_free/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"datafree\")\nclass DataFreePipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: Optional[DataLoader],\n        dataset_args: \"DatasetArguments\",\n    ):\n        \"\"\"\n        A pipeline for data-free calibration\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        LifecycleCallbacks.calibration_epoch_start()\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.DataFreePipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>A pipeline for data-free calibration</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>Optional[DataLoader]</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/data_free/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: Optional[DataLoader],\n    dataset_args: \"DatasetArguments\",\n):\n    \"\"\"\n    A pipeline for data-free calibration\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    LifecycleCallbacks.calibration_epoch_start()\n    LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.IndependentPipeline","title":"<code>IndependentPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/independent/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"independent\")\nclass IndependentPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: \"DatasetArguments\",\n    ):\n        \"\"\"\n        Data pipeline where each modifier is assigned its own calibration epoch and data\n        pipeline\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        _logger = logger.patch(lambda r: r.update(function=\"IndependentPipeline\"))\n\n        session = active_session()\n        modifiers = session.get_modifiers()\n        with patch_attr(session.lifecycle, \"modifiers\", None):\n            for index, modifier in enumerate(modifiers):\n                mod_type = str(type(modifier).__name__)\n                session.lifecycle.modifiers = [\n                    StageModifiers(modifiers=[modifier], group=mod_type, index=index)\n                ]\n\n                pipeline = CalibrationPipeline.from_modifiers([modifier])\n                pipeline_name = pipeline.__class__.__name__\n                _logger.info(f\"Inferred `{pipeline_name}` for `{mod_type}`\")\n\n                pipeline(model, dataloader, dataset_args)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.IndependentPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Data pipeline where each modifier is assigned its own calibration epoch and data pipeline</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/independent/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    dataset_args: \"DatasetArguments\",\n):\n    \"\"\"\n    Data pipeline where each modifier is assigned its own calibration epoch and data\n    pipeline\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    _logger = logger.patch(lambda r: r.update(function=\"IndependentPipeline\"))\n\n    session = active_session()\n    modifiers = session.get_modifiers()\n    with patch_attr(session.lifecycle, \"modifiers\", None):\n        for index, modifier in enumerate(modifiers):\n            mod_type = str(type(modifier).__name__)\n            session.lifecycle.modifiers = [\n                StageModifiers(modifiers=[modifier], group=mod_type, index=index)\n            ]\n\n            pipeline = CalibrationPipeline.from_modifiers([modifier])\n            pipeline_name = pipeline.__class__.__name__\n            _logger.info(f\"Inferred `{pipeline_name}` for `{mod_type}`\")\n\n            pipeline(model, dataloader, dataset_args)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.LayerSequentialPipeline","title":"<code>LayerSequentialPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/layer_sequential/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"layer_sequential\")\nclass LayerSequentialPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n    ):\n        \"\"\"\n        Run a layer-wise sequential data pipeline according to the following steps:\n\n        1. Layers are identified according to `sequential_targets`\n        2. A hook is attached to the first layer. This hook raises an exception which is\n            then caught and used to capture the input arguments to the first layer\n        3. The inputs to the first layer are used to calibrate the first layer, and the\n            output of the previous layer is used as inputs to calibrate the next layer\n\n        This pipeline requires that the model have distinct layers defined in its\n        architecture and that the outputs of the previous layer are exactly the inputs\n        to the next layer. This is violated by encoder-decoder architectures, among\n        others.\n\n        If your model architecture violates these assumptions, consider using the\n        sequential pipeline (see llmcompressor.pipelines.sequential). Architectures\n        which are known to fail these assumptions include GPT-J and most vision models\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        session = active_session()\n\n        # find layers\n        modifiers = session.get_modifiers()\n        sequential_targets, _ = get_targets_from_modifiers(modifiers, model)\n        layers = match_modules(model, sequential_targets)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model), DisableQuantization(model):\n            # prepare intermediates cache\n            intermediates: IntermediatesCache = capture_first_layer_intermediates(\n                model, layers[0], dataloader\n            )\n\n            num_layers = len(layers)\n            for layer_index, layer in enumerate(layers):\n                # prepare tqdm description texts\n                calib_desc = f\"({layer_index + 1}/{num_layers}): Calibrating\"\n                prop_desc = f\"({layer_index + 1}/{num_layers}): Propagating\"\n\n                # do an preliminary pass to trigger modifier hooks\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                    inputs = intermediates.fetch(batch_idx)\n                    layer(**inputs)\n\n                # trigger compression\n                LifecycleCallbacks.sequential_epoch_end()\n\n                # this pass does not trigger modifier hooks\n                # and is only used for capturing outputs from newly compressed modules\n                with HooksMixin.disable_hooks():\n                    for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                        inputs = intermediates.fetch(batch_idx)\n                        output = layer(**inputs)\n\n                        if layer_index &lt; num_layers - 1:\n                            next_layer = layers[layer_index + 1]\n                            output = to_next_layer_kwargs(output, next_layer)\n                            output = maybe_inject_pos_embeddings(\n                                output, next_layer, inputs\n                            )\n\n                            intermediates.delete(batch_idx)\n                            intermediates.update(batch_idx, output)\n\n            # redudant, finish any remaining compression\n            LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.LayerSequentialPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a layer-wise sequential data pipeline according to the following steps:</p> <ol> <li>Layers are identified according to <code>sequential_targets</code></li> <li>A hook is attached to the first layer. This hook raises an exception which is     then caught and used to capture the input arguments to the first layer</li> <li>The inputs to the first layer are used to calibrate the first layer, and the     output of the previous layer is used as inputs to calibrate the next layer</li> </ol> <p>This pipeline requires that the model have distinct layers defined in its architecture and that the outputs of the previous layer are exactly the inputs to the next layer. This is violated by encoder-decoder architectures, among others.</p> <p>If your model architecture violates these assumptions, consider using the sequential pipeline (see llmcompressor.pipelines.sequential). Architectures which are known to fail these assumptions include GPT-J and most vision models</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/layer_sequential/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n):\n    \"\"\"\n    Run a layer-wise sequential data pipeline according to the following steps:\n\n    1. Layers are identified according to `sequential_targets`\n    2. A hook is attached to the first layer. This hook raises an exception which is\n        then caught and used to capture the input arguments to the first layer\n    3. The inputs to the first layer are used to calibrate the first layer, and the\n        output of the previous layer is used as inputs to calibrate the next layer\n\n    This pipeline requires that the model have distinct layers defined in its\n    architecture and that the outputs of the previous layer are exactly the inputs\n    to the next layer. This is violated by encoder-decoder architectures, among\n    others.\n\n    If your model architecture violates these assumptions, consider using the\n    sequential pipeline (see llmcompressor.pipelines.sequential). Architectures\n    which are known to fail these assumptions include GPT-J and most vision models\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    session = active_session()\n\n    # find layers\n    modifiers = session.get_modifiers()\n    sequential_targets, _ = get_targets_from_modifiers(modifiers, model)\n    layers = match_modules(model, sequential_targets)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model), DisableQuantization(model):\n        # prepare intermediates cache\n        intermediates: IntermediatesCache = capture_first_layer_intermediates(\n            model, layers[0], dataloader\n        )\n\n        num_layers = len(layers)\n        for layer_index, layer in enumerate(layers):\n            # prepare tqdm description texts\n            calib_desc = f\"({layer_index + 1}/{num_layers}): Calibrating\"\n            prop_desc = f\"({layer_index + 1}/{num_layers}): Propagating\"\n\n            # do an preliminary pass to trigger modifier hooks\n            for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                inputs = intermediates.fetch(batch_idx)\n                layer(**inputs)\n\n            # trigger compression\n            LifecycleCallbacks.sequential_epoch_end()\n\n            # this pass does not trigger modifier hooks\n            # and is only used for capturing outputs from newly compressed modules\n            with HooksMixin.disable_hooks():\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                    inputs = intermediates.fetch(batch_idx)\n                    output = layer(**inputs)\n\n                    if layer_index &lt; num_layers - 1:\n                        next_layer = layers[layer_index + 1]\n                        output = to_next_layer_kwargs(output, next_layer)\n                        output = maybe_inject_pos_embeddings(\n                            output, next_layer, inputs\n                        )\n\n                        intermediates.delete(batch_idx)\n                        intermediates.update(batch_idx, output)\n\n        # redudant, finish any remaining compression\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"sequential\")\nclass SequentialPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n    ):\n        \"\"\"\n        Run a sequential data pipeline according to the following steps:\n\n        1. The model is partitioned into subgraphs according to `sequential_targets`\n        2. Data passes through each subgraph sequentially. Data is passed through each\n            subgraph twice, once to trigger calibration hooks, then a second time in\n            order to capture activations after quantization has occurred through hooks.\n        3. The intermediate activations between each subgraph are cached and offloaded\n            to the cpu between each batch in order to save memory\n\n        This pipeline requires that the model be traceable with respect to data from the\n        data loader. This may be an issue for vision models with vision datasets, due\n        to specialized input processing in the model.\n\n        In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A\n        model can be made traceable by wrapping the untraceable functions (see\n        llmcompressor.transformers.tracing)\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        session = active_session()\n\n        # infer sequential targets\n        modifiers = session.get_modifiers()\n        sequential_targets, ignore = get_targets_from_modifiers(modifiers, model)\n\n        # trace subgraphs\n        sample_input = next(iter(dataloader))\n        subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model), DisableQuantization(model):\n            # prepare intermediates cache\n            model_device = get_execution_device(model)\n            intermediates = IntermediatesCache.from_dataloader(dataloader, model_device)\n\n            num_subgraphs = len(subgraphs)\n            for subgraph_index, subgraph in enumerate(subgraphs):\n                # prepare tqdm description texts\n                calib_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Calibrating\"\n                prop_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Propagating\"\n\n                # do an preliminary pass to trigger modifier hooks\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                    inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                    subgraph.forward(model, **inputs)\n\n                # trigger compression\n                LifecycleCallbacks.sequential_epoch_end()\n\n                # this pass does not trigger modifier hooks\n                # and is only used for capturing outputs from newly compressed modules\n                with HooksMixin.disable_hooks():\n                    for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                        inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                        output = subgraph.forward(model, **inputs)\n\n                        if subgraph_index &lt; num_subgraphs - 1:\n                            intermediates.update(batch_idx, output)\n                            intermediates.delete(batch_idx, subgraph.consumed_names)\n\n            # redudant, finish any remaining compression\n            LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.SequentialPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a sequential data pipeline according to the following steps:</p> <ol> <li>The model is partitioned into subgraphs according to <code>sequential_targets</code></li> <li>Data passes through each subgraph sequentially. Data is passed through each     subgraph twice, once to trigger calibration hooks, then a second time in     order to capture activations after quantization has occurred through hooks.</li> <li>The intermediate activations between each subgraph are cached and offloaded     to the cpu between each batch in order to save memory</li> </ol> <p>This pipeline requires that the model be traceable with respect to data from the data loader. This may be an issue for vision models with vision datasets, due to specialized input processing in the model.</p> <p>In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A model can be made traceable by wrapping the untraceable functions (see llmcompressor.transformers.tracing)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n):\n    \"\"\"\n    Run a sequential data pipeline according to the following steps:\n\n    1. The model is partitioned into subgraphs according to `sequential_targets`\n    2. Data passes through each subgraph sequentially. Data is passed through each\n        subgraph twice, once to trigger calibration hooks, then a second time in\n        order to capture activations after quantization has occurred through hooks.\n    3. The intermediate activations between each subgraph are cached and offloaded\n        to the cpu between each batch in order to save memory\n\n    This pipeline requires that the model be traceable with respect to data from the\n    data loader. This may be an issue for vision models with vision datasets, due\n    to specialized input processing in the model.\n\n    In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A\n    model can be made traceable by wrapping the untraceable functions (see\n    llmcompressor.transformers.tracing)\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    session = active_session()\n\n    # infer sequential targets\n    modifiers = session.get_modifiers()\n    sequential_targets, ignore = get_targets_from_modifiers(modifiers, model)\n\n    # trace subgraphs\n    sample_input = next(iter(dataloader))\n    subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model), DisableQuantization(model):\n        # prepare intermediates cache\n        model_device = get_execution_device(model)\n        intermediates = IntermediatesCache.from_dataloader(dataloader, model_device)\n\n        num_subgraphs = len(subgraphs)\n        for subgraph_index, subgraph in enumerate(subgraphs):\n            # prepare tqdm description texts\n            calib_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Calibrating\"\n            prop_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Propagating\"\n\n            # do an preliminary pass to trigger modifier hooks\n            for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                subgraph.forward(model, **inputs)\n\n            # trigger compression\n            LifecycleCallbacks.sequential_epoch_end()\n\n            # this pass does not trigger modifier hooks\n            # and is only used for capturing outputs from newly compressed modules\n            with HooksMixin.disable_hooks():\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                    inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                    output = subgraph.forward(model, **inputs)\n\n                    if subgraph_index &lt; num_subgraphs - 1:\n                        intermediates.update(batch_idx, output)\n                        intermediates.delete(batch_idx, subgraph.consumed_names)\n\n        # redudant, finish any remaining compression\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/#llmcompressor.pipelines.get_targets_from_modifiers","title":"<code>get_targets_from_modifiers(modifiers, model)</code>","text":"<p>Infer sequential targets and ignore list from modifiers list</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>model being calibrated</p> required <code>modifiers</code> <code>List[Modifier]</code> <p>list of modifiers being applied during calibration</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>list of sequential targets and list of modules to ignore for tracing</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def get_targets_from_modifiers(\n    modifiers: List[Modifier], model: PreTrainedModel\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Infer sequential targets and ignore list from modifiers list\n\n    :param model: model being calibrated\n    :param modifiers: list of modifiers being applied during calibration\n    :return: list of sequential targets and list of modules to ignore for tracing\n    \"\"\"\n    # avoid circular import\n    from llmcompressor.pipelines.registry import SEQUENTIAL_MODIFIERS\n\n    sequential_modifiers = [\n        modifier for modifier in modifiers if isinstance(modifier, SEQUENTIAL_MODIFIERS)\n    ]\n\n    if len(sequential_modifiers) &gt;= 2:\n        types = [type(modifier) for modifier in sequential_modifiers]\n        logger.warning(\n            \"Cannot infer sequential targets from multiple sequential modifiers \"\n            f\"({types}). Defaulting to {types[0]}\"\n        )\n    elif len(sequential_modifiers) &lt;= 0:\n        types = [type(modifier) for modifier in modifiers]\n        raise ValueError(f\"Cannot infer sequential targets from list of {types}\")\n\n    modifier = sequential_modifiers[0]\n\n    # infer sequential targets\n    if modifier.sequential_targets is None:\n        sequential_targets = get_no_split_params(model)\n    elif isinstance(modifier.sequential_targets, str):\n        sequential_targets = [modifier.sequential_targets]\n    else:\n        sequential_targets = modifier.sequential_targets\n\n    return sequential_targets, modifier.ignore\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/","title":"llmcompressor.pipelines.cache","text":""},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediateValue","title":"<code>IntermediateValue</code>  <code>dataclass</code>","text":"<p>Dataclass which recursively defines offloaded values and which device to onload to</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[Tensor, IntermediateValue, Any]</code> <p>either an offloaded Tensor, an primative value, or a recursable value</p> required <code>device</code> <code>Union[device, None]</code> <p>if the value is a Tensor, then the device to onload the tensor to, otherwise None</p> required Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>@dataclass\nclass IntermediateValue:\n    \"\"\"\n    Dataclass which recursively defines offloaded values and which device to onload to\n\n    :param value: either an offloaded Tensor, an primative value, or a recursable value\n    :param device: if the value is a Tensor, then the device to onload the tensor to,\n        otherwise None\n    \"\"\"\n\n    value: Union[torch.Tensor, \"IntermediateValue\", Any]\n    device: Union[torch.device, None]\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediatesCache","title":"<code>IntermediatesCache</code>","text":"<p>Cache which stores intermediate values (activations) produced by batched, sequential execution of models. Values are offloaded to the <code>offload_device</code> when stored in the cache and onloaded to their original device when fetched from the cache</p> <p>Currently supports nested offloading of dataclass instances and tuples</p> <p>Construct using <code>empty</code> and <code>from_dataloader</code> class methods</p> Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>class IntermediatesCache:\n    \"\"\"\n    Cache which stores intermediate values (activations) produced by batched, sequential\n    execution of models. Values are offloaded to the `offload_device` when stored in\n    the cache and onloaded to their original device when fetched from the cache\n\n    Currently supports nested offloading of dataclass instances and tuples\n\n    Construct using `empty` and `from_dataloader` class methods\n    \"\"\"\n\n    batch_intermediates: List[Dict[str, IntermediateValue]]\n    offload_device: torch.device\n\n    def __init__(\n        self,\n        batch_intermediates: List[Dict[str, IntermediateValue]],\n        offload_device: torch.device,\n    ):\n        self.batch_intermediates = batch_intermediates\n        self.offload_device = offload_device\n\n    @classmethod\n    def empty(cls, num_batches: int, offload_device: torch.device):\n        \"\"\"\n        Construct an empty cache\n\n        :param num_batches: the expected number of batches to be stored\n        :param offload_device: device to offload values to\n        \"\"\"\n        batch_intermediates = [{} for _ in range(num_batches)]\n        return cls(batch_intermediates, offload_device)\n\n    @classmethod\n    def from_dataloader(\n        cls,\n        dataloader: torch.utils.data.DataLoader,\n        model_device: torch.device,\n        mask_padding: bool = True,\n        offload_device: torch.device = torch.device(\"cpu\"),\n    ):\n        \"\"\"\n        Initialize a cache with data from the provided dataloader\n\n        :param dataloader: dataloader which generates values to be cached\n        :param model_device: device which values will be onloaded to when fetched\n        :param mask_padding: zero out padding tokens if True. This affects modifiers\n            such as GPTQ and SparseGPT\n        :param offload_device: device to offload values to\n        \"\"\"\n        # note: list comprehesion was found to not improve performance\n        batch_intermediates = []\n        for batch in tqdm.tqdm(dataloader, desc=\"Preparing intermediates cache\"):\n            intermediate = {}\n            for key, value in batch.items():\n                if mask_padding and key == \"input_ids\":\n                    value = cls._mask_padding(value, batch[\"attention_mask\"])\n                intermediate[key] = IntermediateValue(value=value, device=model_device)\n\n            batch_intermediates.append(intermediate)\n\n        return cls(batch_intermediates, offload_device)\n\n    def fetch(\n        self, batch_index: int, input_names: Optional[List[str]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Fetch values belonging to a batch\n\n        :param batch_index: index of batch whose values are being fetched\n        :param input_names: list of keys whose values are being fetched\n        :return: dictionary mapping keys to onloaded values\n        \"\"\"\n        intermediates = self.batch_intermediates[batch_index]\n\n        return {\n            key: self._onload_value(subgraph_input)\n            for key, subgraph_input in intermediates.items()\n            if input_names is None or key in input_names\n        }\n\n    def update(self, batch_index: int, values: Dict[str, Any]):\n        \"\"\"\n        Update/put values belonging to a batch\n\n        :param batch_index: index of batch whose values will be updated\n        :param values: dictionary mapping keys to values used for update\n        \"\"\"\n        intermediates = {k: self._offload_value(v) for k, v in values.items()}\n        self.batch_intermediates[batch_index].update(intermediates)\n\n    def delete(self, batch_index: int, consumed_names: Optional[List[str]] = None):\n        \"\"\"\n        Delete values from the cache\n\n        :param batch_index: index of batch whose values will be deleted\n        :param consumed_names: list of keys whose values will be deleted, defaults to\n            removing all keys\n        \"\"\"\n        intermediates = self.batch_intermediates[batch_index]\n\n        if consumed_names is None:\n            consumed_names = list(intermediates.keys())\n\n        for name in consumed_names:\n            del intermediates[name]\n\n    def _onload_value(self, intermediate: IntermediateValue) -&gt; Any:\n        value = intermediate.value\n        device = intermediate.device\n\n        if isinstance(value, torch.Tensor):\n            return value.to(device=device)\n\n        if is_dataclass(value):\n            for field in fields(value):  # `asdict` is recursive, not applicable here\n                v = getattr(value, field.name)\n                setattr(value, field.name, self._onload_value(v))\n\n            return value\n\n        if isinstance(value, tuple):\n            return tuple(self._onload_value(v) for v in value)\n\n        return value\n\n    def _offload_value(self, value: Any) -&gt; IntermediateValue:\n        if isinstance(value, torch.Tensor):\n            return IntermediateValue(\n                value=value.to(device=self.offload_device), device=value.device\n            )\n\n        if is_dataclass(value):\n            for field in fields(value):  # `asdict` is recursive, not applicable here\n                v = getattr(value, field.name)\n                setattr(value, field.name, self._offload_value(v))\n\n            return IntermediateValue(value=value, device=None)\n\n        if isinstance(value, tuple):\n            return IntermediateValue(\n                value=tuple(self._offload_value(v) for v in value), device=None\n            )\n\n        if not isinstance(\n            value, (int, str, float, bool, torch.dtype, torch.device, type(None))\n        ):\n            warnings.warn(f\"Offloading not implemented for type {type(value)}.\")\n\n        return IntermediateValue(value=value, device=None)\n\n    @staticmethod\n    def _mask_padding(\n        input_ids: torch.Tensor, attention_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        if attention_mask.dim() == 4:\n            # some attention masks, such as those from pixtral, are are 4d\n            attention_mask = attention_mask[0, 0, 0].unsqueeze(0)\n\n        # Assumes that `attention_mask` only contains zeros and ones\n        return input_ids * attention_mask\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediatesCache.delete","title":"<code>delete(batch_index, consumed_names=None)</code>","text":"<p>Delete values from the cache</p> <p>Parameters:</p> Name Type Description Default <code>batch_index</code> <code>int</code> <p>index of batch whose values will be deleted</p> required <code>consumed_names</code> <code>Optional[List[str]]</code> <p>list of keys whose values will be deleted, defaults to removing all keys</p> <code>None</code> Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>def delete(self, batch_index: int, consumed_names: Optional[List[str]] = None):\n    \"\"\"\n    Delete values from the cache\n\n    :param batch_index: index of batch whose values will be deleted\n    :param consumed_names: list of keys whose values will be deleted, defaults to\n        removing all keys\n    \"\"\"\n    intermediates = self.batch_intermediates[batch_index]\n\n    if consumed_names is None:\n        consumed_names = list(intermediates.keys())\n\n    for name in consumed_names:\n        del intermediates[name]\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediatesCache.empty","title":"<code>empty(num_batches, offload_device)</code>  <code>classmethod</code>","text":"<p>Construct an empty cache</p> <p>Parameters:</p> Name Type Description Default <code>num_batches</code> <code>int</code> <p>the expected number of batches to be stored</p> required <code>offload_device</code> <code>device</code> <p>device to offload values to</p> required Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>@classmethod\ndef empty(cls, num_batches: int, offload_device: torch.device):\n    \"\"\"\n    Construct an empty cache\n\n    :param num_batches: the expected number of batches to be stored\n    :param offload_device: device to offload values to\n    \"\"\"\n    batch_intermediates = [{} for _ in range(num_batches)]\n    return cls(batch_intermediates, offload_device)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediatesCache.fetch","title":"<code>fetch(batch_index, input_names=None)</code>","text":"<p>Fetch values belonging to a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch_index</code> <code>int</code> <p>index of batch whose values are being fetched</p> required <code>input_names</code> <code>Optional[List[str]]</code> <p>list of keys whose values are being fetched</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>dictionary mapping keys to onloaded values</p> Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>def fetch(\n    self, batch_index: int, input_names: Optional[List[str]] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Fetch values belonging to a batch\n\n    :param batch_index: index of batch whose values are being fetched\n    :param input_names: list of keys whose values are being fetched\n    :return: dictionary mapping keys to onloaded values\n    \"\"\"\n    intermediates = self.batch_intermediates[batch_index]\n\n    return {\n        key: self._onload_value(subgraph_input)\n        for key, subgraph_input in intermediates.items()\n        if input_names is None or key in input_names\n    }\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediatesCache.from_dataloader","title":"<code>from_dataloader(dataloader, model_device, mask_padding=True, offload_device=torch.device('cpu'))</code>  <code>classmethod</code>","text":"<p>Initialize a cache with data from the provided dataloader</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>dataloader which generates values to be cached</p> required <code>model_device</code> <code>device</code> <p>device which values will be onloaded to when fetched</p> required <code>mask_padding</code> <code>bool</code> <p>zero out padding tokens if True. This affects modifiers such as GPTQ and SparseGPT</p> <code>True</code> <code>offload_device</code> <code>device</code> <p>device to offload values to</p> <code>device('cpu')</code> Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>@classmethod\ndef from_dataloader(\n    cls,\n    dataloader: torch.utils.data.DataLoader,\n    model_device: torch.device,\n    mask_padding: bool = True,\n    offload_device: torch.device = torch.device(\"cpu\"),\n):\n    \"\"\"\n    Initialize a cache with data from the provided dataloader\n\n    :param dataloader: dataloader which generates values to be cached\n    :param model_device: device which values will be onloaded to when fetched\n    :param mask_padding: zero out padding tokens if True. This affects modifiers\n        such as GPTQ and SparseGPT\n    :param offload_device: device to offload values to\n    \"\"\"\n    # note: list comprehesion was found to not improve performance\n    batch_intermediates = []\n    for batch in tqdm.tqdm(dataloader, desc=\"Preparing intermediates cache\"):\n        intermediate = {}\n        for key, value in batch.items():\n            if mask_padding and key == \"input_ids\":\n                value = cls._mask_padding(value, batch[\"attention_mask\"])\n            intermediate[key] = IntermediateValue(value=value, device=model_device)\n\n        batch_intermediates.append(intermediate)\n\n    return cls(batch_intermediates, offload_device)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/cache/#llmcompressor.pipelines.cache.IntermediatesCache.update","title":"<code>update(batch_index, values)</code>","text":"<p>Update/put values belonging to a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch_index</code> <code>int</code> <p>index of batch whose values will be updated</p> required <code>values</code> <code>Dict[str, Any]</code> <p>dictionary mapping keys to values used for update</p> required Source code in <code>src/llmcompressor/pipelines/cache.py</code> <pre><code>def update(self, batch_index: int, values: Dict[str, Any]):\n    \"\"\"\n    Update/put values belonging to a batch\n\n    :param batch_index: index of batch whose values will be updated\n    :param values: dictionary mapping keys to values used for update\n    \"\"\"\n    intermediates = {k: self._offload_value(v) for k, v in values.items()}\n    self.batch_intermediates[batch_index].update(intermediates)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/registry/","title":"llmcompressor.pipelines.registry","text":""},{"location":"reference/llmcompressor/pipelines/registry/#llmcompressor.pipelines.registry.CalibrationPipeline","title":"<code>CalibrationPipeline</code>","text":"<p>               Bases: <code>ABC</code>, <code>RegistryMixin</code></p> Source code in <code>src/llmcompressor/pipelines/registry.py</code> <pre><code>class CalibrationPipeline(ABC, RegistryMixin):\n    @staticmethod\n    @abstractmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: \"DatasetArguments\",\n    ):\n        raise NotImplementedError()\n\n    @classmethod\n    def from_modifiers(\n        cls, modifiers: List[Modifier], user: Optional[str] = None\n    ) -&gt; \"CalibrationPipeline\":\n        \"\"\"\n        Infer which calibration pipeline to use based on the available modifiers and\n        any user specifications\n\n        :param modifiers: modifiers to apply to model\n        :param user: pipeline name passed by user\n        :return: CalibrationPipeline instance to be called with data (if not datafree)\n        \"\"\"\n        user = standardize_lookup_name(user) if user else None\n        inferred = standardize_lookup_name(cls._validate_infer_pipeline(modifiers))\n        independent = standardize_lookup_name(\"independent\")\n\n        if user == independent:\n            inferred = independent\n\n        if user is not None and user != inferred:\n            logger.warning(\n                f\"Calibration pipeline is set to `{user}`, but it is recommended to \"\n                f\"use `{inferred}`\"\n            )\n\n        pipeline = user or inferred\n        return cls.load_from_registry(pipeline)\n\n    @staticmethod\n    def _validate_infer_pipeline(modifiers: List[Modifier]) -&gt; str:\n        if any(isinstance(modifier, AWQModifier) for modifier in modifiers):\n            if len(modifiers) &gt; 1:\n                logger.warning(\n                    \"AWQ does not currently support sharing a data pipeline with other \"\n                    \"modifiers. Inferring `independent` calibration pipeline\"\n                )\n                return \"independent\"\n            return \"datafree\"\n\n        if any(isinstance(modifier, SEQUENTIAL_MODIFIERS) for modifier in modifiers):\n            return \"sequential\"\n\n        active_qmods = _get_active_quant_modifiers(modifiers)\n        if len(active_qmods) &gt; 1:\n            raise ValueError(\n                f\"Recipe contains more than one active quantization config \"\n                f\"({active_qmods}). These configs may be conflicting, Please modify \"\n                \"your recipe to use at most one quantization config\"\n            )\n\n        if len(active_qmods) == 1:\n            quant_modifier = active_qmods[0]\n            config = quant_modifier.resolve_quantization_config()\n            if config.requires_calibration_data():\n                return \"basic\"\n            else:\n                return \"datafree\"\n\n        if any(isinstance(modifier, SmoothQuantModifier) for modifier in modifiers):\n            return \"basic\"\n\n        return \"datafree\"\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/registry/#llmcompressor.pipelines.registry.CalibrationPipeline.from_modifiers","title":"<code>from_modifiers(modifiers, user=None)</code>  <code>classmethod</code>","text":"<p>Infer which calibration pipeline to use based on the available modifiers and any user specifications</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <code>List[Modifier]</code> <p>modifiers to apply to model</p> required <code>user</code> <code>Optional[str]</code> <p>pipeline name passed by user</p> <code>None</code> <p>Returns:</p> Type Description <code>CalibrationPipeline</code> <p>CalibrationPipeline instance to be called with data (if not datafree)</p> Source code in <code>src/llmcompressor/pipelines/registry.py</code> <pre><code>@classmethod\ndef from_modifiers(\n    cls, modifiers: List[Modifier], user: Optional[str] = None\n) -&gt; \"CalibrationPipeline\":\n    \"\"\"\n    Infer which calibration pipeline to use based on the available modifiers and\n    any user specifications\n\n    :param modifiers: modifiers to apply to model\n    :param user: pipeline name passed by user\n    :return: CalibrationPipeline instance to be called with data (if not datafree)\n    \"\"\"\n    user = standardize_lookup_name(user) if user else None\n    inferred = standardize_lookup_name(cls._validate_infer_pipeline(modifiers))\n    independent = standardize_lookup_name(\"independent\")\n\n    if user == independent:\n        inferred = independent\n\n    if user is not None and user != inferred:\n        logger.warning(\n            f\"Calibration pipeline is set to `{user}`, but it is recommended to \"\n            f\"use `{inferred}`\"\n        )\n\n    pipeline = user or inferred\n    return cls.load_from_registry(pipeline)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/basic/","title":"llmcompressor.pipelines.basic","text":""},{"location":"reference/llmcompressor/pipelines/basic/#llmcompressor.pipelines.basic.BasicPipeline","title":"<code>BasicPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/basic/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"basic\")\nclass BasicPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: Union[\"DatasetArguments\", None],\n    ):\n        \"\"\"\n        Run a basic data pipeline.\n\n        Batches are fetched from the data loader and are used to perform forward passes\n        through the model. This pipeline is typically used for basic model calibration\n        and, unlike the sequential pipelines, does not propagate compression error when\n        used to calibrate model compression\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        model_device = get_execution_device(model)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model):\n            for batch in tqdm.tqdm(dataloader, desc=\"Calibrating\"):\n                batch = apply_pad_mask_to_batch(batch)\n                batch = tensors_to_device(batch, model_device)\n                model(**batch)\n\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/basic/#llmcompressor.pipelines.basic.BasicPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a basic data pipeline.</p> <p>Batches are fetched from the data loader and are used to perform forward passes through the model. This pipeline is typically used for basic model calibration and, unlike the sequential pipelines, does not propagate compression error when used to calibrate model compression</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>Union[DatasetArguments, None]</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/basic/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    dataset_args: Union[\"DatasetArguments\", None],\n):\n    \"\"\"\n    Run a basic data pipeline.\n\n    Batches are fetched from the data loader and are used to perform forward passes\n    through the model. This pipeline is typically used for basic model calibration\n    and, unlike the sequential pipelines, does not propagate compression error when\n    used to calibrate model compression\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    model_device = get_execution_device(model)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model):\n        for batch in tqdm.tqdm(dataloader, desc=\"Calibrating\"):\n            batch = apply_pad_mask_to_batch(batch)\n            batch = tensors_to_device(batch, model_device)\n            model(**batch)\n\n    LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/basic/pipeline/","title":"llmcompressor.pipelines.basic.pipeline","text":""},{"location":"reference/llmcompressor/pipelines/basic/pipeline/#llmcompressor.pipelines.basic.pipeline.BasicPipeline","title":"<code>BasicPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/basic/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"basic\")\nclass BasicPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: Union[\"DatasetArguments\", None],\n    ):\n        \"\"\"\n        Run a basic data pipeline.\n\n        Batches are fetched from the data loader and are used to perform forward passes\n        through the model. This pipeline is typically used for basic model calibration\n        and, unlike the sequential pipelines, does not propagate compression error when\n        used to calibrate model compression\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        model_device = get_execution_device(model)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model):\n            for batch in tqdm.tqdm(dataloader, desc=\"Calibrating\"):\n                batch = apply_pad_mask_to_batch(batch)\n                batch = tensors_to_device(batch, model_device)\n                model(**batch)\n\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/basic/pipeline/#llmcompressor.pipelines.basic.pipeline.BasicPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a basic data pipeline.</p> <p>Batches are fetched from the data loader and are used to perform forward passes through the model. This pipeline is typically used for basic model calibration and, unlike the sequential pipelines, does not propagate compression error when used to calibrate model compression</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>Union[DatasetArguments, None]</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/basic/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    dataset_args: Union[\"DatasetArguments\", None],\n):\n    \"\"\"\n    Run a basic data pipeline.\n\n    Batches are fetched from the data loader and are used to perform forward passes\n    through the model. This pipeline is typically used for basic model calibration\n    and, unlike the sequential pipelines, does not propagate compression error when\n    used to calibrate model compression\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    model_device = get_execution_device(model)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model):\n        for batch in tqdm.tqdm(dataloader, desc=\"Calibrating\"):\n            batch = apply_pad_mask_to_batch(batch)\n            batch = tensors_to_device(batch, model_device)\n            model(**batch)\n\n    LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/data_free/","title":"llmcompressor.pipelines.data_free","text":""},{"location":"reference/llmcompressor/pipelines/data_free/#llmcompressor.pipelines.data_free.DataFreePipeline","title":"<code>DataFreePipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/data_free/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"datafree\")\nclass DataFreePipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: Optional[DataLoader],\n        dataset_args: \"DatasetArguments\",\n    ):\n        \"\"\"\n        A pipeline for data-free calibration\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        LifecycleCallbacks.calibration_epoch_start()\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/data_free/#llmcompressor.pipelines.data_free.DataFreePipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>A pipeline for data-free calibration</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>Optional[DataLoader]</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/data_free/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: Optional[DataLoader],\n    dataset_args: \"DatasetArguments\",\n):\n    \"\"\"\n    A pipeline for data-free calibration\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    LifecycleCallbacks.calibration_epoch_start()\n    LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/data_free/pipeline/","title":"llmcompressor.pipelines.data_free.pipeline","text":""},{"location":"reference/llmcompressor/pipelines/data_free/pipeline/#llmcompressor.pipelines.data_free.pipeline.DataFreePipeline","title":"<code>DataFreePipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/data_free/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"datafree\")\nclass DataFreePipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: Optional[DataLoader],\n        dataset_args: \"DatasetArguments\",\n    ):\n        \"\"\"\n        A pipeline for data-free calibration\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        LifecycleCallbacks.calibration_epoch_start()\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/data_free/pipeline/#llmcompressor.pipelines.data_free.pipeline.DataFreePipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>A pipeline for data-free calibration</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>Optional[DataLoader]</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/data_free/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: Optional[DataLoader],\n    dataset_args: \"DatasetArguments\",\n):\n    \"\"\"\n    A pipeline for data-free calibration\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    LifecycleCallbacks.calibration_epoch_start()\n    LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/independent/","title":"llmcompressor.pipelines.independent","text":""},{"location":"reference/llmcompressor/pipelines/independent/#llmcompressor.pipelines.independent.IndependentPipeline","title":"<code>IndependentPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/independent/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"independent\")\nclass IndependentPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: \"DatasetArguments\",\n    ):\n        \"\"\"\n        Data pipeline where each modifier is assigned its own calibration epoch and data\n        pipeline\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        _logger = logger.patch(lambda r: r.update(function=\"IndependentPipeline\"))\n\n        session = active_session()\n        modifiers = session.get_modifiers()\n        with patch_attr(session.lifecycle, \"modifiers\", None):\n            for index, modifier in enumerate(modifiers):\n                mod_type = str(type(modifier).__name__)\n                session.lifecycle.modifiers = [\n                    StageModifiers(modifiers=[modifier], group=mod_type, index=index)\n                ]\n\n                pipeline = CalibrationPipeline.from_modifiers([modifier])\n                pipeline_name = pipeline.__class__.__name__\n                _logger.info(f\"Inferred `{pipeline_name}` for `{mod_type}`\")\n\n                pipeline(model, dataloader, dataset_args)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/independent/#llmcompressor.pipelines.independent.IndependentPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Data pipeline where each modifier is assigned its own calibration epoch and data pipeline</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/independent/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    dataset_args: \"DatasetArguments\",\n):\n    \"\"\"\n    Data pipeline where each modifier is assigned its own calibration epoch and data\n    pipeline\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    _logger = logger.patch(lambda r: r.update(function=\"IndependentPipeline\"))\n\n    session = active_session()\n    modifiers = session.get_modifiers()\n    with patch_attr(session.lifecycle, \"modifiers\", None):\n        for index, modifier in enumerate(modifiers):\n            mod_type = str(type(modifier).__name__)\n            session.lifecycle.modifiers = [\n                StageModifiers(modifiers=[modifier], group=mod_type, index=index)\n            ]\n\n            pipeline = CalibrationPipeline.from_modifiers([modifier])\n            pipeline_name = pipeline.__class__.__name__\n            _logger.info(f\"Inferred `{pipeline_name}` for `{mod_type}`\")\n\n            pipeline(model, dataloader, dataset_args)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/independent/pipeline/","title":"llmcompressor.pipelines.independent.pipeline","text":""},{"location":"reference/llmcompressor/pipelines/independent/pipeline/#llmcompressor.pipelines.independent.pipeline.IndependentPipeline","title":"<code>IndependentPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/independent/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"independent\")\nclass IndependentPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module,\n        dataloader: DataLoader,\n        dataset_args: \"DatasetArguments\",\n    ):\n        \"\"\"\n        Data pipeline where each modifier is assigned its own calibration epoch and data\n        pipeline\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        _logger = logger.patch(lambda r: r.update(function=\"IndependentPipeline\"))\n\n        session = active_session()\n        modifiers = session.get_modifiers()\n        with patch_attr(session.lifecycle, \"modifiers\", None):\n            for index, modifier in enumerate(modifiers):\n                mod_type = str(type(modifier).__name__)\n                session.lifecycle.modifiers = [\n                    StageModifiers(modifiers=[modifier], group=mod_type, index=index)\n                ]\n\n                pipeline = CalibrationPipeline.from_modifiers([modifier])\n                pipeline_name = pipeline.__class__.__name__\n                _logger.info(f\"Inferred `{pipeline_name}` for `{mod_type}`\")\n\n                pipeline(model, dataloader, dataset_args)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/independent/pipeline/#llmcompressor.pipelines.independent.pipeline.IndependentPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Data pipeline where each modifier is assigned its own calibration epoch and data pipeline</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/independent/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    dataset_args: \"DatasetArguments\",\n):\n    \"\"\"\n    Data pipeline where each modifier is assigned its own calibration epoch and data\n    pipeline\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    _logger = logger.patch(lambda r: r.update(function=\"IndependentPipeline\"))\n\n    session = active_session()\n    modifiers = session.get_modifiers()\n    with patch_attr(session.lifecycle, \"modifiers\", None):\n        for index, modifier in enumerate(modifiers):\n            mod_type = str(type(modifier).__name__)\n            session.lifecycle.modifiers = [\n                StageModifiers(modifiers=[modifier], group=mod_type, index=index)\n            ]\n\n            pipeline = CalibrationPipeline.from_modifiers([modifier])\n            pipeline_name = pipeline.__class__.__name__\n            _logger.info(f\"Inferred `{pipeline_name}` for `{mod_type}`\")\n\n            pipeline(model, dataloader, dataset_args)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/","title":"llmcompressor.pipelines.layer_sequential","text":""},{"location":"reference/llmcompressor/pipelines/layer_sequential/#llmcompressor.pipelines.layer_sequential.LayerSequentialPipeline","title":"<code>LayerSequentialPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/layer_sequential/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"layer_sequential\")\nclass LayerSequentialPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n    ):\n        \"\"\"\n        Run a layer-wise sequential data pipeline according to the following steps:\n\n        1. Layers are identified according to `sequential_targets`\n        2. A hook is attached to the first layer. This hook raises an exception which is\n            then caught and used to capture the input arguments to the first layer\n        3. The inputs to the first layer are used to calibrate the first layer, and the\n            output of the previous layer is used as inputs to calibrate the next layer\n\n        This pipeline requires that the model have distinct layers defined in its\n        architecture and that the outputs of the previous layer are exactly the inputs\n        to the next layer. This is violated by encoder-decoder architectures, among\n        others.\n\n        If your model architecture violates these assumptions, consider using the\n        sequential pipeline (see llmcompressor.pipelines.sequential). Architectures\n        which are known to fail these assumptions include GPT-J and most vision models\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        session = active_session()\n\n        # find layers\n        modifiers = session.get_modifiers()\n        sequential_targets, _ = get_targets_from_modifiers(modifiers, model)\n        layers = match_modules(model, sequential_targets)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model), DisableQuantization(model):\n            # prepare intermediates cache\n            intermediates: IntermediatesCache = capture_first_layer_intermediates(\n                model, layers[0], dataloader\n            )\n\n            num_layers = len(layers)\n            for layer_index, layer in enumerate(layers):\n                # prepare tqdm description texts\n                calib_desc = f\"({layer_index + 1}/{num_layers}): Calibrating\"\n                prop_desc = f\"({layer_index + 1}/{num_layers}): Propagating\"\n\n                # do an preliminary pass to trigger modifier hooks\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                    inputs = intermediates.fetch(batch_idx)\n                    layer(**inputs)\n\n                # trigger compression\n                LifecycleCallbacks.sequential_epoch_end()\n\n                # this pass does not trigger modifier hooks\n                # and is only used for capturing outputs from newly compressed modules\n                with HooksMixin.disable_hooks():\n                    for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                        inputs = intermediates.fetch(batch_idx)\n                        output = layer(**inputs)\n\n                        if layer_index &lt; num_layers - 1:\n                            next_layer = layers[layer_index + 1]\n                            output = to_next_layer_kwargs(output, next_layer)\n                            output = maybe_inject_pos_embeddings(\n                                output, next_layer, inputs\n                            )\n\n                            intermediates.delete(batch_idx)\n                            intermediates.update(batch_idx, output)\n\n            # redudant, finish any remaining compression\n            LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/#llmcompressor.pipelines.layer_sequential.LayerSequentialPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a layer-wise sequential data pipeline according to the following steps:</p> <ol> <li>Layers are identified according to <code>sequential_targets</code></li> <li>A hook is attached to the first layer. This hook raises an exception which is     then caught and used to capture the input arguments to the first layer</li> <li>The inputs to the first layer are used to calibrate the first layer, and the     output of the previous layer is used as inputs to calibrate the next layer</li> </ol> <p>This pipeline requires that the model have distinct layers defined in its architecture and that the outputs of the previous layer are exactly the inputs to the next layer. This is violated by encoder-decoder architectures, among others.</p> <p>If your model architecture violates these assumptions, consider using the sequential pipeline (see llmcompressor.pipelines.sequential). Architectures which are known to fail these assumptions include GPT-J and most vision models</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/layer_sequential/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n):\n    \"\"\"\n    Run a layer-wise sequential data pipeline according to the following steps:\n\n    1. Layers are identified according to `sequential_targets`\n    2. A hook is attached to the first layer. This hook raises an exception which is\n        then caught and used to capture the input arguments to the first layer\n    3. The inputs to the first layer are used to calibrate the first layer, and the\n        output of the previous layer is used as inputs to calibrate the next layer\n\n    This pipeline requires that the model have distinct layers defined in its\n    architecture and that the outputs of the previous layer are exactly the inputs\n    to the next layer. This is violated by encoder-decoder architectures, among\n    others.\n\n    If your model architecture violates these assumptions, consider using the\n    sequential pipeline (see llmcompressor.pipelines.sequential). Architectures\n    which are known to fail these assumptions include GPT-J and most vision models\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    session = active_session()\n\n    # find layers\n    modifiers = session.get_modifiers()\n    sequential_targets, _ = get_targets_from_modifiers(modifiers, model)\n    layers = match_modules(model, sequential_targets)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model), DisableQuantization(model):\n        # prepare intermediates cache\n        intermediates: IntermediatesCache = capture_first_layer_intermediates(\n            model, layers[0], dataloader\n        )\n\n        num_layers = len(layers)\n        for layer_index, layer in enumerate(layers):\n            # prepare tqdm description texts\n            calib_desc = f\"({layer_index + 1}/{num_layers}): Calibrating\"\n            prop_desc = f\"({layer_index + 1}/{num_layers}): Propagating\"\n\n            # do an preliminary pass to trigger modifier hooks\n            for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                inputs = intermediates.fetch(batch_idx)\n                layer(**inputs)\n\n            # trigger compression\n            LifecycleCallbacks.sequential_epoch_end()\n\n            # this pass does not trigger modifier hooks\n            # and is only used for capturing outputs from newly compressed modules\n            with HooksMixin.disable_hooks():\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                    inputs = intermediates.fetch(batch_idx)\n                    output = layer(**inputs)\n\n                    if layer_index &lt; num_layers - 1:\n                        next_layer = layers[layer_index + 1]\n                        output = to_next_layer_kwargs(output, next_layer)\n                        output = maybe_inject_pos_embeddings(\n                            output, next_layer, inputs\n                        )\n\n                        intermediates.delete(batch_idx)\n                        intermediates.update(batch_idx, output)\n\n        # redudant, finish any remaining compression\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/helpers/","title":"llmcompressor.pipelines.layer_sequential.helpers","text":""},{"location":"reference/llmcompressor/pipelines/layer_sequential/helpers/#llmcompressor.pipelines.layer_sequential.helpers.EarlyStopException","title":"<code>EarlyStopException</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Dataclass for storing model activations</p> <p>Note: Attribute names <code>args</code> and <code>kwargs</code> are reserved for <code>dataclass.GenericAlias</code></p> Source code in <code>src/llmcompressor/pipelines/layer_sequential/helpers.py</code> <pre><code>@dataclass\nclass EarlyStopException(Exception):\n    \"\"\"\n    Dataclass for storing model activations\n\n    Note: Attribute names `args` and `kwargs` are reserved for `dataclass.GenericAlias`\n    \"\"\"\n\n    _args: Tuple[Any, ...]\n    _kwargs: Dict[str, Any]\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/helpers/#llmcompressor.pipelines.layer_sequential.helpers.capture_first_layer_intermediates","title":"<code>capture_first_layer_intermediates(model, first_layer, dataloader, mask_padding=True)</code>","text":"<p>Captures the intermediate activations directly before the first model layer. This is meant to capture any model preprocessing before model layers are executed</p> <p>Note that if any modules compressed prior to the execution of the first layer, the compression error induced by compressing those modules will not be propagated to subsequent activations, as they would be for modules which are compressed within a layer</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model containing layers</p> required <code>first_layer</code> <code>Module</code> <p>the first layer of the model</p> required <code>dataloader</code> <code>DataLoader</code> <p>dataloader of calibration inputs</p> required <code>mask_padding</code> <code>bool</code> <p>zero out padding tokens if True. This affects modifiers such as GPTQ and SparseGPT</p> <code>True</code> Source code in <code>src/llmcompressor/pipelines/layer_sequential/helpers.py</code> <pre><code>def capture_first_layer_intermediates(\n    model: Module,\n    first_layer: Module,\n    dataloader: DataLoader,\n    mask_padding: bool = True,\n) -&gt; IntermediatesCache:\n    \"\"\"\n    Captures the intermediate activations directly before the first model layer.\n    This is meant to capture any model preprocessing before model layers are executed\n\n    Note that if any modules compressed prior to the execution of the first layer, the\n    compression error induced by compressing those modules will not be propagated to\n    subsequent activations, as they would be for modules which are compressed within\n    a layer\n\n    :param model: model containing layers\n    :param first_layer: the first layer of the model\n    :param dataloader: dataloader of calibration inputs\n    :param mask_padding: zero out padding tokens if True. This affects modifiers such as\n        GPTQ and SparseGPT\n    \"\"\"\n    model_device = get_execution_device(model)\n    intermediates = IntermediatesCache.empty(len(dataloader), torch.device(\"cpu\"))\n    signature = inspect.signature(first_layer.forward)\n\n    with calibration_forward_context(model), early_stop_hook(first_layer):\n        desc = \"Preparing intermediates cache\"\n        for batch_index, batch in enumerate(tqdm.tqdm(dataloader, desc=desc)):\n            batch = apply_pad_mask_to_batch(batch) if mask_padding else batch\n            batch = tensors_to_device(batch, model_device)\n\n            try:\n                model(**batch)\n            except EarlyStopException as exception:\n                layer_args = args_to_kwargs(exception._args, signature)\n                assert not set(layer_args.keys()) &amp; set(exception._kwargs.keys())\n                layer_args.update(exception._kwargs)\n\n                intermediates.update(batch_index, layer_args)\n            else:\n                raise ValueError(\n                    \"Attempted to capture first layer intermediates, but \"\n                    \"EarlyStopException was not raised\"\n                )\n\n    return intermediates\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/helpers/#llmcompressor.pipelines.layer_sequential.helpers.match_modules","title":"<code>match_modules(model, target_names)</code>","text":"<p>Find all submodules which match the <code>target_names</code> and sort them by name</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to search for submodules in</p> required <code>target_names</code> <code>List[str]</code> <p>patterns of submodule names to match</p> required <p>Returns:</p> Type Description <code>List[Module]</code> <p>list of submodules</p> Source code in <code>src/llmcompressor/pipelines/layer_sequential/helpers.py</code> <pre><code>def match_modules(model: Module, target_names: List[str]) -&gt; List[Module]:\n    \"\"\"\n    Find all submodules which match the `target_names` and sort them by name\n\n    :param model: model to search for submodules in\n    :param target_names: patterns of submodule names to match\n    :return: list of submodules\n    \"\"\"\n    names_layers = [\n        (name, module)\n        for name, module in model.named_modules()\n        if find_name_or_class_matches(name, module, target_names)\n    ]\n\n    names_layers = sorted(names_layers, key=lambda name_layer: name_layer[0])\n    return [layer for _name, layer in names_layers]\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/helpers/#llmcompressor.pipelines.layer_sequential.helpers.maybe_inject_pos_embeddings","title":"<code>maybe_inject_pos_embeddings(output, next_layer, inputs)</code>","text":"<p>As of https://github.com/huggingface/transformers/pull/34858, positional embeddings must be passed into each decoder call as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Dict[str, Any]</code> <p>output of the previous layer</p> required <code>next_layer</code> <code>Module</code> <p>next layer to call</p> required <code>inputs</code> <code>Dict[str, Any]</code> <p>inputs to next layer</p> required Source code in <code>src/llmcompressor/pipelines/layer_sequential/helpers.py</code> <pre><code>def maybe_inject_pos_embeddings(\n    output: Dict[str, Any],\n    next_layer: Module,\n    inputs: Dict[str, Any],\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    As of https://github.com/huggingface/transformers/pull/34858, positional embeddings\n    must be passed into each decoder call as kwargs\n\n    :param output: output of the previous layer\n    :param next_layer: next layer to call\n    :param inputs: inputs to next layer\n    \"\"\"\n    signature = inspect.signature(next_layer.forward)\n    if (\n        \"position_embeddings\" in signature.parameters.keys()\n        and \"position_embeddings\" in inputs\n        and \"position_embeddings\" not in output\n    ):\n        output[\"position_embeddings\"] = inputs[\"position_embeddings\"]\n\n    return output\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/helpers/#llmcompressor.pipelines.layer_sequential.helpers.to_next_layer_kwargs","title":"<code>to_next_layer_kwargs(args, next_layer)</code>","text":"<p>Convert a list of arguments to a dictionary of keyword arguments which match the next layer's function signature</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Tuple[Any, ...]</code> <p>list of argument values</p> required <code>next_layer</code> <code>Module</code> <p>the next layer whose function signature must be matched</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>dictionary mapping function signature keywords to argument values</p> Source code in <code>src/llmcompressor/pipelines/layer_sequential/helpers.py</code> <pre><code>def to_next_layer_kwargs(args: Tuple[Any, ...], next_layer: Module) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert a list of arguments to a dictionary of keyword arguments which match the\n    next layer's function signature\n\n    :param args: list of argument values\n    :param next_layer: the next layer whose function signature must be matched\n    :return: dictionary mapping function signature keywords to argument values\n    \"\"\"\n    signature = inspect.signature(next_layer.forward)\n    return args_to_kwargs(args, signature)\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/pipeline/","title":"llmcompressor.pipelines.layer_sequential.pipeline","text":""},{"location":"reference/llmcompressor/pipelines/layer_sequential/pipeline/#llmcompressor.pipelines.layer_sequential.pipeline.LayerSequentialPipeline","title":"<code>LayerSequentialPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/layer_sequential/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"layer_sequential\")\nclass LayerSequentialPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n    ):\n        \"\"\"\n        Run a layer-wise sequential data pipeline according to the following steps:\n\n        1. Layers are identified according to `sequential_targets`\n        2. A hook is attached to the first layer. This hook raises an exception which is\n            then caught and used to capture the input arguments to the first layer\n        3. The inputs to the first layer are used to calibrate the first layer, and the\n            output of the previous layer is used as inputs to calibrate the next layer\n\n        This pipeline requires that the model have distinct layers defined in its\n        architecture and that the outputs of the previous layer are exactly the inputs\n        to the next layer. This is violated by encoder-decoder architectures, among\n        others.\n\n        If your model architecture violates these assumptions, consider using the\n        sequential pipeline (see llmcompressor.pipelines.sequential). Architectures\n        which are known to fail these assumptions include GPT-J and most vision models\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        session = active_session()\n\n        # find layers\n        modifiers = session.get_modifiers()\n        sequential_targets, _ = get_targets_from_modifiers(modifiers, model)\n        layers = match_modules(model, sequential_targets)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model), DisableQuantization(model):\n            # prepare intermediates cache\n            intermediates: IntermediatesCache = capture_first_layer_intermediates(\n                model, layers[0], dataloader\n            )\n\n            num_layers = len(layers)\n            for layer_index, layer in enumerate(layers):\n                # prepare tqdm description texts\n                calib_desc = f\"({layer_index + 1}/{num_layers}): Calibrating\"\n                prop_desc = f\"({layer_index + 1}/{num_layers}): Propagating\"\n\n                # do an preliminary pass to trigger modifier hooks\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                    inputs = intermediates.fetch(batch_idx)\n                    layer(**inputs)\n\n                # trigger compression\n                LifecycleCallbacks.sequential_epoch_end()\n\n                # this pass does not trigger modifier hooks\n                # and is only used for capturing outputs from newly compressed modules\n                with HooksMixin.disable_hooks():\n                    for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                        inputs = intermediates.fetch(batch_idx)\n                        output = layer(**inputs)\n\n                        if layer_index &lt; num_layers - 1:\n                            next_layer = layers[layer_index + 1]\n                            output = to_next_layer_kwargs(output, next_layer)\n                            output = maybe_inject_pos_embeddings(\n                                output, next_layer, inputs\n                            )\n\n                            intermediates.delete(batch_idx)\n                            intermediates.update(batch_idx, output)\n\n            # redudant, finish any remaining compression\n            LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/layer_sequential/pipeline/#llmcompressor.pipelines.layer_sequential.pipeline.LayerSequentialPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a layer-wise sequential data pipeline according to the following steps:</p> <ol> <li>Layers are identified according to <code>sequential_targets</code></li> <li>A hook is attached to the first layer. This hook raises an exception which is     then caught and used to capture the input arguments to the first layer</li> <li>The inputs to the first layer are used to calibrate the first layer, and the     output of the previous layer is used as inputs to calibrate the next layer</li> </ol> <p>This pipeline requires that the model have distinct layers defined in its architecture and that the outputs of the previous layer are exactly the inputs to the next layer. This is violated by encoder-decoder architectures, among others.</p> <p>If your model architecture violates these assumptions, consider using the sequential pipeline (see llmcompressor.pipelines.sequential). Architectures which are known to fail these assumptions include GPT-J and most vision models</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/layer_sequential/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n):\n    \"\"\"\n    Run a layer-wise sequential data pipeline according to the following steps:\n\n    1. Layers are identified according to `sequential_targets`\n    2. A hook is attached to the first layer. This hook raises an exception which is\n        then caught and used to capture the input arguments to the first layer\n    3. The inputs to the first layer are used to calibrate the first layer, and the\n        output of the previous layer is used as inputs to calibrate the next layer\n\n    This pipeline requires that the model have distinct layers defined in its\n    architecture and that the outputs of the previous layer are exactly the inputs\n    to the next layer. This is violated by encoder-decoder architectures, among\n    others.\n\n    If your model architecture violates these assumptions, consider using the\n    sequential pipeline (see llmcompressor.pipelines.sequential). Architectures\n    which are known to fail these assumptions include GPT-J and most vision models\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    session = active_session()\n\n    # find layers\n    modifiers = session.get_modifiers()\n    sequential_targets, _ = get_targets_from_modifiers(modifiers, model)\n    layers = match_modules(model, sequential_targets)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model), DisableQuantization(model):\n        # prepare intermediates cache\n        intermediates: IntermediatesCache = capture_first_layer_intermediates(\n            model, layers[0], dataloader\n        )\n\n        num_layers = len(layers)\n        for layer_index, layer in enumerate(layers):\n            # prepare tqdm description texts\n            calib_desc = f\"({layer_index + 1}/{num_layers}): Calibrating\"\n            prop_desc = f\"({layer_index + 1}/{num_layers}): Propagating\"\n\n            # do an preliminary pass to trigger modifier hooks\n            for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                inputs = intermediates.fetch(batch_idx)\n                layer(**inputs)\n\n            # trigger compression\n            LifecycleCallbacks.sequential_epoch_end()\n\n            # this pass does not trigger modifier hooks\n            # and is only used for capturing outputs from newly compressed modules\n            with HooksMixin.disable_hooks():\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                    inputs = intermediates.fetch(batch_idx)\n                    output = layer(**inputs)\n\n                    if layer_index &lt; num_layers - 1:\n                        next_layer = layers[layer_index + 1]\n                        output = to_next_layer_kwargs(output, next_layer)\n                        output = maybe_inject_pos_embeddings(\n                            output, next_layer, inputs\n                        )\n\n                        intermediates.delete(batch_idx)\n                        intermediates.update(batch_idx, output)\n\n        # redudant, finish any remaining compression\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/","title":"llmcompressor.pipelines.sequential","text":""},{"location":"reference/llmcompressor/pipelines/sequential/#llmcompressor.pipelines.sequential.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"sequential\")\nclass SequentialPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n    ):\n        \"\"\"\n        Run a sequential data pipeline according to the following steps:\n\n        1. The model is partitioned into subgraphs according to `sequential_targets`\n        2. Data passes through each subgraph sequentially. Data is passed through each\n            subgraph twice, once to trigger calibration hooks, then a second time in\n            order to capture activations after quantization has occurred through hooks.\n        3. The intermediate activations between each subgraph are cached and offloaded\n            to the cpu between each batch in order to save memory\n\n        This pipeline requires that the model be traceable with respect to data from the\n        data loader. This may be an issue for vision models with vision datasets, due\n        to specialized input processing in the model.\n\n        In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A\n        model can be made traceable by wrapping the untraceable functions (see\n        llmcompressor.transformers.tracing)\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        session = active_session()\n\n        # infer sequential targets\n        modifiers = session.get_modifiers()\n        sequential_targets, ignore = get_targets_from_modifiers(modifiers, model)\n\n        # trace subgraphs\n        sample_input = next(iter(dataloader))\n        subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model), DisableQuantization(model):\n            # prepare intermediates cache\n            model_device = get_execution_device(model)\n            intermediates = IntermediatesCache.from_dataloader(dataloader, model_device)\n\n            num_subgraphs = len(subgraphs)\n            for subgraph_index, subgraph in enumerate(subgraphs):\n                # prepare tqdm description texts\n                calib_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Calibrating\"\n                prop_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Propagating\"\n\n                # do an preliminary pass to trigger modifier hooks\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                    inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                    subgraph.forward(model, **inputs)\n\n                # trigger compression\n                LifecycleCallbacks.sequential_epoch_end()\n\n                # this pass does not trigger modifier hooks\n                # and is only used for capturing outputs from newly compressed modules\n                with HooksMixin.disable_hooks():\n                    for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                        inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                        output = subgraph.forward(model, **inputs)\n\n                        if subgraph_index &lt; num_subgraphs - 1:\n                            intermediates.update(batch_idx, output)\n                            intermediates.delete(batch_idx, subgraph.consumed_names)\n\n            # redudant, finish any remaining compression\n            LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/#llmcompressor.pipelines.sequential.SequentialPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a sequential data pipeline according to the following steps:</p> <ol> <li>The model is partitioned into subgraphs according to <code>sequential_targets</code></li> <li>Data passes through each subgraph sequentially. Data is passed through each     subgraph twice, once to trigger calibration hooks, then a second time in     order to capture activations after quantization has occurred through hooks.</li> <li>The intermediate activations between each subgraph are cached and offloaded     to the cpu between each batch in order to save memory</li> </ol> <p>This pipeline requires that the model be traceable with respect to data from the data loader. This may be an issue for vision models with vision datasets, due to specialized input processing in the model.</p> <p>In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A model can be made traceable by wrapping the untraceable functions (see llmcompressor.transformers.tracing)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n):\n    \"\"\"\n    Run a sequential data pipeline according to the following steps:\n\n    1. The model is partitioned into subgraphs according to `sequential_targets`\n    2. Data passes through each subgraph sequentially. Data is passed through each\n        subgraph twice, once to trigger calibration hooks, then a second time in\n        order to capture activations after quantization has occurred through hooks.\n    3. The intermediate activations between each subgraph are cached and offloaded\n        to the cpu between each batch in order to save memory\n\n    This pipeline requires that the model be traceable with respect to data from the\n    data loader. This may be an issue for vision models with vision datasets, due\n    to specialized input processing in the model.\n\n    In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A\n    model can be made traceable by wrapping the untraceable functions (see\n    llmcompressor.transformers.tracing)\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    session = active_session()\n\n    # infer sequential targets\n    modifiers = session.get_modifiers()\n    sequential_targets, ignore = get_targets_from_modifiers(modifiers, model)\n\n    # trace subgraphs\n    sample_input = next(iter(dataloader))\n    subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model), DisableQuantization(model):\n        # prepare intermediates cache\n        model_device = get_execution_device(model)\n        intermediates = IntermediatesCache.from_dataloader(dataloader, model_device)\n\n        num_subgraphs = len(subgraphs)\n        for subgraph_index, subgraph in enumerate(subgraphs):\n            # prepare tqdm description texts\n            calib_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Calibrating\"\n            prop_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Propagating\"\n\n            # do an preliminary pass to trigger modifier hooks\n            for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                subgraph.forward(model, **inputs)\n\n            # trigger compression\n            LifecycleCallbacks.sequential_epoch_end()\n\n            # this pass does not trigger modifier hooks\n            # and is only used for capturing outputs from newly compressed modules\n            with HooksMixin.disable_hooks():\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                    inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                    output = subgraph.forward(model, **inputs)\n\n                    if subgraph_index &lt; num_subgraphs - 1:\n                        intermediates.update(batch_idx, output)\n                        intermediates.delete(batch_idx, subgraph.consumed_names)\n\n        # redudant, finish any remaining compression\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/","title":"llmcompressor.pipelines.sequential.helpers","text":""},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.Subgraph","title":"<code>Subgraph</code>  <code>dataclass</code>","text":"<p>Dataclass specifying an executable subgraph of a model graph</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>subgraph of model graph</p> required <code>input_names</code> <code>Set[str]</code> <p>argument names of the compiled forward function</p> required <code>consumed_names</code> <code>Set[str]</code> <p>argument names which are not used by any subsequent subgraphs and can therefore be deleted from the intermediates cache</p> required Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>@dataclass\nclass Subgraph:\n    \"\"\"\n    Dataclass specifying an executable subgraph of a model graph\n\n    :param graph: subgraph of model graph\n    :param input_names: argument names of the compiled forward function\n    :param consumed_names: argument names which are not used by any subsequent subgraphs\n        and can therefore be deleted from the intermediates cache\n    \"\"\"\n\n    graph: Graph\n    input_names: Set[str]\n    consumed_names: Set[str]\n    _code: Optional[PythonCode] = None\n\n    def forward(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute the operations within the subgraph\n\n        :param \\\\*args: argument inputs to subgraph forward function\n        :param \\\\**kwargs: keyword inputs to subgraph forward function\n        :return keyword outputs of subgraph forward function (non-consumed variables):\n        \"\"\"\n        if self._code is None:\n            self._code = self.graph.python_code(\"self\")\n            exec(self._code.src, self._code.globals)\n\n        forward_fn = self._code.globals.get(\"forward\")\n\n        try:\n            outputs = forward_fn(*args, **kwargs)\n        except Exception as exception:\n            raise RuntimeError(\n                \"Raised an exception during execution of the following code:\\n\"\n                f\"```\\n{add_line_numbers(self._code.src)}\\n```\\n\"\n                \"This is likely due to a violation of shape assumptions made when \"\n                \"tracing\"\n            ) from exception\n\n        return outputs\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.Subgraph.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Execute the operations within the subgraph</p> <p>Parameters:</p> Name Type Description Default <code>\\*args</code> <p>argument inputs to subgraph forward function</p> required <code>\\**kwargs</code> <p>keyword inputs to subgraph forward function</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute the operations within the subgraph\n\n    :param \\\\*args: argument inputs to subgraph forward function\n    :param \\\\**kwargs: keyword inputs to subgraph forward function\n    :return keyword outputs of subgraph forward function (non-consumed variables):\n    \"\"\"\n    if self._code is None:\n        self._code = self.graph.python_code(\"self\")\n        exec(self._code.src, self._code.globals)\n\n    forward_fn = self._code.globals.get(\"forward\")\n\n    try:\n        outputs = forward_fn(*args, **kwargs)\n    except Exception as exception:\n        raise RuntimeError(\n            \"Raised an exception during execution of the following code:\\n\"\n            f\"```\\n{add_line_numbers(self._code.src)}\\n```\\n\"\n            \"This is likely due to a violation of shape assumptions made when \"\n            \"tracing\"\n        ) from exception\n\n    return outputs\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.find_target_nodes","title":"<code>find_target_nodes(graph, targets)</code>","text":"<p>Find all nodes whose execution is equivalent to executing the target modules. Note that these nodes are guaranteed to be treated as leaf nodes by SequentialTracer</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>GraphModule</code> <p>graph containing target nodes</p> required <code>targets</code> <code>Set[Module]</code> <p>modules whose nodes are being searched for</p> required <p>Returns:</p> Type Description <code>Set[Node]</code> <p>set of all nodes which call the target modules</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def find_target_nodes(graph: GraphModule, targets: Set[Module]) -&gt; Set[Node]:\n    \"\"\"\n    Find all nodes whose execution is equivalent to executing the target modules.\n    Note that these nodes are guaranteed to be treated as leaf nodes by SequentialTracer\n\n    :param graph: graph containing target nodes\n    :param targets: modules whose nodes are being searched for\n    :return: set of all nodes which call the target modules\n    \"\"\"\n    return set(\n        node\n        for node in graph.graph.nodes\n        if node.op == \"call_module\" and graph.get_submodule(node.target) in targets\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.get_sequential_ancestors","title":"<code>get_sequential_ancestors(model, targets)</code>","text":"<p>Find modules which are call graph ancestors of the given sequential targets</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model containing sequential targets</p> required <code>targets</code> <code>Set[Module]</code> <p>sequential targets to find ancestors of</p> required <p>Returns:</p> Type Description <code>Set[Module]</code> <p>call graph ancestors of sequential targets</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def get_sequential_ancestors(model: Module, targets: Set[Module]) -&gt; Set[Module]:\n    \"\"\"\n    Find modules which are call graph ancestors of the given sequential targets\n\n    :param model: model containing sequential targets\n    :param targets: sequential targets to find ancestors of\n    :return: call graph ancestors of sequential targets\n    \"\"\"\n    ancestors = set()\n\n    def is_ancestor(module: Module) -&gt; bool:\n        if module in ancestors or module in targets:\n            return True\n\n        # eagerly compute list in order to avoid early stopping and :. missing ancestors\n        _is_ancestor = any([is_ancestor(child) for child in module.children()])\n        if _is_ancestor:\n            ancestors.add(module)\n\n        return _is_ancestor\n\n    is_ancestor(model)\n    return ancestors\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.get_targets_from_modifiers","title":"<code>get_targets_from_modifiers(modifiers, model)</code>","text":"<p>Infer sequential targets and ignore list from modifiers list</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>model being calibrated</p> required <code>modifiers</code> <code>List[Modifier]</code> <p>list of modifiers being applied during calibration</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>list of sequential targets and list of modules to ignore for tracing</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def get_targets_from_modifiers(\n    modifiers: List[Modifier], model: PreTrainedModel\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Infer sequential targets and ignore list from modifiers list\n\n    :param model: model being calibrated\n    :param modifiers: list of modifiers being applied during calibration\n    :return: list of sequential targets and list of modules to ignore for tracing\n    \"\"\"\n    # avoid circular import\n    from llmcompressor.pipelines.registry import SEQUENTIAL_MODIFIERS\n\n    sequential_modifiers = [\n        modifier for modifier in modifiers if isinstance(modifier, SEQUENTIAL_MODIFIERS)\n    ]\n\n    if len(sequential_modifiers) &gt;= 2:\n        types = [type(modifier) for modifier in sequential_modifiers]\n        logger.warning(\n            \"Cannot infer sequential targets from multiple sequential modifiers \"\n            f\"({types}). Defaulting to {types[0]}\"\n        )\n    elif len(sequential_modifiers) &lt;= 0:\n        types = [type(modifier) for modifier in modifiers]\n        raise ValueError(f\"Cannot infer sequential targets from list of {types}\")\n\n    modifier = sequential_modifiers[0]\n\n    # infer sequential targets\n    if modifier.sequential_targets is None:\n        sequential_targets = get_no_split_params(model)\n    elif isinstance(modifier.sequential_targets, str):\n        sequential_targets = [modifier.sequential_targets]\n    else:\n        sequential_targets = modifier.sequential_targets\n\n    return sequential_targets, modifier.ignore\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.get_tracer","title":"<code>get_tracer(model, sequential_targets, ignore)</code>","text":"<p>Get a tracer specialized for the given model. The resulting tracer will not trace inside of sequential targets, nor any modules which are not call graph ancestors of sequential targets</p> <p>Tracing within sequential targets is unnecessary, and tracing within offloaded modules may result in meta tensors being added to the model graph</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being traced</p> required <code>sequential_targets</code> <code>Set[Module]</code> <p>modules which are sequential targets</p> required <code>ignore</code> <code>Set[Module]</code> <p>modules to ignore during tracing, in the future will specify functions and methods to skip during tracing</p> required Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def get_tracer(\n    model: Module, sequential_targets: Set[Module], ignore: Set[Module]\n) -&gt; HFTracer:\n    \"\"\"\n    Get a tracer specialized for the given model. The resulting tracer will not trace\n    inside of sequential targets, nor any modules which are not call graph ancestors of\n    sequential targets\n\n    Tracing within sequential targets is unnecessary, and tracing within offloaded\n    modules may result in meta tensors being added to the model graph\n\n    :param model: model being traced\n    :param sequential_targets: modules which are sequential targets\n    :param ignore: modules to ignore during tracing, in the future will specify\n        functions and methods to skip during tracing\n    \"\"\"\n    sequential_ancestors = get_sequential_ancestors(model, sequential_targets)\n    offloaded_modules = set(m for m in model.modules() if has_offloaded_params(m))\n\n    # check unlikely case that ancestors have direct params which are offloaded\n    offloaded_ancestors = offloaded_modules &amp; sequential_ancestors\n    if offloaded_ancestors:\n        names = set(module.__class__.__name__ for module in offloaded_ancestors)\n        logger.warning(\n            \"The following modules are call graph ancestors of sequential targets,\"\n            f\"but also contain offloaded modules: {names}.\\n\"\n            \"These modules will not be traced, and any sequential target children will \"\n            \"be executed jointly, which may lead to OOM errors\"\n        )\n\n    class SequentialTracer(HFTracer):\n        def create_arg(self, a: Any) -&gt; Argument:\n            # special extension allows models which depend on config values to be traced\n            if isinstance(a, PretrainedConfig):\n                kwargs = {k: self.create_arg(v) for k, v in a.to_dict().items()}\n                return self.create_node(\"call_function\", a.__class__, (), kwargs)\n\n            else:\n                return super().create_arg(a)\n\n        def is_leaf_module(self, module: Module, module_qualified_name: str) -&gt; bool:\n            return (\n                module not in sequential_ancestors\n                or module in offloaded_modules\n                or module in ignore\n            )\n\n        def trace(self, root: Union[Module, Callable], *args, **kwargs) -&gt; Graph:\n            if isinstance(root, Module):\n                # due to a bug in Tracer.create_args_for_root (_patch_function),\n                # we must unwrap function wrappers prior to tracing, for example\n                # the `deprecate_kwarg` by transformers which wraps forward\n                unwrapped_forward = inspect.unwrap(type(root).forward)\n\n                # we override the class method because the\n                # class method is the one being traced\n                with patch_attr(type(root), \"forward\", unwrapped_forward):\n                    return super().trace(root, *args, **kwargs)\n\n            else:\n                return super().trace(root, *args, **kwargs)\n\n    return SequentialTracer()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.graph_is_well_formed","title":"<code>graph_is_well_formed(graph)</code>","text":"<p>A graph is well formed if and only if <code>nodeA in NodeB.users &lt;=&gt; nodeB in Node.A.all_input_nodes</code></p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>graph being checked</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the graph is well formed, False otherwise</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def graph_is_well_formed(graph: Graph) -&gt; bool:\n    \"\"\"\n    A graph is well formed if and only if\n    `nodeA in NodeB.users &lt;=&gt; nodeB in Node.A.all_input_nodes`\n\n    :param graph: graph being checked\n    :return: True if the graph is well formed, False otherwise\n    \"\"\"\n    for node in graph.nodes:\n        for user in node.users:\n            if node not in user.all_input_nodes:\n                return False\n\n        for input_node in node.all_input_nodes:\n            if node not in input_node.users:\n                return False\n\n        if len(node.users) != len(set(node.users)) or len(node.all_input_nodes) != len(\n            set(node.all_input_nodes)\n        ):\n            return False\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.match_modules","title":"<code>match_modules(model, target_names)</code>","text":"<p>Find modules whose names match the patterns given by <code>target_names</code></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model containing submodules to find</p> required <code>target_names</code> <code>List[str]</code> <p>target patterns to find</p> required <p>Returns:</p> Type Description <code>Set[Module]</code> <p>all submodules matching <code>target_names</code></p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def match_modules(model: Module, target_names: List[str]) -&gt; Set[Module]:\n    \"\"\"\n    Find modules whose names match the patterns given by `target_names`\n\n    :param model: model containing submodules to find\n    :param target_names: target patterns to find\n    :return: all submodules matching `target_names`\n    \"\"\"\n    return set(\n        module\n        for name, module in model.named_modules()\n        if find_name_or_class_matches(name, module, target_names)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.partition_graph","title":"<code>partition_graph(model, partitions)</code>","text":"<p>Convert each partition into a Subgraph. Each Subgraph returns a dictionary mapping of output node names to their computed values. Note that the <code>consumed_names</code> attribute of each Subgraph remains empty, to be later populated by <code>trace_consumed_names</code></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model which owns the produced Subgraphs</p> required <code>partitions</code> <code>List[List[Node]]</code> <p>list of partitions, where each partition is a list of nodes belonging to that partition</p> required <p>Returns:</p> Type Description <code>List[Subgraph]</code> <p>list of subgraphs in order of execution</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def partition_graph(model: Module, partitions: List[List[Node]]) -&gt; List[Subgraph]:\n    \"\"\"\n    Convert each partition into a Subgraph. Each Subgraph returns a dictionary mapping\n    of output node names to their computed values. Note that the `consumed_names`\n    attribute of each Subgraph remains empty, to be later populated by\n    `trace_consumed_names`\n\n    :param model: model which owns the produced Subgraphs\n    :param partitions: list of partitions, where each partition is a list of nodes\n        belonging to that partition\n    :return: list of subgraphs in order of execution\n    \"\"\"\n    subgraphs = []\n\n    # create subgraphs\n    for partition_nodes in partitions:\n        # create a new graph for the partition\n        graph = Graph(model)\n        node_map = {}\n\n        # add placeholders for inputs not in this subgraph. use set to deduplicate\n        new_input_nodes = {\n            input_node\n            for node in partition_nodes\n            for input_node in node.all_input_nodes\n            if input_node not in partition_nodes and input_node.op\n        }\n        for input_node in new_input_nodes:\n            node_map[input_node] = graph.placeholder(input_node.name)\n\n        # add the nodes to subgraph\n        for node in partition_nodes:\n            node_map[node] = graph.node_copy(node, lambda n: node_map[n])\n\n        # add an output node to collect all subgraph outputs into a dictionary\n        if len(graph.find_nodes(op=\"output\")) &lt;= 0:\n            output_dict = {\n                node.name: node_map[node]\n                for node in partition_nodes\n                if any(user not in partition_nodes for user in node.users.keys())\n            }\n            graph.output(output_dict)\n\n        # save the subgraph for this partition\n        graph.lint()\n        input_names = set(node.name for node in graph.nodes if node.op == \"placeholder\")\n        subgraphs.append(\n            Subgraph(\n                graph=graph,\n                input_names=input_names,\n                consumed_names=set(),  # populated later\n            )\n        )\n\n        assert graph_is_well_formed(graph)\n\n    return subgraphs\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.populate_concrete_args","title":"<code>populate_concrete_args(model, sample_input)</code>","text":"<p>Creates concrete args which, unlike the equivalent function provided by transformers.utils.fx, creates default values for variadic arguments, which are needed by some models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being traced</p> required <code>sample_input</code> <code>Dict</code> <p>values used to symbolically trace the model. All arguments to the model.forward function which are not in the sample_input are considered concrete args</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>dictionary mapping concrete argument names to their default values</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def populate_concrete_args(model: Module, sample_input: Dict) -&gt; Dict:\n    \"\"\"\n    Creates concrete args which, unlike the equivalent function provided by\n    transformers.utils.fx, creates default values for variadic arguments, which are\n    needed by some models.\n\n    :param model: model being traced\n    :param sample_input: values used to symbolically trace the model. All arguments\n        to the model.forward function which are not in the sample_input are considered\n        concrete args\n    :return: dictionary mapping concrete argument names to their default values\n    \"\"\"\n    sig = inspect.signature(model.forward)\n\n    concrete_args = {}\n    for parameter in sig.parameters.values():\n        if parameter.name in sample_input:\n            continue\n        if parameter.kind == inspect._ParameterKind.VAR_POSITIONAL:\n            value = list()\n        elif parameter.kind == inspect._ParameterKind.VAR_KEYWORD:\n            value = dict()\n        elif parameter.name == \"use_cache\":\n            value = False\n        else:\n            value = parameter.default\n\n        concrete_args[parameter.name] = value\n\n    return concrete_args\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.topological_partition","title":"<code>topological_partition(graph, targets)</code>","text":"<p>Partition the graph into partitions such that each <code>target</code> belongs to exactly one partition and executing each partition depends only on intermediate values produced by executing the partitions before it.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>GraphModule</code> <p>graph being partitioned</p> required <code>targets</code> <code>Set[Module]</code> <p>target modules which will be assigned to disjoint partitions</p> required <p>Returns:</p> Type Description <code>List[List[Node]]</code> <p>list of partitions, where each partition is a list of nodes belonging to that partition</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def topological_partition(graph: GraphModule, targets: Set[Module]) -&gt; List[List[Node]]:\n    \"\"\"\n    Partition the graph into partitions such that each `target` belongs to exactly one\n    partition and executing each partition depends only on intermediate values produced\n    by executing the partitions before it.\n\n    :param graph: graph being partitioned\n    :param targets: target modules which will be assigned to disjoint partitions\n    :return: list of partitions, where each partition is a list of nodes belonging to\n        that partition\n    \"\"\"\n    assert graph_is_well_formed(graph.graph)\n    target_nodes = find_target_nodes(graph, targets)\n\n    partitions: List[List[Node]] = [[]]\n    remaining_indegrees = {\n        node: len([node for node in node.all_input_nodes if node.op != \"get_attr\"])\n        for node in graph.graph.nodes\n    }\n    partition_index = 0  # global counter\n\n    # start with graph input nodes,\n    # but delay the `get_attr` nodes as long as possible\n    queue = deque(\n        node\n        for node in graph.graph.nodes\n        if remaining_indegrees[node] == 0 and node.op != \"get_attr\"\n    )\n    while len(queue) &gt; 0:\n        node = queue.popleft()\n\n        # assign to partition\n        partitions[partition_index].append(node)\n\n        # guarantee targets are assigned to disjoint partitions\n        if node in target_nodes:\n            partition_index += 1\n            partitions.append([])\n\n        # recurse on last indegree only in order to guarantee that\n        # the node is assigned to maximal partition\n        for user in node.users:\n            remaining_indegrees[user] -= 1\n            if remaining_indegrees[user] == 0:\n                queue.append(user)\n\n    # an ideal implementation would involve implicitly consolidating partition indices\n    # so that each node is assigned to the maximum partition possible (in order to delay\n    # execution as long as possible), but saving these nodes for last covers the most\n    # common and costly case (get_attr)\n    for node in graph.graph.find_nodes(op=\"get_attr\"):\n        user_partitions = []\n        for user in node.users:\n            for index in range(len(partitions)):\n                if user in partitions[index]:\n                    user_partitions.append(index)\n                    break\n        partition_index = min(user_partitions)\n        partitions[partition_index].insert(0, node)\n\n    assert set().union(*partitions) == set(graph.graph.nodes)\n    return partitions\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.trace_consumed_names","title":"<code>trace_consumed_names(subgraphs)</code>","text":"<p>Populate the <code>consumed_names</code> attribute of each Subgraph according to when inputs are last used in order to vacate the <code>intermediates</code> cache and save memory</p> <p>Parameters:</p> Name Type Description Default <code>subgraphs</code> <code>List[Subgraph]</code> <p>list of subgraphs with empty <code>consumed_names</code> attributes</p> required Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def trace_consumed_names(subgraphs: List[Subgraph]):\n    \"\"\"\n    Populate the `consumed_names` attribute of each Subgraph according to when inputs\n    are last used in order to vacate the `intermediates` cache and save memory\n\n    :param subgraphs: list of subgraphs with empty `consumed_names` attributes\n    \"\"\"\n    # populate consumed_names according to when inputs are last used\n    # in order to vacate the `intermediates` cache and save memory\n    all_input_names = set().union(*(subgraph.input_names for subgraph in subgraphs))\n    for input_name in all_input_names:\n        for subgraph in reversed(subgraphs):\n            if input_name in subgraph.input_names:\n                subgraph.consumed_names.add(input_name)\n                break\n        else:\n            raise ValueError(f\"Could not find input name {input_name} in subgraphs\")\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/helpers/#llmcompressor.pipelines.sequential.helpers.trace_subgraphs","title":"<code>trace_subgraphs(model, sample_input, sequential_targets, ignore)</code>","text":"<p>Trace a model to produce subgraphs, where each sequential target belongs to exactly one subgraph and where executing each subgraph in order is equivalent to executing the original model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>model being traced</p> required <code>sample_input</code> <code>Dict[str, Any]</code> <p>inputs whose values will change during execution but whose len, bool, and contains values are assumed constant across batches</p> required <code>sequential_targets</code> <code>List[str]</code> <p>list of patterns matching sequential targets</p> required <code>ignore</code> <code>List[str]</code> <p>modules to ignore during tracing, in the future will specify functions and methods to skip during tracing</p> required <p>Returns:</p> Type Description <code>List[Subgraph]</code> <p>a list of Subgraphs in order of execution</p> Source code in <code>src/llmcompressor/pipelines/sequential/helpers.py</code> <pre><code>def trace_subgraphs(\n    model: PreTrainedModel,\n    sample_input: Dict[str, Any],\n    sequential_targets: List[str],\n    ignore: List[str],\n) -&gt; List[Subgraph]:\n    \"\"\"\n    Trace a model to produce subgraphs, where each sequential target belongs to exactly\n    one subgraph and where executing each subgraph in order is equivalent to executing\n    the original model\n\n    :param model: model being traced\n    :param sample_input: inputs whose values will change during execution but whose\n        __len__, __bool__, and __contains__ values are assumed constant across batches\n    :param sequential_targets: list of patterns matching sequential targets\n    :param ignore: modules to ignore during tracing, in the future will specify\n        functions and methods to skip during tracing\n    :return: a list of Subgraphs in order of execution\n    \"\"\"\n    # find modules\n    sequential_targets = match_modules(model, sequential_targets)\n    ignore = match_modules(model, ignore)\n\n    # initialize arguments\n    tracer = get_tracer(model, sequential_targets, ignore)\n    concrete_args = populate_concrete_args(model, sample_input)\n\n    # trace\n    with calibration_forward_context(model), HooksMixin.disable_hooks():\n        graph = GraphModule(\n            model,\n            tracer.trace(\n                model,\n                dummy_inputs=sample_input,\n                concrete_args=concrete_args,\n                complete_concrete_args_with_inputs_not_in_dummy_inputs=False,\n                # bug in trace throws an error for variadic\n                # args and kwargs in function signature\n            ),\n        )\n\n    # copy metadata\n    graph.config = model.config\n    graph.class_for_deserialization = model.__class__\n    graph.device = model.device\n\n    # perform subgraph partition\n    partitions = topological_partition(graph, sequential_targets)\n    subgraphs = partition_graph(model, partitions)\n    trace_consumed_names(subgraphs)\n\n    return subgraphs\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/pipeline/","title":"llmcompressor.pipelines.sequential.pipeline","text":""},{"location":"reference/llmcompressor/pipelines/sequential/pipeline/#llmcompressor.pipelines.sequential.pipeline.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>CalibrationPipeline</code></p> Source code in <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> <pre><code>@CalibrationPipeline.register(\"sequential\")\nclass SequentialPipeline(CalibrationPipeline):\n    @staticmethod\n    def __call__(\n        model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n    ):\n        \"\"\"\n        Run a sequential data pipeline according to the following steps:\n\n        1. The model is partitioned into subgraphs according to `sequential_targets`\n        2. Data passes through each subgraph sequentially. Data is passed through each\n            subgraph twice, once to trigger calibration hooks, then a second time in\n            order to capture activations after quantization has occurred through hooks.\n        3. The intermediate activations between each subgraph are cached and offloaded\n            to the cpu between each batch in order to save memory\n\n        This pipeline requires that the model be traceable with respect to data from the\n        data loader. This may be an issue for vision models with vision datasets, due\n        to specialized input processing in the model.\n\n        In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A\n        model can be made traceable by wrapping the untraceable functions (see\n        llmcompressor.transformers.tracing)\n\n        :param model: model being calibrated\n        :param dataloader: loads data for calibration\n        :param dataset_args: dataset arguments relevant to pipelines\n        \"\"\"\n        session = active_session()\n\n        # infer sequential targets\n        modifiers = session.get_modifiers()\n        sequential_targets, ignore = get_targets_from_modifiers(modifiers, model)\n\n        # trace subgraphs\n        sample_input = next(iter(dataloader))\n        subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n\n        LifecycleCallbacks.calibration_epoch_start()\n\n        with calibration_forward_context(model), DisableQuantization(model):\n            # prepare intermediates cache\n            model_device = get_execution_device(model)\n            intermediates = IntermediatesCache.from_dataloader(dataloader, model_device)\n\n            num_subgraphs = len(subgraphs)\n            for subgraph_index, subgraph in enumerate(subgraphs):\n                # prepare tqdm description texts\n                calib_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Calibrating\"\n                prop_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Propagating\"\n\n                # do an preliminary pass to trigger modifier hooks\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                    inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                    subgraph.forward(model, **inputs)\n\n                # trigger compression\n                LifecycleCallbacks.sequential_epoch_end()\n\n                # this pass does not trigger modifier hooks\n                # and is only used for capturing outputs from newly compressed modules\n                with HooksMixin.disable_hooks():\n                    for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                        inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                        output = subgraph.forward(model, **inputs)\n\n                        if subgraph_index &lt; num_subgraphs - 1:\n                            intermediates.update(batch_idx, output)\n                            intermediates.delete(batch_idx, subgraph.consumed_names)\n\n            # redudant, finish any remaining compression\n            LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pipelines/sequential/pipeline/#llmcompressor.pipelines.sequential.pipeline.SequentialPipeline.__call__","title":"<code>__call__(model, dataloader, dataset_args)</code>  <code>staticmethod</code>","text":"<p>Run a sequential data pipeline according to the following steps:</p> <ol> <li>The model is partitioned into subgraphs according to <code>sequential_targets</code></li> <li>Data passes through each subgraph sequentially. Data is passed through each     subgraph twice, once to trigger calibration hooks, then a second time in     order to capture activations after quantization has occurred through hooks.</li> <li>The intermediate activations between each subgraph are cached and offloaded     to the cpu between each batch in order to save memory</li> </ol> <p>This pipeline requires that the model be traceable with respect to data from the data loader. This may be an issue for vision models with vision datasets, due to specialized input processing in the model.</p> <p>In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A model can be made traceable by wrapping the untraceable functions (see llmcompressor.transformers.tracing)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model being calibrated</p> required <code>dataloader</code> <code>DataLoader</code> <p>loads data for calibration</p> required <code>dataset_args</code> <code>DatasetArguments</code> <p>dataset arguments relevant to pipelines</p> required Source code in <code>src/llmcompressor/pipelines/sequential/pipeline.py</code> <pre><code>@staticmethod\ndef __call__(\n    model: torch.nn.Module, dataloader: DataLoader, dataset_args: \"DatasetArguments\"\n):\n    \"\"\"\n    Run a sequential data pipeline according to the following steps:\n\n    1. The model is partitioned into subgraphs according to `sequential_targets`\n    2. Data passes through each subgraph sequentially. Data is passed through each\n        subgraph twice, once to trigger calibration hooks, then a second time in\n        order to capture activations after quantization has occurred through hooks.\n    3. The intermediate activations between each subgraph are cached and offloaded\n        to the cpu between each batch in order to save memory\n\n    This pipeline requires that the model be traceable with respect to data from the\n    data loader. This may be an issue for vision models with vision datasets, due\n    to specialized input processing in the model.\n\n    In the event that tracing fails, a torch.fx.proxy.TraceError will be raised. A\n    model can be made traceable by wrapping the untraceable functions (see\n    llmcompressor.transformers.tracing)\n\n    :param model: model being calibrated\n    :param dataloader: loads data for calibration\n    :param dataset_args: dataset arguments relevant to pipelines\n    \"\"\"\n    session = active_session()\n\n    # infer sequential targets\n    modifiers = session.get_modifiers()\n    sequential_targets, ignore = get_targets_from_modifiers(modifiers, model)\n\n    # trace subgraphs\n    sample_input = next(iter(dataloader))\n    subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n\n    LifecycleCallbacks.calibration_epoch_start()\n\n    with calibration_forward_context(model), DisableQuantization(model):\n        # prepare intermediates cache\n        model_device = get_execution_device(model)\n        intermediates = IntermediatesCache.from_dataloader(dataloader, model_device)\n\n        num_subgraphs = len(subgraphs)\n        for subgraph_index, subgraph in enumerate(subgraphs):\n            # prepare tqdm description texts\n            calib_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Calibrating\"\n            prop_desc = f\"({subgraph_index + 1}/{num_subgraphs}): Propagating\"\n\n            # do an preliminary pass to trigger modifier hooks\n            for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=calib_desc):\n                inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                subgraph.forward(model, **inputs)\n\n            # trigger compression\n            LifecycleCallbacks.sequential_epoch_end()\n\n            # this pass does not trigger modifier hooks\n            # and is only used for capturing outputs from newly compressed modules\n            with HooksMixin.disable_hooks():\n                for batch_idx in tqdm.tqdm(range(len(dataloader)), desc=prop_desc):\n                    inputs = intermediates.fetch(batch_idx, subgraph.input_names)\n                    output = subgraph.forward(model, **inputs)\n\n                    if subgraph_index &lt; num_subgraphs - 1:\n                        intermediates.update(batch_idx, output)\n                        intermediates.delete(batch_idx, subgraph.consumed_names)\n\n        # redudant, finish any remaining compression\n        LifecycleCallbacks.calibration_epoch_end()\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/","title":"llmcompressor.pytorch","text":"<p>Functionality for working with and sparsifying Models in the PyTorch framework</p>"},{"location":"reference/llmcompressor/pytorch/model_load/","title":"llmcompressor.pytorch.model_load","text":""},{"location":"reference/llmcompressor/pytorch/model_load/helpers/","title":"llmcompressor.pytorch.model_load.helpers","text":""},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.fallback_to_cpu","title":"<code>fallback_to_cpu(device)</code>","text":"<p>Takes in a device string and forces it to cpu if cuda is not available</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>device id to check</p> required <p>Returns:</p> Type Description <code>str</code> <p>device modified for CUDA status</p> Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def fallback_to_cpu(device: str) -&gt; str:\n    \"\"\"\n    Takes in a device string and forces it to cpu if cuda is not available\n\n    :param device: device id to check\n    :return: device modified for CUDA status\n    \"\"\"\n    if \"cuda\" in device and not torch.cuda.is_available():\n        logger.warning(\n            f\"Requested {device} but CUDA is not available, falling back to CPU\"\n        )\n        return \"cpu\"\n\n    return device\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.get_completed_stages","title":"<code>get_completed_stages(checkpoint_dir)</code>","text":"<p>Given a checkpoint directory for a staged run, get the list of stages that have completed in a prior run if the checkpoint_dir is a string</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Any</code> <p>path to staged checkpoint</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>list of completed stage names</p> Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def get_completed_stages(checkpoint_dir: Any) -&gt; List[str]:\n    \"\"\"\n    Given a checkpoint directory for a staged run, get the list of stages that\n    have completed in a prior run if the checkpoint_dir is a string\n\n    :param checkpoint_dir: path to staged checkpoint\n    :return: list of completed stage names\n    \"\"\"\n    if isinstance(checkpoint_dir, str):\n        stage_path = os.path.join(checkpoint_dir, COMPLETED_STAGES_FILENAME)\n        if os.path.exists(stage_path):\n            with open(stage_path) as stage_file:\n                stage_data = json.load(stage_file)\n                return stage_data[\"completed\"]\n\n    return []\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.get_session_model","title":"<code>get_session_model()</code>","text":"<p>Returns:</p> Type Description <code>Optional[Module]</code> <p>pytorch module stored by the active CompressionSession, or None if no session is active</p> Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def get_session_model() -&gt; Optional[Module]:\n    \"\"\"\n    :return: pytorch module stored by the active CompressionSession,\n        or None if no session is active\n    \"\"\"\n    session = active_session()\n    if not session:\n        return None\n\n    active_model = session.state.model\n    return active_model\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.load_safetensors_state_dict","title":"<code>load_safetensors_state_dict(file_path)</code>","text":"<p>Load a safetensors file from disk</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to the safetensors file</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>dictionary of safetensors data</p> Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def load_safetensors_state_dict(file_path: str) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Load a safetensors file from disk\n\n    :param file_path: path to the safetensors file\n    :return: dictionary of safetensors data\n    \"\"\"\n    with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n        return {key: f.get_tensor(key) for key in f.keys()}\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.parse_dtype","title":"<code>parse_dtype(dtype_arg)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dtype_arg</code> <code>Union[str, dtype]</code> <p>dtype or string to parse</p> required <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype parsed from input string</p> Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def parse_dtype(dtype_arg: Union[str, torch.dtype]) -&gt; torch.dtype:\n    \"\"\"\n    :param dtype_arg: dtype or string to parse\n    :return: torch.dtype parsed from input string\n    \"\"\"\n    dtype_arg = str(dtype_arg)\n    dtype = \"auto\"  # get precision from model by default\n    if dtype_arg in (\"half\", \"float16\", \"torch.float16\"):\n        dtype = torch.float16\n    elif dtype_arg in (\"torch.bfloat16\", \"bfloat16\"):\n        dtype = torch.bfloat16\n    elif dtype_arg in (\"full\", \"float32\", \"torch.float32\"):\n        dtype = torch.float32\n\n    return dtype\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.save_checkpoint","title":"<code>save_checkpoint(save_path, model, processor=None, save_safetensors=True, save_compressed=True, skip_sparsity_compression_stats=False)</code>","text":"<p>Save a model, processor, and recipe</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path used to save model and processor</p> required <code>model</code> <code>PreTrainedModel</code> <p>model to save</p> required <code>processor</code> <code>Optional[Processor]</code> <p>processor to save</p> <code>None</code> <code>save_safetensors</code> <code>bool</code> <p>save model checkpoint using safetensors file type</p> <code>True</code> <code>save_compressed</code> <code>bool</code> <p>save model checkpoint using compressed-tensors format</p> <code>True</code> Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def save_checkpoint(\n    save_path: str,\n    model: PreTrainedModel,\n    processor: Optional[Processor] = None,\n    save_safetensors: bool = True,\n    save_compressed: bool = True,\n    skip_sparsity_compression_stats: bool = False,\n):\n    \"\"\"\n    Save a model, processor, and recipe\n\n    :param save_path: Path used to save model and processor\n    :param model: model to save\n    :param processor: processor to save\n    :param save_safetensors: save model checkpoint using safetensors file type\n    :param save_compressed: save model checkpoint using compressed-tensors format\n    \"\"\"\n    # saving the model also saves the recipe\n    model.save_pretrained(\n        save_path,\n        save_safetensors=save_safetensors,\n        save_compressed=save_compressed,\n        skip_sparsity_compression_stats=skip_sparsity_compression_stats,\n    )\n    if processor is not None:\n        processor.save_pretrained(save_path)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/model_load/helpers/#llmcompressor.pytorch.model_load.helpers.save_completed_stages","title":"<code>save_completed_stages(checkpoint_dir, completed_stages)</code>","text":"<p>Save a list of completed stages to a checkpoint directory</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>model checkpoint directory to save stages to</p> required <code>completed_stages</code> <code>List[str]</code> <p>list of stage names that have been run</p> required Source code in <code>src/llmcompressor/pytorch/model_load/helpers.py</code> <pre><code>def save_completed_stages(checkpoint_dir: str, completed_stages: List[str]):\n    \"\"\"\n    Save a list of completed stages to a checkpoint directory\n\n    :param checkpoint_dir: model checkpoint directory to save stages to\n    :param completed_stages: list of stage names that have been run\n    \"\"\"\n    stage_path = os.path.join(checkpoint_dir, COMPLETED_STAGES_FILENAME)\n    with open(stage_path, \"w\") as out_file:\n        json.dump({\"completed\": completed_stages}, out_file)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/","title":"llmcompressor.pytorch.utils","text":"<p>Generic code used as utilities and helpers for PyTorch</p>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.ModuleSparsificationInfo","title":"<code>ModuleSparsificationInfo</code>","text":"<p>Helper class for providing information related to torch Module parameters and the amount of sparsification applied. Includes information for pruning and quantization</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>torch Module to analyze</p> required <code>state_dict</code> <code>Optional[Dict[str, Tensor]]</code> <p>optional state_dict to analyze in place of the torch model. This is used when analyzing an FSDP model, where the full weights may not be accessible</p> <code>None</code> Source code in <code>src/llmcompressor/pytorch/utils/sparsification.py</code> <pre><code>class ModuleSparsificationInfo:\n    \"\"\"\n    Helper class for providing information related to torch Module parameters\n    and the amount of sparsification applied. Includes information for pruning\n    and quantization\n\n    :param module: torch Module to analyze\n    :param state_dict: optional state_dict to analyze in place of the torch model. This\n    is used when analyzing an FSDP model, where the full weights may not be accessible\n    \"\"\"\n\n    def __init__(\n        self, module: Module, state_dict: Optional[Dict[str, torch.Tensor]] = None\n    ):\n        self.module = module\n\n        if state_dict is not None:\n            # when analyzing an FSDP model, the state_dict does not differentiate\n            # between trainable and non-trainable parameters\n            # (e.g. it can contain buffers) this means that the\n            # self.trainable_parameters may be overestimated\n            self.trainable_params = state_dict\n        else:\n            if hasattr(module, \"_hf_hook\"):\n                self.trainable_params = get_state_dict_offloaded_model(module)\n            else:\n                self.trainable_params = {\n                    k: v for k, v in self.module.named_parameters() if v.requires_grad\n                }\n\n    def __str__(self):\n        return json.dumps(\n            {\n                \"params_summary\": {\n                    \"total\": self.params_total,\n                    \"sparse\": self.params_sparse,\n                    \"sparsity_percent\": self.params_sparse_percent,\n                    \"quantized\": self.params_quantized,\n                    \"quantized_percent\": self.params_quantized_percent,\n                },\n                \"params_info\": self.params_info,\n            }\n        )\n\n    @property\n    def params_total(self) -&gt; int:\n        \"\"\"\n        :return: total number of trainable parameters in the model\n        \"\"\"\n        return sum(torch.numel(param) for param in self.trainable_params.values())\n\n    @property\n    def params_sparse(self) -&gt; int:\n        \"\"\"\n        :return: total number of sparse (0) trainable parameters in the model\n        \"\"\"\n        return sum(\n            round(tensor_sparsity(param).item() * torch.numel(param))\n            for param in tqdm(\n                self.trainable_params.values(), desc=\"Calculating model sparsity\"\n            )\n        )\n\n    @property\n    def params_sparse_percent(self) -&gt; float:\n        \"\"\"\n        :return: percent of sparsified parameters in the entire model\n        \"\"\"\n        return self.params_sparse / float(self.params_total) * 100\n\n    @property\n    def params_quantized(self) -&gt; int:\n        \"\"\"\n        :return: number of parameters across quantized layers\n        \"\"\"\n        num_params = 0\n        for name, layer in get_quantized_layers(self.module):\n            if getattr(layer, \"weight\", None) is not None:\n                num_params += torch.numel(layer.weight)\n            if getattr(layer, \"bias\", None) is not None:\n                num_params += torch.numel(layer.bias)\n\n        return num_params\n\n    @property\n    def params_quantized_percent(self) -&gt; float:\n        \"\"\"\n        :return: percentage of parameters that have been quantized\n        \"\"\"\n        return self.params_quantized / float(self.params_total) * 100\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.ModuleSparsificationInfo.params_quantized","title":"<code>params_quantized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>number of parameters across quantized layers</p>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.ModuleSparsificationInfo.params_quantized_percent","title":"<code>params_quantized_percent</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>percentage of parameters that have been quantized</p>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.ModuleSparsificationInfo.params_sparse","title":"<code>params_sparse</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>total number of sparse (0) trainable parameters in the model</p>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.ModuleSparsificationInfo.params_sparse_percent","title":"<code>params_sparse_percent</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>percent of sparsified parameters in the entire model</p>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.ModuleSparsificationInfo.params_total","title":"<code>params_total</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>total number of trainable parameters in the model</p>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.get_linear_layers","title":"<code>get_linear_layers(module)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>the module to grab all linear layers for</p> required <p>Returns:</p> Type Description <code>Dict[str, Module]</code> <p>a list of all linear layers in the module</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def get_linear_layers(module: Module) -&gt; Dict[str, Module]:\n    \"\"\"\n    :param module: the module to grab all linear layers for\n    :return: a list of all linear layers in the module\n    \"\"\"\n    return {\n        name: mod for name, mod in module.named_modules() if isinstance(mod, Linear)\n    }\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.get_quantized_layers","title":"<code>get_quantized_layers(module)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>the module to get the quantized layers from</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, Module]]</code> <p>a list containing the names and modules of the quantized layers (Embedding, Linear, Conv2d, Conv3d)</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def get_quantized_layers(module: Module) -&gt; List[Tuple[str, Module]]:\n    \"\"\"\n    :param module: the module to get the quantized layers from\n    :return: a list containing the names and modules of the quantized layers\n        (Embedding, Linear, Conv2d, Conv3d)\n    \"\"\"\n\n    quantized_layers = []\n    for name, mod in module.named_modules():\n        if hasattr(mod, \"quantization_scheme\"):\n            weight_scheme = getattr(mod.quantization_scheme, \"weights\", None)\n            if weight_scheme is not None and hasattr(mod, \"weight\"):\n                quantized_layers.append((name, mod))\n\n    return quantized_layers\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.set_deterministic_seeds","title":"<code>set_deterministic_seeds(seed=0)</code>","text":"<p>Manually seeds the numpy, random, and torch packages. Also sets torch.backends.cudnn.deterministic to True</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>the manual seed to use. Default is 0</p> <code>0</code> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def set_deterministic_seeds(seed: int = 0):\n    \"\"\"\n    Manually seeds the numpy, random, and torch packages.\n    Also sets torch.backends.cudnn.deterministic to True\n    :param seed: the manual seed to use. Default is 0\n    \"\"\"\n    numpy.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.tensor_sparsity","title":"<code>tensor_sparsity(tens, dim=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tens</code> <code>Tensor</code> <p>the tensor to calculate the sparsity for</p> required <code>dim</code> <code>Union[None, int, List[int], Tuple[int, ...]]</code> <p>the dimension(s) to split the calculations over; ex, can split over batch, channels, or combos</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>the sparsity of the input tens, ie the fraction of numbers that are zero</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensor_sparsity(\n    tens: Tensor, dim: Union[None, int, List[int], Tuple[int, ...]] = None\n) -&gt; Tensor:\n    \"\"\"\n    :param tens: the tensor to calculate the sparsity for\n    :param dim: the dimension(s) to split the calculations over;\n        ex, can split over batch, channels, or combos\n    :return: the sparsity of the input tens, ie the fraction of numbers that are zero\n    \"\"\"\n    if dim is None:\n        zeros = (tens.cpu() == 0).sum()\n        total = tens.numel()\n\n        return zeros.float() / float(total)\n\n    if isinstance(dim, int):\n        dim = [dim]\n\n    if max(dim) &gt;= len(tens.shape):\n        raise ValueError(\n            \"Unsupported dim given of {} in {} for tensor shape {}\".format(\n                max(dim), dim, tens.shape\n            )\n        )\n\n    sum_dims = [ind for ind in range(len(tens.shape)) if ind not in dim]\n    zeros = (tens == 0).sum(dim=sum_dims) if sum_dims else tens == 0\n    total = numpy.prod(\n        [tens.shape[ind] for ind in range(len(tens.shape)) if ind not in dim]\n    )\n\n    permute_order = sorted(\n        ((d, len(dim) - i - 1) for i, d in enumerate(dim)), reverse=True\n    )\n    permute = [d[1] for d in permute_order]\n\n    if permute != [i for i in range(len(permute))]:\n        # need to permute to get desired dimensions at the front\n        zeros = zeros.permute(*permute).contiguous()\n\n    return zeros.float() / float(total)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.tensors_module_forward","title":"<code>tensors_module_forward(tensors, module, check_feat_lab_inp=True)</code>","text":"<p>Default function for calling into a model with data for a forward execution. Returns the model result. Note, if an iterable the features to be passed into the model are considered to be at index 0 and other indices are for labels.</p> <p>Supported use cases: single tensor, iterable with first tensor taken as the features to pass into the model</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[Tensor, Iterable[Tensor], Mapping[Any, Tensor]]</code> <p>the data to be passed into the model, if an iterable the features to be passed into the model are considered to be at index 0 and other indices are for labels</p> required <code>module</code> <code>Module</code> <p>the module to pass the data into</p> required <code>check_feat_lab_inp</code> <code>bool</code> <p>True to check if the incoming tensors looks like it's made up of features and labels ie a tuple or list with 2 items (typical output from a data loader) and will call into the model with just the first element assuming it's the features False to not check</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of calling into the model for a forward pass</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensors_module_forward(\n    tensors: Union[Tensor, Iterable[Tensor], Mapping[Any, Tensor]],\n    module: Module,\n    check_feat_lab_inp: bool = True,\n) -&gt; Any:\n    \"\"\"\n    Default function for calling into a model with data for a forward execution.\n    Returns the model result.\n    Note, if an iterable the features to be passed into the model are considered\n    to be at index 0 and other indices are for labels.\n\n    Supported use cases: single tensor,\n    iterable with first tensor taken as the features to pass into the model\n\n    :param tensors: the data to be passed into the model, if an iterable the features\n        to be passed into the model are considered to be at index 0 and other indices\n        are for labels\n    :param module: the module to pass the data into\n    :param check_feat_lab_inp: True to check if the incoming tensors looks like\n        it's made up of features and labels ie a tuple or list with 2 items\n        (typical output from a data loader) and will call into the model with just\n        the first element assuming it's the features False to not check\n    :return: the result of calling into the model for a forward pass\n    \"\"\"\n    if (\n        (isinstance(tensors, tuple) or isinstance(tensors, List))\n        and len(tensors) == 2\n        and check_feat_lab_inp\n    ):\n        # assume if this is a list or tuple of 2 items that it is made up of\n        # (features, labels) pass the features into a recursive call for the model\n        return tensors_module_forward(tensors[0], module, check_feat_lab_inp=False)\n\n    if isinstance(tensors, Tensor):\n        return module(tensors)\n\n    if isinstance(tensors, Mapping):\n        return module(**tensors)\n\n    if isinstance(tensors, Iterable):\n        return module(*tensors)\n\n    raise ValueError(\n        \"unrecognized type for data given of {}\".format(tensors.__class__.__name__)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.tensors_to_device","title":"<code>tensors_to_device(tensors, device)</code>","text":"<p>Default function for putting a tensor or collection of tensors to the proper device. Returns the tensor references after being placed on the proper device.</p> <p>Supported use cases:     - single tensor     - Dictionary of single tensors     - Dictionary of iterable of tensors     - Dictionary of dictionary of tensors     - Iterable of single tensors     - Iterable of iterable of tensors     - Iterable of dictionary of tensors</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors or collection of tensors to put onto a device</p> required <code>device</code> <code>str</code> <p>the string representing the device to put the tensors on, ex: 'cpu', 'cuda', 'cuda:1'</p> required <p>Returns:</p> Type Description <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors or collection of tensors after being placed on the device</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensors_to_device(\n    tensors: Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]], device: str\n) -&gt; Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]:\n    \"\"\"\n    Default function for putting a tensor or collection of tensors to the proper device.\n    Returns the tensor references after being placed on the proper device.\n\n    Supported use cases:\n        - single tensor\n        - Dictionary of single tensors\n        - Dictionary of iterable of tensors\n        - Dictionary of dictionary of tensors\n        - Iterable of single tensors\n        - Iterable of iterable of tensors\n        - Iterable of dictionary of tensors\n\n    :param tensors: the tensors or collection of tensors to put onto a device\n    :param device: the string representing the device to put the tensors on,\n        ex: 'cpu', 'cuda', 'cuda:1'\n    :return: the tensors or collection of tensors after being placed on the device\n    \"\"\"\n    if isinstance(tensors, Tensor):\n        return tensors.to(device)\n\n    if isinstance(tensors, OrderedDict):\n        return OrderedDict(\n            [(key, tensors_to_device(tens, device)) for key, tens in tensors.items()]\n        )\n\n    if isinstance(tensors, Mapping):\n        return {key: tensors_to_device(tens, device) for key, tens in tensors.items()}\n\n    if isinstance(tensors, tuple):\n        return tuple(tensors_to_device(tens, device) for tens in tensors)\n\n    if isinstance(tensors, Iterable):\n        return [tensors_to_device(tens, device) for tens in tensors]\n\n    raise ValueError(\n        \"unrecognized type for tensors given of {}\".format(tensors.__class__.__name__)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/#llmcompressor.pytorch.utils.tensors_to_precision","title":"<code>tensors_to_precision(tensors, full_precision)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors to change the precision of</p> required <code>full_precision</code> <code>bool</code> <p>True for full precision (float 32) and False for half (float 16)</p> required <p>Returns:</p> Type Description <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors converted to the desired precision</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensors_to_precision(\n    tensors: Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]], full_precision: bool\n) -&gt; Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]:\n    \"\"\"\n    :param tensors: the tensors to change the precision of\n    :param full_precision: True for full precision (float 32) and\n        False for half (float 16)\n    :return: the tensors converted to the desired precision\n    \"\"\"\n    if isinstance(tensors, Tensor):\n        return tensors.float() if full_precision else tensors.half()\n\n    if isinstance(tensors, Mapping):\n        return {\n            key: tensors_to_precision(tens, full_precision)\n            for key, tens in tensors.items()\n        }\n\n    if isinstance(tensors, tuple):\n        return tuple(tensors_to_precision(tens, full_precision) for tens in tensors)\n\n    if isinstance(tensors, Iterable):\n        return [tensors_to_precision(tens, full_precision) for tens in tensors]\n\n    raise ValueError(\n        \"unrecognized type for tensors given of {}\".format(tensors.__class__.__name__)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/","title":"llmcompressor.pytorch.utils.helpers","text":"<p>Utility / helper functions</p>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.get_linear_layers","title":"<code>get_linear_layers(module)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>the module to grab all linear layers for</p> required <p>Returns:</p> Type Description <code>Dict[str, Module]</code> <p>a list of all linear layers in the module</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def get_linear_layers(module: Module) -&gt; Dict[str, Module]:\n    \"\"\"\n    :param module: the module to grab all linear layers for\n    :return: a list of all linear layers in the module\n    \"\"\"\n    return {\n        name: mod for name, mod in module.named_modules() if isinstance(mod, Linear)\n    }\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.get_quantized_layers","title":"<code>get_quantized_layers(module)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>the module to get the quantized layers from</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, Module]]</code> <p>a list containing the names and modules of the quantized layers (Embedding, Linear, Conv2d, Conv3d)</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def get_quantized_layers(module: Module) -&gt; List[Tuple[str, Module]]:\n    \"\"\"\n    :param module: the module to get the quantized layers from\n    :return: a list containing the names and modules of the quantized layers\n        (Embedding, Linear, Conv2d, Conv3d)\n    \"\"\"\n\n    quantized_layers = []\n    for name, mod in module.named_modules():\n        if hasattr(mod, \"quantization_scheme\"):\n            weight_scheme = getattr(mod.quantization_scheme, \"weights\", None)\n            if weight_scheme is not None and hasattr(mod, \"weight\"):\n                quantized_layers.append((name, mod))\n\n    return quantized_layers\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.set_deterministic_seeds","title":"<code>set_deterministic_seeds(seed=0)</code>","text":"<p>Manually seeds the numpy, random, and torch packages. Also sets torch.backends.cudnn.deterministic to True</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>the manual seed to use. Default is 0</p> <code>0</code> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def set_deterministic_seeds(seed: int = 0):\n    \"\"\"\n    Manually seeds the numpy, random, and torch packages.\n    Also sets torch.backends.cudnn.deterministic to True\n    :param seed: the manual seed to use. Default is 0\n    \"\"\"\n    numpy.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.tensor_sparsity","title":"<code>tensor_sparsity(tens, dim=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tens</code> <code>Tensor</code> <p>the tensor to calculate the sparsity for</p> required <code>dim</code> <code>Union[None, int, List[int], Tuple[int, ...]]</code> <p>the dimension(s) to split the calculations over; ex, can split over batch, channels, or combos</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>the sparsity of the input tens, ie the fraction of numbers that are zero</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensor_sparsity(\n    tens: Tensor, dim: Union[None, int, List[int], Tuple[int, ...]] = None\n) -&gt; Tensor:\n    \"\"\"\n    :param tens: the tensor to calculate the sparsity for\n    :param dim: the dimension(s) to split the calculations over;\n        ex, can split over batch, channels, or combos\n    :return: the sparsity of the input tens, ie the fraction of numbers that are zero\n    \"\"\"\n    if dim is None:\n        zeros = (tens.cpu() == 0).sum()\n        total = tens.numel()\n\n        return zeros.float() / float(total)\n\n    if isinstance(dim, int):\n        dim = [dim]\n\n    if max(dim) &gt;= len(tens.shape):\n        raise ValueError(\n            \"Unsupported dim given of {} in {} for tensor shape {}\".format(\n                max(dim), dim, tens.shape\n            )\n        )\n\n    sum_dims = [ind for ind in range(len(tens.shape)) if ind not in dim]\n    zeros = (tens == 0).sum(dim=sum_dims) if sum_dims else tens == 0\n    total = numpy.prod(\n        [tens.shape[ind] for ind in range(len(tens.shape)) if ind not in dim]\n    )\n\n    permute_order = sorted(\n        ((d, len(dim) - i - 1) for i, d in enumerate(dim)), reverse=True\n    )\n    permute = [d[1] for d in permute_order]\n\n    if permute != [i for i in range(len(permute))]:\n        # need to permute to get desired dimensions at the front\n        zeros = zeros.permute(*permute).contiguous()\n\n    return zeros.float() / float(total)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.tensors_module_forward","title":"<code>tensors_module_forward(tensors, module, check_feat_lab_inp=True)</code>","text":"<p>Default function for calling into a model with data for a forward execution. Returns the model result. Note, if an iterable the features to be passed into the model are considered to be at index 0 and other indices are for labels.</p> <p>Supported use cases: single tensor, iterable with first tensor taken as the features to pass into the model</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[Tensor, Iterable[Tensor], Mapping[Any, Tensor]]</code> <p>the data to be passed into the model, if an iterable the features to be passed into the model are considered to be at index 0 and other indices are for labels</p> required <code>module</code> <code>Module</code> <p>the module to pass the data into</p> required <code>check_feat_lab_inp</code> <code>bool</code> <p>True to check if the incoming tensors looks like it's made up of features and labels ie a tuple or list with 2 items (typical output from a data loader) and will call into the model with just the first element assuming it's the features False to not check</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of calling into the model for a forward pass</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensors_module_forward(\n    tensors: Union[Tensor, Iterable[Tensor], Mapping[Any, Tensor]],\n    module: Module,\n    check_feat_lab_inp: bool = True,\n) -&gt; Any:\n    \"\"\"\n    Default function for calling into a model with data for a forward execution.\n    Returns the model result.\n    Note, if an iterable the features to be passed into the model are considered\n    to be at index 0 and other indices are for labels.\n\n    Supported use cases: single tensor,\n    iterable with first tensor taken as the features to pass into the model\n\n    :param tensors: the data to be passed into the model, if an iterable the features\n        to be passed into the model are considered to be at index 0 and other indices\n        are for labels\n    :param module: the module to pass the data into\n    :param check_feat_lab_inp: True to check if the incoming tensors looks like\n        it's made up of features and labels ie a tuple or list with 2 items\n        (typical output from a data loader) and will call into the model with just\n        the first element assuming it's the features False to not check\n    :return: the result of calling into the model for a forward pass\n    \"\"\"\n    if (\n        (isinstance(tensors, tuple) or isinstance(tensors, List))\n        and len(tensors) == 2\n        and check_feat_lab_inp\n    ):\n        # assume if this is a list or tuple of 2 items that it is made up of\n        # (features, labels) pass the features into a recursive call for the model\n        return tensors_module_forward(tensors[0], module, check_feat_lab_inp=False)\n\n    if isinstance(tensors, Tensor):\n        return module(tensors)\n\n    if isinstance(tensors, Mapping):\n        return module(**tensors)\n\n    if isinstance(tensors, Iterable):\n        return module(*tensors)\n\n    raise ValueError(\n        \"unrecognized type for data given of {}\".format(tensors.__class__.__name__)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.tensors_to_device","title":"<code>tensors_to_device(tensors, device)</code>","text":"<p>Default function for putting a tensor or collection of tensors to the proper device. Returns the tensor references after being placed on the proper device.</p> <p>Supported use cases:     - single tensor     - Dictionary of single tensors     - Dictionary of iterable of tensors     - Dictionary of dictionary of tensors     - Iterable of single tensors     - Iterable of iterable of tensors     - Iterable of dictionary of tensors</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors or collection of tensors to put onto a device</p> required <code>device</code> <code>str</code> <p>the string representing the device to put the tensors on, ex: 'cpu', 'cuda', 'cuda:1'</p> required <p>Returns:</p> Type Description <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors or collection of tensors after being placed on the device</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensors_to_device(\n    tensors: Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]], device: str\n) -&gt; Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]:\n    \"\"\"\n    Default function for putting a tensor or collection of tensors to the proper device.\n    Returns the tensor references after being placed on the proper device.\n\n    Supported use cases:\n        - single tensor\n        - Dictionary of single tensors\n        - Dictionary of iterable of tensors\n        - Dictionary of dictionary of tensors\n        - Iterable of single tensors\n        - Iterable of iterable of tensors\n        - Iterable of dictionary of tensors\n\n    :param tensors: the tensors or collection of tensors to put onto a device\n    :param device: the string representing the device to put the tensors on,\n        ex: 'cpu', 'cuda', 'cuda:1'\n    :return: the tensors or collection of tensors after being placed on the device\n    \"\"\"\n    if isinstance(tensors, Tensor):\n        return tensors.to(device)\n\n    if isinstance(tensors, OrderedDict):\n        return OrderedDict(\n            [(key, tensors_to_device(tens, device)) for key, tens in tensors.items()]\n        )\n\n    if isinstance(tensors, Mapping):\n        return {key: tensors_to_device(tens, device) for key, tens in tensors.items()}\n\n    if isinstance(tensors, tuple):\n        return tuple(tensors_to_device(tens, device) for tens in tensors)\n\n    if isinstance(tensors, Iterable):\n        return [tensors_to_device(tens, device) for tens in tensors]\n\n    raise ValueError(\n        \"unrecognized type for tensors given of {}\".format(tensors.__class__.__name__)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/helpers/#llmcompressor.pytorch.utils.helpers.tensors_to_precision","title":"<code>tensors_to_precision(tensors, full_precision)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors to change the precision of</p> required <code>full_precision</code> <code>bool</code> <p>True for full precision (float 32) and False for half (float 16)</p> required <p>Returns:</p> Type Description <code>Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]</code> <p>the tensors converted to the desired precision</p> Source code in <code>src/llmcompressor/pytorch/utils/helpers.py</code> <pre><code>def tensors_to_precision(\n    tensors: Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]], full_precision: bool\n) -&gt; Union[Tensor, Iterable[Tensor], Dict[Any, Tensor]]:\n    \"\"\"\n    :param tensors: the tensors to change the precision of\n    :param full_precision: True for full precision (float 32) and\n        False for half (float 16)\n    :return: the tensors converted to the desired precision\n    \"\"\"\n    if isinstance(tensors, Tensor):\n        return tensors.float() if full_precision else tensors.half()\n\n    if isinstance(tensors, Mapping):\n        return {\n            key: tensors_to_precision(tens, full_precision)\n            for key, tens in tensors.items()\n        }\n\n    if isinstance(tensors, tuple):\n        return tuple(tensors_to_precision(tens, full_precision) for tens in tensors)\n\n    if isinstance(tensors, Iterable):\n        return [tensors_to_precision(tens, full_precision) for tens in tensors]\n\n    raise ValueError(\n        \"unrecognized type for tensors given of {}\".format(tensors.__class__.__name__)\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/","title":"llmcompressor.pytorch.utils.sparsification","text":"<p>Helper functions for retrieving information related to model sparsification</p>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/#llmcompressor.pytorch.utils.sparsification.ModuleSparsificationInfo","title":"<code>ModuleSparsificationInfo</code>","text":"<p>Helper class for providing information related to torch Module parameters and the amount of sparsification applied. Includes information for pruning and quantization</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>torch Module to analyze</p> required <code>state_dict</code> <code>Optional[Dict[str, Tensor]]</code> <p>optional state_dict to analyze in place of the torch model. This is used when analyzing an FSDP model, where the full weights may not be accessible</p> <code>None</code> Source code in <code>src/llmcompressor/pytorch/utils/sparsification.py</code> <pre><code>class ModuleSparsificationInfo:\n    \"\"\"\n    Helper class for providing information related to torch Module parameters\n    and the amount of sparsification applied. Includes information for pruning\n    and quantization\n\n    :param module: torch Module to analyze\n    :param state_dict: optional state_dict to analyze in place of the torch model. This\n    is used when analyzing an FSDP model, where the full weights may not be accessible\n    \"\"\"\n\n    def __init__(\n        self, module: Module, state_dict: Optional[Dict[str, torch.Tensor]] = None\n    ):\n        self.module = module\n\n        if state_dict is not None:\n            # when analyzing an FSDP model, the state_dict does not differentiate\n            # between trainable and non-trainable parameters\n            # (e.g. it can contain buffers) this means that the\n            # self.trainable_parameters may be overestimated\n            self.trainable_params = state_dict\n        else:\n            if hasattr(module, \"_hf_hook\"):\n                self.trainable_params = get_state_dict_offloaded_model(module)\n            else:\n                self.trainable_params = {\n                    k: v for k, v in self.module.named_parameters() if v.requires_grad\n                }\n\n    def __str__(self):\n        return json.dumps(\n            {\n                \"params_summary\": {\n                    \"total\": self.params_total,\n                    \"sparse\": self.params_sparse,\n                    \"sparsity_percent\": self.params_sparse_percent,\n                    \"quantized\": self.params_quantized,\n                    \"quantized_percent\": self.params_quantized_percent,\n                },\n                \"params_info\": self.params_info,\n            }\n        )\n\n    @property\n    def params_total(self) -&gt; int:\n        \"\"\"\n        :return: total number of trainable parameters in the model\n        \"\"\"\n        return sum(torch.numel(param) for param in self.trainable_params.values())\n\n    @property\n    def params_sparse(self) -&gt; int:\n        \"\"\"\n        :return: total number of sparse (0) trainable parameters in the model\n        \"\"\"\n        return sum(\n            round(tensor_sparsity(param).item() * torch.numel(param))\n            for param in tqdm(\n                self.trainable_params.values(), desc=\"Calculating model sparsity\"\n            )\n        )\n\n    @property\n    def params_sparse_percent(self) -&gt; float:\n        \"\"\"\n        :return: percent of sparsified parameters in the entire model\n        \"\"\"\n        return self.params_sparse / float(self.params_total) * 100\n\n    @property\n    def params_quantized(self) -&gt; int:\n        \"\"\"\n        :return: number of parameters across quantized layers\n        \"\"\"\n        num_params = 0\n        for name, layer in get_quantized_layers(self.module):\n            if getattr(layer, \"weight\", None) is not None:\n                num_params += torch.numel(layer.weight)\n            if getattr(layer, \"bias\", None) is not None:\n                num_params += torch.numel(layer.bias)\n\n        return num_params\n\n    @property\n    def params_quantized_percent(self) -&gt; float:\n        \"\"\"\n        :return: percentage of parameters that have been quantized\n        \"\"\"\n        return self.params_quantized / float(self.params_total) * 100\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/#llmcompressor.pytorch.utils.sparsification.ModuleSparsificationInfo.params_quantized","title":"<code>params_quantized</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>number of parameters across quantized layers</p>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/#llmcompressor.pytorch.utils.sparsification.ModuleSparsificationInfo.params_quantized_percent","title":"<code>params_quantized_percent</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>percentage of parameters that have been quantized</p>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/#llmcompressor.pytorch.utils.sparsification.ModuleSparsificationInfo.params_sparse","title":"<code>params_sparse</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>total number of sparse (0) trainable parameters in the model</p>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/#llmcompressor.pytorch.utils.sparsification.ModuleSparsificationInfo.params_sparse_percent","title":"<code>params_sparse_percent</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>percent of sparsified parameters in the entire model</p>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification/#llmcompressor.pytorch.utils.sparsification.ModuleSparsificationInfo.params_total","title":"<code>params_total</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>total number of trainable parameters in the model</p>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/","title":"llmcompressor.pytorch.utils.sparsification_info","text":""},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/","title":"llmcompressor.pytorch.utils.sparsification_info.configs","text":""},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationInfo","title":"<code>SparsificationInfo</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>class SparsificationInfo(BaseModel, ABC):\n    @classmethod\n    @abstractmethod\n    def from_module(\n        cls,\n        module: torch.nn.Module,\n        **kwargs,\n    ) -&gt; \"SparsificationInfo\":\n        \"\"\"\n        Factory method to create SparsificationInfo object from a module.\n\n        :param module: The module to create the SparsificationInfo object from.\n        :param kwargs: Additional arguments to pass to the SparsificationInfo object.\n        :return: A SparsificationInfo object.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def loggable_items(\n        self,\n        **kwargs,\n    ) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n        \"\"\"\n        Yield the loggable items for SparsificationInfo object.\n\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def filter_loggable_items_percentages_only(\n        items_to_log: Generator[Tuple[str, Any], None, None],\n        percentage_only: bool = False,\n    ):\n        \"\"\"\n        Filter the loggable items to only yield the percentages of the loggable items\n\n        :param items_to_log: A generator that yields the loggable items for this object.\n        :param percentage_only: If True, only yield the percentages of the loggable\n            items. If False, yield both the counts and percentages. Defaults to False\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n\n        def filter_percentage(log):\n            # log tag ends with percent\n            return log[0].endswith(\"percent\")\n\n        yield from SparsificationInfo._filter_items_to_log(\n            items_to_log,\n            filter_function=filter_percentage,\n            to_filter=percentage_only,\n        )\n\n    @staticmethod\n    def filter_loggable_items_non_zero_only(items_to_log, non_zero_only):\n        \"\"\"\n        Filter the loggable items to only yield the non-zero items\n\n        :param items_to_log: A generator that yields the loggable items for this object.\n        :param non_zero_only: If True, only yield information for non-zero items.\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n\n        def filter_non_zero_values(log):\n            # log value must be non-zero\n            return log[1] != 0\n\n        yield from SparsificationInfo._filter_items_to_log(\n            items_to_log,\n            filter_function=filter_non_zero_values,\n            to_filter=non_zero_only,\n        )\n\n    @staticmethod\n    def _filter_items_to_log(items_to_log, filter_function, to_filter: bool = True):\n        \"\"\"\n        Utility function to filter the loggable itemsn based on a filter function\n\n        :param items_to_log: A generator that yields the loggable items for this object.\n        :param filter_function: A function that takes in a loggable item and returns\n            True if the item should be yieled, False otherwise.\n        :param to_filter: If True, filter the loggable items. If False, do not filter.\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n        for loggable_item in items_to_log:\n            if not to_filter:\n                yield loggable_item\n            elif filter_function(loggable_item):\n                yield loggable_item\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationInfo.filter_loggable_items_non_zero_only","title":"<code>filter_loggable_items_non_zero_only(items_to_log, non_zero_only)</code>  <code>staticmethod</code>","text":"<p>Filter the loggable items to only yield the non-zero items</p> <p>Parameters:</p> Name Type Description Default <code>items_to_log</code> <p>A generator that yields the loggable items for this object.</p> required <code>non_zero_only</code> <p>If True, only yield information for non-zero items.</p> required <p>Returns:</p> Type Description <p>A generator that yields the loggable items for this object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@staticmethod\ndef filter_loggable_items_non_zero_only(items_to_log, non_zero_only):\n    \"\"\"\n    Filter the loggable items to only yield the non-zero items\n\n    :param items_to_log: A generator that yields the loggable items for this object.\n    :param non_zero_only: If True, only yield information for non-zero items.\n    :return: A generator that yields the loggable items for this object.\n    \"\"\"\n\n    def filter_non_zero_values(log):\n        # log value must be non-zero\n        return log[1] != 0\n\n    yield from SparsificationInfo._filter_items_to_log(\n        items_to_log,\n        filter_function=filter_non_zero_values,\n        to_filter=non_zero_only,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationInfo.filter_loggable_items_percentages_only","title":"<code>filter_loggable_items_percentages_only(items_to_log, percentage_only=False)</code>  <code>staticmethod</code>","text":"<p>Filter the loggable items to only yield the percentages of the loggable items</p> <p>Parameters:</p> Name Type Description Default <code>items_to_log</code> <code>Generator[Tuple[str, Any], None, None]</code> <p>A generator that yields the loggable items for this object.</p> required <code>percentage_only</code> <code>bool</code> <p>If True, only yield the percentages of the loggable items. If False, yield both the counts and percentages. Defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <p>A generator that yields the loggable items for this object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@staticmethod\ndef filter_loggable_items_percentages_only(\n    items_to_log: Generator[Tuple[str, Any], None, None],\n    percentage_only: bool = False,\n):\n    \"\"\"\n    Filter the loggable items to only yield the percentages of the loggable items\n\n    :param items_to_log: A generator that yields the loggable items for this object.\n    :param percentage_only: If True, only yield the percentages of the loggable\n        items. If False, yield both the counts and percentages. Defaults to False\n    :return: A generator that yields the loggable items for this object.\n    \"\"\"\n\n    def filter_percentage(log):\n        # log tag ends with percent\n        return log[0].endswith(\"percent\")\n\n    yield from SparsificationInfo._filter_items_to_log(\n        items_to_log,\n        filter_function=filter_percentage,\n        to_filter=percentage_only,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationInfo.from_module","title":"<code>from_module(module, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Factory method to create SparsificationInfo object from a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to create the SparsificationInfo object from.</p> required <code>kwargs</code> <p>Additional arguments to pass to the SparsificationInfo object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>SparsificationInfo</code> <p>A SparsificationInfo object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_module(\n    cls,\n    module: torch.nn.Module,\n    **kwargs,\n) -&gt; \"SparsificationInfo\":\n    \"\"\"\n    Factory method to create SparsificationInfo object from a module.\n\n    :param module: The module to create the SparsificationInfo object from.\n    :param kwargs: Additional arguments to pass to the SparsificationInfo object.\n    :return: A SparsificationInfo object.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationInfo.loggable_items","title":"<code>loggable_items(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Yield the loggable items for SparsificationInfo object.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]</code> <p>A generator that yields the loggable items for this object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@abstractmethod\ndef loggable_items(\n    self,\n    **kwargs,\n) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n    \"\"\"\n    Yield the loggable items for SparsificationInfo object.\n\n    :return: A generator that yields the loggable items for this object.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationPruning","title":"<code>SparsificationPruning</code>","text":"<p>               Bases: <code>SparsificationInfo</code></p> <p>A model that contains the pruning information for a torch module.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>class SparsificationPruning(SparsificationInfo):\n    \"\"\"\n    A model that contains the pruning information for a torch module.\n    \"\"\"\n\n    sparse_parameters: Dict[str, CountAndPercent] = Field(\n        description=\"A dictionary that maps the name of a parameter \"\n        \"to the number/percent of weights that are zeroed out \"\n        \"in that layer.\"\n    )\n\n    @classmethod\n    def from_module(cls, module: torch.nn.Module) -&gt; \"SparsificationPruning\":\n        \"\"\"\n        Factory method to create a SparsificationPruning object from a module.\n\n        :param module: The module to create the SparsificationPruning object from.\n        :return: A SparsificationPruning object.\n        \"\"\"\n        sparse_parameters_count = defaultdict(CountAndPercent)\n        for param_name, param in module.named_parameters():\n            num_parameters = param.numel()\n            num_zero_parameters = param.numel() - param.count_nonzero().item()\n            num_parameters = max(1, num_parameters)  # avoid FSDP divide by 0\n\n            zero_count = num_zero_parameters\n            zero_count_percent = num_zero_parameters / num_parameters\n\n            sparse_parameters_count[param_name] = CountAndPercent(\n                count=zero_count, percent=zero_count_percent\n            )\n\n        return cls(sparse_parameters=sparse_parameters_count)\n\n    def loggable_items(\n        self,\n        percentages_only: bool = False,\n        non_zero_only: bool = False,\n        **kwargs,\n    ) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n        \"\"\"\n        Yield the loggable items for SparsificationPruning object.\n\n        :param percentages_only: If True, only yield the percentages of the loggable\n            items. If False, yield both the counts and percentages. Default is False.\n        :param non_zero_only: If True, only yield information for non-zero\n            counts/percentages. Default is False.\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n        main_tag = self.__class__.__name__\n        items_to_log = []\n        for param_name, count_and_percent in self.sparse_parameters.items():\n            items_to_log.append(\n                (\n                    f\"{main_tag}/SparseParameters/{param_name}/count\",\n                    count_and_percent.count,\n                )\n            )  # noqa: E501\n            items_to_log.append(\n                (\n                    f\"{main_tag}/SparseParameters/{param_name}/percent\",\n                    count_and_percent.percent,\n                )\n            )  # noqa: E501\n\n        items_to_log = SparsificationInfo.filter_loggable_items_percentages_only(\n            items_to_log, percentages_only\n        )\n        items_to_log = SparsificationInfo.filter_loggable_items_non_zero_only(\n            items_to_log, non_zero_only\n        )\n\n        yield from items_to_log\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationPruning.from_module","title":"<code>from_module(module)</code>  <code>classmethod</code>","text":"<p>Factory method to create a SparsificationPruning object from a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to create the SparsificationPruning object from.</p> required <p>Returns:</p> Type Description <code>SparsificationPruning</code> <p>A SparsificationPruning object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@classmethod\ndef from_module(cls, module: torch.nn.Module) -&gt; \"SparsificationPruning\":\n    \"\"\"\n    Factory method to create a SparsificationPruning object from a module.\n\n    :param module: The module to create the SparsificationPruning object from.\n    :return: A SparsificationPruning object.\n    \"\"\"\n    sparse_parameters_count = defaultdict(CountAndPercent)\n    for param_name, param in module.named_parameters():\n        num_parameters = param.numel()\n        num_zero_parameters = param.numel() - param.count_nonzero().item()\n        num_parameters = max(1, num_parameters)  # avoid FSDP divide by 0\n\n        zero_count = num_zero_parameters\n        zero_count_percent = num_zero_parameters / num_parameters\n\n        sparse_parameters_count[param_name] = CountAndPercent(\n            count=zero_count, percent=zero_count_percent\n        )\n\n    return cls(sparse_parameters=sparse_parameters_count)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationPruning.loggable_items","title":"<code>loggable_items(percentages_only=False, non_zero_only=False, **kwargs)</code>","text":"<p>Yield the loggable items for SparsificationPruning object.</p> <p>Parameters:</p> Name Type Description Default <code>percentages_only</code> <code>bool</code> <p>If True, only yield the percentages of the loggable items. If False, yield both the counts and percentages. Default is False.</p> <code>False</code> <code>non_zero_only</code> <code>bool</code> <p>If True, only yield information for non-zero counts/percentages. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]</code> <p>A generator that yields the loggable items for this object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>def loggable_items(\n    self,\n    percentages_only: bool = False,\n    non_zero_only: bool = False,\n    **kwargs,\n) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n    \"\"\"\n    Yield the loggable items for SparsificationPruning object.\n\n    :param percentages_only: If True, only yield the percentages of the loggable\n        items. If False, yield both the counts and percentages. Default is False.\n    :param non_zero_only: If True, only yield information for non-zero\n        counts/percentages. Default is False.\n    :return: A generator that yields the loggable items for this object.\n    \"\"\"\n    main_tag = self.__class__.__name__\n    items_to_log = []\n    for param_name, count_and_percent in self.sparse_parameters.items():\n        items_to_log.append(\n            (\n                f\"{main_tag}/SparseParameters/{param_name}/count\",\n                count_and_percent.count,\n            )\n        )  # noqa: E501\n        items_to_log.append(\n            (\n                f\"{main_tag}/SparseParameters/{param_name}/percent\",\n                count_and_percent.percent,\n            )\n        )  # noqa: E501\n\n    items_to_log = SparsificationInfo.filter_loggable_items_percentages_only(\n        items_to_log, percentages_only\n    )\n    items_to_log = SparsificationInfo.filter_loggable_items_non_zero_only(\n        items_to_log, non_zero_only\n    )\n\n    yield from items_to_log\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationQuantization","title":"<code>SparsificationQuantization</code>","text":"<p>               Bases: <code>SparsificationInfo</code></p> <p>A model that contains the quantization information for a torch module.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>class SparsificationQuantization(SparsificationInfo):\n    \"\"\"\n    A model that contains the quantization information for a torch module.\n    \"\"\"\n\n    enabled: Dict[str, bool] = Field(\n        description=\"A dictionary that maps the name of an \"\n        \"operation to a boolean flag that indicates whether \"\n        \"the operation is quantized or not.\"\n    )\n    precision: Dict[str, Union[BaseModel, None, int]] = Field(\n        description=\"A dictionary that maps the name of a layer\"\n        \"to the precision of that layer.\"\n    )\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def from_module(\n        cls,\n        module: torch.nn.Module,\n    ) -&gt; \"SparsificationQuantization\":\n        \"\"\"\n        Factory method to create a SparsificationQuantization object from a module.\n\n        :param module: The module to create the SparsificationQuantization object from.\n        :return: A SparsificationQuantization object.\n        \"\"\"\n        operations = get_leaf_operations(module)\n        enabled = defaultdict(bool)\n        precision = defaultdict(str)\n        for op in operations:\n            operation_name = op.__class__.__name__\n            operation_counter = 0\n            # make sure that the operation name is unique\n            while enabled.get(operation_name) is not None:\n                operation_counter += 1\n                operation_name = f\"{op.__class__.__name__}_{operation_counter}\"\n\n            enabled[operation_name] = is_quantized(op)\n            precision[operation_name] = get_precision_information(op)\n\n        return cls(enabled=enabled, precision=precision)\n\n    def loggable_items(\n        self,\n        enabled_only: bool = False,\n        **kwargs,\n    ) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n        \"\"\"\n        Yield the loggable items for SparsificationQuantization object.\n\n        :param enabled_only: If True, only yield loggable items for\n            operations where quantization is enabled. If False, yield irrespective\n            of whether quantization is enabled or not. Defaults to False.\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n        main_tag = self.__class__.__name__\n        for operation in self.enabled.keys():\n            if enabled_only and not self.enabled[operation]:\n                continue\n\n            yield f\"{main_tag}/{operation}/enabled\", self.enabled[operation]\n\n            precision = self.precision[operation]\n            if precision is None:\n                yield f\"{main_tag}/{operation}/precision\", precision\n            elif isinstance(precision, int):\n                yield f\"{main_tag}/{operation}/precision.weights/num_bits\", precision\n            elif isinstance(precision, BaseModel):\n                yield (\n                    f\"{main_tag}/{operation}/precision/weights/num_bits\",\n                    precision.weights.num_bits,\n                )  # noqa: E501\n                yield (\n                    f\"{main_tag}/{operation}/precision/input_activations/num_bits\",\n                    precision.input_activations.num_bits,\n                )  # noqa: E501\n            else:\n                raise ValueError(\n                    f\"The precision is not a valid type {type(precision)}.\"\n                )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationQuantization.from_module","title":"<code>from_module(module)</code>  <code>classmethod</code>","text":"<p>Factory method to create a SparsificationQuantization object from a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to create the SparsificationQuantization object from.</p> required <p>Returns:</p> Type Description <code>SparsificationQuantization</code> <p>A SparsificationQuantization object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@classmethod\ndef from_module(\n    cls,\n    module: torch.nn.Module,\n) -&gt; \"SparsificationQuantization\":\n    \"\"\"\n    Factory method to create a SparsificationQuantization object from a module.\n\n    :param module: The module to create the SparsificationQuantization object from.\n    :return: A SparsificationQuantization object.\n    \"\"\"\n    operations = get_leaf_operations(module)\n    enabled = defaultdict(bool)\n    precision = defaultdict(str)\n    for op in operations:\n        operation_name = op.__class__.__name__\n        operation_counter = 0\n        # make sure that the operation name is unique\n        while enabled.get(operation_name) is not None:\n            operation_counter += 1\n            operation_name = f\"{op.__class__.__name__}_{operation_counter}\"\n\n        enabled[operation_name] = is_quantized(op)\n        precision[operation_name] = get_precision_information(op)\n\n    return cls(enabled=enabled, precision=precision)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationQuantization.loggable_items","title":"<code>loggable_items(enabled_only=False, **kwargs)</code>","text":"<p>Yield the loggable items for SparsificationQuantization object.</p> <p>Parameters:</p> Name Type Description Default <code>enabled_only</code> <code>bool</code> <p>If True, only yield loggable items for operations where quantization is enabled. If False, yield irrespective of whether quantization is enabled or not. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]</code> <p>A generator that yields the loggable items for this object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>def loggable_items(\n    self,\n    enabled_only: bool = False,\n    **kwargs,\n) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n    \"\"\"\n    Yield the loggable items for SparsificationQuantization object.\n\n    :param enabled_only: If True, only yield loggable items for\n        operations where quantization is enabled. If False, yield irrespective\n        of whether quantization is enabled or not. Defaults to False.\n    :return: A generator that yields the loggable items for this object.\n    \"\"\"\n    main_tag = self.__class__.__name__\n    for operation in self.enabled.keys():\n        if enabled_only and not self.enabled[operation]:\n            continue\n\n        yield f\"{main_tag}/{operation}/enabled\", self.enabled[operation]\n\n        precision = self.precision[operation]\n        if precision is None:\n            yield f\"{main_tag}/{operation}/precision\", precision\n        elif isinstance(precision, int):\n            yield f\"{main_tag}/{operation}/precision.weights/num_bits\", precision\n        elif isinstance(precision, BaseModel):\n            yield (\n                f\"{main_tag}/{operation}/precision/weights/num_bits\",\n                precision.weights.num_bits,\n            )  # noqa: E501\n            yield (\n                f\"{main_tag}/{operation}/precision/input_activations/num_bits\",\n                precision.input_activations.num_bits,\n            )  # noqa: E501\n        else:\n            raise ValueError(\n                f\"The precision is not a valid type {type(precision)}.\"\n            )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationSummaries","title":"<code>SparsificationSummaries</code>","text":"<p>               Bases: <code>SparsificationInfo</code></p> <p>A model that contains the sparsification summaries for a torch module.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>class SparsificationSummaries(SparsificationInfo):\n    \"\"\"\n    A model that contains the sparsification summaries for a torch module.\n    \"\"\"\n\n    quantized: CountAndPercent = Field(\n        description=\"A model that contains the number of \"\n        \"operations/the percent of operations that are quantized.\"\n    )\n    pruned: CountAndPercent = Field(\n        description=\"A model that contains the number of \"\n        \"parameters/the percent of parameters that are pruned.\"\n    )\n    parameter_counts: Dict[str, int] = Field(\n        description=\"A dictionary that maps the name of a parameter \"\n        \"to the number of elements (weights) in that parameter.\"\n    )\n    operation_counts: Dict[str, int] = Field(\n        description=\"A dictionary that maps the name of an operation \"\n        \"to the number of times that operation is used in the model.\"\n    )\n\n    @classmethod\n    def from_module(\n        cls,\n        module=torch.nn.Module,\n        pruning_thresholds: Tuple[float, float] = (0.05, 1 - 1e-9),\n    ) -&gt; \"SparsificationSummaries\":\n        \"\"\"\n        Factory method to create a SparsificationSummaries object from a module.\n\n        :param module: The module to create the SparsificationSummaries object from.\n        :param pruning_thresholds: The lower and upper thresholds used to determine\n            whether a parameter is pruned. If it's percentage of zero weights is between\n            the lower and upper thresholds, it is considered pruned.\n        :return: A SparsificationSummaries object.\n        \"\"\"\n        operations = get_leaf_operations(module)\n        num_quantized_ops = sum([is_quantized(op) for op in operations])\n        total_num_params = len(list(module.parameters()))\n\n        lower_threshold_pruning = min(pruning_thresholds)\n        upper_threshold_pruning = max(pruning_thresholds)\n        total_num_params_pruned = 0\n        count_parameters = defaultdict(int)\n\n        for param_name, param in module.named_parameters():\n            num_parameters = param.numel()\n            num_zero_parameters = param.numel() - param.count_nonzero().item()\n            num_parameters = max(1, num_parameters)  # avoid FSDP divide by 0\n\n            if (\n                lower_threshold_pruning\n                &lt;= num_zero_parameters / num_parameters\n                &lt;= upper_threshold_pruning\n            ):\n                total_num_params_pruned += 1\n\n            count_parameters[param_name] = num_parameters\n\n        return cls(\n            pruned=CountAndPercent(\n                count=total_num_params_pruned,\n                percent=total_num_params_pruned / total_num_params,\n            ),\n            quantized=CountAndPercent(\n                count=num_quantized_ops, percent=num_quantized_ops / len(operations)\n            ),\n            parameter_counts=count_parameters,\n            operation_counts=Counter([op.__class__.__name__ for op in operations]),\n        )\n\n    def loggable_items(\n        self,\n        non_zero_only: bool = False,\n        percentages_only: bool = True,\n        **kwargs,\n    ) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n        \"\"\"\n        Yield the loggable items for SparsificationSummaries object.\n\n        :param non_zero_only: If True, only yield information for non-zero items.\n        :param percentages_only: If True, only yield the percentages of the loggable\n            items. If False, yield both the counts and percentages. Defaults to True\n        :return: A generator that yields the loggable items for this object.\n        \"\"\"\n        main_tag = self.__class__.__name__\n        yield f\"{main_tag}/OperationCounts\", self.operation_counts\n        yield f\"{main_tag}/ParameterCounts\", self.parameter_counts\n\n        items_to_log = (\n            (f\"{main_tag}/QuantizedOperations/count\", self.quantized.count),\n            (f\"{main_tag}/QuantizedOperations/percent\", self.quantized.percent),\n            (f\"{main_tag}/PrunedParameters/count\", self.pruned.count),\n            (f\"{main_tag}/PrunedParameters/percent\", self.pruned.percent),\n        )\n\n        items_to_log = SparsificationInfo.filter_loggable_items_percentages_only(\n            items_to_log, percentages_only\n        )\n        items_to_log = SparsificationInfo.filter_loggable_items_non_zero_only(\n            items_to_log, non_zero_only\n        )\n\n        yield from items_to_log\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationSummaries.from_module","title":"<code>from_module(module=torch.nn.Module, pruning_thresholds=(0.05, 1 - 1e-09))</code>  <code>classmethod</code>","text":"<p>Factory method to create a SparsificationSummaries object from a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <p>The module to create the SparsificationSummaries object from.</p> <code>Module</code> <code>pruning_thresholds</code> <code>Tuple[float, float]</code> <p>The lower and upper thresholds used to determine whether a parameter is pruned. If it's percentage of zero weights is between the lower and upper thresholds, it is considered pruned.</p> <code>(0.05, 1 - 1e-09)</code> <p>Returns:</p> Type Description <code>SparsificationSummaries</code> <p>A SparsificationSummaries object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>@classmethod\ndef from_module(\n    cls,\n    module=torch.nn.Module,\n    pruning_thresholds: Tuple[float, float] = (0.05, 1 - 1e-9),\n) -&gt; \"SparsificationSummaries\":\n    \"\"\"\n    Factory method to create a SparsificationSummaries object from a module.\n\n    :param module: The module to create the SparsificationSummaries object from.\n    :param pruning_thresholds: The lower and upper thresholds used to determine\n        whether a parameter is pruned. If it's percentage of zero weights is between\n        the lower and upper thresholds, it is considered pruned.\n    :return: A SparsificationSummaries object.\n    \"\"\"\n    operations = get_leaf_operations(module)\n    num_quantized_ops = sum([is_quantized(op) for op in operations])\n    total_num_params = len(list(module.parameters()))\n\n    lower_threshold_pruning = min(pruning_thresholds)\n    upper_threshold_pruning = max(pruning_thresholds)\n    total_num_params_pruned = 0\n    count_parameters = defaultdict(int)\n\n    for param_name, param in module.named_parameters():\n        num_parameters = param.numel()\n        num_zero_parameters = param.numel() - param.count_nonzero().item()\n        num_parameters = max(1, num_parameters)  # avoid FSDP divide by 0\n\n        if (\n            lower_threshold_pruning\n            &lt;= num_zero_parameters / num_parameters\n            &lt;= upper_threshold_pruning\n        ):\n            total_num_params_pruned += 1\n\n        count_parameters[param_name] = num_parameters\n\n    return cls(\n        pruned=CountAndPercent(\n            count=total_num_params_pruned,\n            percent=total_num_params_pruned / total_num_params,\n        ),\n        quantized=CountAndPercent(\n            count=num_quantized_ops, percent=num_quantized_ops / len(operations)\n        ),\n        parameter_counts=count_parameters,\n        operation_counts=Counter([op.__class__.__name__ for op in operations]),\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/configs/#llmcompressor.pytorch.utils.sparsification_info.configs.SparsificationSummaries.loggable_items","title":"<code>loggable_items(non_zero_only=False, percentages_only=True, **kwargs)</code>","text":"<p>Yield the loggable items for SparsificationSummaries object.</p> <p>Parameters:</p> Name Type Description Default <code>non_zero_only</code> <code>bool</code> <p>If True, only yield information for non-zero items.</p> <code>False</code> <code>percentages_only</code> <code>bool</code> <p>If True, only yield the percentages of the loggable items. If False, yield both the counts and percentages. Defaults to True</p> <code>True</code> <p>Returns:</p> Type Description <code>Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]</code> <p>A generator that yields the loggable items for this object.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/configs.py</code> <pre><code>def loggable_items(\n    self,\n    non_zero_only: bool = False,\n    percentages_only: bool = True,\n    **kwargs,\n) -&gt; Generator[Tuple[str, Union[Dict[str, int], float, int]], None, None]:\n    \"\"\"\n    Yield the loggable items for SparsificationSummaries object.\n\n    :param non_zero_only: If True, only yield information for non-zero items.\n    :param percentages_only: If True, only yield the percentages of the loggable\n        items. If False, yield both the counts and percentages. Defaults to True\n    :return: A generator that yields the loggable items for this object.\n    \"\"\"\n    main_tag = self.__class__.__name__\n    yield f\"{main_tag}/OperationCounts\", self.operation_counts\n    yield f\"{main_tag}/ParameterCounts\", self.parameter_counts\n\n    items_to_log = (\n        (f\"{main_tag}/QuantizedOperations/count\", self.quantized.count),\n        (f\"{main_tag}/QuantizedOperations/percent\", self.quantized.percent),\n        (f\"{main_tag}/PrunedParameters/count\", self.pruned.count),\n        (f\"{main_tag}/PrunedParameters/percent\", self.pruned.percent),\n    )\n\n    items_to_log = SparsificationInfo.filter_loggable_items_percentages_only(\n        items_to_log, percentages_only\n    )\n    items_to_log = SparsificationInfo.filter_loggable_items_non_zero_only(\n        items_to_log, non_zero_only\n    )\n\n    yield from items_to_log\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/helpers/","title":"llmcompressor.pytorch.utils.sparsification_info.helpers","text":""},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/helpers/#llmcompressor.pytorch.utils.sparsification_info.helpers.get_leaf_operations","title":"<code>get_leaf_operations(model, operations_to_skip=None, operations_to_unwrap=None)</code>","text":"<p>Get the leaf operations in the model (those that do not have operations as children)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model to get the leaf operations from</p> required <code>operations_to_skip</code> <code>Optional[List[Module]]</code> <p>a list of leaf operations that will be omitted when getting the leaf operations. If None passed, by default the Identity operation will be skipped</p> <code>None</code> <code>operations_to_unwrap</code> <code>Optional[List[Module]]</code> <p>a list of operations that will be unwrapped when getting the leaf operations. Unwrapping means that we directly add the module(s) that is/are wrapped by the operation (i.e. operation's <code>module</code> attribute) to the list of leaf operations. If None passed, by default the QuantWrapper operation will be unwrapped</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Module]</code> <p>a list of the leaf operations</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/helpers.py</code> <pre><code>def get_leaf_operations(\n    model: torch.nn.Module,\n    operations_to_skip: Optional[List[torch.nn.Module]] = None,\n    operations_to_unwrap: Optional[List[torch.nn.Module]] = None,\n) -&gt; List[torch.nn.Module]:\n    \"\"\"\n    Get the leaf operations in the model\n    (those that do not have operations as children)\n\n    :param model: the model to get the leaf operations from\n    :param operations_to_skip: a list of leaf operations that will be\n        omitted when getting the leaf operations. If None passed, by\n        default the Identity operation will be skipped\n    :param operations_to_unwrap: a list of operations that will be unwrapped\n        when getting the leaf operations. Unwrapping means that we directly\n        add the module(s) that is/are wrapped by the operation (i.e. operation's\n        `module` attribute) to the list\n        of leaf operations. If None passed, by default the QuantWrapper\n        operation will be unwrapped\n    :return: a list of the leaf operations\n    \"\"\"\n    if operations_to_skip is None:\n        operations_to_skip = [Identity]\n\n    if operations_to_unwrap is None:\n        operations_to_unwrap = [QuantWrapper]\n\n    leaf_operations = []\n    children = list(model.children())\n\n    if children == []:\n        return model\n    else:\n        for child in children:\n            if isinstance(child, tuple(operations_to_unwrap)):\n                leaf_operations.append(child.module)\n                continue\n            try:\n                leaf_operations.extend(get_leaf_operations(child))\n            except TypeError:\n                leaf_operations.append(get_leaf_operations(child))\n    leaf_operations = [\n        op for op in leaf_operations if not isinstance(op, tuple(operations_to_skip))\n    ]\n    return leaf_operations\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/helpers/#llmcompressor.pytorch.utils.sparsification_info.helpers.get_precision_information","title":"<code>get_precision_information(operation)</code>","text":"<p>Get the information about the precision of the operation.</p> <p>1)  If operation is quantized, returns the quantization     scheme of the operation. 2)  If operation is not quantized, returns the numer of bits     of the operation's weights. 3)  If operation is not quantized and does not have a weights,     returns None.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>Module</code> <p>the operation to get the quantization scheme from</p> required <p>Returns:</p> Type Description <code>Union[None, int, QuantizationScheme]</code> <p>the quantization scheme of the operation, the number of bits of the operation's weights, or None if the operation is not quantized and does not have a weight</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/helpers.py</code> <pre><code>def get_precision_information(\n    operation: torch.nn.Module,\n) -&gt; Union[None, int, \"QuantizationScheme\"]:  # noqa F821\n    \"\"\"\n    Get the information about the precision of the operation.\n\n    1)  If operation is quantized, returns the quantization\n        scheme of the operation.\n    2)  If operation is not quantized, returns the numer of bits\n        of the operation's weights.\n    3)  If operation is not quantized and does not have a weights,\n        returns None.\n\n    :param operation: the operation to get the quantization scheme from\n    :return: the quantization scheme of the operation, the number of bits\n        of the operation's weights, or None if the operation is not quantized\n        and does not have a weight\n    \"\"\"\n\n    if hasattr(operation, \"quantization_scheme\"):\n        return getattr(operation, \"quantization_scheme\")\n    elif hasattr(operation, \"weight\"):\n        return _get_num_bits(operation.weight.dtype)\n    else:\n        return None\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/helpers/#llmcompressor.pytorch.utils.sparsification_info.helpers.is_quantized","title":"<code>is_quantized(operation)</code>","text":"<p>Check whether the operation is quantized (contains a quantization scheme)</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/helpers.py</code> <pre><code>def is_quantized(operation: torch.nn.Module) -&gt; bool:\n    \"\"\"\n    Check whether the operation is quantized (contains\n    a quantization scheme)\n    \"\"\"\n    return hasattr(operation, \"quantization_scheme\")\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info/","title":"llmcompressor.pytorch.utils.sparsification_info.module_sparsification_info","text":""},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info/#llmcompressor.pytorch.utils.sparsification_info.module_sparsification_info.ModuleSparsificationInfo","title":"<code>ModuleSparsificationInfo</code>","text":"<p>               Bases: <code>SparsificationInfo</code></p> <p>Pydantic model for storing sparsification information of a torch module.</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info.py</code> <pre><code>class ModuleSparsificationInfo(SparsificationInfo):\n    \"\"\"\n    Pydantic model for storing sparsification information of a torch module.\n    \"\"\"\n\n    summary_info: SparsificationSummaries = Field(\n        description=\"Model that holds the sparsification summary info of the module\"\n    )\n    pruning_info: SparsificationPruning = Field(\n        description=\"Model that holds the pruning info of the module\"\n    )\n    quantization_info: SparsificationQuantization = Field(\n        description=\"Model that holds the quantization info of the module\"\n    )\n\n    @classmethod\n    def from_module(cls, module: torch.nn.Module) -&gt; \"ModuleSparsificationInfo\":\n        \"\"\"\n        Factory method to create a ModuleSparsificationInfo object from a torch module.\n\n        :param module: the module to create the ModuleSparsificationInfo object from\n        :return: the ModuleSparsificationInfo object created from the module\n        \"\"\"\n        if not isinstance(module, torch.nn.Module):\n            raise ValueError(\n                \"Module must be a torch.nn.Module, not {}\".format(type(module))\n            )\n\n        return cls(\n            summary_info=SparsificationSummaries.from_module(module),\n            pruning_info=SparsificationPruning.from_module(module),\n            quantization_info=SparsificationQuantization.from_module(module),\n        )\n\n    def loggable_items(self, **kwargs) -&gt; Generator[Tuple[str, Any], None, None]:\n        \"\"\"\n        A generator that yields the loggable items of\n        the ModuleSparsificationInfo object.\n\n        :param kwargs: additional kwargs to pass to the loggable items\n        :return a generator that yields a tuple of:\n            - the name of the loggable item\n            - the value of the loggable item\n        \"\"\"\n        for info in [self.summary_info, self.pruning_info, self.quantization_info]:\n            yield from info.loggable_items(**kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info/#llmcompressor.pytorch.utils.sparsification_info.module_sparsification_info.ModuleSparsificationInfo.from_module","title":"<code>from_module(module)</code>  <code>classmethod</code>","text":"<p>Factory method to create a ModuleSparsificationInfo object from a torch module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>the module to create the ModuleSparsificationInfo object from</p> required <p>Returns:</p> Type Description <code>ModuleSparsificationInfo</code> <p>the ModuleSparsificationInfo object created from the module</p> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info.py</code> <pre><code>@classmethod\ndef from_module(cls, module: torch.nn.Module) -&gt; \"ModuleSparsificationInfo\":\n    \"\"\"\n    Factory method to create a ModuleSparsificationInfo object from a torch module.\n\n    :param module: the module to create the ModuleSparsificationInfo object from\n    :return: the ModuleSparsificationInfo object created from the module\n    \"\"\"\n    if not isinstance(module, torch.nn.Module):\n        raise ValueError(\n            \"Module must be a torch.nn.Module, not {}\".format(type(module))\n        )\n\n    return cls(\n        summary_info=SparsificationSummaries.from_module(module),\n        pruning_info=SparsificationPruning.from_module(module),\n        quantization_info=SparsificationQuantization.from_module(module),\n    )\n</code></pre>"},{"location":"reference/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info/#llmcompressor.pytorch.utils.sparsification_info.module_sparsification_info.ModuleSparsificationInfo.loggable_items","title":"<code>loggable_items(**kwargs)</code>","text":"<p>A generator that yields the loggable items of the ModuleSparsificationInfo object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>additional kwargs to pass to the loggable items</p> <code>{}</code> <p>Returns:</p> Type Description <code>Generator[Tuple[str, Any], None, None]</code> <ul> <li>the name of the loggable item - the value of the loggable item</li> </ul> Source code in <code>src/llmcompressor/pytorch/utils/sparsification_info/module_sparsification_info.py</code> <pre><code>def loggable_items(self, **kwargs) -&gt; Generator[Tuple[str, Any], None, None]:\n    \"\"\"\n    A generator that yields the loggable items of\n    the ModuleSparsificationInfo object.\n\n    :param kwargs: additional kwargs to pass to the loggable items\n    :return a generator that yields a tuple of:\n        - the name of the loggable item\n        - the value of the loggable item\n    \"\"\"\n    for info in [self.summary_info, self.pruning_info, self.quantization_info]:\n        yield from info.loggable_items(**kwargs)\n</code></pre>"},{"location":"reference/llmcompressor/recipe/","title":"llmcompressor.recipe","text":""},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe","title":"<code>Recipe</code>","text":"<p>               Bases: <code>RecipeBase</code></p> <p>A class to represent a recipe for a model. Recipes encode the instructions needed for modifying the model and/or training process as a list of modifiers.</p> <p>Recipes can be created from a file, string, or HuggingFace stub. Acceptable file formats include both json and yaml, however, when serializing a recipe, yaml will be used by default.</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>class Recipe(RecipeBase):\n    \"\"\"\n    A class to represent a recipe for a model.\n    Recipes encode the instructions needed for modifying\n    the model and/or training process as a list of modifiers.\n\n    Recipes can be created from a file, string, or HuggingFace stub.\n    Acceptable file formats include both json and yaml, however,\n    when serializing a recipe, yaml will be used by default.\n    \"\"\"\n\n    @classmethod\n    def from_modifiers(\n        cls,\n        modifiers: Union[Modifier, List[Modifier]],\n        modifier_group_name: Optional[str] = None,\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        Create a recipe instance from a list of modifiers\n\n        (Note: all modifiers are wrapped into a single stage\n        with the modifier_group_name as the stage name. If modifier_group_name is None,\n        the default run type is `oneshot`)\n\n        Lfecycle:\n        | - Validate Modifiers\n        | - Create recipe string from modifiers\n        | - Create recipe instance from recipe string\n\n        :param modifiers: The list of RecipeModifier instances\n        :param modifier_group_name: The stage_name of the recipe,\n            if `oneshot` or `train` the run_type of the recipe will be\n            inferred from the modifier_group_name, if None, a dummy default\n            group_name will be assigned.\n        :return: The Recipe instance created from the modifiers\n        \"\"\"\n        logger.info(\"Creating recipe from modifiers\")\n\n        if isinstance(modifiers, Modifier):\n            modifiers = [modifiers]\n\n        if any(not isinstance(modifier, Modifier) for modifier in modifiers):\n            raise ValueError(\"modifiers must be a list of Modifier instances\")\n\n        group_name = modifier_group_name or \"default\"\n\n        recipe_modifiers: List[RecipeModifier] = [\n            RecipeModifier(\n                type=modifier.__class__.__name__,\n                group=group_name,\n                args=modifier.model_dump(exclude_unset=True),\n            )\n            for modifier in modifiers\n        ]\n        # assume one stage for modifier instances\n        stages: List[RecipeStage] = [\n            RecipeStage(group=group_name, modifiers=recipe_modifiers)\n        ]\n        recipe = cls()\n        recipe.stages = stages\n        return recipe\n\n    @classmethod\n    def create_instance(\n        cls,\n        path_or_modifiers: Union[str, Modifier, List[Modifier], \"Recipe\"],\n        modifier_group_name: Optional[str] = None,\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        Create a recipe instance from a file, string, or RecipeModifier objects\n\n\n        Using a recipe string or file is supported:\n        &gt;&gt;&gt; recipe_str = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n\n        :param path_or_modifiers: The path to the recipe file or\n            or the recipe string (must be a valid\n            json/yaml file or a valid json/yaml string). Can also\n            accept a RecipeModifier instance, or a list of\n            RecipeModifiers\n        :param modifier_group_name: The stage_name of the recipe,\n            if `oneshot` or `train` the run_type of the recipe will be\n            inferred from the modifier_group_name, if None, a dummy default\n            group_name will be assigned. This argument is only used\n            when creating a recipe from a Modifier/list of Modifier(s)\n            instance, else it's ignored.\n        :return: The Recipe instance created from the path or modifiers,\n            or a valid recipe string in yaml/json format\n        \"\"\"\n\n        if isinstance(path_or_modifiers, Recipe):\n            # already a recipe\n            return path_or_modifiers\n\n        if isinstance(path_or_modifiers, (Modifier, list)):\n            return cls.from_modifiers(\n                modifiers=path_or_modifiers, modifier_group_name=modifier_group_name\n            )\n\n        if not os.path.isfile(path_or_modifiers):\n            # not a local file\n            # assume it's a string\n            logger.debug(\n                \"Could not initialize recipe as a file path or zoo stub, \"\n                \"attempting to process as a string.\"\n            )\n            logger.debug(f\"Input string: {path_or_modifiers}\")\n            obj = _load_json_or_yaml_string(path_or_modifiers)\n            return Recipe.model_validate(obj)\n        else:\n            logger.info(f\"Loading recipe from file {path_or_modifiers}\")\n\n        with open(path_or_modifiers, \"r\") as file:\n            content = file.read().strip()\n            if path_or_modifiers.lower().endswith(\".md\"):\n                content = _parse_recipe_from_md(path_or_modifiers, content)\n\n            if path_or_modifiers.lower().endswith(\".json\"):\n                obj = json.loads(content)\n            elif path_or_modifiers.lower().endswith(\n                \".yaml\"\n            ) or path_or_modifiers.lower().endswith(\".yml\"):\n                obj = yaml.safe_load(content)\n            else:\n                try:\n                    obj = _load_json_or_yaml_string(content)\n                except ValueError:\n                    raise ValueError(\n                        f\"Could not parse recipe from path {path_or_modifiers}\"\n                    )\n            return Recipe.model_validate(obj)\n\n    @staticmethod\n    def simplify_recipe(\n        recipe: Union[str, \"Recipe\", \"RecipeTuple\"], shift: Optional[int] = None\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        Simplify a RecipeTuple by removing stages that are not in the target_stages\n        and shifting the start and end of the recipe by the shift amount\n\n\n        Using a RecipeTuple instance with shift:\n        &gt;&gt;&gt; recipe_str = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n        &gt;&gt;&gt; recipe_tuple = RecipeTuple(recipe, [\"test\"], {})\n        &gt;&gt;&gt; simplified = Recipe.simplify_recipe(recipe_tuple)\n\n        :param recipe: The Recipe or RecipeTuple instance to simplify\n        :return: The simplified Recipe instance\n        \"\"\"\n        if isinstance(recipe, str):\n            recipe = Recipe.create_instance(recipe)\n\n        if isinstance(recipe, Recipe):\n            return recipe\n\n        # RecipeTuple case\n        stages = []\n        stage_names = recipe.target_stages\n        if stage_names is None:\n            stages = recipe.recipe.stages\n        else:\n            for stage in recipe.recipe.stages:\n                if any(stage.group in stage_name for stage_name in stage_names):\n                    stages.append(stage)\n\n        # default args in recipe\n        args = recipe.recipe.args if isinstance(recipe, RecipeTuple) else recipe.args\n\n        # overwrite with args passed in through CLI\n        for key, val in recipe.override_args.items():\n            args[key] = val\n        version = recipe.version if isinstance(recipe, Recipe) else None\n\n        simplified = Recipe()\n        simplified.version = version\n        simplified.args = args\n        simplified.stages = stages\n\n        return simplified\n\n    @staticmethod\n    def simplify_combine_recipes(\n        recipes: List[Union[str, \"Recipe\", \"RecipeTuple\"]],\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        A method to combine multiple recipes into one recipe\n        Automatically calculates the start and end of the combined recipe\n        and shifts the start and end of the recipes accordingly\n\n        Using two RecipeTuple instances:\n        &gt;&gt;&gt; recipe_str_1 = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe_str_2 = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 3.0\n        ...             end: 5.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe_1, recipe_2 = (Recipe.create_instance(recipe_str_1),\n        ... Recipe.create_instance(recipe_str_2))\n        &gt;&gt;&gt; combined = Recipe.simplify_combine_recipes(\n        ... [RecipeTuple(recipe_1, [\"test\"], {}), RecipeTuple(recipe_2, [\"test\"], {})])\n        &gt;&gt;&gt; len(combined.stages)\n        2\n\n        :param recipes: The list of Recipe/RecipeTuple instances to combine\n        :return: The combined Recipe instance\n        \"\"\"\n\n        combined = Recipe()\n\n        for recipe in recipes:\n            simplified = Recipe.simplify_recipe(\n                recipe=recipe,\n            )\n            combined.version = simplified.version\n            combined.stages.extend(simplified.stages)\n            combined.args.update(simplified.args)\n\n        return combined\n\n    version: str = None\n    args: Dict[str, Any] = Field(default_factory=dict)\n    stages: List[RecipeStage] = Field(default_factory=list)\n\n    def create_modifier(self) -&gt; List[\"StageModifiers\"]:\n        \"\"\"\n        Create and return a list of StageModifiers for each stage in the recipe\n\n        &gt;&gt;&gt; recipe_str = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n        &gt;&gt;&gt; stage_modifiers = recipe.create_modifier()\n        &gt;&gt;&gt; len(stage_modifiers) == 1\n        True\n        &gt;&gt;&gt; len(stage_modifiers[0].modifiers) == 1\n        True\n\n        :return: A list of StageModifiers for each stage in the recipe\n        \"\"\"\n        modifiers = []\n\n        for index, stage in enumerate(self.stages):\n            stage_modifiers = stage.create_modifier()\n            stage_modifiers.index = index\n            stage_modifiers.group = stage.group\n            modifiers.append(stage_modifiers)\n\n        return modifiers\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def remap_stages(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        stages = []\n\n        modifiers = RecipeStage.extract_dict_modifiers(values)\n        if modifiers:\n            default_stage = {\"modifiers\": modifiers, \"group\": \"default\"}\n            stages.append(default_stage)\n\n        extracted = Recipe.extract_dict_stages(values)\n        stages.extend(extracted)\n        formatted_values = {}\n\n        # fill out stages\n        formatted_values[\"stages\"] = stages\n\n        # fill out any default argument values\n        args = {}\n        for key, val in values.items():\n            args[key] = val\n        formatted_values[\"args\"] = args\n\n        return formatted_values\n\n    @staticmethod\n    def extract_dict_stages(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extract stages from a dict of values, acceptable dictionary structures\n        are shown below\n\n        Accepted stage formats:\n        - stages:\n          first_stage:\n            modifiers: ...\n          second_stage:\n            modifiers: ...\n\n        - first_stage:\n          modifiers: ...\n        - second_stage:\n          modifiers: ...\n\n        Accepted modifier formats default stage:\n        - modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n\n        - first_modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n\n        &gt;&gt;&gt; values = {\n        ... \"stages\": {\n        ...     \"first_stage\": {\n        ...         \"modifiers\": {\n        ...             \"ModifierTypeOne\": {\n        ...                 \"start\": 0.0,\n        ...                 \"end\": 2.0,\n        ...                 }\n        ...         }\n        ...     }\n        ... }\n        ... }\n        &gt;&gt;&gt; Recipe.extract_dict_stages(values) # doctest: +NORMALIZE_WHITESPACE\n        [{'modifiers': {'ModifierTypeOne': {'start': 0.0, 'end': 2.0}},\n        'group': 'first_stage'}]\n\n        :param values: The values dict to extract stages from\n        :return: A list of stages, where each stage is a dict of\n            modifiers and their group\n        \"\"\"\n\n        stages = []\n        remove_keys = []\n\n        default_modifiers = RecipeStage.extract_dict_modifiers(values)\n        if default_modifiers:\n            default_stage = {\"modifiers\": default_modifiers, \"group\": \"default\"}\n            stages.append(default_stage)\n\n        if \"stages\" in values and values[\"stages\"]:\n            assert isinstance(\n                values[\"stages\"], dict\n            ), f\"stages must be a dict, given {values['stages']}\"\n            remove_keys.append(\"stages\")\n\n            for key, value in values[\"stages\"].items():\n                assert isinstance(value, dict), f\"stage must be a dict, given {value}\"\n                value[\"group\"] = key\n                stages.append(value)\n\n        for key, value in list(values.items()):\n            if key.endswith(\"_stage\"):\n                remove_keys.append(key)\n                value[\"group\"] = key.rsplit(\"_stage\", 1)[0]\n                stages.append(value)\n\n        for key in remove_keys:\n            del values[key]\n\n        return stages\n\n    def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        :return: A dictionary representation of the recipe\n        \"\"\"\n        dict_ = super().model_dump(*args, **kwargs)\n        stages = {}\n\n        for stage in dict_[\"stages\"]:\n            name = f\"{stage['group']}_stage\"\n            del stage[\"group\"]\n\n            if name not in stages:\n                stages[name] = []\n\n            stages[name].append(stage)\n\n        dict_[\"stages\"] = stages\n\n        return dict_\n\n    def yaml(self, file_path: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Return a yaml string representation of the recipe.\n\n        :param file_path: optional file path to save yaml to\n        :return: The yaml string representation of the recipe\n        \"\"\"\n        file_stream = None if file_path is None else open(file_path, \"w\")\n        yaml_dict = self._get_yaml_dict()\n\n        ret = yaml.dump(\n            yaml_dict,\n            stream=file_stream,\n            allow_unicode=True,\n            sort_keys=False,\n            default_flow_style=None,\n            width=88,\n        )\n\n        if file_stream is not None:\n            file_stream.close()\n\n        return ret\n\n    def _get_yaml_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get a dictionary representation of the recipe for yaml serialization\n        The returned dict will only contain information necessary for yaml\n        serialization and must not be used in place of the dict method\n\n        :return: A dictionary representation of the recipe for yaml serialization\n        \"\"\"\n\n        original_recipe_dict = self.dict()\n        yaml_recipe_dict = {}\n\n        # populate recipe level attributes\n        recipe_level_attributes = [\"version\", \"args\"]\n\n        for attribute in recipe_level_attributes:\n            if attribute_value := original_recipe_dict.get(attribute):\n                yaml_recipe_dict[attribute] = attribute_value\n\n        # populate stages\n        stages = original_recipe_dict[\"stages\"]\n        for stage_name, stage_list in stages.items():\n            for idx, stage in enumerate(stage_list):\n                if len(stage_list) &gt; 1:\n                    # resolve name clashes caused by combining recipes with\n                    # duplicate stage names\n                    final_stage_name = f\"{stage_name}_{idx}\"\n                else:\n                    final_stage_name = stage_name\n                stage_dict = get_yaml_serializable_stage_dict(\n                    modifiers=stage[\"modifiers\"]\n                )\n\n                # infer run_type from stage\n                if run_type := stage.get(\"run_type\"):\n                    stage_dict[\"run_type\"] = run_type\n\n                yaml_recipe_dict[final_stage_name] = stage_dict\n\n        return yaml_recipe_dict\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.create_instance","title":"<code>create_instance(path_or_modifiers, modifier_group_name=None)</code>  <code>classmethod</code>","text":"<p>Create a recipe instance from a file, string, or RecipeModifier objects</p> <p>Using a recipe string or file is supported:</p> <p>recipe_str = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.*weight'] ... ''' recipe = Recipe.create_instance(recipe_str)</p> <p>Parameters:</p> Name Type Description Default <code>path_or_modifiers</code> <code>Union[str, Modifier, List[Modifier], Recipe]</code> <p>The path to the recipe file or or the recipe string (must be a valid json/yaml file or a valid json/yaml string). Can also accept a RecipeModifier instance, or a list of RecipeModifiers</p> required <code>modifier_group_name</code> <code>Optional[str]</code> <p>The stage_name of the recipe, if <code>oneshot</code> or <code>train</code> the run_type of the recipe will be inferred from the modifier_group_name, if None, a dummy default group_name will be assigned. This argument is only used when creating a recipe from a Modifier/list of Modifier(s) instance, else it's ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Recipe</code> <p>The Recipe instance created from the path or modifiers, or a valid recipe string in yaml/json format</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@classmethod\ndef create_instance(\n    cls,\n    path_or_modifiers: Union[str, Modifier, List[Modifier], \"Recipe\"],\n    modifier_group_name: Optional[str] = None,\n) -&gt; \"Recipe\":\n    \"\"\"\n    Create a recipe instance from a file, string, or RecipeModifier objects\n\n\n    Using a recipe string or file is supported:\n    &gt;&gt;&gt; recipe_str = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n\n    :param path_or_modifiers: The path to the recipe file or\n        or the recipe string (must be a valid\n        json/yaml file or a valid json/yaml string). Can also\n        accept a RecipeModifier instance, or a list of\n        RecipeModifiers\n    :param modifier_group_name: The stage_name of the recipe,\n        if `oneshot` or `train` the run_type of the recipe will be\n        inferred from the modifier_group_name, if None, a dummy default\n        group_name will be assigned. This argument is only used\n        when creating a recipe from a Modifier/list of Modifier(s)\n        instance, else it's ignored.\n    :return: The Recipe instance created from the path or modifiers,\n        or a valid recipe string in yaml/json format\n    \"\"\"\n\n    if isinstance(path_or_modifiers, Recipe):\n        # already a recipe\n        return path_or_modifiers\n\n    if isinstance(path_or_modifiers, (Modifier, list)):\n        return cls.from_modifiers(\n            modifiers=path_or_modifiers, modifier_group_name=modifier_group_name\n        )\n\n    if not os.path.isfile(path_or_modifiers):\n        # not a local file\n        # assume it's a string\n        logger.debug(\n            \"Could not initialize recipe as a file path or zoo stub, \"\n            \"attempting to process as a string.\"\n        )\n        logger.debug(f\"Input string: {path_or_modifiers}\")\n        obj = _load_json_or_yaml_string(path_or_modifiers)\n        return Recipe.model_validate(obj)\n    else:\n        logger.info(f\"Loading recipe from file {path_or_modifiers}\")\n\n    with open(path_or_modifiers, \"r\") as file:\n        content = file.read().strip()\n        if path_or_modifiers.lower().endswith(\".md\"):\n            content = _parse_recipe_from_md(path_or_modifiers, content)\n\n        if path_or_modifiers.lower().endswith(\".json\"):\n            obj = json.loads(content)\n        elif path_or_modifiers.lower().endswith(\n            \".yaml\"\n        ) or path_or_modifiers.lower().endswith(\".yml\"):\n            obj = yaml.safe_load(content)\n        else:\n            try:\n                obj = _load_json_or_yaml_string(content)\n            except ValueError:\n                raise ValueError(\n                    f\"Could not parse recipe from path {path_or_modifiers}\"\n                )\n        return Recipe.model_validate(obj)\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.create_modifier","title":"<code>create_modifier()</code>","text":"<p>Create and return a list of StageModifiers for each stage in the recipe</p> <p>recipe_str = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.*weight'] ... ''' recipe = Recipe.create_instance(recipe_str) stage_modifiers = recipe.create_modifier() len(stage_modifiers) == 1 True len(stage_modifiers[0].modifiers) == 1 True</p> <p>Returns:</p> Type Description <code>List[StageModifiers]</code> <p>A list of StageModifiers for each stage in the recipe</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def create_modifier(self) -&gt; List[\"StageModifiers\"]:\n    \"\"\"\n    Create and return a list of StageModifiers for each stage in the recipe\n\n    &gt;&gt;&gt; recipe_str = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n    &gt;&gt;&gt; stage_modifiers = recipe.create_modifier()\n    &gt;&gt;&gt; len(stage_modifiers) == 1\n    True\n    &gt;&gt;&gt; len(stage_modifiers[0].modifiers) == 1\n    True\n\n    :return: A list of StageModifiers for each stage in the recipe\n    \"\"\"\n    modifiers = []\n\n    for index, stage in enumerate(self.stages):\n        stage_modifiers = stage.create_modifier()\n        stage_modifiers.index = index\n        stage_modifiers.group = stage.group\n        modifiers.append(stage_modifiers)\n\n    return modifiers\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representation of the recipe</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    :return: A dictionary representation of the recipe\n    \"\"\"\n    dict_ = super().model_dump(*args, **kwargs)\n    stages = {}\n\n    for stage in dict_[\"stages\"]:\n        name = f\"{stage['group']}_stage\"\n        del stage[\"group\"]\n\n        if name not in stages:\n            stages[name] = []\n\n        stages[name].append(stage)\n\n    dict_[\"stages\"] = stages\n\n    return dict_\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.extract_dict_stages","title":"<code>extract_dict_stages(values)</code>  <code>staticmethod</code>","text":"<p>Extract stages from a dict of values, acceptable dictionary structures are shown below</p> <p>Accepted stage formats: - stages:   first_stage:     modifiers: ...   second_stage:     modifiers: ...</p> <ul> <li>first_stage:   modifiers: ...</li> <li>second_stage:   modifiers: ...</li> </ul> <p>Accepted modifier formats default stage: - modifiers:   - ModifierTypeOne     ...   - ModifierTypeTwo     ...</p> <ul> <li>first_modifiers:</li> <li>ModifierTypeOne     ...</li> <li>ModifierTypeTwo     ...</li> </ul> <p>values = { ... \"stages\": { ...     \"first_stage\": { ...         \"modifiers\": { ...             \"ModifierTypeOne\": { ...                 \"start\": 0.0, ...                 \"end\": 2.0, ...                 } ...         } ...     } ... } ... } Recipe.extract_dict_stages(values) # doctest: +NORMALIZE_WHITESPACE [{'modifiers': {'ModifierTypeOne': {'start': 0.0, 'end': 2.0}}, 'group': 'first_stage'}]</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict[str, Any]</code> <p>The values dict to extract stages from</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of stages, where each stage is a dict of modifiers and their group</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@staticmethod\ndef extract_dict_stages(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Extract stages from a dict of values, acceptable dictionary structures\n    are shown below\n\n    Accepted stage formats:\n    - stages:\n      first_stage:\n        modifiers: ...\n      second_stage:\n        modifiers: ...\n\n    - first_stage:\n      modifiers: ...\n    - second_stage:\n      modifiers: ...\n\n    Accepted modifier formats default stage:\n    - modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n\n    - first_modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n\n    &gt;&gt;&gt; values = {\n    ... \"stages\": {\n    ...     \"first_stage\": {\n    ...         \"modifiers\": {\n    ...             \"ModifierTypeOne\": {\n    ...                 \"start\": 0.0,\n    ...                 \"end\": 2.0,\n    ...                 }\n    ...         }\n    ...     }\n    ... }\n    ... }\n    &gt;&gt;&gt; Recipe.extract_dict_stages(values) # doctest: +NORMALIZE_WHITESPACE\n    [{'modifiers': {'ModifierTypeOne': {'start': 0.0, 'end': 2.0}},\n    'group': 'first_stage'}]\n\n    :param values: The values dict to extract stages from\n    :return: A list of stages, where each stage is a dict of\n        modifiers and their group\n    \"\"\"\n\n    stages = []\n    remove_keys = []\n\n    default_modifiers = RecipeStage.extract_dict_modifiers(values)\n    if default_modifiers:\n        default_stage = {\"modifiers\": default_modifiers, \"group\": \"default\"}\n        stages.append(default_stage)\n\n    if \"stages\" in values and values[\"stages\"]:\n        assert isinstance(\n            values[\"stages\"], dict\n        ), f\"stages must be a dict, given {values['stages']}\"\n        remove_keys.append(\"stages\")\n\n        for key, value in values[\"stages\"].items():\n            assert isinstance(value, dict), f\"stage must be a dict, given {value}\"\n            value[\"group\"] = key\n            stages.append(value)\n\n    for key, value in list(values.items()):\n        if key.endswith(\"_stage\"):\n            remove_keys.append(key)\n            value[\"group\"] = key.rsplit(\"_stage\", 1)[0]\n            stages.append(value)\n\n    for key in remove_keys:\n        del values[key]\n\n    return stages\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.from_modifiers","title":"<code>from_modifiers(modifiers, modifier_group_name=None)</code>  <code>classmethod</code>","text":"<p>Create a recipe instance from a list of modifiers</p> <p>(Note: all modifiers are wrapped into a single stage with the modifier_group_name as the stage name. If modifier_group_name is None, the default run type is <code>oneshot</code>)</p> <p>Lfecycle: | - Validate Modifiers | - Create recipe string from modifiers | - Create recipe instance from recipe string</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <code>Union[Modifier, List[Modifier]]</code> <p>The list of RecipeModifier instances</p> required <code>modifier_group_name</code> <code>Optional[str]</code> <p>The stage_name of the recipe, if <code>oneshot</code> or <code>train</code> the run_type of the recipe will be inferred from the modifier_group_name, if None, a dummy default group_name will be assigned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Recipe</code> <p>The Recipe instance created from the modifiers</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@classmethod\ndef from_modifiers(\n    cls,\n    modifiers: Union[Modifier, List[Modifier]],\n    modifier_group_name: Optional[str] = None,\n) -&gt; \"Recipe\":\n    \"\"\"\n    Create a recipe instance from a list of modifiers\n\n    (Note: all modifiers are wrapped into a single stage\n    with the modifier_group_name as the stage name. If modifier_group_name is None,\n    the default run type is `oneshot`)\n\n    Lfecycle:\n    | - Validate Modifiers\n    | - Create recipe string from modifiers\n    | - Create recipe instance from recipe string\n\n    :param modifiers: The list of RecipeModifier instances\n    :param modifier_group_name: The stage_name of the recipe,\n        if `oneshot` or `train` the run_type of the recipe will be\n        inferred from the modifier_group_name, if None, a dummy default\n        group_name will be assigned.\n    :return: The Recipe instance created from the modifiers\n    \"\"\"\n    logger.info(\"Creating recipe from modifiers\")\n\n    if isinstance(modifiers, Modifier):\n        modifiers = [modifiers]\n\n    if any(not isinstance(modifier, Modifier) for modifier in modifiers):\n        raise ValueError(\"modifiers must be a list of Modifier instances\")\n\n    group_name = modifier_group_name or \"default\"\n\n    recipe_modifiers: List[RecipeModifier] = [\n        RecipeModifier(\n            type=modifier.__class__.__name__,\n            group=group_name,\n            args=modifier.model_dump(exclude_unset=True),\n        )\n        for modifier in modifiers\n    ]\n    # assume one stage for modifier instances\n    stages: List[RecipeStage] = [\n        RecipeStage(group=group_name, modifiers=recipe_modifiers)\n    ]\n    recipe = cls()\n    recipe.stages = stages\n    return recipe\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.simplify_combine_recipes","title":"<code>simplify_combine_recipes(recipes)</code>  <code>staticmethod</code>","text":"<p>A method to combine multiple recipes into one recipe Automatically calculates the start and end of the combined recipe and shifts the start and end of the recipes accordingly</p> <p>Using two RecipeTuple instances:</p> <p>recipe_str_1 = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.weight'] ... ''' recipe_str_2 = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 3.0 ...             end: 5.0 ...             targets: ['re:.weight'] ... ''' recipe_1, recipe_2 = (Recipe.create_instance(recipe_str_1), ... Recipe.create_instance(recipe_str_2)) combined = Recipe.simplify_combine_recipes( ... [RecipeTuple(recipe_1, [\"test\"], {}), RecipeTuple(recipe_2, [\"test\"], {})]) len(combined.stages) 2</p> <p>Parameters:</p> Name Type Description Default <code>recipes</code> <code>List[Union[str, Recipe, RecipeTuple]]</code> <p>The list of Recipe/RecipeTuple instances to combine</p> required <p>Returns:</p> Type Description <code>Recipe</code> <p>The combined Recipe instance</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@staticmethod\ndef simplify_combine_recipes(\n    recipes: List[Union[str, \"Recipe\", \"RecipeTuple\"]],\n) -&gt; \"Recipe\":\n    \"\"\"\n    A method to combine multiple recipes into one recipe\n    Automatically calculates the start and end of the combined recipe\n    and shifts the start and end of the recipes accordingly\n\n    Using two RecipeTuple instances:\n    &gt;&gt;&gt; recipe_str_1 = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe_str_2 = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 3.0\n    ...             end: 5.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe_1, recipe_2 = (Recipe.create_instance(recipe_str_1),\n    ... Recipe.create_instance(recipe_str_2))\n    &gt;&gt;&gt; combined = Recipe.simplify_combine_recipes(\n    ... [RecipeTuple(recipe_1, [\"test\"], {}), RecipeTuple(recipe_2, [\"test\"], {})])\n    &gt;&gt;&gt; len(combined.stages)\n    2\n\n    :param recipes: The list of Recipe/RecipeTuple instances to combine\n    :return: The combined Recipe instance\n    \"\"\"\n\n    combined = Recipe()\n\n    for recipe in recipes:\n        simplified = Recipe.simplify_recipe(\n            recipe=recipe,\n        )\n        combined.version = simplified.version\n        combined.stages.extend(simplified.stages)\n        combined.args.update(simplified.args)\n\n    return combined\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.simplify_recipe","title":"<code>simplify_recipe(recipe, shift=None)</code>  <code>staticmethod</code>","text":"<p>Simplify a RecipeTuple by removing stages that are not in the target_stages and shifting the start and end of the recipe by the shift amount</p> <p>Using a RecipeTuple instance with shift:</p> <p>recipe_str = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.*weight'] ... ''' recipe = Recipe.create_instance(recipe_str) recipe_tuple = RecipeTuple(recipe, [\"test\"], {}) simplified = Recipe.simplify_recipe(recipe_tuple)</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>Union[str, Recipe, RecipeTuple]</code> <p>The Recipe or RecipeTuple instance to simplify</p> required <p>Returns:</p> Type Description <code>Recipe</code> <p>The simplified Recipe instance</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@staticmethod\ndef simplify_recipe(\n    recipe: Union[str, \"Recipe\", \"RecipeTuple\"], shift: Optional[int] = None\n) -&gt; \"Recipe\":\n    \"\"\"\n    Simplify a RecipeTuple by removing stages that are not in the target_stages\n    and shifting the start and end of the recipe by the shift amount\n\n\n    Using a RecipeTuple instance with shift:\n    &gt;&gt;&gt; recipe_str = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n    &gt;&gt;&gt; recipe_tuple = RecipeTuple(recipe, [\"test\"], {})\n    &gt;&gt;&gt; simplified = Recipe.simplify_recipe(recipe_tuple)\n\n    :param recipe: The Recipe or RecipeTuple instance to simplify\n    :return: The simplified Recipe instance\n    \"\"\"\n    if isinstance(recipe, str):\n        recipe = Recipe.create_instance(recipe)\n\n    if isinstance(recipe, Recipe):\n        return recipe\n\n    # RecipeTuple case\n    stages = []\n    stage_names = recipe.target_stages\n    if stage_names is None:\n        stages = recipe.recipe.stages\n    else:\n        for stage in recipe.recipe.stages:\n            if any(stage.group in stage_name for stage_name in stage_names):\n                stages.append(stage)\n\n    # default args in recipe\n    args = recipe.recipe.args if isinstance(recipe, RecipeTuple) else recipe.args\n\n    # overwrite with args passed in through CLI\n    for key, val in recipe.override_args.items():\n        args[key] = val\n    version = recipe.version if isinstance(recipe, Recipe) else None\n\n    simplified = Recipe()\n    simplified.version = version\n    simplified.args = args\n    simplified.stages = stages\n\n    return simplified\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.Recipe.yaml","title":"<code>yaml(file_path=None)</code>","text":"<p>Return a yaml string representation of the recipe.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Optional[str]</code> <p>optional file path to save yaml to</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The yaml string representation of the recipe</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def yaml(self, file_path: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Return a yaml string representation of the recipe.\n\n    :param file_path: optional file path to save yaml to\n    :return: The yaml string representation of the recipe\n    \"\"\"\n    file_stream = None if file_path is None else open(file_path, \"w\")\n    yaml_dict = self._get_yaml_dict()\n\n    ret = yaml.dump(\n        yaml_dict,\n        stream=file_stream,\n        allow_unicode=True,\n        sort_keys=False,\n        default_flow_style=None,\n        width=88,\n    )\n\n    if file_stream is not None:\n        file_stream.close()\n\n    return ret\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeBase","title":"<code>RecipeBase</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Defines the contract that <code>Recipe</code> and its components such as <code>RecipeModifier</code> and <code>RecipeStage</code> must follow.</p> <p>All inheritors of this class must implement the following methods:     - calculate_start     - calculate_end     - evaluate     - create_modifier</p> Source code in <code>src/llmcompressor/recipe/base.py</code> <pre><code>class RecipeBase(BaseModel, ABC):\n    \"\"\"\n    Defines the contract that `Recipe` and its components\n    such as `RecipeModifier` and `RecipeStage` must follow.\n\n    All inheritors of this class must implement the following methods:\n        - calculate_start\n        - calculate_end\n        - evaluate\n        - create_modifier\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def create_modifier(self) -&gt; Any:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeContainer","title":"<code>RecipeContainer</code>  <code>dataclass</code>","text":"<p>A container for recipes to be used in a session. Provides utilities to update the recipes and compile them into a single recipe.</p> <p>Parameters:</p> Name Type Description Default <code>compiled_recipe</code> <code>Optional[Recipe]</code> <p>the compiled recipe from the recipes list</p> <code>None</code> <code>recipes</code> <code>List[RecipeTuple]</code> <p>the list of RecipeTuple instances to be compiled</p> <code>list()</code> <code>applied_stages</code> <code>List[str]</code> <p>list of recipe stages that have already been applied</p> <code>list()</code> Source code in <code>src/llmcompressor/recipe/container.py</code> <pre><code>@dataclass\nclass RecipeContainer:\n    \"\"\"\n    A container for recipes to be used in a session. Provides utilities\n    to update the recipes and compile them into a single recipe.\n\n    :param compiled_recipe: the compiled recipe from the recipes list\n    :param recipes: the list of RecipeTuple instances to be compiled\n    :param applied_stages: list of recipe stages that have already been applied\n    \"\"\"\n\n    compiled_recipe: Optional[Recipe] = None\n    recipes: List[RecipeTuple] = field(default_factory=list)\n    applied_stages: List[str] = field(default_factory=list)\n\n    def prepend(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n    ):\n        recipe_tuples = self._prepare_tuples(recipe, recipe_stage, recipe_args)\n        self.recipes = recipe_tuples + self.recipes\n        self._check_compile_recipe()\n\n    def append(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n    ):\n        recipe_tuples = self._prepare_tuples(recipe, recipe_stage, recipe_args)\n        self.recipes = self.recipes + recipe_tuples\n        self._check_compile_recipe()\n\n    def get_modifiers(self) -&gt; List[Modifier]:\n        if self.compiled_recipe is None:\n            return []\n\n        return self.compiled_recipe.create_modifier()\n\n    def _prepare_tuples(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n    ) -&gt; List[RecipeTuple]:\n        if recipe is None or (isinstance(recipe, list) and len(recipe) == 0):\n            return []\n\n        # prepare recipe\n        if isinstance(recipe, Modifier) or (\n            isinstance(recipe, list)\n            and all(isinstance(mod, Modifier) for mod in recipe)\n        ):\n            recipe = Recipe.create_instance(recipe)\n\n        if not isinstance(recipe, list):\n            recipe = [recipe]\n\n        recipe = [\n            Recipe.create_instance(rec) if isinstance(rec, str) else rec\n            for rec in recipe\n        ]\n\n        # prepare stage\n        if recipe_stage is None:\n            recipe_stage = [None] * len(recipe)\n        else:\n            if not isinstance(recipe_stage, list):\n                recipe_stage = [[recipe_stage]] * len(recipe)\n            if not isinstance(recipe_stage[0], list):\n                recipe_stage = [recipe_stage] * len(recipe)\n\n        # prepare args\n        if recipe_args is None:\n            recipe_args = [{}] * len(recipe)\n        elif not isinstance(recipe_args, list):\n            recipe_args = [recipe_args] * len(recipe)\n\n        # validation\n        if len(recipe) != len(recipe_stage) or len(recipe) != len(recipe_args):\n            raise ValueError(\n                \"recipe, recipe_stage, and recipe_args must be the same length\"\n            )\n\n        # create tuples\n        return [\n            RecipeTuple(rec, stage, args)\n            for rec, stage, args in zip(recipe, recipe_stage, recipe_args)\n        ]\n\n    def update_applied_stages(self, new_stages: List[str]):\n        \"\"\"\n        Updates the applied_stages list with new stages, indicating their structure\n        has already been applied\n\n        :param new_stages: new stage names to add\n        \"\"\"\n        for stage in new_stages:\n            if stage not in self.applied_stages:\n                self.applied_stages.append(stage)\n\n    def _check_compile_recipe(self):\n        \"\"\"\n        Check if the recipes need to be compiled into a single recipe and\n        compile them if they do.\n\n        :return: True if the recipes were compiled, False otherwise\n        \"\"\"\n        if self.recipes:\n            self.compiled_recipe = Recipe.simplify_combine_recipes(self.recipes)\n\n    def check_any_recipe_exists(self) -&gt; bool:\n        \"\"\"\n        Checks if any recipes have been added to the container, compiled or not\n\n        :return: True if any recipes exist in the container, False otherwise\n        \"\"\"\n        if self.compiled_recipe is not None:\n            return True\n        if len(self.recipes) &gt; 0:\n            return True\n\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeContainer.check_any_recipe_exists","title":"<code>check_any_recipe_exists()</code>","text":"<p>Checks if any recipes have been added to the container, compiled or not</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if any recipes exist in the container, False otherwise</p> Source code in <code>src/llmcompressor/recipe/container.py</code> <pre><code>def check_any_recipe_exists(self) -&gt; bool:\n    \"\"\"\n    Checks if any recipes have been added to the container, compiled or not\n\n    :return: True if any recipes exist in the container, False otherwise\n    \"\"\"\n    if self.compiled_recipe is not None:\n        return True\n    if len(self.recipes) &gt; 0:\n        return True\n\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeContainer.update_applied_stages","title":"<code>update_applied_stages(new_stages)</code>","text":"<p>Updates the applied_stages list with new stages, indicating their structure has already been applied</p> <p>Parameters:</p> Name Type Description Default <code>new_stages</code> <code>List[str]</code> <p>new stage names to add</p> required Source code in <code>src/llmcompressor/recipe/container.py</code> <pre><code>def update_applied_stages(self, new_stages: List[str]):\n    \"\"\"\n    Updates the applied_stages list with new stages, indicating their structure\n    has already been applied\n\n    :param new_stages: new stage names to add\n    \"\"\"\n    for stage in new_stages:\n        if stage not in self.applied_stages:\n            self.applied_stages.append(stage)\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeModifier","title":"<code>RecipeModifier</code>","text":"<p>               Bases: <code>RecipeBase</code></p> <p>A RecipeModifier is a modifier that is defined in a recipe and can be evaluated and used to create a  Modifier instance using the ModifierFactory.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <p>the type of modifier to create</p> required <code>group</code> <p>the group to assign the modifier to</p> required <code>args</code> <p>the args to use for the modifier</p> required Source code in <code>src/llmcompressor/recipe/modifier.py</code> <pre><code>class RecipeModifier(RecipeBase):\n    \"\"\"\n    A RecipeModifier is a modifier that is defined in a recipe and can be\n    evaluated and used to create a  Modifier instance using\n    the ModifierFactory.\n\n    :param type: the type of modifier to create\n    :param group: the group to assign the modifier to\n    :param args: the args to use for the modifier\n    \"\"\"\n\n    type: str\n    group: Optional[str] = None\n    args: Optional[Dict[str, Any]] = None\n\n    def create_modifier(self) -&gt; \"Modifier\":\n        \"\"\"\n        Create a Modifier instance using the ModifierFactory\n\n        :return: the created modifier\n        \"\"\"\n        if not ModifierFactory._loaded:\n            ModifierFactory.refresh()\n        return ModifierFactory.create(\n            self.type,\n            allow_registered=True,\n            allow_experimental=True,\n            **self.args,\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def extract_modifier_type(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if len(values) == 2:\n            if \"group\" not in values:\n                raise ValueError(\n                    \"Invalid format: expected keys 'group' and one modifier \"\n                    f\"type, but got keys: {list(values.keys())}\"\n                )\n\n            # values contains only group and the Modifier type as keys\n            group = values.pop(\"group\")\n            modifier_type, args = values.popitem()\n            return {\"group\": group, \"type\": modifier_type, \"args\": args}\n\n        # values already in the correct format\n        return values\n\n    def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        :return: the dictionary representation of the modifier\n        \"\"\"\n        return {self.type: self.args, \"group\": f\"{self.group}_modifiers\"}\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeModifier.create_modifier","title":"<code>create_modifier()</code>","text":"<p>Create a Modifier instance using the ModifierFactory</p> <p>Returns:</p> Type Description <code>Modifier</code> <p>the created modifier</p> Source code in <code>src/llmcompressor/recipe/modifier.py</code> <pre><code>def create_modifier(self) -&gt; \"Modifier\":\n    \"\"\"\n    Create a Modifier instance using the ModifierFactory\n\n    :return: the created modifier\n    \"\"\"\n    if not ModifierFactory._loaded:\n        ModifierFactory.refresh()\n    return ModifierFactory.create(\n        self.type,\n        allow_registered=True,\n        allow_experimental=True,\n        **self.args,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeModifier.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the dictionary representation of the modifier</p> Source code in <code>src/llmcompressor/recipe/modifier.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    :return: the dictionary representation of the modifier\n    \"\"\"\n    return {self.type: self.args, \"group\": f\"{self.group}_modifiers\"}\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeStage","title":"<code>RecipeStage</code>","text":"<p>               Bases: <code>RecipeBase</code></p> <p>Represents a stage in a recipe.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>Name of the current stage</p> required <code>run_type</code> <p>Whether this is a oneshot or training stage</p> required <code>args</code> <p>Optional recipe args to use for this stage</p> required <code>enabled</code> <p>True to enable the stage, False otherwise</p> required <code>modifiers</code> <p>list of RecipeModifiers that are a part of this stage</p> required <code>exclude_default</code> <p>True to exclude the default modifiers from the stage, False otherwise</p> required Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>class RecipeStage(RecipeBase):\n    \"\"\"\n    Represents a stage in a recipe.\n\n    :param group: Name of the current stage\n    :param run_type: Whether this is a oneshot or training stage\n    :param args: Optional recipe args to use for this stage\n    :param enabled: True to enable the stage, False otherwise\n    :param modifiers: list of RecipeModifiers that are a part of this stage\n    :param exclude_default: True to exclude the default modifiers from the stage,\n        False otherwise\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    group: Optional[str] = None\n    args: Optional[Dict[str, Any]] = None\n    enabled: bool = True\n    modifiers: List[RecipeModifier] = Field(default_factory=list)\n    exclude_default: bool = False\n\n    def create_modifier(self) -&gt; StageModifiers:\n        \"\"\"\n        The StageModifiers instance will contain instantiated\n        specific modifiers for the stage with the group and index set\n\n            | for each recipe_modifier in stage\n            |   | instantiate modifier\n            |   | set group and index of modifier\n            |   | append modifier to StageModifiers.modifiers\n            | return StageModifiers instance\n\n        :return: the StageModifiers for the stage\n        \"\"\"\n\n        stage_modifiers = StageModifiers()\n        for index, modifier in enumerate(self.modifiers):\n            modifier = modifier.create_modifier()\n            modifier.group = self.group\n            modifier.index = index\n            stage_modifiers.modifiers.append(modifier)\n\n        return stage_modifiers\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def remap_modifiers(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        modifiers = RecipeStage.extract_dict_modifiers(values)\n        values[\"modifiers\"] = modifiers\n\n        return values\n\n    @staticmethod\n    def extract_dict_modifiers(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extracts modifiers from a dict of values and returns a list of modifiers\n        with the group set to the key of the modifier in the dict\n\n        &gt;&gt;&gt; values = {\n        ...     \"pruning_modifiers\": {\n        ...         \"ModifierTypeOne\": {\"param\": 1},\n        ...         \"ModifierTypeTwo\": {\"param\": 2},\n        ...     },\n        ... }\n        &gt;&gt;&gt; RecipeStage.extract_dict_modifiers(values) # doctest: +NORMALIZE_WHITESPACE\n        [{'ModifierTypeOne': {'param': 1}, 'group': 'pruning'},\n        {'ModifierTypeTwo': {'param': 2}, 'group': 'pruning'}]\n\n        Accepted formats:\n        - modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n\n        - first_modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n        \"\"\"\n\n        modifiers = []\n\n        if \"modifiers\" in values:\n            modifier_values = values.pop(\"modifiers\")\n            if \"stages\" in values:\n                for mod_key, mod_value in values.pop(\"stages\").items():\n                    modifiers.append({mod_key: mod_value, \"group\": \"default\"})\n            else:\n                values[\"default_stage\"] = {\n                    \"default_modifiers\": {mod.type: mod.args for mod in modifier_values}\n                }\n                modifiers.extend(\n                    {mod.type: mod.args, \"group\": \"default\"} for mod in modifier_values\n                )\n\n        for key in [k for k in values if k.endswith(\"_modifiers\")]:\n            group = key.rsplit(\"_modifiers\", 1)[0]\n            modifiers.extend(\n                {mod_key: mod_value, \"group\": group}\n                for mod_key, mod_value in values.pop(key).items()\n            )\n\n        return modifiers\n\n    def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        :return: a dictionary representation of the stage\n        \"\"\"\n        dict_ = super().dict(*args, **kwargs)\n        modifiers = {}\n\n        for modifier in dict_[\"modifiers\"]:\n            group = modifier[\"group\"]\n            del modifier[\"group\"]\n            if group not in modifiers:\n                modifiers[group] = []\n            modifiers[group].append(modifier)\n\n        dict_[\"modifiers\"] = modifiers\n\n        return dict_\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeStage.create_modifier","title":"<code>create_modifier()</code>","text":"<p>The StageModifiers instance will contain instantiated specific modifiers for the stage with the group and index set</p> <pre><code>| for each recipe_modifier in stage\n|   | instantiate modifier\n|   | set group and index of modifier\n|   | append modifier to StageModifiers.modifiers\n| return StageModifiers instance\n</code></pre> <p>Returns:</p> Type Description <code>StageModifiers</code> <p>the StageModifiers for the stage</p> Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>def create_modifier(self) -&gt; StageModifiers:\n    \"\"\"\n    The StageModifiers instance will contain instantiated\n    specific modifiers for the stage with the group and index set\n\n        | for each recipe_modifier in stage\n        |   | instantiate modifier\n        |   | set group and index of modifier\n        |   | append modifier to StageModifiers.modifiers\n        | return StageModifiers instance\n\n    :return: the StageModifiers for the stage\n    \"\"\"\n\n    stage_modifiers = StageModifiers()\n    for index, modifier in enumerate(self.modifiers):\n        modifier = modifier.create_modifier()\n        modifier.group = self.group\n        modifier.index = index\n        stage_modifiers.modifiers.append(modifier)\n\n    return stage_modifiers\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeStage.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>a dictionary representation of the stage</p> Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    :return: a dictionary representation of the stage\n    \"\"\"\n    dict_ = super().dict(*args, **kwargs)\n    modifiers = {}\n\n    for modifier in dict_[\"modifiers\"]:\n        group = modifier[\"group\"]\n        del modifier[\"group\"]\n        if group not in modifiers:\n            modifiers[group] = []\n        modifiers[group].append(modifier)\n\n    dict_[\"modifiers\"] = modifiers\n\n    return dict_\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeStage.extract_dict_modifiers","title":"<code>extract_dict_modifiers(values)</code>  <code>staticmethod</code>","text":"<p>Extracts modifiers from a dict of values and returns a list of modifiers with the group set to the key of the modifier in the dict</p> <p>values = { ...     \"pruning_modifiers\": { ...         \"ModifierTypeOne\": {\"param\": 1}, ...         \"ModifierTypeTwo\": {\"param\": 2}, ...     }, ... } RecipeStage.extract_dict_modifiers(values) # doctest: +NORMALIZE_WHITESPACE [{'ModifierTypeOne': {'param': 1}, 'group': 'pruning'}, {'ModifierTypeTwo': {'param': 2}, 'group': 'pruning'}]</p> <p>Accepted formats: - modifiers:   - ModifierTypeOne     ...   - ModifierTypeTwo     ...</p> <ul> <li>first_modifiers:</li> <li>ModifierTypeOne     ...</li> <li>ModifierTypeTwo     ...</li> </ul> Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>@staticmethod\ndef extract_dict_modifiers(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Extracts modifiers from a dict of values and returns a list of modifiers\n    with the group set to the key of the modifier in the dict\n\n    &gt;&gt;&gt; values = {\n    ...     \"pruning_modifiers\": {\n    ...         \"ModifierTypeOne\": {\"param\": 1},\n    ...         \"ModifierTypeTwo\": {\"param\": 2},\n    ...     },\n    ... }\n    &gt;&gt;&gt; RecipeStage.extract_dict_modifiers(values) # doctest: +NORMALIZE_WHITESPACE\n    [{'ModifierTypeOne': {'param': 1}, 'group': 'pruning'},\n    {'ModifierTypeTwo': {'param': 2}, 'group': 'pruning'}]\n\n    Accepted formats:\n    - modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n\n    - first_modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n    \"\"\"\n\n    modifiers = []\n\n    if \"modifiers\" in values:\n        modifier_values = values.pop(\"modifiers\")\n        if \"stages\" in values:\n            for mod_key, mod_value in values.pop(\"stages\").items():\n                modifiers.append({mod_key: mod_value, \"group\": \"default\"})\n        else:\n            values[\"default_stage\"] = {\n                \"default_modifiers\": {mod.type: mod.args for mod in modifier_values}\n            }\n            modifiers.extend(\n                {mod.type: mod.args, \"group\": \"default\"} for mod in modifier_values\n            )\n\n    for key in [k for k in values if k.endswith(\"_modifiers\")]:\n        group = key.rsplit(\"_modifiers\", 1)[0]\n        modifiers.extend(\n            {mod_key: mod_value, \"group\": group}\n            for mod_key, mod_value in values.pop(key).items()\n        )\n\n    return modifiers\n</code></pre>"},{"location":"reference/llmcompressor/recipe/#llmcompressor.recipe.RecipeTuple","title":"<code>RecipeTuple</code>  <code>dataclass</code>","text":"<p>A simple dataclass to hold a recipe, its target_stages, and override_args</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>Recipe</code> <p>The Recipe instance to hold</p> required <code>target_stages</code> <code>List[str]</code> <p>The stages to target when simplifying the recipe (Note: Stages not in the target_stages will be removed during simplification)</p> required <code>override_args</code> <code>Dict[str, Any]</code> <p>The args used to override existing recipe args associated with the supplied <code>recipe</code></p> required Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@dataclass\nclass RecipeTuple:\n    \"\"\"\n    A simple dataclass to hold a recipe, its target_stages, and override_args\n\n    :param recipe: The Recipe instance to hold\n    :param target_stages: The stages to target when simplifying the recipe\n        (Note: Stages not in the target_stages will be removed during\n        simplification)\n    :param override_args: The args used to override existing recipe args\n        associated with the supplied `recipe`\n    \"\"\"\n\n    recipe: Recipe\n    target_stages: List[str]\n    override_args: Dict[str, Any]\n</code></pre>"},{"location":"reference/llmcompressor/recipe/base/","title":"llmcompressor.recipe.base","text":""},{"location":"reference/llmcompressor/recipe/base/#llmcompressor.recipe.base.RecipeBase","title":"<code>RecipeBase</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Defines the contract that <code>Recipe</code> and its components such as <code>RecipeModifier</code> and <code>RecipeStage</code> must follow.</p> <p>All inheritors of this class must implement the following methods:     - calculate_start     - calculate_end     - evaluate     - create_modifier</p> Source code in <code>src/llmcompressor/recipe/base.py</code> <pre><code>class RecipeBase(BaseModel, ABC):\n    \"\"\"\n    Defines the contract that `Recipe` and its components\n    such as `RecipeModifier` and `RecipeStage` must follow.\n\n    All inheritors of this class must implement the following methods:\n        - calculate_start\n        - calculate_end\n        - evaluate\n        - create_modifier\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def create_modifier(self) -&gt; Any:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/llmcompressor/recipe/container/","title":"llmcompressor.recipe.container","text":""},{"location":"reference/llmcompressor/recipe/container/#llmcompressor.recipe.container.RecipeContainer","title":"<code>RecipeContainer</code>  <code>dataclass</code>","text":"<p>A container for recipes to be used in a session. Provides utilities to update the recipes and compile them into a single recipe.</p> <p>Parameters:</p> Name Type Description Default <code>compiled_recipe</code> <code>Optional[Recipe]</code> <p>the compiled recipe from the recipes list</p> <code>None</code> <code>recipes</code> <code>List[RecipeTuple]</code> <p>the list of RecipeTuple instances to be compiled</p> <code>list()</code> <code>applied_stages</code> <code>List[str]</code> <p>list of recipe stages that have already been applied</p> <code>list()</code> Source code in <code>src/llmcompressor/recipe/container.py</code> <pre><code>@dataclass\nclass RecipeContainer:\n    \"\"\"\n    A container for recipes to be used in a session. Provides utilities\n    to update the recipes and compile them into a single recipe.\n\n    :param compiled_recipe: the compiled recipe from the recipes list\n    :param recipes: the list of RecipeTuple instances to be compiled\n    :param applied_stages: list of recipe stages that have already been applied\n    \"\"\"\n\n    compiled_recipe: Optional[Recipe] = None\n    recipes: List[RecipeTuple] = field(default_factory=list)\n    applied_stages: List[str] = field(default_factory=list)\n\n    def prepend(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n    ):\n        recipe_tuples = self._prepare_tuples(recipe, recipe_stage, recipe_args)\n        self.recipes = recipe_tuples + self.recipes\n        self._check_compile_recipe()\n\n    def append(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n    ):\n        recipe_tuples = self._prepare_tuples(recipe, recipe_stage, recipe_args)\n        self.recipes = self.recipes + recipe_tuples\n        self._check_compile_recipe()\n\n    def get_modifiers(self) -&gt; List[Modifier]:\n        if self.compiled_recipe is None:\n            return []\n\n        return self.compiled_recipe.create_modifier()\n\n    def _prepare_tuples(\n        self,\n        recipe: Optional[RecipeInput] = None,\n        recipe_stage: Optional[RecipeStageInput] = None,\n        recipe_args: Optional[RecipeArgsInput] = None,\n    ) -&gt; List[RecipeTuple]:\n        if recipe is None or (isinstance(recipe, list) and len(recipe) == 0):\n            return []\n\n        # prepare recipe\n        if isinstance(recipe, Modifier) or (\n            isinstance(recipe, list)\n            and all(isinstance(mod, Modifier) for mod in recipe)\n        ):\n            recipe = Recipe.create_instance(recipe)\n\n        if not isinstance(recipe, list):\n            recipe = [recipe]\n\n        recipe = [\n            Recipe.create_instance(rec) if isinstance(rec, str) else rec\n            for rec in recipe\n        ]\n\n        # prepare stage\n        if recipe_stage is None:\n            recipe_stage = [None] * len(recipe)\n        else:\n            if not isinstance(recipe_stage, list):\n                recipe_stage = [[recipe_stage]] * len(recipe)\n            if not isinstance(recipe_stage[0], list):\n                recipe_stage = [recipe_stage] * len(recipe)\n\n        # prepare args\n        if recipe_args is None:\n            recipe_args = [{}] * len(recipe)\n        elif not isinstance(recipe_args, list):\n            recipe_args = [recipe_args] * len(recipe)\n\n        # validation\n        if len(recipe) != len(recipe_stage) or len(recipe) != len(recipe_args):\n            raise ValueError(\n                \"recipe, recipe_stage, and recipe_args must be the same length\"\n            )\n\n        # create tuples\n        return [\n            RecipeTuple(rec, stage, args)\n            for rec, stage, args in zip(recipe, recipe_stage, recipe_args)\n        ]\n\n    def update_applied_stages(self, new_stages: List[str]):\n        \"\"\"\n        Updates the applied_stages list with new stages, indicating their structure\n        has already been applied\n\n        :param new_stages: new stage names to add\n        \"\"\"\n        for stage in new_stages:\n            if stage not in self.applied_stages:\n                self.applied_stages.append(stage)\n\n    def _check_compile_recipe(self):\n        \"\"\"\n        Check if the recipes need to be compiled into a single recipe and\n        compile them if they do.\n\n        :return: True if the recipes were compiled, False otherwise\n        \"\"\"\n        if self.recipes:\n            self.compiled_recipe = Recipe.simplify_combine_recipes(self.recipes)\n\n    def check_any_recipe_exists(self) -&gt; bool:\n        \"\"\"\n        Checks if any recipes have been added to the container, compiled or not\n\n        :return: True if any recipes exist in the container, False otherwise\n        \"\"\"\n        if self.compiled_recipe is not None:\n            return True\n        if len(self.recipes) &gt; 0:\n            return True\n\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/recipe/container/#llmcompressor.recipe.container.RecipeContainer.check_any_recipe_exists","title":"<code>check_any_recipe_exists()</code>","text":"<p>Checks if any recipes have been added to the container, compiled or not</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if any recipes exist in the container, False otherwise</p> Source code in <code>src/llmcompressor/recipe/container.py</code> <pre><code>def check_any_recipe_exists(self) -&gt; bool:\n    \"\"\"\n    Checks if any recipes have been added to the container, compiled or not\n\n    :return: True if any recipes exist in the container, False otherwise\n    \"\"\"\n    if self.compiled_recipe is not None:\n        return True\n    if len(self.recipes) &gt; 0:\n        return True\n\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/recipe/container/#llmcompressor.recipe.container.RecipeContainer.update_applied_stages","title":"<code>update_applied_stages(new_stages)</code>","text":"<p>Updates the applied_stages list with new stages, indicating their structure has already been applied</p> <p>Parameters:</p> Name Type Description Default <code>new_stages</code> <code>List[str]</code> <p>new stage names to add</p> required Source code in <code>src/llmcompressor/recipe/container.py</code> <pre><code>def update_applied_stages(self, new_stages: List[str]):\n    \"\"\"\n    Updates the applied_stages list with new stages, indicating their structure\n    has already been applied\n\n    :param new_stages: new stage names to add\n    \"\"\"\n    for stage in new_stages:\n        if stage not in self.applied_stages:\n            self.applied_stages.append(stage)\n</code></pre>"},{"location":"reference/llmcompressor/recipe/metadata/","title":"llmcompressor.recipe.metadata","text":""},{"location":"reference/llmcompressor/recipe/modifier/","title":"llmcompressor.recipe.modifier","text":""},{"location":"reference/llmcompressor/recipe/modifier/#llmcompressor.recipe.modifier.RecipeModifier","title":"<code>RecipeModifier</code>","text":"<p>               Bases: <code>RecipeBase</code></p> <p>A RecipeModifier is a modifier that is defined in a recipe and can be evaluated and used to create a  Modifier instance using the ModifierFactory.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <p>the type of modifier to create</p> required <code>group</code> <p>the group to assign the modifier to</p> required <code>args</code> <p>the args to use for the modifier</p> required Source code in <code>src/llmcompressor/recipe/modifier.py</code> <pre><code>class RecipeModifier(RecipeBase):\n    \"\"\"\n    A RecipeModifier is a modifier that is defined in a recipe and can be\n    evaluated and used to create a  Modifier instance using\n    the ModifierFactory.\n\n    :param type: the type of modifier to create\n    :param group: the group to assign the modifier to\n    :param args: the args to use for the modifier\n    \"\"\"\n\n    type: str\n    group: Optional[str] = None\n    args: Optional[Dict[str, Any]] = None\n\n    def create_modifier(self) -&gt; \"Modifier\":\n        \"\"\"\n        Create a Modifier instance using the ModifierFactory\n\n        :return: the created modifier\n        \"\"\"\n        if not ModifierFactory._loaded:\n            ModifierFactory.refresh()\n        return ModifierFactory.create(\n            self.type,\n            allow_registered=True,\n            allow_experimental=True,\n            **self.args,\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def extract_modifier_type(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if len(values) == 2:\n            if \"group\" not in values:\n                raise ValueError(\n                    \"Invalid format: expected keys 'group' and one modifier \"\n                    f\"type, but got keys: {list(values.keys())}\"\n                )\n\n            # values contains only group and the Modifier type as keys\n            group = values.pop(\"group\")\n            modifier_type, args = values.popitem()\n            return {\"group\": group, \"type\": modifier_type, \"args\": args}\n\n        # values already in the correct format\n        return values\n\n    def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        :return: the dictionary representation of the modifier\n        \"\"\"\n        return {self.type: self.args, \"group\": f\"{self.group}_modifiers\"}\n</code></pre>"},{"location":"reference/llmcompressor/recipe/modifier/#llmcompressor.recipe.modifier.RecipeModifier.create_modifier","title":"<code>create_modifier()</code>","text":"<p>Create a Modifier instance using the ModifierFactory</p> <p>Returns:</p> Type Description <code>Modifier</code> <p>the created modifier</p> Source code in <code>src/llmcompressor/recipe/modifier.py</code> <pre><code>def create_modifier(self) -&gt; \"Modifier\":\n    \"\"\"\n    Create a Modifier instance using the ModifierFactory\n\n    :return: the created modifier\n    \"\"\"\n    if not ModifierFactory._loaded:\n        ModifierFactory.refresh()\n    return ModifierFactory.create(\n        self.type,\n        allow_registered=True,\n        allow_experimental=True,\n        **self.args,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/recipe/modifier/#llmcompressor.recipe.modifier.RecipeModifier.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the dictionary representation of the modifier</p> Source code in <code>src/llmcompressor/recipe/modifier.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    :return: the dictionary representation of the modifier\n    \"\"\"\n    return {self.type: self.args, \"group\": f\"{self.group}_modifiers\"}\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/","title":"llmcompressor.recipe.recipe","text":""},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe","title":"<code>Recipe</code>","text":"<p>               Bases: <code>RecipeBase</code></p> <p>A class to represent a recipe for a model. Recipes encode the instructions needed for modifying the model and/or training process as a list of modifiers.</p> <p>Recipes can be created from a file, string, or HuggingFace stub. Acceptable file formats include both json and yaml, however, when serializing a recipe, yaml will be used by default.</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>class Recipe(RecipeBase):\n    \"\"\"\n    A class to represent a recipe for a model.\n    Recipes encode the instructions needed for modifying\n    the model and/or training process as a list of modifiers.\n\n    Recipes can be created from a file, string, or HuggingFace stub.\n    Acceptable file formats include both json and yaml, however,\n    when serializing a recipe, yaml will be used by default.\n    \"\"\"\n\n    @classmethod\n    def from_modifiers(\n        cls,\n        modifiers: Union[Modifier, List[Modifier]],\n        modifier_group_name: Optional[str] = None,\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        Create a recipe instance from a list of modifiers\n\n        (Note: all modifiers are wrapped into a single stage\n        with the modifier_group_name as the stage name. If modifier_group_name is None,\n        the default run type is `oneshot`)\n\n        Lfecycle:\n        | - Validate Modifiers\n        | - Create recipe string from modifiers\n        | - Create recipe instance from recipe string\n\n        :param modifiers: The list of RecipeModifier instances\n        :param modifier_group_name: The stage_name of the recipe,\n            if `oneshot` or `train` the run_type of the recipe will be\n            inferred from the modifier_group_name, if None, a dummy default\n            group_name will be assigned.\n        :return: The Recipe instance created from the modifiers\n        \"\"\"\n        logger.info(\"Creating recipe from modifiers\")\n\n        if isinstance(modifiers, Modifier):\n            modifiers = [modifiers]\n\n        if any(not isinstance(modifier, Modifier) for modifier in modifiers):\n            raise ValueError(\"modifiers must be a list of Modifier instances\")\n\n        group_name = modifier_group_name or \"default\"\n\n        recipe_modifiers: List[RecipeModifier] = [\n            RecipeModifier(\n                type=modifier.__class__.__name__,\n                group=group_name,\n                args=modifier.model_dump(exclude_unset=True),\n            )\n            for modifier in modifiers\n        ]\n        # assume one stage for modifier instances\n        stages: List[RecipeStage] = [\n            RecipeStage(group=group_name, modifiers=recipe_modifiers)\n        ]\n        recipe = cls()\n        recipe.stages = stages\n        return recipe\n\n    @classmethod\n    def create_instance(\n        cls,\n        path_or_modifiers: Union[str, Modifier, List[Modifier], \"Recipe\"],\n        modifier_group_name: Optional[str] = None,\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        Create a recipe instance from a file, string, or RecipeModifier objects\n\n\n        Using a recipe string or file is supported:\n        &gt;&gt;&gt; recipe_str = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n\n        :param path_or_modifiers: The path to the recipe file or\n            or the recipe string (must be a valid\n            json/yaml file or a valid json/yaml string). Can also\n            accept a RecipeModifier instance, or a list of\n            RecipeModifiers\n        :param modifier_group_name: The stage_name of the recipe,\n            if `oneshot` or `train` the run_type of the recipe will be\n            inferred from the modifier_group_name, if None, a dummy default\n            group_name will be assigned. This argument is only used\n            when creating a recipe from a Modifier/list of Modifier(s)\n            instance, else it's ignored.\n        :return: The Recipe instance created from the path or modifiers,\n            or a valid recipe string in yaml/json format\n        \"\"\"\n\n        if isinstance(path_or_modifiers, Recipe):\n            # already a recipe\n            return path_or_modifiers\n\n        if isinstance(path_or_modifiers, (Modifier, list)):\n            return cls.from_modifiers(\n                modifiers=path_or_modifiers, modifier_group_name=modifier_group_name\n            )\n\n        if not os.path.isfile(path_or_modifiers):\n            # not a local file\n            # assume it's a string\n            logger.debug(\n                \"Could not initialize recipe as a file path or zoo stub, \"\n                \"attempting to process as a string.\"\n            )\n            logger.debug(f\"Input string: {path_or_modifiers}\")\n            obj = _load_json_or_yaml_string(path_or_modifiers)\n            return Recipe.model_validate(obj)\n        else:\n            logger.info(f\"Loading recipe from file {path_or_modifiers}\")\n\n        with open(path_or_modifiers, \"r\") as file:\n            content = file.read().strip()\n            if path_or_modifiers.lower().endswith(\".md\"):\n                content = _parse_recipe_from_md(path_or_modifiers, content)\n\n            if path_or_modifiers.lower().endswith(\".json\"):\n                obj = json.loads(content)\n            elif path_or_modifiers.lower().endswith(\n                \".yaml\"\n            ) or path_or_modifiers.lower().endswith(\".yml\"):\n                obj = yaml.safe_load(content)\n            else:\n                try:\n                    obj = _load_json_or_yaml_string(content)\n                except ValueError:\n                    raise ValueError(\n                        f\"Could not parse recipe from path {path_or_modifiers}\"\n                    )\n            return Recipe.model_validate(obj)\n\n    @staticmethod\n    def simplify_recipe(\n        recipe: Union[str, \"Recipe\", \"RecipeTuple\"], shift: Optional[int] = None\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        Simplify a RecipeTuple by removing stages that are not in the target_stages\n        and shifting the start and end of the recipe by the shift amount\n\n\n        Using a RecipeTuple instance with shift:\n        &gt;&gt;&gt; recipe_str = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n        &gt;&gt;&gt; recipe_tuple = RecipeTuple(recipe, [\"test\"], {})\n        &gt;&gt;&gt; simplified = Recipe.simplify_recipe(recipe_tuple)\n\n        :param recipe: The Recipe or RecipeTuple instance to simplify\n        :return: The simplified Recipe instance\n        \"\"\"\n        if isinstance(recipe, str):\n            recipe = Recipe.create_instance(recipe)\n\n        if isinstance(recipe, Recipe):\n            return recipe\n\n        # RecipeTuple case\n        stages = []\n        stage_names = recipe.target_stages\n        if stage_names is None:\n            stages = recipe.recipe.stages\n        else:\n            for stage in recipe.recipe.stages:\n                if any(stage.group in stage_name for stage_name in stage_names):\n                    stages.append(stage)\n\n        # default args in recipe\n        args = recipe.recipe.args if isinstance(recipe, RecipeTuple) else recipe.args\n\n        # overwrite with args passed in through CLI\n        for key, val in recipe.override_args.items():\n            args[key] = val\n        version = recipe.version if isinstance(recipe, Recipe) else None\n\n        simplified = Recipe()\n        simplified.version = version\n        simplified.args = args\n        simplified.stages = stages\n\n        return simplified\n\n    @staticmethod\n    def simplify_combine_recipes(\n        recipes: List[Union[str, \"Recipe\", \"RecipeTuple\"]],\n    ) -&gt; \"Recipe\":\n        \"\"\"\n        A method to combine multiple recipes into one recipe\n        Automatically calculates the start and end of the combined recipe\n        and shifts the start and end of the recipes accordingly\n\n        Using two RecipeTuple instances:\n        &gt;&gt;&gt; recipe_str_1 = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe_str_2 = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 3.0\n        ...             end: 5.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe_1, recipe_2 = (Recipe.create_instance(recipe_str_1),\n        ... Recipe.create_instance(recipe_str_2))\n        &gt;&gt;&gt; combined = Recipe.simplify_combine_recipes(\n        ... [RecipeTuple(recipe_1, [\"test\"], {}), RecipeTuple(recipe_2, [\"test\"], {})])\n        &gt;&gt;&gt; len(combined.stages)\n        2\n\n        :param recipes: The list of Recipe/RecipeTuple instances to combine\n        :return: The combined Recipe instance\n        \"\"\"\n\n        combined = Recipe()\n\n        for recipe in recipes:\n            simplified = Recipe.simplify_recipe(\n                recipe=recipe,\n            )\n            combined.version = simplified.version\n            combined.stages.extend(simplified.stages)\n            combined.args.update(simplified.args)\n\n        return combined\n\n    version: str = None\n    args: Dict[str, Any] = Field(default_factory=dict)\n    stages: List[RecipeStage] = Field(default_factory=list)\n\n    def create_modifier(self) -&gt; List[\"StageModifiers\"]:\n        \"\"\"\n        Create and return a list of StageModifiers for each stage in the recipe\n\n        &gt;&gt;&gt; recipe_str = '''\n        ... test_stage:\n        ...     pruning_modifiers:\n        ...         ConstantPruningModifier:\n        ...             start: 0.0\n        ...             end: 2.0\n        ...             targets: ['re:.*weight']\n        ... '''\n        &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n        &gt;&gt;&gt; stage_modifiers = recipe.create_modifier()\n        &gt;&gt;&gt; len(stage_modifiers) == 1\n        True\n        &gt;&gt;&gt; len(stage_modifiers[0].modifiers) == 1\n        True\n\n        :return: A list of StageModifiers for each stage in the recipe\n        \"\"\"\n        modifiers = []\n\n        for index, stage in enumerate(self.stages):\n            stage_modifiers = stage.create_modifier()\n            stage_modifiers.index = index\n            stage_modifiers.group = stage.group\n            modifiers.append(stage_modifiers)\n\n        return modifiers\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def remap_stages(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        stages = []\n\n        modifiers = RecipeStage.extract_dict_modifiers(values)\n        if modifiers:\n            default_stage = {\"modifiers\": modifiers, \"group\": \"default\"}\n            stages.append(default_stage)\n\n        extracted = Recipe.extract_dict_stages(values)\n        stages.extend(extracted)\n        formatted_values = {}\n\n        # fill out stages\n        formatted_values[\"stages\"] = stages\n\n        # fill out any default argument values\n        args = {}\n        for key, val in values.items():\n            args[key] = val\n        formatted_values[\"args\"] = args\n\n        return formatted_values\n\n    @staticmethod\n    def extract_dict_stages(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extract stages from a dict of values, acceptable dictionary structures\n        are shown below\n\n        Accepted stage formats:\n        - stages:\n          first_stage:\n            modifiers: ...\n          second_stage:\n            modifiers: ...\n\n        - first_stage:\n          modifiers: ...\n        - second_stage:\n          modifiers: ...\n\n        Accepted modifier formats default stage:\n        - modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n\n        - first_modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n\n        &gt;&gt;&gt; values = {\n        ... \"stages\": {\n        ...     \"first_stage\": {\n        ...         \"modifiers\": {\n        ...             \"ModifierTypeOne\": {\n        ...                 \"start\": 0.0,\n        ...                 \"end\": 2.0,\n        ...                 }\n        ...         }\n        ...     }\n        ... }\n        ... }\n        &gt;&gt;&gt; Recipe.extract_dict_stages(values) # doctest: +NORMALIZE_WHITESPACE\n        [{'modifiers': {'ModifierTypeOne': {'start': 0.0, 'end': 2.0}},\n        'group': 'first_stage'}]\n\n        :param values: The values dict to extract stages from\n        :return: A list of stages, where each stage is a dict of\n            modifiers and their group\n        \"\"\"\n\n        stages = []\n        remove_keys = []\n\n        default_modifiers = RecipeStage.extract_dict_modifiers(values)\n        if default_modifiers:\n            default_stage = {\"modifiers\": default_modifiers, \"group\": \"default\"}\n            stages.append(default_stage)\n\n        if \"stages\" in values and values[\"stages\"]:\n            assert isinstance(\n                values[\"stages\"], dict\n            ), f\"stages must be a dict, given {values['stages']}\"\n            remove_keys.append(\"stages\")\n\n            for key, value in values[\"stages\"].items():\n                assert isinstance(value, dict), f\"stage must be a dict, given {value}\"\n                value[\"group\"] = key\n                stages.append(value)\n\n        for key, value in list(values.items()):\n            if key.endswith(\"_stage\"):\n                remove_keys.append(key)\n                value[\"group\"] = key.rsplit(\"_stage\", 1)[0]\n                stages.append(value)\n\n        for key in remove_keys:\n            del values[key]\n\n        return stages\n\n    def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        :return: A dictionary representation of the recipe\n        \"\"\"\n        dict_ = super().model_dump(*args, **kwargs)\n        stages = {}\n\n        for stage in dict_[\"stages\"]:\n            name = f\"{stage['group']}_stage\"\n            del stage[\"group\"]\n\n            if name not in stages:\n                stages[name] = []\n\n            stages[name].append(stage)\n\n        dict_[\"stages\"] = stages\n\n        return dict_\n\n    def yaml(self, file_path: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Return a yaml string representation of the recipe.\n\n        :param file_path: optional file path to save yaml to\n        :return: The yaml string representation of the recipe\n        \"\"\"\n        file_stream = None if file_path is None else open(file_path, \"w\")\n        yaml_dict = self._get_yaml_dict()\n\n        ret = yaml.dump(\n            yaml_dict,\n            stream=file_stream,\n            allow_unicode=True,\n            sort_keys=False,\n            default_flow_style=None,\n            width=88,\n        )\n\n        if file_stream is not None:\n            file_stream.close()\n\n        return ret\n\n    def _get_yaml_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get a dictionary representation of the recipe for yaml serialization\n        The returned dict will only contain information necessary for yaml\n        serialization and must not be used in place of the dict method\n\n        :return: A dictionary representation of the recipe for yaml serialization\n        \"\"\"\n\n        original_recipe_dict = self.dict()\n        yaml_recipe_dict = {}\n\n        # populate recipe level attributes\n        recipe_level_attributes = [\"version\", \"args\"]\n\n        for attribute in recipe_level_attributes:\n            if attribute_value := original_recipe_dict.get(attribute):\n                yaml_recipe_dict[attribute] = attribute_value\n\n        # populate stages\n        stages = original_recipe_dict[\"stages\"]\n        for stage_name, stage_list in stages.items():\n            for idx, stage in enumerate(stage_list):\n                if len(stage_list) &gt; 1:\n                    # resolve name clashes caused by combining recipes with\n                    # duplicate stage names\n                    final_stage_name = f\"{stage_name}_{idx}\"\n                else:\n                    final_stage_name = stage_name\n                stage_dict = get_yaml_serializable_stage_dict(\n                    modifiers=stage[\"modifiers\"]\n                )\n\n                # infer run_type from stage\n                if run_type := stage.get(\"run_type\"):\n                    stage_dict[\"run_type\"] = run_type\n\n                yaml_recipe_dict[final_stage_name] = stage_dict\n\n        return yaml_recipe_dict\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.create_instance","title":"<code>create_instance(path_or_modifiers, modifier_group_name=None)</code>  <code>classmethod</code>","text":"<p>Create a recipe instance from a file, string, or RecipeModifier objects</p> <p>Using a recipe string or file is supported:</p> <p>recipe_str = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.*weight'] ... ''' recipe = Recipe.create_instance(recipe_str)</p> <p>Parameters:</p> Name Type Description Default <code>path_or_modifiers</code> <code>Union[str, Modifier, List[Modifier], Recipe]</code> <p>The path to the recipe file or or the recipe string (must be a valid json/yaml file or a valid json/yaml string). Can also accept a RecipeModifier instance, or a list of RecipeModifiers</p> required <code>modifier_group_name</code> <code>Optional[str]</code> <p>The stage_name of the recipe, if <code>oneshot</code> or <code>train</code> the run_type of the recipe will be inferred from the modifier_group_name, if None, a dummy default group_name will be assigned. This argument is only used when creating a recipe from a Modifier/list of Modifier(s) instance, else it's ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Recipe</code> <p>The Recipe instance created from the path or modifiers, or a valid recipe string in yaml/json format</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@classmethod\ndef create_instance(\n    cls,\n    path_or_modifiers: Union[str, Modifier, List[Modifier], \"Recipe\"],\n    modifier_group_name: Optional[str] = None,\n) -&gt; \"Recipe\":\n    \"\"\"\n    Create a recipe instance from a file, string, or RecipeModifier objects\n\n\n    Using a recipe string or file is supported:\n    &gt;&gt;&gt; recipe_str = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n\n    :param path_or_modifiers: The path to the recipe file or\n        or the recipe string (must be a valid\n        json/yaml file or a valid json/yaml string). Can also\n        accept a RecipeModifier instance, or a list of\n        RecipeModifiers\n    :param modifier_group_name: The stage_name of the recipe,\n        if `oneshot` or `train` the run_type of the recipe will be\n        inferred from the modifier_group_name, if None, a dummy default\n        group_name will be assigned. This argument is only used\n        when creating a recipe from a Modifier/list of Modifier(s)\n        instance, else it's ignored.\n    :return: The Recipe instance created from the path or modifiers,\n        or a valid recipe string in yaml/json format\n    \"\"\"\n\n    if isinstance(path_or_modifiers, Recipe):\n        # already a recipe\n        return path_or_modifiers\n\n    if isinstance(path_or_modifiers, (Modifier, list)):\n        return cls.from_modifiers(\n            modifiers=path_or_modifiers, modifier_group_name=modifier_group_name\n        )\n\n    if not os.path.isfile(path_or_modifiers):\n        # not a local file\n        # assume it's a string\n        logger.debug(\n            \"Could not initialize recipe as a file path or zoo stub, \"\n            \"attempting to process as a string.\"\n        )\n        logger.debug(f\"Input string: {path_or_modifiers}\")\n        obj = _load_json_or_yaml_string(path_or_modifiers)\n        return Recipe.model_validate(obj)\n    else:\n        logger.info(f\"Loading recipe from file {path_or_modifiers}\")\n\n    with open(path_or_modifiers, \"r\") as file:\n        content = file.read().strip()\n        if path_or_modifiers.lower().endswith(\".md\"):\n            content = _parse_recipe_from_md(path_or_modifiers, content)\n\n        if path_or_modifiers.lower().endswith(\".json\"):\n            obj = json.loads(content)\n        elif path_or_modifiers.lower().endswith(\n            \".yaml\"\n        ) or path_or_modifiers.lower().endswith(\".yml\"):\n            obj = yaml.safe_load(content)\n        else:\n            try:\n                obj = _load_json_or_yaml_string(content)\n            except ValueError:\n                raise ValueError(\n                    f\"Could not parse recipe from path {path_or_modifiers}\"\n                )\n        return Recipe.model_validate(obj)\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.create_modifier","title":"<code>create_modifier()</code>","text":"<p>Create and return a list of StageModifiers for each stage in the recipe</p> <p>recipe_str = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.*weight'] ... ''' recipe = Recipe.create_instance(recipe_str) stage_modifiers = recipe.create_modifier() len(stage_modifiers) == 1 True len(stage_modifiers[0].modifiers) == 1 True</p> <p>Returns:</p> Type Description <code>List[StageModifiers]</code> <p>A list of StageModifiers for each stage in the recipe</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def create_modifier(self) -&gt; List[\"StageModifiers\"]:\n    \"\"\"\n    Create and return a list of StageModifiers for each stage in the recipe\n\n    &gt;&gt;&gt; recipe_str = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n    &gt;&gt;&gt; stage_modifiers = recipe.create_modifier()\n    &gt;&gt;&gt; len(stage_modifiers) == 1\n    True\n    &gt;&gt;&gt; len(stage_modifiers[0].modifiers) == 1\n    True\n\n    :return: A list of StageModifiers for each stage in the recipe\n    \"\"\"\n    modifiers = []\n\n    for index, stage in enumerate(self.stages):\n        stage_modifiers = stage.create_modifier()\n        stage_modifiers.index = index\n        stage_modifiers.group = stage.group\n        modifiers.append(stage_modifiers)\n\n    return modifiers\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representation of the recipe</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    :return: A dictionary representation of the recipe\n    \"\"\"\n    dict_ = super().model_dump(*args, **kwargs)\n    stages = {}\n\n    for stage in dict_[\"stages\"]:\n        name = f\"{stage['group']}_stage\"\n        del stage[\"group\"]\n\n        if name not in stages:\n            stages[name] = []\n\n        stages[name].append(stage)\n\n    dict_[\"stages\"] = stages\n\n    return dict_\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.extract_dict_stages","title":"<code>extract_dict_stages(values)</code>  <code>staticmethod</code>","text":"<p>Extract stages from a dict of values, acceptable dictionary structures are shown below</p> <p>Accepted stage formats: - stages:   first_stage:     modifiers: ...   second_stage:     modifiers: ...</p> <ul> <li>first_stage:   modifiers: ...</li> <li>second_stage:   modifiers: ...</li> </ul> <p>Accepted modifier formats default stage: - modifiers:   - ModifierTypeOne     ...   - ModifierTypeTwo     ...</p> <ul> <li>first_modifiers:</li> <li>ModifierTypeOne     ...</li> <li>ModifierTypeTwo     ...</li> </ul> <p>values = { ... \"stages\": { ...     \"first_stage\": { ...         \"modifiers\": { ...             \"ModifierTypeOne\": { ...                 \"start\": 0.0, ...                 \"end\": 2.0, ...                 } ...         } ...     } ... } ... } Recipe.extract_dict_stages(values) # doctest: +NORMALIZE_WHITESPACE [{'modifiers': {'ModifierTypeOne': {'start': 0.0, 'end': 2.0}}, 'group': 'first_stage'}]</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict[str, Any]</code> <p>The values dict to extract stages from</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of stages, where each stage is a dict of modifiers and their group</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@staticmethod\ndef extract_dict_stages(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Extract stages from a dict of values, acceptable dictionary structures\n    are shown below\n\n    Accepted stage formats:\n    - stages:\n      first_stage:\n        modifiers: ...\n      second_stage:\n        modifiers: ...\n\n    - first_stage:\n      modifiers: ...\n    - second_stage:\n      modifiers: ...\n\n    Accepted modifier formats default stage:\n    - modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n\n    - first_modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n\n    &gt;&gt;&gt; values = {\n    ... \"stages\": {\n    ...     \"first_stage\": {\n    ...         \"modifiers\": {\n    ...             \"ModifierTypeOne\": {\n    ...                 \"start\": 0.0,\n    ...                 \"end\": 2.0,\n    ...                 }\n    ...         }\n    ...     }\n    ... }\n    ... }\n    &gt;&gt;&gt; Recipe.extract_dict_stages(values) # doctest: +NORMALIZE_WHITESPACE\n    [{'modifiers': {'ModifierTypeOne': {'start': 0.0, 'end': 2.0}},\n    'group': 'first_stage'}]\n\n    :param values: The values dict to extract stages from\n    :return: A list of stages, where each stage is a dict of\n        modifiers and their group\n    \"\"\"\n\n    stages = []\n    remove_keys = []\n\n    default_modifiers = RecipeStage.extract_dict_modifiers(values)\n    if default_modifiers:\n        default_stage = {\"modifiers\": default_modifiers, \"group\": \"default\"}\n        stages.append(default_stage)\n\n    if \"stages\" in values and values[\"stages\"]:\n        assert isinstance(\n            values[\"stages\"], dict\n        ), f\"stages must be a dict, given {values['stages']}\"\n        remove_keys.append(\"stages\")\n\n        for key, value in values[\"stages\"].items():\n            assert isinstance(value, dict), f\"stage must be a dict, given {value}\"\n            value[\"group\"] = key\n            stages.append(value)\n\n    for key, value in list(values.items()):\n        if key.endswith(\"_stage\"):\n            remove_keys.append(key)\n            value[\"group\"] = key.rsplit(\"_stage\", 1)[0]\n            stages.append(value)\n\n    for key in remove_keys:\n        del values[key]\n\n    return stages\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.from_modifiers","title":"<code>from_modifiers(modifiers, modifier_group_name=None)</code>  <code>classmethod</code>","text":"<p>Create a recipe instance from a list of modifiers</p> <p>(Note: all modifiers are wrapped into a single stage with the modifier_group_name as the stage name. If modifier_group_name is None, the default run type is <code>oneshot</code>)</p> <p>Lfecycle: | - Validate Modifiers | - Create recipe string from modifiers | - Create recipe instance from recipe string</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <code>Union[Modifier, List[Modifier]]</code> <p>The list of RecipeModifier instances</p> required <code>modifier_group_name</code> <code>Optional[str]</code> <p>The stage_name of the recipe, if <code>oneshot</code> or <code>train</code> the run_type of the recipe will be inferred from the modifier_group_name, if None, a dummy default group_name will be assigned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Recipe</code> <p>The Recipe instance created from the modifiers</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@classmethod\ndef from_modifiers(\n    cls,\n    modifiers: Union[Modifier, List[Modifier]],\n    modifier_group_name: Optional[str] = None,\n) -&gt; \"Recipe\":\n    \"\"\"\n    Create a recipe instance from a list of modifiers\n\n    (Note: all modifiers are wrapped into a single stage\n    with the modifier_group_name as the stage name. If modifier_group_name is None,\n    the default run type is `oneshot`)\n\n    Lfecycle:\n    | - Validate Modifiers\n    | - Create recipe string from modifiers\n    | - Create recipe instance from recipe string\n\n    :param modifiers: The list of RecipeModifier instances\n    :param modifier_group_name: The stage_name of the recipe,\n        if `oneshot` or `train` the run_type of the recipe will be\n        inferred from the modifier_group_name, if None, a dummy default\n        group_name will be assigned.\n    :return: The Recipe instance created from the modifiers\n    \"\"\"\n    logger.info(\"Creating recipe from modifiers\")\n\n    if isinstance(modifiers, Modifier):\n        modifiers = [modifiers]\n\n    if any(not isinstance(modifier, Modifier) for modifier in modifiers):\n        raise ValueError(\"modifiers must be a list of Modifier instances\")\n\n    group_name = modifier_group_name or \"default\"\n\n    recipe_modifiers: List[RecipeModifier] = [\n        RecipeModifier(\n            type=modifier.__class__.__name__,\n            group=group_name,\n            args=modifier.model_dump(exclude_unset=True),\n        )\n        for modifier in modifiers\n    ]\n    # assume one stage for modifier instances\n    stages: List[RecipeStage] = [\n        RecipeStage(group=group_name, modifiers=recipe_modifiers)\n    ]\n    recipe = cls()\n    recipe.stages = stages\n    return recipe\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.simplify_combine_recipes","title":"<code>simplify_combine_recipes(recipes)</code>  <code>staticmethod</code>","text":"<p>A method to combine multiple recipes into one recipe Automatically calculates the start and end of the combined recipe and shifts the start and end of the recipes accordingly</p> <p>Using two RecipeTuple instances:</p> <p>recipe_str_1 = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.weight'] ... ''' recipe_str_2 = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 3.0 ...             end: 5.0 ...             targets: ['re:.weight'] ... ''' recipe_1, recipe_2 = (Recipe.create_instance(recipe_str_1), ... Recipe.create_instance(recipe_str_2)) combined = Recipe.simplify_combine_recipes( ... [RecipeTuple(recipe_1, [\"test\"], {}), RecipeTuple(recipe_2, [\"test\"], {})]) len(combined.stages) 2</p> <p>Parameters:</p> Name Type Description Default <code>recipes</code> <code>List[Union[str, Recipe, RecipeTuple]]</code> <p>The list of Recipe/RecipeTuple instances to combine</p> required <p>Returns:</p> Type Description <code>Recipe</code> <p>The combined Recipe instance</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@staticmethod\ndef simplify_combine_recipes(\n    recipes: List[Union[str, \"Recipe\", \"RecipeTuple\"]],\n) -&gt; \"Recipe\":\n    \"\"\"\n    A method to combine multiple recipes into one recipe\n    Automatically calculates the start and end of the combined recipe\n    and shifts the start and end of the recipes accordingly\n\n    Using two RecipeTuple instances:\n    &gt;&gt;&gt; recipe_str_1 = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe_str_2 = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 3.0\n    ...             end: 5.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe_1, recipe_2 = (Recipe.create_instance(recipe_str_1),\n    ... Recipe.create_instance(recipe_str_2))\n    &gt;&gt;&gt; combined = Recipe.simplify_combine_recipes(\n    ... [RecipeTuple(recipe_1, [\"test\"], {}), RecipeTuple(recipe_2, [\"test\"], {})])\n    &gt;&gt;&gt; len(combined.stages)\n    2\n\n    :param recipes: The list of Recipe/RecipeTuple instances to combine\n    :return: The combined Recipe instance\n    \"\"\"\n\n    combined = Recipe()\n\n    for recipe in recipes:\n        simplified = Recipe.simplify_recipe(\n            recipe=recipe,\n        )\n        combined.version = simplified.version\n        combined.stages.extend(simplified.stages)\n        combined.args.update(simplified.args)\n\n    return combined\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.simplify_recipe","title":"<code>simplify_recipe(recipe, shift=None)</code>  <code>staticmethod</code>","text":"<p>Simplify a RecipeTuple by removing stages that are not in the target_stages and shifting the start and end of the recipe by the shift amount</p> <p>Using a RecipeTuple instance with shift:</p> <p>recipe_str = ''' ... test_stage: ...     pruning_modifiers: ...         ConstantPruningModifier: ...             start: 0.0 ...             end: 2.0 ...             targets: ['re:.*weight'] ... ''' recipe = Recipe.create_instance(recipe_str) recipe_tuple = RecipeTuple(recipe, [\"test\"], {}) simplified = Recipe.simplify_recipe(recipe_tuple)</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>Union[str, Recipe, RecipeTuple]</code> <p>The Recipe or RecipeTuple instance to simplify</p> required <p>Returns:</p> Type Description <code>Recipe</code> <p>The simplified Recipe instance</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@staticmethod\ndef simplify_recipe(\n    recipe: Union[str, \"Recipe\", \"RecipeTuple\"], shift: Optional[int] = None\n) -&gt; \"Recipe\":\n    \"\"\"\n    Simplify a RecipeTuple by removing stages that are not in the target_stages\n    and shifting the start and end of the recipe by the shift amount\n\n\n    Using a RecipeTuple instance with shift:\n    &gt;&gt;&gt; recipe_str = '''\n    ... test_stage:\n    ...     pruning_modifiers:\n    ...         ConstantPruningModifier:\n    ...             start: 0.0\n    ...             end: 2.0\n    ...             targets: ['re:.*weight']\n    ... '''\n    &gt;&gt;&gt; recipe = Recipe.create_instance(recipe_str)\n    &gt;&gt;&gt; recipe_tuple = RecipeTuple(recipe, [\"test\"], {})\n    &gt;&gt;&gt; simplified = Recipe.simplify_recipe(recipe_tuple)\n\n    :param recipe: The Recipe or RecipeTuple instance to simplify\n    :return: The simplified Recipe instance\n    \"\"\"\n    if isinstance(recipe, str):\n        recipe = Recipe.create_instance(recipe)\n\n    if isinstance(recipe, Recipe):\n        return recipe\n\n    # RecipeTuple case\n    stages = []\n    stage_names = recipe.target_stages\n    if stage_names is None:\n        stages = recipe.recipe.stages\n    else:\n        for stage in recipe.recipe.stages:\n            if any(stage.group in stage_name for stage_name in stage_names):\n                stages.append(stage)\n\n    # default args in recipe\n    args = recipe.recipe.args if isinstance(recipe, RecipeTuple) else recipe.args\n\n    # overwrite with args passed in through CLI\n    for key, val in recipe.override_args.items():\n        args[key] = val\n    version = recipe.version if isinstance(recipe, Recipe) else None\n\n    simplified = Recipe()\n    simplified.version = version\n    simplified.args = args\n    simplified.stages = stages\n\n    return simplified\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.Recipe.yaml","title":"<code>yaml(file_path=None)</code>","text":"<p>Return a yaml string representation of the recipe.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Optional[str]</code> <p>optional file path to save yaml to</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The yaml string representation of the recipe</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def yaml(self, file_path: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Return a yaml string representation of the recipe.\n\n    :param file_path: optional file path to save yaml to\n    :return: The yaml string representation of the recipe\n    \"\"\"\n    file_stream = None if file_path is None else open(file_path, \"w\")\n    yaml_dict = self._get_yaml_dict()\n\n    ret = yaml.dump(\n        yaml_dict,\n        stream=file_stream,\n        allow_unicode=True,\n        sort_keys=False,\n        default_flow_style=None,\n        width=88,\n    )\n\n    if file_stream is not None:\n        file_stream.close()\n\n    return ret\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.RecipeTuple","title":"<code>RecipeTuple</code>  <code>dataclass</code>","text":"<p>A simple dataclass to hold a recipe, its target_stages, and override_args</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>Recipe</code> <p>The Recipe instance to hold</p> required <code>target_stages</code> <code>List[str]</code> <p>The stages to target when simplifying the recipe (Note: Stages not in the target_stages will be removed during simplification)</p> required <code>override_args</code> <code>Dict[str, Any]</code> <p>The args used to override existing recipe args associated with the supplied <code>recipe</code></p> required Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>@dataclass\nclass RecipeTuple:\n    \"\"\"\n    A simple dataclass to hold a recipe, its target_stages, and override_args\n\n    :param recipe: The Recipe instance to hold\n    :param target_stages: The stages to target when simplifying the recipe\n        (Note: Stages not in the target_stages will be removed during\n        simplification)\n    :param override_args: The args used to override existing recipe args\n        associated with the supplied `recipe`\n    \"\"\"\n\n    recipe: Recipe\n    target_stages: List[str]\n    override_args: Dict[str, Any]\n</code></pre>"},{"location":"reference/llmcompressor/recipe/recipe/#llmcompressor.recipe.recipe.get_yaml_serializable_stage_dict","title":"<code>get_yaml_serializable_stage_dict(modifiers)</code>","text":"<p>This function is used to convert a list of modifiers into a dictionary where the keys are the group names and the values are the modifiers which in turn are dictionaries with the modifier type as the key and the modifier args as the value.</p> <p>This is needed to conform to our recipe structure during yaml serialization where each stage, modifier_groups, and modifiers are represented as valid yaml dictionaries.</p> <p>Note: This function assumes that modifier groups do not contain the same modifier type more than once in a group. This assumption is also held by Recipe.create_instance(...) method.</p> <p>Parameters:</p> Name Type Description Default <code>modifiers</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries where each dictionary holds all information about a modifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary where the keys are the group names and the values are the modifiers which in turn are dictionaries with the modifier type as the key and the modifier args as the value.</p> Source code in <code>src/llmcompressor/recipe/recipe.py</code> <pre><code>def get_yaml_serializable_stage_dict(modifiers: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    This function is used to convert a list of modifiers into a dictionary\n    where the keys are the group names and the values are the modifiers\n    which in turn are dictionaries with the modifier type as the key and\n    the modifier args as the value.\n\n    This is needed to conform to our recipe structure during yaml serialization\n    where each stage, modifier_groups, and modifiers are represented as\n    valid yaml dictionaries.\n\n    Note: This function assumes that modifier groups do not contain the same\n    modifier type more than once in a group. This assumption is also held by\n    Recipe.create_instance(...) method.\n\n    :param modifiers: A list of dictionaries where each dictionary\n        holds all information about a modifier\n    :return: A dictionary where the keys are the group names and the values\n        are the modifiers which in turn are dictionaries with the modifier\n        type as the key and the modifier args as the value.\n    \"\"\"\n    stage_dict = {}\n    for modifier in modifiers:\n        group_name = f\"{modifier['group']}_modifiers\"\n        modifier_type = modifier[\"type\"]\n        if group_name not in stage_dict:\n            stage_dict[group_name] = {}\n        stage_dict[group_name][modifier_type] = modifier[\"args\"]\n    return stage_dict\n</code></pre>"},{"location":"reference/llmcompressor/recipe/stage/","title":"llmcompressor.recipe.stage","text":""},{"location":"reference/llmcompressor/recipe/stage/#llmcompressor.recipe.stage.RecipeStage","title":"<code>RecipeStage</code>","text":"<p>               Bases: <code>RecipeBase</code></p> <p>Represents a stage in a recipe.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>Name of the current stage</p> required <code>run_type</code> <p>Whether this is a oneshot or training stage</p> required <code>args</code> <p>Optional recipe args to use for this stage</p> required <code>enabled</code> <p>True to enable the stage, False otherwise</p> required <code>modifiers</code> <p>list of RecipeModifiers that are a part of this stage</p> required <code>exclude_default</code> <p>True to exclude the default modifiers from the stage, False otherwise</p> required Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>class RecipeStage(RecipeBase):\n    \"\"\"\n    Represents a stage in a recipe.\n\n    :param group: Name of the current stage\n    :param run_type: Whether this is a oneshot or training stage\n    :param args: Optional recipe args to use for this stage\n    :param enabled: True to enable the stage, False otherwise\n    :param modifiers: list of RecipeModifiers that are a part of this stage\n    :param exclude_default: True to exclude the default modifiers from the stage,\n        False otherwise\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    group: Optional[str] = None\n    args: Optional[Dict[str, Any]] = None\n    enabled: bool = True\n    modifiers: List[RecipeModifier] = Field(default_factory=list)\n    exclude_default: bool = False\n\n    def create_modifier(self) -&gt; StageModifiers:\n        \"\"\"\n        The StageModifiers instance will contain instantiated\n        specific modifiers for the stage with the group and index set\n\n            | for each recipe_modifier in stage\n            |   | instantiate modifier\n            |   | set group and index of modifier\n            |   | append modifier to StageModifiers.modifiers\n            | return StageModifiers instance\n\n        :return: the StageModifiers for the stage\n        \"\"\"\n\n        stage_modifiers = StageModifiers()\n        for index, modifier in enumerate(self.modifiers):\n            modifier = modifier.create_modifier()\n            modifier.group = self.group\n            modifier.index = index\n            stage_modifiers.modifiers.append(modifier)\n\n        return stage_modifiers\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def remap_modifiers(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        modifiers = RecipeStage.extract_dict_modifiers(values)\n        values[\"modifiers\"] = modifiers\n\n        return values\n\n    @staticmethod\n    def extract_dict_modifiers(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extracts modifiers from a dict of values and returns a list of modifiers\n        with the group set to the key of the modifier in the dict\n\n        &gt;&gt;&gt; values = {\n        ...     \"pruning_modifiers\": {\n        ...         \"ModifierTypeOne\": {\"param\": 1},\n        ...         \"ModifierTypeTwo\": {\"param\": 2},\n        ...     },\n        ... }\n        &gt;&gt;&gt; RecipeStage.extract_dict_modifiers(values) # doctest: +NORMALIZE_WHITESPACE\n        [{'ModifierTypeOne': {'param': 1}, 'group': 'pruning'},\n        {'ModifierTypeTwo': {'param': 2}, 'group': 'pruning'}]\n\n        Accepted formats:\n        - modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n\n        - first_modifiers:\n          - ModifierTypeOne\n            ...\n          - ModifierTypeTwo\n            ...\n        \"\"\"\n\n        modifiers = []\n\n        if \"modifiers\" in values:\n            modifier_values = values.pop(\"modifiers\")\n            if \"stages\" in values:\n                for mod_key, mod_value in values.pop(\"stages\").items():\n                    modifiers.append({mod_key: mod_value, \"group\": \"default\"})\n            else:\n                values[\"default_stage\"] = {\n                    \"default_modifiers\": {mod.type: mod.args for mod in modifier_values}\n                }\n                modifiers.extend(\n                    {mod.type: mod.args, \"group\": \"default\"} for mod in modifier_values\n                )\n\n        for key in [k for k in values if k.endswith(\"_modifiers\")]:\n            group = key.rsplit(\"_modifiers\", 1)[0]\n            modifiers.extend(\n                {mod_key: mod_value, \"group\": group}\n                for mod_key, mod_value in values.pop(key).items()\n            )\n\n        return modifiers\n\n    def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        :return: a dictionary representation of the stage\n        \"\"\"\n        dict_ = super().dict(*args, **kwargs)\n        modifiers = {}\n\n        for modifier in dict_[\"modifiers\"]:\n            group = modifier[\"group\"]\n            del modifier[\"group\"]\n            if group not in modifiers:\n                modifiers[group] = []\n            modifiers[group].append(modifier)\n\n        dict_[\"modifiers\"] = modifiers\n\n        return dict_\n</code></pre>"},{"location":"reference/llmcompressor/recipe/stage/#llmcompressor.recipe.stage.RecipeStage.create_modifier","title":"<code>create_modifier()</code>","text":"<p>The StageModifiers instance will contain instantiated specific modifiers for the stage with the group and index set</p> <pre><code>| for each recipe_modifier in stage\n|   | instantiate modifier\n|   | set group and index of modifier\n|   | append modifier to StageModifiers.modifiers\n| return StageModifiers instance\n</code></pre> <p>Returns:</p> Type Description <code>StageModifiers</code> <p>the StageModifiers for the stage</p> Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>def create_modifier(self) -&gt; StageModifiers:\n    \"\"\"\n    The StageModifiers instance will contain instantiated\n    specific modifiers for the stage with the group and index set\n\n        | for each recipe_modifier in stage\n        |   | instantiate modifier\n        |   | set group and index of modifier\n        |   | append modifier to StageModifiers.modifiers\n        | return StageModifiers instance\n\n    :return: the StageModifiers for the stage\n    \"\"\"\n\n    stage_modifiers = StageModifiers()\n    for index, modifier in enumerate(self.modifiers):\n        modifier = modifier.create_modifier()\n        modifier.group = self.group\n        modifier.index = index\n        stage_modifiers.modifiers.append(modifier)\n\n    return stage_modifiers\n</code></pre>"},{"location":"reference/llmcompressor/recipe/stage/#llmcompressor.recipe.stage.RecipeStage.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>a dictionary representation of the stage</p> Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    :return: a dictionary representation of the stage\n    \"\"\"\n    dict_ = super().dict(*args, **kwargs)\n    modifiers = {}\n\n    for modifier in dict_[\"modifiers\"]:\n        group = modifier[\"group\"]\n        del modifier[\"group\"]\n        if group not in modifiers:\n            modifiers[group] = []\n        modifiers[group].append(modifier)\n\n    dict_[\"modifiers\"] = modifiers\n\n    return dict_\n</code></pre>"},{"location":"reference/llmcompressor/recipe/stage/#llmcompressor.recipe.stage.RecipeStage.extract_dict_modifiers","title":"<code>extract_dict_modifiers(values)</code>  <code>staticmethod</code>","text":"<p>Extracts modifiers from a dict of values and returns a list of modifiers with the group set to the key of the modifier in the dict</p> <p>values = { ...     \"pruning_modifiers\": { ...         \"ModifierTypeOne\": {\"param\": 1}, ...         \"ModifierTypeTwo\": {\"param\": 2}, ...     }, ... } RecipeStage.extract_dict_modifiers(values) # doctest: +NORMALIZE_WHITESPACE [{'ModifierTypeOne': {'param': 1}, 'group': 'pruning'}, {'ModifierTypeTwo': {'param': 2}, 'group': 'pruning'}]</p> <p>Accepted formats: - modifiers:   - ModifierTypeOne     ...   - ModifierTypeTwo     ...</p> <ul> <li>first_modifiers:</li> <li>ModifierTypeOne     ...</li> <li>ModifierTypeTwo     ...</li> </ul> Source code in <code>src/llmcompressor/recipe/stage.py</code> <pre><code>@staticmethod\ndef extract_dict_modifiers(values: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Extracts modifiers from a dict of values and returns a list of modifiers\n    with the group set to the key of the modifier in the dict\n\n    &gt;&gt;&gt; values = {\n    ...     \"pruning_modifiers\": {\n    ...         \"ModifierTypeOne\": {\"param\": 1},\n    ...         \"ModifierTypeTwo\": {\"param\": 2},\n    ...     },\n    ... }\n    &gt;&gt;&gt; RecipeStage.extract_dict_modifiers(values) # doctest: +NORMALIZE_WHITESPACE\n    [{'ModifierTypeOne': {'param': 1}, 'group': 'pruning'},\n    {'ModifierTypeTwo': {'param': 2}, 'group': 'pruning'}]\n\n    Accepted formats:\n    - modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n\n    - first_modifiers:\n      - ModifierTypeOne\n        ...\n      - ModifierTypeTwo\n        ...\n    \"\"\"\n\n    modifiers = []\n\n    if \"modifiers\" in values:\n        modifier_values = values.pop(\"modifiers\")\n        if \"stages\" in values:\n            for mod_key, mod_value in values.pop(\"stages\").items():\n                modifiers.append({mod_key: mod_value, \"group\": \"default\"})\n        else:\n            values[\"default_stage\"] = {\n                \"default_modifiers\": {mod.type: mod.args for mod in modifier_values}\n            }\n            modifiers.extend(\n                {mod.type: mod.args, \"group\": \"default\"} for mod in modifier_values\n            )\n\n    for key in [k for k in values if k.endswith(\"_modifiers\")]:\n        group = key.rsplit(\"_modifiers\", 1)[0]\n        modifiers.extend(\n            {mod_key: mod_value, \"group\": group}\n            for mod_key, mod_value in values.pop(key).items()\n        )\n\n    return modifiers\n</code></pre>"},{"location":"reference/llmcompressor/transformers/","title":"llmcompressor.transformers","text":"<p>Tools for integrating LLM Compressor with transformers training flows</p>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn","title":"<code>SessionManagerMixIn</code>","text":"<p>Mix-In class to extend the Hugging Face Trainer class to support LLM Compressor recipes for one-shot and finetuning flows.</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>str</code> <p>path to recipe file to apply during training</p> required <code>recipe_args</code> <code>Optional[Union[Dict[str, Any], str]]</code> <p>additional kwargs to use for evaluating recipe</p> <code>None</code> <code>dataset_args</code> <code>Optional[DatasetArguments]</code> <p>kwargs for configuring dataset loading</p> <code>None</code> <code>teacher</code> <code>Optional[Union[Module, str]]</code> <p>optional teacher model to use for distillation</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>class SessionManagerMixIn:\n    \"\"\"\n    Mix-In class to extend the Hugging Face Trainer class to support LLM Compressor\n    recipes for one-shot and finetuning flows.\n\n    :param recipe: path to recipe file to apply during training\n    :param recipe_args: additional kwargs to use for evaluating recipe\n    :param dataset_args: kwargs for configuring dataset loading\n    :param teacher: optional teacher model to use for distillation\n    \"\"\"\n\n    def __init__(\n        self,\n        recipe: str,\n        model_args: \"ModelArguments\",\n        dataset_args: Optional[\"DatasetArguments\"] = None,\n        teacher: Optional[Union[Module, str]] = None,\n        recipe_args: Optional[Union[Dict[str, Any], str]] = None,\n        **kwargs,\n    ):\n        self.recipe = recipe\n        self.recipe_args = recipe_args\n        self.model_args = model_args\n        self.teacher = teacher\n\n        # parse training and metadata args\n        training_args = kwargs.get(\"args\")\n\n        self.metadata = None\n        if training_args is not None:\n            # trl_sft_trainer pathway. Both training_args and dataset_args\n            # have `max_seq_length` which causes collision error. This is the\n            # only shared parameter, where training arg is `TRLSFTConfig` that\n            # inherits HuggingFace's `TrainingArguments`\n            training_args_dict = training_args.to_dict()\n            if \"max_seq_length\" in training_args_dict:\n                training_args_dict[\"training_args_max_seq_length\"] = (\n                    training_args_dict.pop(\"max_seq_length\")\n                )\n                logger.warning(\n                    \"Detected `max_seq_length` in both dataset_args \",\n                    \"and training_args. This is expected for TRL in distillation. \",\n                    \"Updating metadata to `training_args_max_seq_length`\",\n                )\n\n            self.metadata = self._extract_metadata(\n                metadata_args=METADATA_ARGS,\n                training_args_dict=training_args_dict,\n                dataset_args_dict=asdict(dataset_args) if dataset_args else {},\n            )\n\n        # setup metrics and session\n        self.logger_manager = LoggerManager(log_python=False)\n        create_session()\n\n        # call Trainer initialization\n        super().__init__(**kwargs)\n        self.accelerator.wait_for_everyone()\n\n        # setup callbacks and loss\n        self.optim_callbacks = TrainingLoopCallbacks(self)\n        self.callback_handler.add_callback(self.optim_callbacks)\n        self.callback_disable_fp16 = DisableHalfPrecisionCallback(self)\n        self.callback_handler.add_callback(self.callback_disable_fp16)\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        model_signature = inspect.signature(self.model.forward)\n        self._signature_columns = list(model_signature.parameters.keys())\n\n        if self.teacher is not None and teacher not in (\"disable\", \"self\"):\n            teacher_signature = inspect.signature(self.teacher.forward)\n            self._teacher_signature_columns = list(teacher_signature.parameters.keys())\n        else:\n            self._teacher_signature_columns = None\n\n        if self.is_fsdp_enabled:\n            self._prepare_model_for_fsdp()\n\n        if dataset_args is not None:\n            self.min_tokens_per_module = dataset_args.min_tokens_per_module\n\n    def initialize_session(\n        self,\n        epoch: float,\n        checkpoint: Optional[str] = None,\n        stage: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the CompressionSession from the specified epoch, evaluates the recipe\n        and initialized the modifiers for the training session\n\n        :param epoch: Epoch to initialize session from, usually 0 unless loading\n        from a checkpoint\n        :param checkpoint: Optional checkpoint to initialize from to continue training\n        :param stage: Optional stage of recipe to run, or None to run all stages\n        \"\"\"\n        session = active_session()\n        if session.lifecycle.initialized_ or session.lifecycle.finalized:\n            return False\n\n        train_data = self.get_train_dataloader()\n\n        self.accelerator.wait_for_everyone()\n        with summon_full_params_context(self.model, offload_to_cpu=True):\n            active_session().initialize(\n                recipe=self.recipe,\n                recipe_stage=stage,\n                recipe_args=self.recipe_args,\n                model=self.model,\n                teacher_model=self.teacher,  # TODO: what about for self/disable?\n                train_data=train_data,\n                start=epoch,\n                copy_data=False,\n                attach_optim_callbacks=True,\n                fsdp_active=self.is_fsdp_enabled,\n                metadata=self.metadata,\n            )\n\n        self.accelerator.wait_for_everyone()\n        model = get_session_model()\n        self.model_wrapped = self.model = model\n\n        if self.recipe is None:\n            logger.warning(\n                \"No training recipe was provided, finetuning will be run \"\n                \"without event callbacks to LLM Compressor. To supply a recipe \"\n                \"pass a yaml file or string to the `recipe` argument.\"\n            )\n\n        torch.cuda.empty_cache()\n\n    def finalize_session(self):\n        \"\"\"\n        Wrap up training by finalizing all modifiers initialized in the current session\n        \"\"\"\n        session = active_session()\n        if not session.lifecycle.initialized_ or session.lifecycle.finalized:\n            return False\n\n        with summon_full_params_context(self.model, offload_to_cpu=True):\n            # in order to update each layer we need to gathers all its parameters\n            active_session().finalize()\n        logger.info(\"Finalized LLM Compressor session\")\n        model = get_session_model()\n        self.model = model\n        torch.cuda.empty_cache()\n\n    def create_optimizer(self):\n        \"\"\"\n        Override the optimizer to apply and update the recipe while training.\n        create_optimizer must exist in the parent class and should set\n        self.optimizer to the optimizer state and optionally set self.scaler\n        if using amp.\n        \"\"\"\n\n        self._check_super_defined(\"create_optimizer\")\n        super().create_optimizer()\n\n        # n_gpu handled internally by dataloader\n        total_batch_size = (\n            self.args.per_device_train_batch_size\n            * self.args.gradient_accumulation_steps\n        )\n\n        if isinstance(self.train_dataset, IterableDataset):\n            logger.warning(\n                \"Training is being run with a streamed dataset, \"\n                \"steps_per_epoch cannot be determined and will default to \"\n                \"1. LLM Compressor modifiers utilizing this statistic may not \"\n                \"behave as expected. \"\n            )\n            self.total_steps_per_epoch = 1\n        else:\n            self.total_steps_per_epoch = math.ceil(\n                len(self.train_dataset) / total_batch_size\n            )\n\n        active_session().initialize(\n            optimizer=self.optimizer, steps_per_epoch=self.total_steps_per_epoch\n        )\n\n        return self.optimizer\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: torch.optim.Optimizer = None\n    ):\n        \"\"\"\n        Create an LR scheduler to work with the applied recipes. This is a placeholder\n        that just calls the super method, but would be expanded upon if we ever\n        implement a LearningRateModifier.\n\n        :param num_training_steps: the total number of training steps\n        :param optimizer: pre-initialized optimizer\n        \"\"\"\n\n        # TODO: we don't currently have a LR scheduler in the new modifier framework\n        self._check_super_defined(\"create_scheduler\")\n        return super().create_scheduler(\n            num_training_steps=num_training_steps, optimizer=optimizer\n        )\n\n    def training_step(\n        self,\n        model: torch.nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        num_items_in_batch: Optional[int] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Overrides the Trainer's training step to trigger the batch_start callback to\n        the modifiers, then calls the parent function.\n\n        :param model: the model to compute the loss for\n        :param inputs: the inputs to pass through the model for calculating the loss\n        :return: output of the model\n        \"\"\"\n        self._check_super_defined(\"training_step\")\n\n        callbacks.batch_start(batch_data=inputs, global_step=self.state.epoch)\n        model_outputs = super().training_step(\n            model=model, inputs=inputs, num_items_in_batch=num_items_in_batch\n        )\n\n        return model_outputs\n\n    def compute_loss(\n        self,\n        model: Module,\n        inputs: Dict[str, Any],\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, Any]]:\n        \"\"\"\n        Override for the compute_loss to factor trigger callbacks and filter columns\n\n        :param model: the model to compute the loss for\n        :param inputs: the inputs to pass through the model for calculating the loss\n        :param return_outputs: True to return the outputs with the loss,\n            False otherwise\n        :return: the resulting loss if not return_outputs, otherwise a tuple\n            containing the loss and the model's outputs\n        \"\"\"\n        self._check_super_defined(\"compute_loss\")\n\n        # TODO: do we need these model signature columns?\n        inputs = {k: inputs[k] for k in inputs if k in self._signature_columns}\n        loss = super().compute_loss(\n            model=model,\n            inputs=inputs,\n            return_outputs=return_outputs,\n            num_items_in_batch=num_items_in_batch,\n        )\n\n        # take the mean across multiple GPUs\n        # this is done outside the compute_loss function in the parent, replicating it\n        # here for LLM Compressor logging and distillation\n        loss = loss.mean()\n\n        # Log step-wise loss and perplexity, for llama-recipes comparison\n        # we want this before distillation loss so perplexity isn't thrown off\n        do_log = self.state.global_step % self.args.logging_steps == 0\n        if do_log:\n            log = {}\n            log[\"step_loss\"] = loss.item()\n            log[\"perplexity\"] = torch.exp(loss).item()\n\n        if active_session().lifecycle.initialized_:\n            state = callbacks.loss_calculated(loss=loss)\n            if state and state.loss is not None:\n                loss = state.loss\n                if do_log:\n                    log[\"distill_step_loss\"] = loss.item() - log[\"step_loss\"]\n            callbacks.optim_pre_step()\n\n        if do_log:\n            self.log(log)\n\n        return loss\n\n    def train(self, *args, stage: Optional[str] = None, **kwargs):\n        \"\"\"\n        Run a sparsification training cycle. Runs initialization for the sparse session\n        before calling super().train() and finalization of the session after.\n\n        Logs sparsification details for the trained model.\n\n        :param args: positional args to pass to super().train()\n        :param stage: Optional stage of recipe to run, or None to run all stages\n        :param kwargs: keyword args to pass to super().train()\n        :return: the output from super.train()\n        \"\"\"\n\n        # lifecycle\n        checkpoint, epoch = self._calculate_checkpoint_info(kwargs)\n        self.initialize_session(epoch=epoch, checkpoint=checkpoint, stage=stage)\n\n        # do not save checkpoints as compressed\n        original_save_compressed = self.model_args.save_compressed\n        self.model_args.save_compressed = False\n\n        # train with accelerator\n        self.accelerator.wait_for_everyone()\n        output = super().train(*args, **kwargs)\n        self.accelerator.wait_for_everyone()\n\n        # restore original setting for saving final model\n        self.model_args.save_compressed = original_save_compressed\n\n        # lifecycle\n        self.finalize_session()\n        self.accelerator.wait_for_everyone()\n\n        # log model sparsity\n        self.maybe_log_model_sparsification()\n        self.accelerator.wait_for_everyone()\n\n        return output\n\n    # TODO: support all save args, not just skip_sparsity_compression_stats\n    def save_model(\n        self,\n        output_dir: str,\n        _internal_call: bool = False,\n        skip_sparsity_compression_stats: Optional[bool] = False,\n    ):\n        \"\"\"\n        Override of the save_model function and expects it to exist in the parent.\n        Calls into super() to save the model and additionally saves any recipes\n        that were used with the model within the model folder.\n\n        :param output_dir: the path to save the recipes into\n        :param _internal_call: True if this is an internal call from\n            the trainer in super(). Called from\n            self.save_model(output_dir, _internal_call=True)\n            in transformers/trainer/Trainer::_save_checkpoint\n\n        \"\"\"\n        if active_session() is None:\n            logger.warning(\n                \"No active session found, skipping saving of recipes and model.\"\n            )\n            return\n\n        # knowledge distillation requires making wrappers transparent during\n        if isinstance(self.model, KDModelWrapper):\n            self.model.prepare_for_save()  # TODO: move to finalize\n\n        # save checkpoint\n        self.save_state()\n        if self.accelerator.is_main_process:\n            processor = getattr(self, \"processing_class\", self.tokenizer)\n            # TODO: need to port over all saving parameters so that all\n            # checkpoints are saved in the same way\n            save_checkpoint(\n                output_dir,\n                model=self.model,\n                processor=processor,\n                save_safetensors=self.args.save_safetensors,\n                save_compressed=self.model_args.save_compressed,\n                skip_sparsity_compression_stats=skip_sparsity_compression_stats,\n            )\n        self.accelerator.wait_for_everyone()\n\n        if isinstance(self.model, KDModelWrapper):\n            self.model.finish_save()\n\n    def maybe_log_model_sparsification(self):\n        \"\"\"\n        Log info on model sparsity and quantization if possible. Only print logs on the\n        main process, and avoid logging for quantized FSDP models\n        \"\"\"\n        with summon_full_params_context(self.model, offload_to_cpu=True):\n            # offload to avoid OOM errors\n            if not self.accelerator.is_main_process:\n                # only calculate stats rank0 GPU\n                return\n            if self.is_fsdp_enabled and qat_active(self.model):\n                # due to state dict changes we can't log sparsity info with quantized\n                # models in FSDP\n                return\n\n            self.log_model_sparsification()\n\n    def log_model_sparsification(self):\n        \"\"\"\n        Log the current model sparsification info including pruned and quantized states\n        \"\"\"\n        sparsification_info = ModuleSparsificationInfo(self.model)\n\n        logger.info(\n            f\"Sparsification info for {type(self.model).__name__}: \"\n            f\"{sparsification_info.params_total} total params. \"\n        )\n        sparsity_percent_formatted = \"{:.2f}\".format(\n            sparsification_info.params_sparse_percent\n        )\n        logger.info(\n            f\"There are {sparsification_info.params_total} prunable \"\n            f\"params which have {sparsity_percent_formatted}% \"\n            \"avg sparsity.\"\n        )\n\n        quant_percent_formatted = \"{:.2f}\".format(\n            sparsification_info.params_quantized_percent\n        )\n        logger.info(\n            f\"There are {sparsification_info.params_total} quantizable \"\n            f\"params, with a quantization percentage of \"\n            f\"{quant_percent_formatted}%.\"\n        )\n\n    def _prepare_model_for_fsdp(self):\n        \"\"\"\n        Sets up FSDP ahead of time so we can run one-shot in FSDP mode\n        \"\"\"\n        self.model.to(\"cpu\")\n        self.model = self.accelerator.prepare(self.model)\n        self.accelerator.wait_for_everyone()\n\n        if self.teacher is not None:\n            self.teacher.to(\"cpu\")\n            for n, p in self.teacher.named_parameters():\n                p.requires_grad = False\n            self.teacher = self.accelerator.prepare(self.teacher)\n            self.teacher.eval()\n            self.accelerator.wait_for_everyone()\n\n    def _extract_metadata(\n        self,\n        metadata_args: List[str],\n        training_args_dict: Dict[str, Any],\n        dataset_args_dict: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        metadata = {}\n        if not training_args_dict.keys().isdisjoint(dataset_args_dict.keys()):\n            raise ValueError(\n                \"Found common keys in `training_args` and `data args`. \"\n                \"This is prohibitive and may lead to undesired behavior.\"\n            )\n\n        args_dict = {**training_args_dict, **dataset_args_dict}\n\n        for arg in metadata_args:\n            if arg not in args_dict.keys():\n                logger.warning(\n                    f\"Required metadata argument {arg} was not found \"\n                    f\"in the training arguments. Setting {arg} to None.\"\n                )\n                metadata[arg] = None\n            else:\n                metadata[arg] = args_dict[arg]\n\n        return metadata\n\n    def _check_super_defined(self, func: str):\n        if not hasattr(super(), func):\n            raise NotImplementedError(\n                f\"The super class for SessionManagerMixIn must define a {func} function\"\n            )\n\n    def _calculate_checkpoint_info(self, kwargs) -&gt; Tuple[Optional[str], float]:\n        \"\"\"\n        If resuming from checkpoint is set, get checkpoint and epoch to resume from\n        \"\"\"\n        checkpoint = None\n        epoch = 0.0\n\n        if not kwargs or \"resume_from_checkpoint\" not in kwargs:\n            logger.warning(\n                \"resume_from_checkpoint not passed into LLM Compressor Trainer.train. \"\n                \"This will cause issues with restoring recipes when \"\n                \"running from a checkpoint.\"\n            )\n        elif kwargs[\"resume_from_checkpoint\"]:\n            if (\n                isinstance(kwargs[\"resume_from_checkpoint\"], bool)\n                and kwargs[\"resume_from_checkpoint\"]\n            ):\n                checkpoint = get_last_checkpoint(self.args.output_dir)\n            else:\n                checkpoint = kwargs[\"resume_from_checkpoint\"]\n            epoch = TrainerState.load_from_json(\n                os.path.join(checkpoint, TRAINER_STATE_NAME)\n            ).epoch\n\n        return checkpoint, epoch\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.compute_loss","title":"<code>compute_loss(model, inputs, return_outputs=False, num_items_in_batch=None)</code>","text":"<p>Override for the compute_loss to factor trigger callbacks and filter columns</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model to compute the loss for</p> required <code>inputs</code> <code>Dict[str, Any]</code> <p>the inputs to pass through the model for calculating the loss</p> required <code>return_outputs</code> <code>bool</code> <p>True to return the outputs with the loss, False otherwise</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, Any]]</code> <p>the resulting loss if not return_outputs, otherwise a tuple containing the loss and the model's outputs</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def compute_loss(\n    self,\n    model: Module,\n    inputs: Dict[str, Any],\n    return_outputs: bool = False,\n    num_items_in_batch: Optional[int] = None,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, Any]]:\n    \"\"\"\n    Override for the compute_loss to factor trigger callbacks and filter columns\n\n    :param model: the model to compute the loss for\n    :param inputs: the inputs to pass through the model for calculating the loss\n    :param return_outputs: True to return the outputs with the loss,\n        False otherwise\n    :return: the resulting loss if not return_outputs, otherwise a tuple\n        containing the loss and the model's outputs\n    \"\"\"\n    self._check_super_defined(\"compute_loss\")\n\n    # TODO: do we need these model signature columns?\n    inputs = {k: inputs[k] for k in inputs if k in self._signature_columns}\n    loss = super().compute_loss(\n        model=model,\n        inputs=inputs,\n        return_outputs=return_outputs,\n        num_items_in_batch=num_items_in_batch,\n    )\n\n    # take the mean across multiple GPUs\n    # this is done outside the compute_loss function in the parent, replicating it\n    # here for LLM Compressor logging and distillation\n    loss = loss.mean()\n\n    # Log step-wise loss and perplexity, for llama-recipes comparison\n    # we want this before distillation loss so perplexity isn't thrown off\n    do_log = self.state.global_step % self.args.logging_steps == 0\n    if do_log:\n        log = {}\n        log[\"step_loss\"] = loss.item()\n        log[\"perplexity\"] = torch.exp(loss).item()\n\n    if active_session().lifecycle.initialized_:\n        state = callbacks.loss_calculated(loss=loss)\n        if state and state.loss is not None:\n            loss = state.loss\n            if do_log:\n                log[\"distill_step_loss\"] = loss.item() - log[\"step_loss\"]\n        callbacks.optim_pre_step()\n\n    if do_log:\n        self.log(log)\n\n    return loss\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.create_optimizer","title":"<code>create_optimizer()</code>","text":"<p>Override the optimizer to apply and update the recipe while training. create_optimizer must exist in the parent class and should set self.optimizer to the optimizer state and optionally set self.scaler if using amp.</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def create_optimizer(self):\n    \"\"\"\n    Override the optimizer to apply and update the recipe while training.\n    create_optimizer must exist in the parent class and should set\n    self.optimizer to the optimizer state and optionally set self.scaler\n    if using amp.\n    \"\"\"\n\n    self._check_super_defined(\"create_optimizer\")\n    super().create_optimizer()\n\n    # n_gpu handled internally by dataloader\n    total_batch_size = (\n        self.args.per_device_train_batch_size\n        * self.args.gradient_accumulation_steps\n    )\n\n    if isinstance(self.train_dataset, IterableDataset):\n        logger.warning(\n            \"Training is being run with a streamed dataset, \"\n            \"steps_per_epoch cannot be determined and will default to \"\n            \"1. LLM Compressor modifiers utilizing this statistic may not \"\n            \"behave as expected. \"\n        )\n        self.total_steps_per_epoch = 1\n    else:\n        self.total_steps_per_epoch = math.ceil(\n            len(self.train_dataset) / total_batch_size\n        )\n\n    active_session().initialize(\n        optimizer=self.optimizer, steps_per_epoch=self.total_steps_per_epoch\n    )\n\n    return self.optimizer\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.create_scheduler","title":"<code>create_scheduler(num_training_steps, optimizer=None)</code>","text":"<p>Create an LR scheduler to work with the applied recipes. This is a placeholder that just calls the super method, but would be expanded upon if we ever implement a LearningRateModifier.</p> <p>Parameters:</p> Name Type Description Default <code>num_training_steps</code> <code>int</code> <p>the total number of training steps</p> required <code>optimizer</code> <code>Optimizer</code> <p>pre-initialized optimizer</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def create_scheduler(\n    self, num_training_steps: int, optimizer: torch.optim.Optimizer = None\n):\n    \"\"\"\n    Create an LR scheduler to work with the applied recipes. This is a placeholder\n    that just calls the super method, but would be expanded upon if we ever\n    implement a LearningRateModifier.\n\n    :param num_training_steps: the total number of training steps\n    :param optimizer: pre-initialized optimizer\n    \"\"\"\n\n    # TODO: we don't currently have a LR scheduler in the new modifier framework\n    self._check_super_defined(\"create_scheduler\")\n    return super().create_scheduler(\n        num_training_steps=num_training_steps, optimizer=optimizer\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.finalize_session","title":"<code>finalize_session()</code>","text":"<p>Wrap up training by finalizing all modifiers initialized in the current session</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def finalize_session(self):\n    \"\"\"\n    Wrap up training by finalizing all modifiers initialized in the current session\n    \"\"\"\n    session = active_session()\n    if not session.lifecycle.initialized_ or session.lifecycle.finalized:\n        return False\n\n    with summon_full_params_context(self.model, offload_to_cpu=True):\n        # in order to update each layer we need to gathers all its parameters\n        active_session().finalize()\n    logger.info(\"Finalized LLM Compressor session\")\n    model = get_session_model()\n    self.model = model\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.initialize_session","title":"<code>initialize_session(epoch, checkpoint=None, stage=None)</code>","text":"<p>Initialize the CompressionSession from the specified epoch, evaluates the recipe and initialized the modifiers for the training session</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>float</code> <p>Epoch to initialize session from, usually 0 unless loading from a checkpoint</p> required <code>checkpoint</code> <code>Optional[str]</code> <p>Optional checkpoint to initialize from to continue training</p> <code>None</code> <code>stage</code> <code>Optional[str]</code> <p>Optional stage of recipe to run, or None to run all stages</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def initialize_session(\n    self,\n    epoch: float,\n    checkpoint: Optional[str] = None,\n    stage: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the CompressionSession from the specified epoch, evaluates the recipe\n    and initialized the modifiers for the training session\n\n    :param epoch: Epoch to initialize session from, usually 0 unless loading\n    from a checkpoint\n    :param checkpoint: Optional checkpoint to initialize from to continue training\n    :param stage: Optional stage of recipe to run, or None to run all stages\n    \"\"\"\n    session = active_session()\n    if session.lifecycle.initialized_ or session.lifecycle.finalized:\n        return False\n\n    train_data = self.get_train_dataloader()\n\n    self.accelerator.wait_for_everyone()\n    with summon_full_params_context(self.model, offload_to_cpu=True):\n        active_session().initialize(\n            recipe=self.recipe,\n            recipe_stage=stage,\n            recipe_args=self.recipe_args,\n            model=self.model,\n            teacher_model=self.teacher,  # TODO: what about for self/disable?\n            train_data=train_data,\n            start=epoch,\n            copy_data=False,\n            attach_optim_callbacks=True,\n            fsdp_active=self.is_fsdp_enabled,\n            metadata=self.metadata,\n        )\n\n    self.accelerator.wait_for_everyone()\n    model = get_session_model()\n    self.model_wrapped = self.model = model\n\n    if self.recipe is None:\n        logger.warning(\n            \"No training recipe was provided, finetuning will be run \"\n            \"without event callbacks to LLM Compressor. To supply a recipe \"\n            \"pass a yaml file or string to the `recipe` argument.\"\n        )\n\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.log_model_sparsification","title":"<code>log_model_sparsification()</code>","text":"<p>Log the current model sparsification info including pruned and quantized states</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def log_model_sparsification(self):\n    \"\"\"\n    Log the current model sparsification info including pruned and quantized states\n    \"\"\"\n    sparsification_info = ModuleSparsificationInfo(self.model)\n\n    logger.info(\n        f\"Sparsification info for {type(self.model).__name__}: \"\n        f\"{sparsification_info.params_total} total params. \"\n    )\n    sparsity_percent_formatted = \"{:.2f}\".format(\n        sparsification_info.params_sparse_percent\n    )\n    logger.info(\n        f\"There are {sparsification_info.params_total} prunable \"\n        f\"params which have {sparsity_percent_formatted}% \"\n        \"avg sparsity.\"\n    )\n\n    quant_percent_formatted = \"{:.2f}\".format(\n        sparsification_info.params_quantized_percent\n    )\n    logger.info(\n        f\"There are {sparsification_info.params_total} quantizable \"\n        f\"params, with a quantization percentage of \"\n        f\"{quant_percent_formatted}%.\"\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.maybe_log_model_sparsification","title":"<code>maybe_log_model_sparsification()</code>","text":"<p>Log info on model sparsity and quantization if possible. Only print logs on the main process, and avoid logging for quantized FSDP models</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def maybe_log_model_sparsification(self):\n    \"\"\"\n    Log info on model sparsity and quantization if possible. Only print logs on the\n    main process, and avoid logging for quantized FSDP models\n    \"\"\"\n    with summon_full_params_context(self.model, offload_to_cpu=True):\n        # offload to avoid OOM errors\n        if not self.accelerator.is_main_process:\n            # only calculate stats rank0 GPU\n            return\n        if self.is_fsdp_enabled and qat_active(self.model):\n            # due to state dict changes we can't log sparsity info with quantized\n            # models in FSDP\n            return\n\n        self.log_model_sparsification()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.save_model","title":"<code>save_model(output_dir, _internal_call=False, skip_sparsity_compression_stats=False)</code>","text":"<p>Override of the save_model function and expects it to exist in the parent. Calls into super() to save the model and additionally saves any recipes that were used with the model within the model folder.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>the path to save the recipes into</p> required <code>_internal_call</code> <code>bool</code> <p>True if this is an internal call from the trainer in super(). Called from self.save_model(output_dir, _internal_call=True) in transformers/trainer/Trainer::_save_checkpoint</p> <code>False</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def save_model(\n    self,\n    output_dir: str,\n    _internal_call: bool = False,\n    skip_sparsity_compression_stats: Optional[bool] = False,\n):\n    \"\"\"\n    Override of the save_model function and expects it to exist in the parent.\n    Calls into super() to save the model and additionally saves any recipes\n    that were used with the model within the model folder.\n\n    :param output_dir: the path to save the recipes into\n    :param _internal_call: True if this is an internal call from\n        the trainer in super(). Called from\n        self.save_model(output_dir, _internal_call=True)\n        in transformers/trainer/Trainer::_save_checkpoint\n\n    \"\"\"\n    if active_session() is None:\n        logger.warning(\n            \"No active session found, skipping saving of recipes and model.\"\n        )\n        return\n\n    # knowledge distillation requires making wrappers transparent during\n    if isinstance(self.model, KDModelWrapper):\n        self.model.prepare_for_save()  # TODO: move to finalize\n\n    # save checkpoint\n    self.save_state()\n    if self.accelerator.is_main_process:\n        processor = getattr(self, \"processing_class\", self.tokenizer)\n        # TODO: need to port over all saving parameters so that all\n        # checkpoints are saved in the same way\n        save_checkpoint(\n            output_dir,\n            model=self.model,\n            processor=processor,\n            save_safetensors=self.args.save_safetensors,\n            save_compressed=self.model_args.save_compressed,\n            skip_sparsity_compression_stats=skip_sparsity_compression_stats,\n        )\n    self.accelerator.wait_for_everyone()\n\n    if isinstance(self.model, KDModelWrapper):\n        self.model.finish_save()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.train","title":"<code>train(*args, stage=None, **kwargs)</code>","text":"<p>Run a sparsification training cycle. Runs initialization for the sparse session before calling super().train() and finalization of the session after.</p> <p>Logs sparsification details for the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional args to pass to super().train()</p> <code>()</code> <code>stage</code> <code>Optional[str]</code> <p>Optional stage of recipe to run, or None to run all stages</p> <code>None</code> <code>kwargs</code> <p>keyword args to pass to super().train()</p> <code>{}</code> <p>Returns:</p> Type Description <p>the output from super.train()</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def train(self, *args, stage: Optional[str] = None, **kwargs):\n    \"\"\"\n    Run a sparsification training cycle. Runs initialization for the sparse session\n    before calling super().train() and finalization of the session after.\n\n    Logs sparsification details for the trained model.\n\n    :param args: positional args to pass to super().train()\n    :param stage: Optional stage of recipe to run, or None to run all stages\n    :param kwargs: keyword args to pass to super().train()\n    :return: the output from super.train()\n    \"\"\"\n\n    # lifecycle\n    checkpoint, epoch = self._calculate_checkpoint_info(kwargs)\n    self.initialize_session(epoch=epoch, checkpoint=checkpoint, stage=stage)\n\n    # do not save checkpoints as compressed\n    original_save_compressed = self.model_args.save_compressed\n    self.model_args.save_compressed = False\n\n    # train with accelerator\n    self.accelerator.wait_for_everyone()\n    output = super().train(*args, **kwargs)\n    self.accelerator.wait_for_everyone()\n\n    # restore original setting for saving final model\n    self.model_args.save_compressed = original_save_compressed\n\n    # lifecycle\n    self.finalize_session()\n    self.accelerator.wait_for_everyone()\n\n    # log model sparsity\n    self.maybe_log_model_sparsification()\n    self.accelerator.wait_for_everyone()\n\n    return output\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.SessionManagerMixIn.training_step","title":"<code>training_step(model, inputs, num_items_in_batch=None)</code>","text":"<p>Overrides the Trainer's training step to trigger the batch_start callback to the modifiers, then calls the parent function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model to compute the loss for</p> required <code>inputs</code> <code>Dict[str, Union[Tensor, Any]]</code> <p>the inputs to pass through the model for calculating the loss</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output of the model</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def training_step(\n    self,\n    model: torch.nn.Module,\n    inputs: Dict[str, Union[torch.Tensor, Any]],\n    num_items_in_batch: Optional[int] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Overrides the Trainer's training step to trigger the batch_start callback to\n    the modifiers, then calls the parent function.\n\n    :param model: the model to compute the loss for\n    :param inputs: the inputs to pass through the model for calculating the loss\n    :return: output of the model\n    \"\"\"\n    self._check_super_defined(\"training_step\")\n\n    callbacks.batch_start(batch_data=inputs, global_step=self.state.epoch)\n    model_outputs = super().training_step(\n        model=model, inputs=inputs, num_items_in_batch=num_items_in_batch\n    )\n\n    return model_outputs\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.TextGenerationDataset","title":"<code>TextGenerationDataset</code>","text":"<p>               Bases: <code>RegistryMixin</code></p> <p>Base class for text datasets. Applies the following transformations to a dataset in order to prepare the dataset to be loaded by a dataloader</p> <ol> <li>Load dataset from huggingface or local cache</li> <li>Preprocess dataset according to preprocess function or chat/dataset template</li> <li>Tokenize dataset using model tokenizer/processor</li> <li>Apply post processing such as grouping text and/or adding labels for finetuning</li> </ol> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/base.py</code> <pre><code>class TextGenerationDataset(RegistryMixin):\n    \"\"\"\n    Base class for text datasets. Applies the following transformations to a dataset\n    in order to prepare the dataset to be loaded by a dataloader\n\n    1. Load dataset from huggingface or local cache\n    2. Preprocess dataset according to preprocess function or chat/dataset template\n    3. Tokenize dataset using model tokenizer/processor\n    4. Apply post processing such as grouping text and/or adding labels for finetuning\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    # used to mask out the prompt so prompt tokens do not contribute to training loss\n    PROMPT_KEY = \"prompt\"\n\n    def __init__(\n        self,\n        dataset_args: DatasetArguments,\n        split: str,\n        processor: Processor,\n    ):\n        self.dataset_args = dataset_args\n        self.split = split\n        self.processor = processor\n\n        # get tokenizer\n        self.tokenizer = getattr(self.processor, \"tokenizer\", self.processor)\n\n        if self.tokenizer is not None:\n            # fill in pad token\n            if not self.tokenizer.pad_token:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n\n            # configure sequence length\n            max_seq_length = dataset_args.max_seq_length\n            if dataset_args.max_seq_length &gt; self.tokenizer.model_max_length:\n                logger.warning(\n                    f\"The max_seq_length passed ({max_seq_length}) is larger than \"\n                    f\"maximum length for model ({self.tokenizer.model_max_length}). \"\n                    f\"Using max_seq_length={self.tokenizer.model_max_length}.\"\n                )\n            self.max_seq_length = min(\n                dataset_args.max_seq_length, self.tokenizer.model_max_length\n            )\n\n            # configure padding\n            self.padding = (\n                False\n                if self.dataset_args.concatenate_data\n                else \"max_length\"\n                if self.dataset_args.pad_to_max_length\n                else False\n            )\n\n        else:\n            self.max_seq_length = None\n            self.padding = False\n\n    def __call__(self, add_labels: bool = True) -&gt; DatasetType:\n        dataset = self.dataset_args.dataset\n\n        if isinstance(dataset, str):\n            # load dataset: load from huggingface or disk\n            dataset = self.load_dataset()\n        logger.debug(f\"Raw dataset: {get_columns(dataset)}\")\n\n        if self.preprocess is not None:\n            # preprocess: apply template or preprocessing function\n            dataset = self.map(\n                dataset,\n                self.preprocess,\n                batched=False,\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Preprocessing\",\n            )\n            logger.debug(f\"Dataset after preprocessing: {get_columns(dataset)}\")\n\n        # rename and remove columns match processor kwargs\n        dataset = self.rename_columns(dataset)\n        logger.debug(f\"Dataset after column renaming: {get_columns(dataset)}\")\n\n        # use processor.model_input_names to determine if the ds is already tokenized\n        model_input_names = getattr(self.processor, \"model_input_names\", [\"input_ids\"])\n        if not any(col_name in model_input_names for col_name in get_columns(dataset)):\n            # tokenize/ process\n            dataset = self.filter_tokenizer_args(dataset)\n            logger.debug(f\"Tokenizer args after filtering: {get_columns(dataset)}\")\n            dataset = self.map(\n                dataset,\n                self.tokenize,\n                batched=False,  # batching is not well supported for vision processors\n                keep_in_memory=True,  # bug occurs when not batched and not in memory,\n                # subsequent ds.map calls are always batched,\n                # regardless of `batched` argument\n                remove_columns=get_columns(dataset),  # assumes that input names\n                # and output names are disjoint\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Tokenizing\",\n            )\n            logger.debug(f\"Model kwargs after tokenizing: {get_columns(dataset)}\")\n\n        if self.dataset_args.concatenate_data:\n            # postprocess: group text\n            dataset = self.map(\n                dataset,\n                self.group_text,\n                batched=True,\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Concatenating data\",\n            )\n            logger.debug(f\"Model kwargs after concatenating: {get_columns(dataset)}\")\n\n        if add_labels:\n            # postprocess: add labels\n            dataset = self.map(\n                dataset,\n                self.add_labels,\n                batched=False,  # not compatible with batching, need row lengths\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Adding labels\",\n            )\n            logger.debug(f\"Model kwargs after adding labels: {get_columns(dataset)}\")\n\n        elif self.PROMPT_KEY in get_columns(dataset):\n            dataset = dataset.remove_columns(self.PROMPT_KEY)\n            logger.debug(\"Removed prompt key\")\n\n        logger.debug(f\"Model kwargs after postprocessing: {get_columns(dataset)}\")\n        return dataset\n\n    def load_dataset(self):\n        \"\"\"\n        Load the raw dataset from Hugging Face, using cached copy if available\n\n        :param cache_dir: disk location to search for cached dataset\n        :return: the requested dataset\n        \"\"\"\n        if self.dataset_args.dataset_path is not None:\n            if self.dataset_args.dvc_data_repository is not None:\n                self.dataset_args.raw_kwargs[\"storage_options\"] = {\n                    \"url\": self.dataset_args.dvc_data_repository\n                }\n                self.dataset_args.raw_kwargs[\"data_files\"] = (\n                    self.dataset_args.dataset_path\n                )\n            else:\n                self.dataset_args.raw_kwargs[\"data_files\"] = (\n                    get_custom_datasets_from_path(\n                        self.dataset_args.dataset_path,\n                        self.dataset_args.dataset\n                        if hasattr(self.dataset_args, \"dataset\")\n                        else self.dataset_args.dataset_name,\n                    )\n                )\n\n        logger.debug(f\"Loading dataset {self.dataset_args.dataset}\")\n        return get_raw_dataset(\n            self.dataset_args,\n            None,\n            split=self.split,\n            streaming=self.dataset_args.streaming,\n            **self.dataset_args.raw_kwargs,\n        )\n\n    @cached_property\n    def preprocess(self) -&gt; Union[Callable[[LazyRow], Any], None]:\n        \"\"\"\n        The function must return keys which correspond to processor/tokenizer kwargs,\n        optionally including PROMPT_KEY\n        \"\"\"\n        preprocessing_func = self.dataset_args.preprocessing_func\n\n        if callable(preprocessing_func):\n            return preprocessing_func\n\n        if isinstance(preprocessing_func, str):\n            if \":\" in preprocessing_func:\n                # load func_name from \"/path/to/file.py:func_name\"\n                return import_from_path(preprocessing_func)\n            else:\n                # load from the registry\n                return PreprocessingFunctionRegistry.get_value_from_registry(\n                    name=preprocessing_func\n                )\n\n        return self.dataset_template\n\n    @property\n    def dataset_template(self) -&gt; Union[Callable[[Any], Any], None]:\n        return None\n\n    def rename_columns(self, dataset: DatasetType) -&gt; DatasetType:\n        # rename columns to match processor/tokenizer kwargs\n        column_names = get_columns(dataset)\n        if self.dataset_args.text_column in column_names and \"text\" not in column_names:\n            logger.debug(f\"Renaming column `{self.dataset_args.text_column}` to `text`\")\n            dataset = dataset.rename_column(self.dataset_args.text_column, \"text\")\n\n        return dataset\n\n    def filter_tokenizer_args(self, dataset: DatasetType) -&gt; DatasetType:\n        # assumes that inputs are not passed via self.processor.__call__ args and kwargs\n        signature = inspect.signature(self.processor.__call__)\n        tokenizer_args = set(\n            key\n            for key, param in signature.parameters.items()\n            if param.kind not in (Kind.VAR_POSITIONAL, Kind.VAR_KEYWORD)\n        )\n        logger.debug(\n            f\"Found processor args `{tokenizer_args}`. Removing all other columns\"\n        )\n\n        column_names = get_columns(dataset)\n        return dataset.remove_columns(\n            list(set(column_names) - set(tokenizer_args) - set([self.PROMPT_KEY]))\n        )\n\n    def tokenize(self, data: LazyRow) -&gt; Dict[str, Any]:\n        # separate prompt\n        prompt = data.pop(self.PROMPT_KEY, None)\n\n        # tokenize\n        data = self.processor(\n            **data,\n            padding=self.padding,\n            max_length=self.max_seq_length,\n            truncation=True,\n        )\n\n        # store unpadded prompt so we can mask out correct number of elements in labels\n        if prompt is not None:\n            data[self.PROMPT_KEY] = self.processor(\n                text=prompt,\n                max_length=self.max_seq_length,\n                truncation=True,\n            )[\"input_ids\"]\n\n        return data\n\n    def group_text(self, data: LazyRow) -&gt; Dict[str, Any]:\n        concatenated_data = {k: sum(data[k], []) for k in data.keys()}\n        total_length = len(concatenated_data[list(data.keys())[0]])\n        total_length = (total_length // self.max_seq_length) * self.max_seq_length\n        result = {\n            k: [\n                t[i : i + self.max_seq_length]\n                for i in range(0, total_length, self.max_seq_length)\n            ]\n            for k, t in concatenated_data.items()\n        }\n        return result\n\n    def add_labels(self, data: LazyRow) -&gt; LazyRow:\n        if \"pixel_values\" in data:\n            raise NotImplementedError(\n                \"Label masking for vision datasets has not been implemented yet\"\n            )\n\n        # if the dataset uses prompts, mask them out so they don't contribute\n        # to the loss calculation\n        prompt_len = 0\n        if self.PROMPT_KEY in data:\n            prompt_len = len(data[self.PROMPT_KEY])\n        data[\"labels\"] = data[\"input_ids\"].copy()\n        data[\"labels\"][:prompt_len] = [LABELS_MASK_VALUE] * prompt_len\n\n        # mask out padding in the labels as well\n        padding = len(data[\"attention_mask\"]) - sum(data[\"attention_mask\"])\n        if padding &gt; 0:\n            data[\"labels\"][-padding:] = [LABELS_MASK_VALUE] * padding\n        return data\n\n    def map(\n        self,\n        dataset: Union[Dataset, IterableDataset],\n        function: Callable[[Any], Any],\n        **kwargs,\n    ) -&gt; Union[Dataset, IterableDataset]:\n        \"\"\"\n        Wrapper function around Dataset.map and IterableDataset.map.\n\n        If the dataset is streaming (in the case of IterableDataset), non-applicable\n        arguments are ignored and the dataset features are resolved\n        \"\"\"\n        if isinstance(dataset, IterableDataset):\n            # remove arguments that don't apply to streaming\n            kwargs.pop(\"num_proc\", None)\n            kwargs.pop(\"load_from_cache_file\", None)\n            kwargs.pop(\"desc\", None)\n            kwargs.pop(\"keep_in_memory\", None)\n\n        dataset = dataset.map(function, **kwargs)\n\n        if isinstance(dataset, IterableDataset):\n            dataset = dataset._resolve_features()\n\n        return dataset\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.TextGenerationDataset.preprocess","title":"<code>preprocess</code>  <code>cached</code> <code>property</code>","text":"<p>The function must return keys which correspond to processor/tokenizer kwargs, optionally including PROMPT_KEY</p>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.TextGenerationDataset.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load the raw dataset from Hugging Face, using cached copy if available</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <p>disk location to search for cached dataset</p> required <p>Returns:</p> Type Description <p>the requested dataset</p> Source code in <code>src/llmcompressor/transformers/finetune/data/base.py</code> <pre><code>def load_dataset(self):\n    \"\"\"\n    Load the raw dataset from Hugging Face, using cached copy if available\n\n    :param cache_dir: disk location to search for cached dataset\n    :return: the requested dataset\n    \"\"\"\n    if self.dataset_args.dataset_path is not None:\n        if self.dataset_args.dvc_data_repository is not None:\n            self.dataset_args.raw_kwargs[\"storage_options\"] = {\n                \"url\": self.dataset_args.dvc_data_repository\n            }\n            self.dataset_args.raw_kwargs[\"data_files\"] = (\n                self.dataset_args.dataset_path\n            )\n        else:\n            self.dataset_args.raw_kwargs[\"data_files\"] = (\n                get_custom_datasets_from_path(\n                    self.dataset_args.dataset_path,\n                    self.dataset_args.dataset\n                    if hasattr(self.dataset_args, \"dataset\")\n                    else self.dataset_args.dataset_name,\n                )\n            )\n\n    logger.debug(f\"Loading dataset {self.dataset_args.dataset}\")\n    return get_raw_dataset(\n        self.dataset_args,\n        None,\n        split=self.split,\n        streaming=self.dataset_args.streaming,\n        **self.dataset_args.raw_kwargs,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.TextGenerationDataset.map","title":"<code>map(dataset, function, **kwargs)</code>","text":"<p>Wrapper function around Dataset.map and IterableDataset.map.</p> <p>If the dataset is streaming (in the case of IterableDataset), non-applicable arguments are ignored and the dataset features are resolved</p> Source code in <code>src/llmcompressor/transformers/finetune/data/base.py</code> <pre><code>def map(\n    self,\n    dataset: Union[Dataset, IterableDataset],\n    function: Callable[[Any], Any],\n    **kwargs,\n) -&gt; Union[Dataset, IterableDataset]:\n    \"\"\"\n    Wrapper function around Dataset.map and IterableDataset.map.\n\n    If the dataset is streaming (in the case of IterableDataset), non-applicable\n    arguments are ignored and the dataset features are resolved\n    \"\"\"\n    if isinstance(dataset, IterableDataset):\n        # remove arguments that don't apply to streaming\n        kwargs.pop(\"num_proc\", None)\n        kwargs.pop(\"load_from_cache_file\", None)\n        kwargs.pop(\"desc\", None)\n        kwargs.pop(\"keep_in_memory\", None)\n\n    dataset = dataset.map(function, **kwargs)\n\n    if isinstance(dataset, IterableDataset):\n        dataset = dataset._resolve_features()\n\n    return dataset\n</code></pre>"},{"location":"reference/llmcompressor/transformers/#llmcompressor.transformers.is_model_ct_quantized_from_path","title":"<code>is_model_ct_quantized_from_path(path)</code>","text":"<p>Determine if model from path is quantized based on the config</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the model or HF stub</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if config contains quantization_config from the given path</p> Source code in <code>src/llmcompressor/transformers/utils/helpers.py</code> <pre><code>def is_model_ct_quantized_from_path(path: str) -&gt; bool:\n    \"\"\"\n    Determine if model from path is quantized based\n    on the config\n\n    :param path: path to the model or HF stub\n    :return: True if config contains quantization_config from the given path\n\n    \"\"\"\n    config = AutoConfig.from_pretrained(path)\n    if config is not None:\n        if (\n            hasattr(config, \"quantization_config\")\n            and config.quantization_config[\"quant_method\"] == \"compressed-tensors\"\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/","title":"llmcompressor.transformers.compression","text":""},{"location":"reference/llmcompressor/transformers/compression/helpers/","title":"llmcompressor.transformers.compression.helpers","text":""},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.calculate_offload_device_map","title":"<code>calculate_offload_device_map(model_stub, reserve_for_hessians=False, num_gpus=1, torch_dtype=torch.float16, model_cls=AutoModelForCausalLM, **model_kwargs)</code>","text":"<p>Calculates the optimal gpu mappings for model_stub stored as torch_dtype. Takes into account extra memory required for quantization and (optionally) GPTQ hessians</p> <p>Parameters:</p> Name Type Description Default <code>model_stub</code> <code>str</code> <p>local path or HF stub to calculate mapping for</p> required <code>reserve_for_hessians</code> <p>whether to reserve memory for GPTQ</p> <code>False</code> <code>num_gpus</code> <code>int</code> <p>number of gpus to utilize</p> <code>1</code> <code>model_cls</code> <code>Type</code> <p>model class to use when initializing model structure, default is AutoModelForCausalLM</p> <code>AutoModelForCausalLM</code> <code>model_kwargs</code> <p>keyword arguments to pass to model initializer</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[Union[int, str], Union[int, str]]</code> <p>memory mapping for layers of model_stub to be passed to from_pretrained()</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def calculate_offload_device_map(\n    model_stub: str,\n    reserve_for_hessians=False,\n    num_gpus: int = 1,\n    torch_dtype: torch.dtype = torch.float16,\n    model_cls: Type = AutoModelForCausalLM,\n    **model_kwargs,\n) -&gt; Dict[Union[int, str], Union[int, str]]:\n    \"\"\"\n    Calculates the optimal gpu mappings for model_stub stored as torch_dtype. Takes\n    into account extra memory required for quantization and (optionally) GPTQ hessians\n\n    :param model_stub: local path or HF stub to calculate mapping for\n    :param reserve_for_hessians: whether to reserve memory for GPTQ\n    :param num_gpus: number of gpus to utilize\n    :param model_cls: model class to use when initializing model structure,\n        default is AutoModelForCausalLM\n    :param model_kwargs: keyword arguments to pass to model initializer\n    :return: memory mapping for layers of model_stub to be passed to from_pretrained()\n    \"\"\"\n    max_cpu_memory = psutil.virtual_memory().available\n    max_gpu_memory = torch.cuda.mem_get_info(0)[0]\n    available_gpus = torch.cuda.device_count()\n    if available_gpus &lt; num_gpus:\n        raise ValueError(\n            f\"Requested {num_gpus} GPUs but only {available_gpus} are available.\"\n        )\n    max_gpu_memory = [max_gpu_memory] * num_gpus\n\n    device_map = {}\n    with init_empty_weights():\n        dummy_model = model_cls.from_pretrained(\n            model_stub, torch_dtype=torch_dtype, **model_kwargs\n        )\n\n        reserved_memory = 0\n        if reserve_for_hessians:\n            reserved_memory = hessian_memory_requirements(dummy_model)\n        reserved_memory += quantization_memory_requirement(dummy_model)\n\n        memory_limits = {\n            idx: (max_memory - reserved_memory)\n            for idx, max_memory in enumerate(max_gpu_memory)\n        }\n        memory_limits[\"cpu\"] = max_cpu_memory\n\n        device_map = infer_auto_device_map(\n            dummy_model,\n            max_memory=memory_limits,\n            no_split_module_classes=dummy_model._no_split_modules,\n        )\n        del dummy_model\n\n    return device_map\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.custom_offload_device_map","title":"<code>custom_offload_device_map(model_stub, max_memory_per_gpu, num_gpus=1, model_cls=AutoModelForCausalLM, **model_kwargs)</code>","text":"<p>Calculates the optimal gpu mappings for model_stub stored as torch_dtype, where each GPU is restricted to allocating a specific amount of memory.</p> <p>Parameters:</p> Name Type Description Default <code>model_stub</code> <code>str</code> <p>local path or HF stub to calculate mapping for</p> required <code>max_memory_per_gpu</code> <code>Union[str, int]</code> <p>Max memory to allocate on each GPU, as either a string such as \"10GB\" or an integer number of bytes</p> required <code>num_gpus</code> <code>int</code> <p>number of gpus to utilize</p> <code>1</code> <code>model_cls</code> <code>Type</code> <p>model class to use when initializing model structure, default is AutoModelForCausalLM</p> <code>AutoModelForCausalLM</code> <code>model_kwargs</code> <p>keyword arguments to pass to model initializer</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[Union[int, str], Union[int, str]]</code> <p>memory mapping for layers of model_stub to be passed to from_pretrained()</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def custom_offload_device_map(\n    model_stub: str,\n    max_memory_per_gpu: Union[str, int],\n    num_gpus: int = 1,\n    model_cls: Type = AutoModelForCausalLM,\n    **model_kwargs,\n) -&gt; Dict[Union[int, str], Union[int, str]]:\n    \"\"\"\n    Calculates the optimal gpu mappings for model_stub stored as torch_dtype, where\n    each GPU is restricted to allocating a specific amount of memory.\n\n    :param model_stub: local path or HF stub to calculate mapping for\n    :param max_memory_per_gpu: Max memory to allocate on each GPU, as either a string\n        such as \"10GB\" or an integer number of bytes\n    :param num_gpus: number of gpus to utilize\n    :param model_cls: model class to use when initializing model structure,\n        default is AutoModelForCausalLM\n    :param model_kwargs: keyword arguments to pass to model initializer\n    :return: memory mapping for layers of model_stub to be passed to from_pretrained()\n    \"\"\"\n    max_cpu_memory = psutil.virtual_memory().available\n    memory_limits = {device: max_memory_per_gpu for device in range(num_gpus)}\n    memory_limits[\"cpu\"] = max_cpu_memory\n\n    device_map = {}\n    with init_empty_weights():\n        dummy_model = model_cls.from_pretrained(model_stub, **model_kwargs)\n        device_map = infer_auto_device_map(\n            dummy_model,\n            max_memory=memory_limits,\n            no_split_module_classes=dummy_model._no_split_modules,\n        )\n        del dummy_model\n\n    return device_map\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.hessian_memory_requirements","title":"<code>hessian_memory_requirements(model)</code>","text":"<p>Determines the number of bytes needed to store Hessian data for a single transformer layer in model. This is used for reserving memory for GPTQ quantization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to calculate requirements for</p> required <p>Returns:</p> Type Description <code>int</code> <p>number of bytes required to reserve for GPTQ on a single layer</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def hessian_memory_requirements(model: torch.nn.Module) -&gt; int:\n    \"\"\"\n    Determines the number of bytes needed to store Hessian data for a single\n    transformer layer in model. This is used for reserving memory for GPTQ\n    quantization\n\n    :param model: model to calculate requirements for\n    :return: number of bytes required to reserve for GPTQ on a single layer\n    \"\"\"\n    transformer_layers = get_layers(get_no_split_params(model), model)\n    total_hessian_elems = {}\n    max_column_size = {}\n    for no_split_name, no_split_layer in transformer_layers.items():\n        total_hessian_elems[no_split_name] = 0\n        max_column_size[no_split_name] = 0\n        for _name, module in no_split_layer.named_modules():\n            if isinstance(module, Linear) and hasattr(module, \"weight\"):\n                column_size = module.weight.shape[1]\n                total_hessian_elems[no_split_name] += column_size * column_size\n                if column_size &gt; max_column_size[no_split_name]:\n                    # max extra memory for inverse calculation\n                    max_column_size[no_split_name] = column_size\n\n    max_total_hessian_elems = max(total_hessian_elems.values())\n    overall_max_column_size = max(max_column_size.values())\n    bytes_per_weight = 32 // 8  # hessians are float32\n    inverse_reserved = overall_max_column_size * overall_max_column_size\n    return (max_total_hessian_elems + inverse_reserved) * bytes_per_weight\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.infer_sparse_targets_and_ignores","title":"<code>infer_sparse_targets_and_ignores(model, sparsity_structure, sparsity_threshold)</code>","text":"<p>Infers the target and ignore layers in the given model to be used for sparsity compression</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to check</p> required <code>sparsity_structure</code> <code>str</code> <p>sparsity structure to check against</p> required <code>sparsity_threshold</code> <code>float</code> <p>threshold for sparsity</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>tuple of target and ignore layers</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def infer_sparse_targets_and_ignores(\n    model: torch.nn.Module,\n    sparsity_structure: str,\n    sparsity_threshold: float,\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Infers the target and ignore layers in the given model\n    to be used for sparsity compression\n\n    :param model: model to check\n    :param sparsity_structure: sparsity structure to check against\n    :param sparsity_threshold: threshold for sparsity\n    :return: tuple of target and ignore layers\n    \"\"\"\n\n    exhaustive_targets, exhaustive_ignore = _get_sparse_targets_ignore_dicts(\n        module=model,\n        sparsity_structure=sparsity_structure,\n        sparsity_threshold=sparsity_threshold,\n    )\n\n    return _reduce_targets_and_ignores_into_lists(\n        exhaustive_targets=exhaustive_targets,\n        exhaustive_ignore=exhaustive_ignore,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.infer_sparsity_structure_from_model","title":"<code>infer_sparsity_structure_from_model(model)</code>","text":"<p>Determines the sparsity structure, if any exists, given the model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to check for sparsity structure</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>sparsity structure as a string or None</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def infer_sparsity_structure_from_model(model: torch.nn.Module) -&gt; Optional[str]:\n    \"\"\"\n    Determines the sparsity structure, if any exists, given the model\n\n    :param model: model to check for sparsity structure\n    :return: sparsity structure as a string or None\n    \"\"\"\n\n    # check for the common sparsity structures\n    structures = {\"2:4\"}\n    for sparsity_structure in structures:\n        linear_modules = get_linear_layers(model)\n        offloaded_params = get_state_dict_offloaded_model(model)\n\n        linear_modules_with_sparsity_structure = [\n            tensor_follows_mask_structure(offloaded_params[f\"{name}.weight\"])\n            for name in tqdm(\n                linear_modules.keys(),\n                desc=\"Checking whether model follows \"\n                f\"{sparsity_structure} sparsity structure\",\n            )\n        ]\n        # if the majority of the linear modules follow the sparsity structure\n        # we can assume that the model follows the sparsity structure\n        # (taking into consideration the fact that some Linear layers like the\n        # embedding layer might not be sparse)\n        if (\n            sum(linear_modules_with_sparsity_structure)\n            &gt; len(linear_modules_with_sparsity_structure) * 0.8\n        ):\n            return sparsity_structure\n\n    return None\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.infer_sparsity_structure_from_stage_modifiers","title":"<code>infer_sparsity_structure_from_stage_modifiers(stage_modifiers)</code>","text":"<p>Determines the sparsity structure, if any exists, given the list of stage modifiers</p> <p>Parameters:</p> Name Type Description Default <code>stage_modifiers</code> <code>List[StageModifier]</code> <p>non-empty list of stage modifiers</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>sparsity structure as a string or None</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def infer_sparsity_structure_from_stage_modifiers(\n    stage_modifiers: List[\"StageModifier\"],  # noqa E501\n) -&gt; Optional[str]:\n    \"\"\"\n    Determines the sparsity structure, if any exists, given the\n    list of stage modifiers\n\n    :param stage_modifiers: non-empty list of stage modifiers\n    :return: sparsity structure as a string or None\n    \"\"\"\n    for stage in stage_modifiers:\n        if stage.applied:\n            for modifier in stage.modifiers:\n                if hasattr(modifier, \"mask_structure\"):\n                    sparsity_structure = modifier.mask_structure\n                    return sparsity_structure\n    return None\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.is_sparse_compression_target","title":"<code>is_sparse_compression_target(module, sparsity_threshold, sparsity_structure)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to check</p> required <code>sparsity_threshold</code> <code>float</code> <p>threshold for sparsity</p> required <code>sparsity_structure</code> <code>str</code> <p>sparsity structure to check against</p> required <p>Returns:</p> Type Description <code>bool</code> <p>whether or not the module is a target for sparsity compression, i.e True if it is sparse and follows the sparsity structure, else False</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def is_sparse_compression_target(\n    module: torch.nn.Module, sparsity_threshold: float, sparsity_structure: str\n) -&gt; bool:\n    \"\"\"\n    :param module: module to check\n    :param sparsity_threshold: threshold for sparsity\n    :param sparsity_structure: sparsity structure to check against\n    :return: whether or not the module is a target for sparsity compression,\n        i.e True if it is sparse and follows the sparsity structure, else False\n    \"\"\"\n    with align_module_device(module):\n        result = (\n            hasattr(module, \"weight\")\n            and tensor_sparsity(module.weight) &gt;= sparsity_threshold\n            and tensor_follows_mask_structure(\n                tensor=module.weight, mask=sparsity_structure\n            )\n        )\n\n    return result\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.quantization_memory_requirement","title":"<code>quantization_memory_requirement(model)</code>","text":"<p>Determines the max number of bytes needed to store quantization scale and zp data</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to calculate requirements for</p> required <p>Returns:</p> Type Description <code>int</code> <p>number of bytes required to reserve for quantization</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def quantization_memory_requirement(model: torch.nn.Module) -&gt; int:\n    \"\"\"\n    Determines the max number of bytes needed to store quantization scale and zp data\n\n    :param model: model to calculate requirements for\n    :return: number of bytes required to reserve for quantization\n    \"\"\"\n\n    total_elements = 0\n    for _, module in model.named_modules():\n        if isinstance(module, Linear):\n            for param in module.parameters():\n                # assume the max of group 128 and static scale/zp\n                # TODO: base this on the recipe instead instead of assuming max\n\n                # potentially just bias term\n                max_quant_shape = param.shape[0] // 128\n\n                if len(param.size()) &gt; 1:  # weights\n                    max_quant_shape *= param.shape[1]\n\n                total_elements += max_quant_shape * 4\n\n    bytes_ratio = 32 // 16  # assuming float16\n    return total_elements * bytes_ratio\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/helpers/#llmcompressor.transformers.compression.helpers.tensor_follows_mask_structure","title":"<code>tensor_follows_mask_structure(tensor, mask='2:4')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>tensor to check</p> required <code>mask</code> <code>str</code> <p>mask structure to check for, in the format \"n:m\", also accepts \"unstructured\" as a valid mask structure</p> <code>'2:4'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the tensor follows the mask structure, False otherwise. Note, some weights can incidentally be zero, so we check for atleast n zeros in each chunk of size m</p> Source code in <code>src/llmcompressor/transformers/compression/helpers.py</code> <pre><code>def tensor_follows_mask_structure(tensor: torch.Tensor, mask: str = \"2:4\") -&gt; bool:\n    \"\"\"\n    :param tensor: tensor to check\n    :param mask: mask structure to check for, in the format \"n:m\", also accepts\n        \"unstructured\" as a valid mask structure\n    :return: True if the tensor follows the mask structure, False otherwise.\n        Note, some weights can incidentally be zero, so we check for\n        atleast n zeros in each chunk of size m\n    \"\"\"\n\n    if mask.lower().strip() == \"unstructured\":\n        return True\n\n    n, m = tuple(map(int, mask.split(\":\")))\n\n    # If n or m is 0, then the tensor follows the mask structure\n    if n == 0 or m == 0:\n        return True\n    # Reshape the tensor into chunks of size m\n    tensor = tensor.view(-1, m)\n\n    # Count the number of zeros in each chunk\n    zero_counts = (tensor == 0).sum(dim=1)\n\n    # Check if the number of zeros in each chunk atleast n\n    # Greater than sign is needed as some weights can incidentally\n    # be zero\n    return torch.all(zero_counts &gt;= n).item()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/quantization_format/","title":"llmcompressor.transformers.compression.quantization_format","text":""},{"location":"reference/llmcompressor/transformers/compression/quantization_format/#llmcompressor.transformers.compression.quantization_format.infer_quantization_format","title":"<code>infer_quantization_format(model, quantization_format=None, save_compressed=False, sparsity_structure=None)</code>","text":"<p>Infers the quantization format for a model based on its state and provided compression arguments.</p> <p>The following table outlines the possible quantization and sparsity formats along with their corresponding compressor formats:</p> <pre><code>+---------------+----------+----------------------+---------------------+\n| Quantization  | Sparsity | Quant Compressor     | Sparsity Compressor |\n|               |          | Format               | Format              |\n+---------------+----------+----------------------+---------------------+\n| W8A8 - int    | None     | int_quantized        | Dense               |\n| W8A8 - float  | None     | float_quantized      | Dense               |\n| W4A16 - int   | None     | pack_quantized       | Dense               |\n| W8A16 - int   | None     | pack_quantized       | Dense               |\n| W8A16 - float | None     | naive_quantized      | Dense               |\n| W8A8 - int    | 2:4      | int_quantized        | Sparse24            |\n| W8A8 - float  | 2:4      | float_quantized      | Sparse24            |\n| W4A16 - int   | 2:4      | marlin_24            | Dense               |\n| W8A16 - int   | 2:4      | marlin_24            | Dense               |\n| W8A16 - float | 2:4      | naive_quantized      | Dense               |\n+---------------+----------+----------------------+---------------------+\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to check for quantization, if the model is not quantized no quantization format is returned</p> required <code>quantization_format</code> <code>Optional[str]</code> <p>user provided quantization format, supercedes any inferred quantization format</p> <code>None</code> <code>save_compressed</code> <code>bool</code> <p>used to infer a quantization format if None is provided</p> <code>False</code> Source code in <code>src/llmcompressor/transformers/compression/quantization_format.py</code> <pre><code>def infer_quantization_format(\n    model,\n    quantization_format: Optional[str] = None,\n    save_compressed: bool = False,\n    sparsity_structure: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Infers the quantization format for a model based on its state and provided\n    compression arguments.\n\n    The following table outlines the possible quantization and sparsity formats\n    along with their corresponding compressor formats:\n\n        +---------------+----------+----------------------+---------------------+\n        | Quantization  | Sparsity | Quant Compressor     | Sparsity Compressor |\n        |               |          | Format               | Format              |\n        +---------------+----------+----------------------+---------------------+\n        | W8A8 - int    | None     | int_quantized        | Dense               |\n        | W8A8 - float  | None     | float_quantized      | Dense               |\n        | W4A16 - int   | None     | pack_quantized       | Dense               |\n        | W8A16 - int   | None     | pack_quantized       | Dense               |\n        | W8A16 - float | None     | naive_quantized      | Dense               |\n        | W8A8 - int    | 2:4      | int_quantized        | Sparse24            |\n        | W8A8 - float  | 2:4      | float_quantized      | Sparse24            |\n        | W4A16 - int   | 2:4      | marlin_24            | Dense               |\n        | W8A16 - int   | 2:4      | marlin_24            | Dense               |\n        | W8A16 - float | 2:4      | naive_quantized      | Dense               |\n        +---------------+----------+----------------------+---------------------+\n\n    :param model: model to check for quantization, if the model is not quantized no\n        quantization format is returned\n    :param quantization_format: user provided quantization format, supercedes any\n        inferred quantization format\n    :param save_compressed: used to infer a quantization format if None is provided\n    :return compression format appropriate for model\n    \"\"\"\n    if not is_model_quantized(model):\n        return None\n\n    if quantization_format is not None:\n        return quantization_format\n\n    if save_compressed:\n        weight_args, input_args = _get_unique_quant_args(model)\n        is_24_structure = (\n            SparsityStructure(sparsity_structure) == SparsityStructure.TWO_FOUR\n        )\n        is_weight_only = len(input_args) == 0 and len(weight_args) &gt; 0\n\n        if is_weight_only:  # w4a16 and w8a16\n            is_valid_pack = all(\n                weight_arg.num_bits in [4, 8]\n                and weight_arg.type == QuantizationType.INT.value\n                for weight_arg in weight_args\n            )\n            if not is_valid_pack:  # packing only valid for int4 and int 8\n                return CompressionFormat.naive_quantized\n            if is_24_structure:\n                for arg in weight_args:\n                    if (\n                        arg.strategy is not QuantizationStrategy.CHANNEL.value\n                        and arg.strategy is not QuantizationStrategy.GROUP.value\n                    ):\n                        # marlin24 kernel only applicable for channel/group quantization\n                        return CompressionFormat.pack_quantized\n                return CompressionFormat.marlin_24\n            return CompressionFormat.pack_quantized\n        else:  # w8a8 float and int\n            if len(weight_args) == 1:\n                if (\n                    weight_args[0].type == QuantizationType.FLOAT.value\n                    and weight_args[0].num_bits == 8\n                ):\n                    return CompressionFormat.float_quantized\n                if weight_args[0].type == QuantizationType.INT.value:\n                    return CompressionFormat.int_quantized\n\n            return CompressionFormat.naive_quantized\n    else:\n        # format will be inferred from config\n        return None\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/","title":"llmcompressor.transformers.compression.sparsity_metadata_config","text":""},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/#llmcompressor.transformers.compression.sparsity_metadata_config.SparsityConfigMetadata","title":"<code>SparsityConfigMetadata</code>","text":"<p>Class of helper functions for filling out a SparsityCompressionConfig with readable metadata from the model</p> Source code in <code>src/llmcompressor/transformers/compression/sparsity_metadata_config.py</code> <pre><code>class SparsityConfigMetadata:\n    \"\"\"\n    Class of helper functions for filling out a SparsityCompressionConfig with readable\n    metadata from the model\n    \"\"\"\n\n    SPARSITY_THRESHOLD: float = 0.49\n\n    @staticmethod\n    def infer_global_sparsity(\n        model: Module, state_dict: Optional[Dict[str, Tensor]] = None\n    ) -&gt; float:\n        \"\"\"\n        Calculates the global percentage of sparse zero weights in the model\n\n        :param model: pytorch model to infer sparsity of\n        :param state_dict: optional state_dict to replace that in model, used for\n        gathering global FSDP model info\n        :return: global sparsity of model\n        \"\"\"\n\n        info = ModuleSparsificationInfo(model, state_dict=state_dict)\n        global_sparsity = info.params_sparse_percent / 100.0  # convert % to float\n        return global_sparsity\n\n    @staticmethod\n    def infer_sparsity_structure(\n        model: Optional[Module] = None, check_only_modifiers: Optional[bool] = False\n    ) -&gt; str:\n        \"\"\"\n        Determines what sparsity structure, if any, was applied.\n\n        First, there is an attempt to dedue the sparsity structure\n        from the currently active sparse session.\n\n        If that fails, the sparsity structure is inferred from the\n        model (if provided)\n\n        Finally, if both fail, the sparsity structure is set to\n        \"unstructured\"\n\n        :return: sparsity structure as a string\n        \"\"\"\n        sparsity_structure = None\n\n        current_session = active_session()\n        stage_modifiers = current_session.lifecycle.modifiers\n        if stage_modifiers:\n            sparsity_structure = infer_sparsity_structure_from_stage_modifiers(\n                stage_modifiers\n            )\n\n        if check_only_modifiers:\n            return sparsity_structure\n\n        if model and sparsity_structure is None:\n            sparsity_structure = infer_sparsity_structure_from_model(model)\n\n        return SparsityStructure(sparsity_structure).value\n\n    @staticmethod\n    def from_pretrained(\n        model: Module,\n        state_dict: Optional[Dict[str, Tensor]] = None,\n        compress: bool = False,\n        quantization_format: Optional[CompressionFormat] = None,\n        disable_sparse_compression: bool = False,\n        sparsity_structure: Optional[str] = None,\n    ) -&gt; Optional[\"SparsityCompressionConfig\"]:\n        \"\"\"\n        Determines compression type and informational parameters for a given model\n\n        :param model: pytorch model to calculate sparsity config for\n        :param state_dict: optional state_dict to replace that in model, used for\n        gathering global FSDP model info\n        :param compress: whether or not to compress the model on disk\n        :param quantization_format: the quantization compression format being used\n            for the model\n        :param disable_sparse_compression: whether or not to compress the model with\n            sparse compressors, If True, the sparse compression format will\n            be dense, default is False.\n        :param sparsity_structure: sparsity structure for the model. Providing it as\n            input will skip the step to infer it from the model directly\n        :return: compression config inferred from the model\n        \"\"\"\n        # TODO: can we remove this? Do we need the state dict?\n        global_sparsity = SparsityConfigMetadata.infer_global_sparsity(\n            model, state_dict=state_dict\n        )\n\n        if sparsity_structure is None:\n            sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure(\n                model=model\n            )\n\n        if (\n            disable_sparse_compression\n            or quantization_format == CompressionFormat.marlin_24\n        ):\n            # sparse compressor should be dense\n            # when no_sparse_compression is True\n            # or when marlin_24 is used\n            format = CompressionFormat.dense.value\n        elif compress and SparsityConfigMetadata.is_sparse24_bitmask_supported(\n            model, sparsity_structure\n        ):\n            format = CompressionFormat.sparse_24_bitmask.value\n        else:\n            format = CompressionFormat.dense.value\n\n        # TODO: eventually should be done similar to quantization\n        # so we do not have to infer\n        targets, ignores = infer_sparse_targets_and_ignores(\n            model,\n            sparsity_structure=sparsity_structure,\n            sparsity_threshold=SparsityConfigMetadata.SPARSITY_THRESHOLD,\n        )\n\n        if not (targets or ignores):\n            # no sparsity config\n            # needed if targets/ignores are empty\n            return None\n\n        return SparsityCompressionConfig.load_from_registry(\n            format,\n            global_sparsity=global_sparsity,\n            sparsity_structure=sparsity_structure,\n            targets=targets,\n            ignore=ignores,\n        )\n\n    @staticmethod\n    def fill_config_details(\n        config: SparsityCompressionConfig,\n        model: Module,\n        state_dict: Optional[Dict[str, Tensor]] = None,\n    ):\n        \"\"\"\n        Fills in informational sparsity parameters from a given model\n\n        :param config: sparsity config to fill in\n        :param model: pytorch model to infer config parameters from\n        :param state_dict: optional state_dict to replace that in model, used for\n        gathering global FSDP model info\n        \"\"\"\n        config.global_sparsity = SparsityConfigMetadata.infer_global_sparsity(\n            model, state_dict=state_dict\n        )\n        config.sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure()\n\n    @staticmethod\n    def is_sparse24_bitmask_supported(\n        model: Module,\n        sparsity_structure: Optional[str] = None,\n    ) -&gt; bool:\n        \"\"\"\n        Determines if sparse 24 bitmask sparse compressor is supported for a given model\n        and its sparsity structure in vLLM\n\n        :param model: pytorch model to check for sparse 24 bit sparsity support\n        :param sparsity_structure: sparsity structure of the model, if\n            not supplied it will be inferred\n        :return: whether or not sparse 24 bitmask compression is supported\n            in vLLM for the given model\n        \"\"\"\n        if sparsity_structure is None:\n            sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure(model)\n\n        if sparsity_structure != SparsityStructure.TWO_FOUR.value:\n            # only supported for 2:4 sparsity\n            return False\n\n        if not is_model_quantized(model):\n            logger.warning(\n                \"Compressed Sparse-only 2:4 models are not supported in vLLM&lt;=0.7.0, \"\n                \"consider saving with `disable_sparse_compression` set, \"\n                \"`model.save_pretrained(..., disable_sparse_compression=True)`\"\n            )\n            return True\n\n        # when model is quantized, and has 2:4 sparsity\n\n        supported_scheme_types: List[str] = [\n            QuantizationType.INT.value,\n            QuantizationType.FLOAT.value,\n        ]\n\n        for _, submodule in iter_named_leaf_modules(model):\n            if is_module_quantized(submodule):\n                weight_scheme = submodule.quantization_scheme.weights\n                input_scheme = submodule.quantization_scheme.input_activations\n\n                if weight_scheme and input_scheme:\n                    # weight and activation quantization\n                    # check schemes are supported\n                    for scheme in [weight_scheme, input_scheme]:\n                        scheme_supported = (\n                            scheme.num_bits == 8\n                            and scheme.type in supported_scheme_types\n                        )\n                        if not scheme_supported:\n                            logger.info(\n                                \"Quantization scheme not supported,\"\n                                \" turning off sparse 24 compression.\"\n                                f\" Invalid Scheme: {scheme}\"\n                            )\n                            return False\n\n                elif weight_scheme or input_scheme:\n                    # weight only quantization\n                    logger.info(\n                        \"Weight only quantization detected, \"\n                        \"turning off sparse 24 compression.\"\n                    )\n                    return False\n\n        return True\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/#llmcompressor.transformers.compression.sparsity_metadata_config.SparsityConfigMetadata.fill_config_details","title":"<code>fill_config_details(config, model, state_dict=None)</code>  <code>staticmethod</code>","text":"<p>Fills in informational sparsity parameters from a given model</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SparsityCompressionConfig</code> <p>sparsity config to fill in</p> required <code>model</code> <code>Module</code> <p>pytorch model to infer config parameters from</p> required <code>state_dict</code> <code>Optional[Dict[str, Tensor]]</code> <p>optional state_dict to replace that in model, used for gathering global FSDP model info</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/compression/sparsity_metadata_config.py</code> <pre><code>@staticmethod\ndef fill_config_details(\n    config: SparsityCompressionConfig,\n    model: Module,\n    state_dict: Optional[Dict[str, Tensor]] = None,\n):\n    \"\"\"\n    Fills in informational sparsity parameters from a given model\n\n    :param config: sparsity config to fill in\n    :param model: pytorch model to infer config parameters from\n    :param state_dict: optional state_dict to replace that in model, used for\n    gathering global FSDP model info\n    \"\"\"\n    config.global_sparsity = SparsityConfigMetadata.infer_global_sparsity(\n        model, state_dict=state_dict\n    )\n    config.sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/#llmcompressor.transformers.compression.sparsity_metadata_config.SparsityConfigMetadata.from_pretrained","title":"<code>from_pretrained(model, state_dict=None, compress=False, quantization_format=None, disable_sparse_compression=False, sparsity_structure=None)</code>  <code>staticmethod</code>","text":"<p>Determines compression type and informational parameters for a given model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>pytorch model to calculate sparsity config for</p> required <code>state_dict</code> <code>Optional[Dict[str, Tensor]]</code> <p>optional state_dict to replace that in model, used for gathering global FSDP model info</p> <code>None</code> <code>compress</code> <code>bool</code> <p>whether or not to compress the model on disk</p> <code>False</code> <code>quantization_format</code> <code>Optional[CompressionFormat]</code> <p>the quantization compression format being used for the model</p> <code>None</code> <code>disable_sparse_compression</code> <code>bool</code> <p>whether or not to compress the model with sparse compressors, If True, the sparse compression format will be dense, default is False.</p> <code>False</code> <code>sparsity_structure</code> <code>Optional[str]</code> <p>sparsity structure for the model. Providing it as input will skip the step to infer it from the model directly</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[SparsityCompressionConfig]</code> <p>compression config inferred from the model</p> Source code in <code>src/llmcompressor/transformers/compression/sparsity_metadata_config.py</code> <pre><code>@staticmethod\ndef from_pretrained(\n    model: Module,\n    state_dict: Optional[Dict[str, Tensor]] = None,\n    compress: bool = False,\n    quantization_format: Optional[CompressionFormat] = None,\n    disable_sparse_compression: bool = False,\n    sparsity_structure: Optional[str] = None,\n) -&gt; Optional[\"SparsityCompressionConfig\"]:\n    \"\"\"\n    Determines compression type and informational parameters for a given model\n\n    :param model: pytorch model to calculate sparsity config for\n    :param state_dict: optional state_dict to replace that in model, used for\n    gathering global FSDP model info\n    :param compress: whether or not to compress the model on disk\n    :param quantization_format: the quantization compression format being used\n        for the model\n    :param disable_sparse_compression: whether or not to compress the model with\n        sparse compressors, If True, the sparse compression format will\n        be dense, default is False.\n    :param sparsity_structure: sparsity structure for the model. Providing it as\n        input will skip the step to infer it from the model directly\n    :return: compression config inferred from the model\n    \"\"\"\n    # TODO: can we remove this? Do we need the state dict?\n    global_sparsity = SparsityConfigMetadata.infer_global_sparsity(\n        model, state_dict=state_dict\n    )\n\n    if sparsity_structure is None:\n        sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure(\n            model=model\n        )\n\n    if (\n        disable_sparse_compression\n        or quantization_format == CompressionFormat.marlin_24\n    ):\n        # sparse compressor should be dense\n        # when no_sparse_compression is True\n        # or when marlin_24 is used\n        format = CompressionFormat.dense.value\n    elif compress and SparsityConfigMetadata.is_sparse24_bitmask_supported(\n        model, sparsity_structure\n    ):\n        format = CompressionFormat.sparse_24_bitmask.value\n    else:\n        format = CompressionFormat.dense.value\n\n    # TODO: eventually should be done similar to quantization\n    # so we do not have to infer\n    targets, ignores = infer_sparse_targets_and_ignores(\n        model,\n        sparsity_structure=sparsity_structure,\n        sparsity_threshold=SparsityConfigMetadata.SPARSITY_THRESHOLD,\n    )\n\n    if not (targets or ignores):\n        # no sparsity config\n        # needed if targets/ignores are empty\n        return None\n\n    return SparsityCompressionConfig.load_from_registry(\n        format,\n        global_sparsity=global_sparsity,\n        sparsity_structure=sparsity_structure,\n        targets=targets,\n        ignore=ignores,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/#llmcompressor.transformers.compression.sparsity_metadata_config.SparsityConfigMetadata.infer_global_sparsity","title":"<code>infer_global_sparsity(model, state_dict=None)</code>  <code>staticmethod</code>","text":"<p>Calculates the global percentage of sparse zero weights in the model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>pytorch model to infer sparsity of</p> required <code>state_dict</code> <code>Optional[Dict[str, Tensor]]</code> <p>optional state_dict to replace that in model, used for gathering global FSDP model info</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>global sparsity of model</p> Source code in <code>src/llmcompressor/transformers/compression/sparsity_metadata_config.py</code> <pre><code>@staticmethod\ndef infer_global_sparsity(\n    model: Module, state_dict: Optional[Dict[str, Tensor]] = None\n) -&gt; float:\n    \"\"\"\n    Calculates the global percentage of sparse zero weights in the model\n\n    :param model: pytorch model to infer sparsity of\n    :param state_dict: optional state_dict to replace that in model, used for\n    gathering global FSDP model info\n    :return: global sparsity of model\n    \"\"\"\n\n    info = ModuleSparsificationInfo(model, state_dict=state_dict)\n    global_sparsity = info.params_sparse_percent / 100.0  # convert % to float\n    return global_sparsity\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/#llmcompressor.transformers.compression.sparsity_metadata_config.SparsityConfigMetadata.infer_sparsity_structure","title":"<code>infer_sparsity_structure(model=None, check_only_modifiers=False)</code>  <code>staticmethod</code>","text":"<p>Determines what sparsity structure, if any, was applied.</p> <p>First, there is an attempt to dedue the sparsity structure from the currently active sparse session.</p> <p>If that fails, the sparsity structure is inferred from the model (if provided)</p> <p>Finally, if both fail, the sparsity structure is set to \"unstructured\"</p> <p>Returns:</p> Type Description <code>str</code> <p>sparsity structure as a string</p> Source code in <code>src/llmcompressor/transformers/compression/sparsity_metadata_config.py</code> <pre><code>@staticmethod\ndef infer_sparsity_structure(\n    model: Optional[Module] = None, check_only_modifiers: Optional[bool] = False\n) -&gt; str:\n    \"\"\"\n    Determines what sparsity structure, if any, was applied.\n\n    First, there is an attempt to dedue the sparsity structure\n    from the currently active sparse session.\n\n    If that fails, the sparsity structure is inferred from the\n    model (if provided)\n\n    Finally, if both fail, the sparsity structure is set to\n    \"unstructured\"\n\n    :return: sparsity structure as a string\n    \"\"\"\n    sparsity_structure = None\n\n    current_session = active_session()\n    stage_modifiers = current_session.lifecycle.modifiers\n    if stage_modifiers:\n        sparsity_structure = infer_sparsity_structure_from_stage_modifiers(\n            stage_modifiers\n        )\n\n    if check_only_modifiers:\n        return sparsity_structure\n\n    if model and sparsity_structure is None:\n        sparsity_structure = infer_sparsity_structure_from_model(model)\n\n    return SparsityStructure(sparsity_structure).value\n</code></pre>"},{"location":"reference/llmcompressor/transformers/compression/sparsity_metadata_config/#llmcompressor.transformers.compression.sparsity_metadata_config.SparsityConfigMetadata.is_sparse24_bitmask_supported","title":"<code>is_sparse24_bitmask_supported(model, sparsity_structure=None)</code>  <code>staticmethod</code>","text":"<p>Determines if sparse 24 bitmask sparse compressor is supported for a given model and its sparsity structure in vLLM</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>pytorch model to check for sparse 24 bit sparsity support</p> required <code>sparsity_structure</code> <code>Optional[str]</code> <p>sparsity structure of the model, if not supplied it will be inferred</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>whether or not sparse 24 bitmask compression is supported in vLLM for the given model</p> Source code in <code>src/llmcompressor/transformers/compression/sparsity_metadata_config.py</code> <pre><code>@staticmethod\ndef is_sparse24_bitmask_supported(\n    model: Module,\n    sparsity_structure: Optional[str] = None,\n) -&gt; bool:\n    \"\"\"\n    Determines if sparse 24 bitmask sparse compressor is supported for a given model\n    and its sparsity structure in vLLM\n\n    :param model: pytorch model to check for sparse 24 bit sparsity support\n    :param sparsity_structure: sparsity structure of the model, if\n        not supplied it will be inferred\n    :return: whether or not sparse 24 bitmask compression is supported\n        in vLLM for the given model\n    \"\"\"\n    if sparsity_structure is None:\n        sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure(model)\n\n    if sparsity_structure != SparsityStructure.TWO_FOUR.value:\n        # only supported for 2:4 sparsity\n        return False\n\n    if not is_model_quantized(model):\n        logger.warning(\n            \"Compressed Sparse-only 2:4 models are not supported in vLLM&lt;=0.7.0, \"\n            \"consider saving with `disable_sparse_compression` set, \"\n            \"`model.save_pretrained(..., disable_sparse_compression=True)`\"\n        )\n        return True\n\n    # when model is quantized, and has 2:4 sparsity\n\n    supported_scheme_types: List[str] = [\n        QuantizationType.INT.value,\n        QuantizationType.FLOAT.value,\n    ]\n\n    for _, submodule in iter_named_leaf_modules(model):\n        if is_module_quantized(submodule):\n            weight_scheme = submodule.quantization_scheme.weights\n            input_scheme = submodule.quantization_scheme.input_activations\n\n            if weight_scheme and input_scheme:\n                # weight and activation quantization\n                # check schemes are supported\n                for scheme in [weight_scheme, input_scheme]:\n                    scheme_supported = (\n                        scheme.num_bits == 8\n                        and scheme.type in supported_scheme_types\n                    )\n                    if not scheme_supported:\n                        logger.info(\n                            \"Quantization scheme not supported,\"\n                            \" turning off sparse 24 compression.\"\n                            f\" Invalid Scheme: {scheme}\"\n                        )\n                        return False\n\n            elif weight_scheme or input_scheme:\n                # weight only quantization\n                logger.info(\n                    \"Weight only quantization detected, \"\n                    \"turning off sparse 24 compression.\"\n                )\n                return False\n\n    return True\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/","title":"llmcompressor.transformers.finetune","text":""},{"location":"reference/llmcompressor/transformers/finetune/callbacks/","title":"llmcompressor.transformers.finetune.callbacks","text":""},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.DisableHalfPrecisionCallback","title":"<code>DisableHalfPrecisionCallback</code>","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>TrainerCallback for disabling FP16 training before QAT training begins</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <p>LLM Compressor trainer that will call back into this object</p> required <code>args</code> <p>args to be passed to base TrainerCallback</p> <code>()</code> <code>kwargs</code> <p>key word arguments to be passed to base TrainerCallback</p> <code>{}</code> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>class DisableHalfPrecisionCallback(TrainerCallback):\n    \"\"\"\n    TrainerCallback for disabling FP16 training before QAT training begins\n\n    :param trainer: LLM Compressor trainer that will call back into this object\n    :param args: args to be passed to base TrainerCallback\n    :param kwargs: key word arguments to be passed to base TrainerCallback\n    \"\"\"\n\n    def __init__(self, trainer, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.trainer = trainer\n        self.on_begin_called = False\n        self.quant_start_epoch = math.inf\n\n    def qat_active(self) -&gt; bool:\n        \"\"\"\n        :return: True if a quantization modifier is active in the current session\n        \"\"\"\n        session = active_session()\n        return session.state.model.qat_active()\n\n    def on_epoch_begin(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        \"\"\"\n        Event called at the beginning of an epoch.\n        \"\"\"\n        super().on_epoch_begin(args, state, control, **kwargs)\n        self.on_begin_called = True\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.DisableHalfPrecisionCallback.on_epoch_begin","title":"<code>on_epoch_begin(args, state, control, **kwargs)</code>","text":"<p>Event called at the beginning of an epoch.</p> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>def on_epoch_begin(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    **kwargs,\n):\n    \"\"\"\n    Event called at the beginning of an epoch.\n    \"\"\"\n    super().on_epoch_begin(args, state, control, **kwargs)\n    self.on_begin_called = True\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.DisableHalfPrecisionCallback.qat_active","title":"<code>qat_active()</code>","text":"<p>Returns:</p> Type Description <code>bool</code> <p>True if a quantization modifier is active in the current session</p> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>def qat_active(self) -&gt; bool:\n    \"\"\"\n    :return: True if a quantization modifier is active in the current session\n    \"\"\"\n    session = active_session()\n    return session.state.model.qat_active()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.TrainingLoopCallbacks","title":"<code>TrainingLoopCallbacks</code>","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>TrainerCallback for triggering CompressionSession callbacks in the training loop. Used to update the model reference(for running with FSDP) and trigger the post- optim callbacks in each modifier.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <p>LLM Compressor trainer that will call back into this object</p> required <code>args</code> <p>args to be passed to base TrainerCallback</p> <code>()</code> <code>kwargs</code> <p>key word arguments to be passed to base TrainerCallback</p> <code>{}</code> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>class TrainingLoopCallbacks(TrainerCallback):\n    \"\"\"\n    TrainerCallback for triggering CompressionSession callbacks in the training loop.\n    Used to update the model reference(for running with FSDP) and trigger the post-\n    optim callbacks in each modifier.\n\n    :param trainer: LLM Compressor trainer that will call back into this object\n    :param args: args to be passed to base TrainerCallback\n    :param kwargs: key word arguments to be passed to base TrainerCallback\n    \"\"\"\n\n    def __init__(self, trainer, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.trainer = trainer\n\n    def on_train_begin(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        \"\"\"\n        Event called at the beginning of training. Update the session reference to the\n        model, as it will have changed to a wrapper if FSDP is enabled\n        \"\"\"\n        super().on_train_begin(args, state, control, **kwargs)\n        session = active_session()\n        session.state.model = self.trainer.model\n\n    def on_step_end(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        \"\"\"\n        Event called at the end of a training step. If using gradient accumulation,\n        one training step might take several inputs.\n\n        Triggers optimizer post_step and batch_end in the active CompressionSession\n        \"\"\"\n        super().on_step_end(args, state, control, **kwargs)\n        session_callbacks.optim_post_step()\n        session_callbacks.batch_end()\n\n    def on_substep_end(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        \"\"\"\n        Event called at the end of an substep during gradient accumulation.\n\n        Triggers optimizer post_step and batch_end in the active CompressionSession\n        \"\"\"\n        super().on_substep_end(args, state, control, **kwargs)\n        session_callbacks.optim_post_step()\n        session_callbacks.batch_end()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.TrainingLoopCallbacks.on_step_end","title":"<code>on_step_end(args, state, control, **kwargs)</code>","text":"<p>Event called at the end of a training step. If using gradient accumulation, one training step might take several inputs.</p> <p>Triggers optimizer post_step and batch_end in the active CompressionSession</p> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>def on_step_end(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    **kwargs,\n):\n    \"\"\"\n    Event called at the end of a training step. If using gradient accumulation,\n    one training step might take several inputs.\n\n    Triggers optimizer post_step and batch_end in the active CompressionSession\n    \"\"\"\n    super().on_step_end(args, state, control, **kwargs)\n    session_callbacks.optim_post_step()\n    session_callbacks.batch_end()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.TrainingLoopCallbacks.on_substep_end","title":"<code>on_substep_end(args, state, control, **kwargs)</code>","text":"<p>Event called at the end of an substep during gradient accumulation.</p> <p>Triggers optimizer post_step and batch_end in the active CompressionSession</p> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>def on_substep_end(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    **kwargs,\n):\n    \"\"\"\n    Event called at the end of an substep during gradient accumulation.\n\n    Triggers optimizer post_step and batch_end in the active CompressionSession\n    \"\"\"\n    super().on_substep_end(args, state, control, **kwargs)\n    session_callbacks.optim_post_step()\n    session_callbacks.batch_end()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/callbacks/#llmcompressor.transformers.finetune.callbacks.TrainingLoopCallbacks.on_train_begin","title":"<code>on_train_begin(args, state, control, **kwargs)</code>","text":"<p>Event called at the beginning of training. Update the session reference to the model, as it will have changed to a wrapper if FSDP is enabled</p> Source code in <code>src/llmcompressor/transformers/finetune/callbacks.py</code> <pre><code>def on_train_begin(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    **kwargs,\n):\n    \"\"\"\n    Event called at the beginning of training. Update the session reference to the\n    model, as it will have changed to a wrapper if FSDP is enabled\n    \"\"\"\n    super().on_train_begin(args, state, control, **kwargs)\n    session = active_session()\n    session.state.model = self.trainer.model\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/","title":"llmcompressor.transformers.finetune.session_mixin","text":""},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn","title":"<code>SessionManagerMixIn</code>","text":"<p>Mix-In class to extend the Hugging Face Trainer class to support LLM Compressor recipes for one-shot and finetuning flows.</p> <p>Parameters:</p> Name Type Description Default <code>recipe</code> <code>str</code> <p>path to recipe file to apply during training</p> required <code>recipe_args</code> <code>Optional[Union[Dict[str, Any], str]]</code> <p>additional kwargs to use for evaluating recipe</p> <code>None</code> <code>dataset_args</code> <code>Optional[DatasetArguments]</code> <p>kwargs for configuring dataset loading</p> <code>None</code> <code>teacher</code> <code>Optional[Union[Module, str]]</code> <p>optional teacher model to use for distillation</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>class SessionManagerMixIn:\n    \"\"\"\n    Mix-In class to extend the Hugging Face Trainer class to support LLM Compressor\n    recipes for one-shot and finetuning flows.\n\n    :param recipe: path to recipe file to apply during training\n    :param recipe_args: additional kwargs to use for evaluating recipe\n    :param dataset_args: kwargs for configuring dataset loading\n    :param teacher: optional teacher model to use for distillation\n    \"\"\"\n\n    def __init__(\n        self,\n        recipe: str,\n        model_args: \"ModelArguments\",\n        dataset_args: Optional[\"DatasetArguments\"] = None,\n        teacher: Optional[Union[Module, str]] = None,\n        recipe_args: Optional[Union[Dict[str, Any], str]] = None,\n        **kwargs,\n    ):\n        self.recipe = recipe\n        self.recipe_args = recipe_args\n        self.model_args = model_args\n        self.teacher = teacher\n\n        # parse training and metadata args\n        training_args = kwargs.get(\"args\")\n\n        self.metadata = None\n        if training_args is not None:\n            # trl_sft_trainer pathway. Both training_args and dataset_args\n            # have `max_seq_length` which causes collision error. This is the\n            # only shared parameter, where training arg is `TRLSFTConfig` that\n            # inherits HuggingFace's `TrainingArguments`\n            training_args_dict = training_args.to_dict()\n            if \"max_seq_length\" in training_args_dict:\n                training_args_dict[\"training_args_max_seq_length\"] = (\n                    training_args_dict.pop(\"max_seq_length\")\n                )\n                logger.warning(\n                    \"Detected `max_seq_length` in both dataset_args \",\n                    \"and training_args. This is expected for TRL in distillation. \",\n                    \"Updating metadata to `training_args_max_seq_length`\",\n                )\n\n            self.metadata = self._extract_metadata(\n                metadata_args=METADATA_ARGS,\n                training_args_dict=training_args_dict,\n                dataset_args_dict=asdict(dataset_args) if dataset_args else {},\n            )\n\n        # setup metrics and session\n        self.logger_manager = LoggerManager(log_python=False)\n        create_session()\n\n        # call Trainer initialization\n        super().__init__(**kwargs)\n        self.accelerator.wait_for_everyone()\n\n        # setup callbacks and loss\n        self.optim_callbacks = TrainingLoopCallbacks(self)\n        self.callback_handler.add_callback(self.optim_callbacks)\n        self.callback_disable_fp16 = DisableHalfPrecisionCallback(self)\n        self.callback_handler.add_callback(self.callback_disable_fp16)\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        model_signature = inspect.signature(self.model.forward)\n        self._signature_columns = list(model_signature.parameters.keys())\n\n        if self.teacher is not None and teacher not in (\"disable\", \"self\"):\n            teacher_signature = inspect.signature(self.teacher.forward)\n            self._teacher_signature_columns = list(teacher_signature.parameters.keys())\n        else:\n            self._teacher_signature_columns = None\n\n        if self.is_fsdp_enabled:\n            self._prepare_model_for_fsdp()\n\n        if dataset_args is not None:\n            self.min_tokens_per_module = dataset_args.min_tokens_per_module\n\n    def initialize_session(\n        self,\n        epoch: float,\n        checkpoint: Optional[str] = None,\n        stage: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the CompressionSession from the specified epoch, evaluates the recipe\n        and initialized the modifiers for the training session\n\n        :param epoch: Epoch to initialize session from, usually 0 unless loading\n        from a checkpoint\n        :param checkpoint: Optional checkpoint to initialize from to continue training\n        :param stage: Optional stage of recipe to run, or None to run all stages\n        \"\"\"\n        session = active_session()\n        if session.lifecycle.initialized_ or session.lifecycle.finalized:\n            return False\n\n        train_data = self.get_train_dataloader()\n\n        self.accelerator.wait_for_everyone()\n        with summon_full_params_context(self.model, offload_to_cpu=True):\n            active_session().initialize(\n                recipe=self.recipe,\n                recipe_stage=stage,\n                recipe_args=self.recipe_args,\n                model=self.model,\n                teacher_model=self.teacher,  # TODO: what about for self/disable?\n                train_data=train_data,\n                start=epoch,\n                copy_data=False,\n                attach_optim_callbacks=True,\n                fsdp_active=self.is_fsdp_enabled,\n                metadata=self.metadata,\n            )\n\n        self.accelerator.wait_for_everyone()\n        model = get_session_model()\n        self.model_wrapped = self.model = model\n\n        if self.recipe is None:\n            logger.warning(\n                \"No training recipe was provided, finetuning will be run \"\n                \"without event callbacks to LLM Compressor. To supply a recipe \"\n                \"pass a yaml file or string to the `recipe` argument.\"\n            )\n\n        torch.cuda.empty_cache()\n\n    def finalize_session(self):\n        \"\"\"\n        Wrap up training by finalizing all modifiers initialized in the current session\n        \"\"\"\n        session = active_session()\n        if not session.lifecycle.initialized_ or session.lifecycle.finalized:\n            return False\n\n        with summon_full_params_context(self.model, offload_to_cpu=True):\n            # in order to update each layer we need to gathers all its parameters\n            active_session().finalize()\n        logger.info(\"Finalized LLM Compressor session\")\n        model = get_session_model()\n        self.model = model\n        torch.cuda.empty_cache()\n\n    def create_optimizer(self):\n        \"\"\"\n        Override the optimizer to apply and update the recipe while training.\n        create_optimizer must exist in the parent class and should set\n        self.optimizer to the optimizer state and optionally set self.scaler\n        if using amp.\n        \"\"\"\n\n        self._check_super_defined(\"create_optimizer\")\n        super().create_optimizer()\n\n        # n_gpu handled internally by dataloader\n        total_batch_size = (\n            self.args.per_device_train_batch_size\n            * self.args.gradient_accumulation_steps\n        )\n\n        if isinstance(self.train_dataset, IterableDataset):\n            logger.warning(\n                \"Training is being run with a streamed dataset, \"\n                \"steps_per_epoch cannot be determined and will default to \"\n                \"1. LLM Compressor modifiers utilizing this statistic may not \"\n                \"behave as expected. \"\n            )\n            self.total_steps_per_epoch = 1\n        else:\n            self.total_steps_per_epoch = math.ceil(\n                len(self.train_dataset) / total_batch_size\n            )\n\n        active_session().initialize(\n            optimizer=self.optimizer, steps_per_epoch=self.total_steps_per_epoch\n        )\n\n        return self.optimizer\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: torch.optim.Optimizer = None\n    ):\n        \"\"\"\n        Create an LR scheduler to work with the applied recipes. This is a placeholder\n        that just calls the super method, but would be expanded upon if we ever\n        implement a LearningRateModifier.\n\n        :param num_training_steps: the total number of training steps\n        :param optimizer: pre-initialized optimizer\n        \"\"\"\n\n        # TODO: we don't currently have a LR scheduler in the new modifier framework\n        self._check_super_defined(\"create_scheduler\")\n        return super().create_scheduler(\n            num_training_steps=num_training_steps, optimizer=optimizer\n        )\n\n    def training_step(\n        self,\n        model: torch.nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        num_items_in_batch: Optional[int] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Overrides the Trainer's training step to trigger the batch_start callback to\n        the modifiers, then calls the parent function.\n\n        :param model: the model to compute the loss for\n        :param inputs: the inputs to pass through the model for calculating the loss\n        :return: output of the model\n        \"\"\"\n        self._check_super_defined(\"training_step\")\n\n        callbacks.batch_start(batch_data=inputs, global_step=self.state.epoch)\n        model_outputs = super().training_step(\n            model=model, inputs=inputs, num_items_in_batch=num_items_in_batch\n        )\n\n        return model_outputs\n\n    def compute_loss(\n        self,\n        model: Module,\n        inputs: Dict[str, Any],\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, Any]]:\n        \"\"\"\n        Override for the compute_loss to factor trigger callbacks and filter columns\n\n        :param model: the model to compute the loss for\n        :param inputs: the inputs to pass through the model for calculating the loss\n        :param return_outputs: True to return the outputs with the loss,\n            False otherwise\n        :return: the resulting loss if not return_outputs, otherwise a tuple\n            containing the loss and the model's outputs\n        \"\"\"\n        self._check_super_defined(\"compute_loss\")\n\n        # TODO: do we need these model signature columns?\n        inputs = {k: inputs[k] for k in inputs if k in self._signature_columns}\n        loss = super().compute_loss(\n            model=model,\n            inputs=inputs,\n            return_outputs=return_outputs,\n            num_items_in_batch=num_items_in_batch,\n        )\n\n        # take the mean across multiple GPUs\n        # this is done outside the compute_loss function in the parent, replicating it\n        # here for LLM Compressor logging and distillation\n        loss = loss.mean()\n\n        # Log step-wise loss and perplexity, for llama-recipes comparison\n        # we want this before distillation loss so perplexity isn't thrown off\n        do_log = self.state.global_step % self.args.logging_steps == 0\n        if do_log:\n            log = {}\n            log[\"step_loss\"] = loss.item()\n            log[\"perplexity\"] = torch.exp(loss).item()\n\n        if active_session().lifecycle.initialized_:\n            state = callbacks.loss_calculated(loss=loss)\n            if state and state.loss is not None:\n                loss = state.loss\n                if do_log:\n                    log[\"distill_step_loss\"] = loss.item() - log[\"step_loss\"]\n            callbacks.optim_pre_step()\n\n        if do_log:\n            self.log(log)\n\n        return loss\n\n    def train(self, *args, stage: Optional[str] = None, **kwargs):\n        \"\"\"\n        Run a sparsification training cycle. Runs initialization for the sparse session\n        before calling super().train() and finalization of the session after.\n\n        Logs sparsification details for the trained model.\n\n        :param args: positional args to pass to super().train()\n        :param stage: Optional stage of recipe to run, or None to run all stages\n        :param kwargs: keyword args to pass to super().train()\n        :return: the output from super.train()\n        \"\"\"\n\n        # lifecycle\n        checkpoint, epoch = self._calculate_checkpoint_info(kwargs)\n        self.initialize_session(epoch=epoch, checkpoint=checkpoint, stage=stage)\n\n        # do not save checkpoints as compressed\n        original_save_compressed = self.model_args.save_compressed\n        self.model_args.save_compressed = False\n\n        # train with accelerator\n        self.accelerator.wait_for_everyone()\n        output = super().train(*args, **kwargs)\n        self.accelerator.wait_for_everyone()\n\n        # restore original setting for saving final model\n        self.model_args.save_compressed = original_save_compressed\n\n        # lifecycle\n        self.finalize_session()\n        self.accelerator.wait_for_everyone()\n\n        # log model sparsity\n        self.maybe_log_model_sparsification()\n        self.accelerator.wait_for_everyone()\n\n        return output\n\n    # TODO: support all save args, not just skip_sparsity_compression_stats\n    def save_model(\n        self,\n        output_dir: str,\n        _internal_call: bool = False,\n        skip_sparsity_compression_stats: Optional[bool] = False,\n    ):\n        \"\"\"\n        Override of the save_model function and expects it to exist in the parent.\n        Calls into super() to save the model and additionally saves any recipes\n        that were used with the model within the model folder.\n\n        :param output_dir: the path to save the recipes into\n        :param _internal_call: True if this is an internal call from\n            the trainer in super(). Called from\n            self.save_model(output_dir, _internal_call=True)\n            in transformers/trainer/Trainer::_save_checkpoint\n\n        \"\"\"\n        if active_session() is None:\n            logger.warning(\n                \"No active session found, skipping saving of recipes and model.\"\n            )\n            return\n\n        # knowledge distillation requires making wrappers transparent during\n        if isinstance(self.model, KDModelWrapper):\n            self.model.prepare_for_save()  # TODO: move to finalize\n\n        # save checkpoint\n        self.save_state()\n        if self.accelerator.is_main_process:\n            processor = getattr(self, \"processing_class\", self.tokenizer)\n            # TODO: need to port over all saving parameters so that all\n            # checkpoints are saved in the same way\n            save_checkpoint(\n                output_dir,\n                model=self.model,\n                processor=processor,\n                save_safetensors=self.args.save_safetensors,\n                save_compressed=self.model_args.save_compressed,\n                skip_sparsity_compression_stats=skip_sparsity_compression_stats,\n            )\n        self.accelerator.wait_for_everyone()\n\n        if isinstance(self.model, KDModelWrapper):\n            self.model.finish_save()\n\n    def maybe_log_model_sparsification(self):\n        \"\"\"\n        Log info on model sparsity and quantization if possible. Only print logs on the\n        main process, and avoid logging for quantized FSDP models\n        \"\"\"\n        with summon_full_params_context(self.model, offload_to_cpu=True):\n            # offload to avoid OOM errors\n            if not self.accelerator.is_main_process:\n                # only calculate stats rank0 GPU\n                return\n            if self.is_fsdp_enabled and qat_active(self.model):\n                # due to state dict changes we can't log sparsity info with quantized\n                # models in FSDP\n                return\n\n            self.log_model_sparsification()\n\n    def log_model_sparsification(self):\n        \"\"\"\n        Log the current model sparsification info including pruned and quantized states\n        \"\"\"\n        sparsification_info = ModuleSparsificationInfo(self.model)\n\n        logger.info(\n            f\"Sparsification info for {type(self.model).__name__}: \"\n            f\"{sparsification_info.params_total} total params. \"\n        )\n        sparsity_percent_formatted = \"{:.2f}\".format(\n            sparsification_info.params_sparse_percent\n        )\n        logger.info(\n            f\"There are {sparsification_info.params_total} prunable \"\n            f\"params which have {sparsity_percent_formatted}% \"\n            \"avg sparsity.\"\n        )\n\n        quant_percent_formatted = \"{:.2f}\".format(\n            sparsification_info.params_quantized_percent\n        )\n        logger.info(\n            f\"There are {sparsification_info.params_total} quantizable \"\n            f\"params, with a quantization percentage of \"\n            f\"{quant_percent_formatted}%.\"\n        )\n\n    def _prepare_model_for_fsdp(self):\n        \"\"\"\n        Sets up FSDP ahead of time so we can run one-shot in FSDP mode\n        \"\"\"\n        self.model.to(\"cpu\")\n        self.model = self.accelerator.prepare(self.model)\n        self.accelerator.wait_for_everyone()\n\n        if self.teacher is not None:\n            self.teacher.to(\"cpu\")\n            for n, p in self.teacher.named_parameters():\n                p.requires_grad = False\n            self.teacher = self.accelerator.prepare(self.teacher)\n            self.teacher.eval()\n            self.accelerator.wait_for_everyone()\n\n    def _extract_metadata(\n        self,\n        metadata_args: List[str],\n        training_args_dict: Dict[str, Any],\n        dataset_args_dict: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        metadata = {}\n        if not training_args_dict.keys().isdisjoint(dataset_args_dict.keys()):\n            raise ValueError(\n                \"Found common keys in `training_args` and `data args`. \"\n                \"This is prohibitive and may lead to undesired behavior.\"\n            )\n\n        args_dict = {**training_args_dict, **dataset_args_dict}\n\n        for arg in metadata_args:\n            if arg not in args_dict.keys():\n                logger.warning(\n                    f\"Required metadata argument {arg} was not found \"\n                    f\"in the training arguments. Setting {arg} to None.\"\n                )\n                metadata[arg] = None\n            else:\n                metadata[arg] = args_dict[arg]\n\n        return metadata\n\n    def _check_super_defined(self, func: str):\n        if not hasattr(super(), func):\n            raise NotImplementedError(\n                f\"The super class for SessionManagerMixIn must define a {func} function\"\n            )\n\n    def _calculate_checkpoint_info(self, kwargs) -&gt; Tuple[Optional[str], float]:\n        \"\"\"\n        If resuming from checkpoint is set, get checkpoint and epoch to resume from\n        \"\"\"\n        checkpoint = None\n        epoch = 0.0\n\n        if not kwargs or \"resume_from_checkpoint\" not in kwargs:\n            logger.warning(\n                \"resume_from_checkpoint not passed into LLM Compressor Trainer.train. \"\n                \"This will cause issues with restoring recipes when \"\n                \"running from a checkpoint.\"\n            )\n        elif kwargs[\"resume_from_checkpoint\"]:\n            if (\n                isinstance(kwargs[\"resume_from_checkpoint\"], bool)\n                and kwargs[\"resume_from_checkpoint\"]\n            ):\n                checkpoint = get_last_checkpoint(self.args.output_dir)\n            else:\n                checkpoint = kwargs[\"resume_from_checkpoint\"]\n            epoch = TrainerState.load_from_json(\n                os.path.join(checkpoint, TRAINER_STATE_NAME)\n            ).epoch\n\n        return checkpoint, epoch\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.compute_loss","title":"<code>compute_loss(model, inputs, return_outputs=False, num_items_in_batch=None)</code>","text":"<p>Override for the compute_loss to factor trigger callbacks and filter columns</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model to compute the loss for</p> required <code>inputs</code> <code>Dict[str, Any]</code> <p>the inputs to pass through the model for calculating the loss</p> required <code>return_outputs</code> <code>bool</code> <p>True to return the outputs with the loss, False otherwise</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, Any]]</code> <p>the resulting loss if not return_outputs, otherwise a tuple containing the loss and the model's outputs</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def compute_loss(\n    self,\n    model: Module,\n    inputs: Dict[str, Any],\n    return_outputs: bool = False,\n    num_items_in_batch: Optional[int] = None,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, Any]]:\n    \"\"\"\n    Override for the compute_loss to factor trigger callbacks and filter columns\n\n    :param model: the model to compute the loss for\n    :param inputs: the inputs to pass through the model for calculating the loss\n    :param return_outputs: True to return the outputs with the loss,\n        False otherwise\n    :return: the resulting loss if not return_outputs, otherwise a tuple\n        containing the loss and the model's outputs\n    \"\"\"\n    self._check_super_defined(\"compute_loss\")\n\n    # TODO: do we need these model signature columns?\n    inputs = {k: inputs[k] for k in inputs if k in self._signature_columns}\n    loss = super().compute_loss(\n        model=model,\n        inputs=inputs,\n        return_outputs=return_outputs,\n        num_items_in_batch=num_items_in_batch,\n    )\n\n    # take the mean across multiple GPUs\n    # this is done outside the compute_loss function in the parent, replicating it\n    # here for LLM Compressor logging and distillation\n    loss = loss.mean()\n\n    # Log step-wise loss and perplexity, for llama-recipes comparison\n    # we want this before distillation loss so perplexity isn't thrown off\n    do_log = self.state.global_step % self.args.logging_steps == 0\n    if do_log:\n        log = {}\n        log[\"step_loss\"] = loss.item()\n        log[\"perplexity\"] = torch.exp(loss).item()\n\n    if active_session().lifecycle.initialized_:\n        state = callbacks.loss_calculated(loss=loss)\n        if state and state.loss is not None:\n            loss = state.loss\n            if do_log:\n                log[\"distill_step_loss\"] = loss.item() - log[\"step_loss\"]\n        callbacks.optim_pre_step()\n\n    if do_log:\n        self.log(log)\n\n    return loss\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.create_optimizer","title":"<code>create_optimizer()</code>","text":"<p>Override the optimizer to apply and update the recipe while training. create_optimizer must exist in the parent class and should set self.optimizer to the optimizer state and optionally set self.scaler if using amp.</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def create_optimizer(self):\n    \"\"\"\n    Override the optimizer to apply and update the recipe while training.\n    create_optimizer must exist in the parent class and should set\n    self.optimizer to the optimizer state and optionally set self.scaler\n    if using amp.\n    \"\"\"\n\n    self._check_super_defined(\"create_optimizer\")\n    super().create_optimizer()\n\n    # n_gpu handled internally by dataloader\n    total_batch_size = (\n        self.args.per_device_train_batch_size\n        * self.args.gradient_accumulation_steps\n    )\n\n    if isinstance(self.train_dataset, IterableDataset):\n        logger.warning(\n            \"Training is being run with a streamed dataset, \"\n            \"steps_per_epoch cannot be determined and will default to \"\n            \"1. LLM Compressor modifiers utilizing this statistic may not \"\n            \"behave as expected. \"\n        )\n        self.total_steps_per_epoch = 1\n    else:\n        self.total_steps_per_epoch = math.ceil(\n            len(self.train_dataset) / total_batch_size\n        )\n\n    active_session().initialize(\n        optimizer=self.optimizer, steps_per_epoch=self.total_steps_per_epoch\n    )\n\n    return self.optimizer\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.create_scheduler","title":"<code>create_scheduler(num_training_steps, optimizer=None)</code>","text":"<p>Create an LR scheduler to work with the applied recipes. This is a placeholder that just calls the super method, but would be expanded upon if we ever implement a LearningRateModifier.</p> <p>Parameters:</p> Name Type Description Default <code>num_training_steps</code> <code>int</code> <p>the total number of training steps</p> required <code>optimizer</code> <code>Optimizer</code> <p>pre-initialized optimizer</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def create_scheduler(\n    self, num_training_steps: int, optimizer: torch.optim.Optimizer = None\n):\n    \"\"\"\n    Create an LR scheduler to work with the applied recipes. This is a placeholder\n    that just calls the super method, but would be expanded upon if we ever\n    implement a LearningRateModifier.\n\n    :param num_training_steps: the total number of training steps\n    :param optimizer: pre-initialized optimizer\n    \"\"\"\n\n    # TODO: we don't currently have a LR scheduler in the new modifier framework\n    self._check_super_defined(\"create_scheduler\")\n    return super().create_scheduler(\n        num_training_steps=num_training_steps, optimizer=optimizer\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.finalize_session","title":"<code>finalize_session()</code>","text":"<p>Wrap up training by finalizing all modifiers initialized in the current session</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def finalize_session(self):\n    \"\"\"\n    Wrap up training by finalizing all modifiers initialized in the current session\n    \"\"\"\n    session = active_session()\n    if not session.lifecycle.initialized_ or session.lifecycle.finalized:\n        return False\n\n    with summon_full_params_context(self.model, offload_to_cpu=True):\n        # in order to update each layer we need to gathers all its parameters\n        active_session().finalize()\n    logger.info(\"Finalized LLM Compressor session\")\n    model = get_session_model()\n    self.model = model\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.initialize_session","title":"<code>initialize_session(epoch, checkpoint=None, stage=None)</code>","text":"<p>Initialize the CompressionSession from the specified epoch, evaluates the recipe and initialized the modifiers for the training session</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>float</code> <p>Epoch to initialize session from, usually 0 unless loading from a checkpoint</p> required <code>checkpoint</code> <code>Optional[str]</code> <p>Optional checkpoint to initialize from to continue training</p> <code>None</code> <code>stage</code> <code>Optional[str]</code> <p>Optional stage of recipe to run, or None to run all stages</p> <code>None</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def initialize_session(\n    self,\n    epoch: float,\n    checkpoint: Optional[str] = None,\n    stage: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the CompressionSession from the specified epoch, evaluates the recipe\n    and initialized the modifiers for the training session\n\n    :param epoch: Epoch to initialize session from, usually 0 unless loading\n    from a checkpoint\n    :param checkpoint: Optional checkpoint to initialize from to continue training\n    :param stage: Optional stage of recipe to run, or None to run all stages\n    \"\"\"\n    session = active_session()\n    if session.lifecycle.initialized_ or session.lifecycle.finalized:\n        return False\n\n    train_data = self.get_train_dataloader()\n\n    self.accelerator.wait_for_everyone()\n    with summon_full_params_context(self.model, offload_to_cpu=True):\n        active_session().initialize(\n            recipe=self.recipe,\n            recipe_stage=stage,\n            recipe_args=self.recipe_args,\n            model=self.model,\n            teacher_model=self.teacher,  # TODO: what about for self/disable?\n            train_data=train_data,\n            start=epoch,\n            copy_data=False,\n            attach_optim_callbacks=True,\n            fsdp_active=self.is_fsdp_enabled,\n            metadata=self.metadata,\n        )\n\n    self.accelerator.wait_for_everyone()\n    model = get_session_model()\n    self.model_wrapped = self.model = model\n\n    if self.recipe is None:\n        logger.warning(\n            \"No training recipe was provided, finetuning will be run \"\n            \"without event callbacks to LLM Compressor. To supply a recipe \"\n            \"pass a yaml file or string to the `recipe` argument.\"\n        )\n\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.log_model_sparsification","title":"<code>log_model_sparsification()</code>","text":"<p>Log the current model sparsification info including pruned and quantized states</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def log_model_sparsification(self):\n    \"\"\"\n    Log the current model sparsification info including pruned and quantized states\n    \"\"\"\n    sparsification_info = ModuleSparsificationInfo(self.model)\n\n    logger.info(\n        f\"Sparsification info for {type(self.model).__name__}: \"\n        f\"{sparsification_info.params_total} total params. \"\n    )\n    sparsity_percent_formatted = \"{:.2f}\".format(\n        sparsification_info.params_sparse_percent\n    )\n    logger.info(\n        f\"There are {sparsification_info.params_total} prunable \"\n        f\"params which have {sparsity_percent_formatted}% \"\n        \"avg sparsity.\"\n    )\n\n    quant_percent_formatted = \"{:.2f}\".format(\n        sparsification_info.params_quantized_percent\n    )\n    logger.info(\n        f\"There are {sparsification_info.params_total} quantizable \"\n        f\"params, with a quantization percentage of \"\n        f\"{quant_percent_formatted}%.\"\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.maybe_log_model_sparsification","title":"<code>maybe_log_model_sparsification()</code>","text":"<p>Log info on model sparsity and quantization if possible. Only print logs on the main process, and avoid logging for quantized FSDP models</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def maybe_log_model_sparsification(self):\n    \"\"\"\n    Log info on model sparsity and quantization if possible. Only print logs on the\n    main process, and avoid logging for quantized FSDP models\n    \"\"\"\n    with summon_full_params_context(self.model, offload_to_cpu=True):\n        # offload to avoid OOM errors\n        if not self.accelerator.is_main_process:\n            # only calculate stats rank0 GPU\n            return\n        if self.is_fsdp_enabled and qat_active(self.model):\n            # due to state dict changes we can't log sparsity info with quantized\n            # models in FSDP\n            return\n\n        self.log_model_sparsification()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.save_model","title":"<code>save_model(output_dir, _internal_call=False, skip_sparsity_compression_stats=False)</code>","text":"<p>Override of the save_model function and expects it to exist in the parent. Calls into super() to save the model and additionally saves any recipes that were used with the model within the model folder.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>the path to save the recipes into</p> required <code>_internal_call</code> <code>bool</code> <p>True if this is an internal call from the trainer in super(). Called from self.save_model(output_dir, _internal_call=True) in transformers/trainer/Trainer::_save_checkpoint</p> <code>False</code> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def save_model(\n    self,\n    output_dir: str,\n    _internal_call: bool = False,\n    skip_sparsity_compression_stats: Optional[bool] = False,\n):\n    \"\"\"\n    Override of the save_model function and expects it to exist in the parent.\n    Calls into super() to save the model and additionally saves any recipes\n    that were used with the model within the model folder.\n\n    :param output_dir: the path to save the recipes into\n    :param _internal_call: True if this is an internal call from\n        the trainer in super(). Called from\n        self.save_model(output_dir, _internal_call=True)\n        in transformers/trainer/Trainer::_save_checkpoint\n\n    \"\"\"\n    if active_session() is None:\n        logger.warning(\n            \"No active session found, skipping saving of recipes and model.\"\n        )\n        return\n\n    # knowledge distillation requires making wrappers transparent during\n    if isinstance(self.model, KDModelWrapper):\n        self.model.prepare_for_save()  # TODO: move to finalize\n\n    # save checkpoint\n    self.save_state()\n    if self.accelerator.is_main_process:\n        processor = getattr(self, \"processing_class\", self.tokenizer)\n        # TODO: need to port over all saving parameters so that all\n        # checkpoints are saved in the same way\n        save_checkpoint(\n            output_dir,\n            model=self.model,\n            processor=processor,\n            save_safetensors=self.args.save_safetensors,\n            save_compressed=self.model_args.save_compressed,\n            skip_sparsity_compression_stats=skip_sparsity_compression_stats,\n        )\n    self.accelerator.wait_for_everyone()\n\n    if isinstance(self.model, KDModelWrapper):\n        self.model.finish_save()\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.train","title":"<code>train(*args, stage=None, **kwargs)</code>","text":"<p>Run a sparsification training cycle. Runs initialization for the sparse session before calling super().train() and finalization of the session after.</p> <p>Logs sparsification details for the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional args to pass to super().train()</p> <code>()</code> <code>stage</code> <code>Optional[str]</code> <p>Optional stage of recipe to run, or None to run all stages</p> <code>None</code> <code>kwargs</code> <p>keyword args to pass to super().train()</p> <code>{}</code> <p>Returns:</p> Type Description <p>the output from super.train()</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def train(self, *args, stage: Optional[str] = None, **kwargs):\n    \"\"\"\n    Run a sparsification training cycle. Runs initialization for the sparse session\n    before calling super().train() and finalization of the session after.\n\n    Logs sparsification details for the trained model.\n\n    :param args: positional args to pass to super().train()\n    :param stage: Optional stage of recipe to run, or None to run all stages\n    :param kwargs: keyword args to pass to super().train()\n    :return: the output from super.train()\n    \"\"\"\n\n    # lifecycle\n    checkpoint, epoch = self._calculate_checkpoint_info(kwargs)\n    self.initialize_session(epoch=epoch, checkpoint=checkpoint, stage=stage)\n\n    # do not save checkpoints as compressed\n    original_save_compressed = self.model_args.save_compressed\n    self.model_args.save_compressed = False\n\n    # train with accelerator\n    self.accelerator.wait_for_everyone()\n    output = super().train(*args, **kwargs)\n    self.accelerator.wait_for_everyone()\n\n    # restore original setting for saving final model\n    self.model_args.save_compressed = original_save_compressed\n\n    # lifecycle\n    self.finalize_session()\n    self.accelerator.wait_for_everyone()\n\n    # log model sparsity\n    self.maybe_log_model_sparsification()\n    self.accelerator.wait_for_everyone()\n\n    return output\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/session_mixin/#llmcompressor.transformers.finetune.session_mixin.SessionManagerMixIn.training_step","title":"<code>training_step(model, inputs, num_items_in_batch=None)</code>","text":"<p>Overrides the Trainer's training step to trigger the batch_start callback to the modifiers, then calls the parent function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model to compute the loss for</p> required <code>inputs</code> <code>Dict[str, Union[Tensor, Any]]</code> <p>the inputs to pass through the model for calculating the loss</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output of the model</p> Source code in <code>src/llmcompressor/transformers/finetune/session_mixin.py</code> <pre><code>def training_step(\n    self,\n    model: torch.nn.Module,\n    inputs: Dict[str, Union[torch.Tensor, Any]],\n    num_items_in_batch: Optional[int] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Overrides the Trainer's training step to trigger the batch_start callback to\n    the modifiers, then calls the parent function.\n\n    :param model: the model to compute the loss for\n    :param inputs: the inputs to pass through the model for calculating the loss\n    :return: output of the model\n    \"\"\"\n    self._check_super_defined(\"training_step\")\n\n    callbacks.batch_start(batch_data=inputs, global_step=self.state.epoch)\n    model_outputs = super().training_step(\n        model=model, inputs=inputs, num_items_in_batch=num_items_in_batch\n    )\n\n    return model_outputs\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/text_generation/","title":"llmcompressor.transformers.finetune.text_generation","text":""},{"location":"reference/llmcompressor/transformers/finetune/trainer/","title":"llmcompressor.transformers.finetune.trainer","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/","title":"llmcompressor.transformers.finetune.data","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/base/","title":"llmcompressor.transformers.finetune.data.base","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/base/#llmcompressor.transformers.finetune.data.base.TextGenerationDataset","title":"<code>TextGenerationDataset</code>","text":"<p>               Bases: <code>RegistryMixin</code></p> <p>Base class for text datasets. Applies the following transformations to a dataset in order to prepare the dataset to be loaded by a dataloader</p> <ol> <li>Load dataset from huggingface or local cache</li> <li>Preprocess dataset according to preprocess function or chat/dataset template</li> <li>Tokenize dataset using model tokenizer/processor</li> <li>Apply post processing such as grouping text and/or adding labels for finetuning</li> </ol> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/base.py</code> <pre><code>class TextGenerationDataset(RegistryMixin):\n    \"\"\"\n    Base class for text datasets. Applies the following transformations to a dataset\n    in order to prepare the dataset to be loaded by a dataloader\n\n    1. Load dataset from huggingface or local cache\n    2. Preprocess dataset according to preprocess function or chat/dataset template\n    3. Tokenize dataset using model tokenizer/processor\n    4. Apply post processing such as grouping text and/or adding labels for finetuning\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    # used to mask out the prompt so prompt tokens do not contribute to training loss\n    PROMPT_KEY = \"prompt\"\n\n    def __init__(\n        self,\n        dataset_args: DatasetArguments,\n        split: str,\n        processor: Processor,\n    ):\n        self.dataset_args = dataset_args\n        self.split = split\n        self.processor = processor\n\n        # get tokenizer\n        self.tokenizer = getattr(self.processor, \"tokenizer\", self.processor)\n\n        if self.tokenizer is not None:\n            # fill in pad token\n            if not self.tokenizer.pad_token:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n\n            # configure sequence length\n            max_seq_length = dataset_args.max_seq_length\n            if dataset_args.max_seq_length &gt; self.tokenizer.model_max_length:\n                logger.warning(\n                    f\"The max_seq_length passed ({max_seq_length}) is larger than \"\n                    f\"maximum length for model ({self.tokenizer.model_max_length}). \"\n                    f\"Using max_seq_length={self.tokenizer.model_max_length}.\"\n                )\n            self.max_seq_length = min(\n                dataset_args.max_seq_length, self.tokenizer.model_max_length\n            )\n\n            # configure padding\n            self.padding = (\n                False\n                if self.dataset_args.concatenate_data\n                else \"max_length\"\n                if self.dataset_args.pad_to_max_length\n                else False\n            )\n\n        else:\n            self.max_seq_length = None\n            self.padding = False\n\n    def __call__(self, add_labels: bool = True) -&gt; DatasetType:\n        dataset = self.dataset_args.dataset\n\n        if isinstance(dataset, str):\n            # load dataset: load from huggingface or disk\n            dataset = self.load_dataset()\n        logger.debug(f\"Raw dataset: {get_columns(dataset)}\")\n\n        if self.preprocess is not None:\n            # preprocess: apply template or preprocessing function\n            dataset = self.map(\n                dataset,\n                self.preprocess,\n                batched=False,\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Preprocessing\",\n            )\n            logger.debug(f\"Dataset after preprocessing: {get_columns(dataset)}\")\n\n        # rename and remove columns match processor kwargs\n        dataset = self.rename_columns(dataset)\n        logger.debug(f\"Dataset after column renaming: {get_columns(dataset)}\")\n\n        # use processor.model_input_names to determine if the ds is already tokenized\n        model_input_names = getattr(self.processor, \"model_input_names\", [\"input_ids\"])\n        if not any(col_name in model_input_names for col_name in get_columns(dataset)):\n            # tokenize/ process\n            dataset = self.filter_tokenizer_args(dataset)\n            logger.debug(f\"Tokenizer args after filtering: {get_columns(dataset)}\")\n            dataset = self.map(\n                dataset,\n                self.tokenize,\n                batched=False,  # batching is not well supported for vision processors\n                keep_in_memory=True,  # bug occurs when not batched and not in memory,\n                # subsequent ds.map calls are always batched,\n                # regardless of `batched` argument\n                remove_columns=get_columns(dataset),  # assumes that input names\n                # and output names are disjoint\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Tokenizing\",\n            )\n            logger.debug(f\"Model kwargs after tokenizing: {get_columns(dataset)}\")\n\n        if self.dataset_args.concatenate_data:\n            # postprocess: group text\n            dataset = self.map(\n                dataset,\n                self.group_text,\n                batched=True,\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Concatenating data\",\n            )\n            logger.debug(f\"Model kwargs after concatenating: {get_columns(dataset)}\")\n\n        if add_labels:\n            # postprocess: add labels\n            dataset = self.map(\n                dataset,\n                self.add_labels,\n                batched=False,  # not compatible with batching, need row lengths\n                num_proc=self.dataset_args.preprocessing_num_workers,\n                load_from_cache_file=not self.dataset_args.overwrite_cache,\n                desc=\"Adding labels\",\n            )\n            logger.debug(f\"Model kwargs after adding labels: {get_columns(dataset)}\")\n\n        elif self.PROMPT_KEY in get_columns(dataset):\n            dataset = dataset.remove_columns(self.PROMPT_KEY)\n            logger.debug(\"Removed prompt key\")\n\n        logger.debug(f\"Model kwargs after postprocessing: {get_columns(dataset)}\")\n        return dataset\n\n    def load_dataset(self):\n        \"\"\"\n        Load the raw dataset from Hugging Face, using cached copy if available\n\n        :param cache_dir: disk location to search for cached dataset\n        :return: the requested dataset\n        \"\"\"\n        if self.dataset_args.dataset_path is not None:\n            if self.dataset_args.dvc_data_repository is not None:\n                self.dataset_args.raw_kwargs[\"storage_options\"] = {\n                    \"url\": self.dataset_args.dvc_data_repository\n                }\n                self.dataset_args.raw_kwargs[\"data_files\"] = (\n                    self.dataset_args.dataset_path\n                )\n            else:\n                self.dataset_args.raw_kwargs[\"data_files\"] = (\n                    get_custom_datasets_from_path(\n                        self.dataset_args.dataset_path,\n                        self.dataset_args.dataset\n                        if hasattr(self.dataset_args, \"dataset\")\n                        else self.dataset_args.dataset_name,\n                    )\n                )\n\n        logger.debug(f\"Loading dataset {self.dataset_args.dataset}\")\n        return get_raw_dataset(\n            self.dataset_args,\n            None,\n            split=self.split,\n            streaming=self.dataset_args.streaming,\n            **self.dataset_args.raw_kwargs,\n        )\n\n    @cached_property\n    def preprocess(self) -&gt; Union[Callable[[LazyRow], Any], None]:\n        \"\"\"\n        The function must return keys which correspond to processor/tokenizer kwargs,\n        optionally including PROMPT_KEY\n        \"\"\"\n        preprocessing_func = self.dataset_args.preprocessing_func\n\n        if callable(preprocessing_func):\n            return preprocessing_func\n\n        if isinstance(preprocessing_func, str):\n            if \":\" in preprocessing_func:\n                # load func_name from \"/path/to/file.py:func_name\"\n                return import_from_path(preprocessing_func)\n            else:\n                # load from the registry\n                return PreprocessingFunctionRegistry.get_value_from_registry(\n                    name=preprocessing_func\n                )\n\n        return self.dataset_template\n\n    @property\n    def dataset_template(self) -&gt; Union[Callable[[Any], Any], None]:\n        return None\n\n    def rename_columns(self, dataset: DatasetType) -&gt; DatasetType:\n        # rename columns to match processor/tokenizer kwargs\n        column_names = get_columns(dataset)\n        if self.dataset_args.text_column in column_names and \"text\" not in column_names:\n            logger.debug(f\"Renaming column `{self.dataset_args.text_column}` to `text`\")\n            dataset = dataset.rename_column(self.dataset_args.text_column, \"text\")\n\n        return dataset\n\n    def filter_tokenizer_args(self, dataset: DatasetType) -&gt; DatasetType:\n        # assumes that inputs are not passed via self.processor.__call__ args and kwargs\n        signature = inspect.signature(self.processor.__call__)\n        tokenizer_args = set(\n            key\n            for key, param in signature.parameters.items()\n            if param.kind not in (Kind.VAR_POSITIONAL, Kind.VAR_KEYWORD)\n        )\n        logger.debug(\n            f\"Found processor args `{tokenizer_args}`. Removing all other columns\"\n        )\n\n        column_names = get_columns(dataset)\n        return dataset.remove_columns(\n            list(set(column_names) - set(tokenizer_args) - set([self.PROMPT_KEY]))\n        )\n\n    def tokenize(self, data: LazyRow) -&gt; Dict[str, Any]:\n        # separate prompt\n        prompt = data.pop(self.PROMPT_KEY, None)\n\n        # tokenize\n        data = self.processor(\n            **data,\n            padding=self.padding,\n            max_length=self.max_seq_length,\n            truncation=True,\n        )\n\n        # store unpadded prompt so we can mask out correct number of elements in labels\n        if prompt is not None:\n            data[self.PROMPT_KEY] = self.processor(\n                text=prompt,\n                max_length=self.max_seq_length,\n                truncation=True,\n            )[\"input_ids\"]\n\n        return data\n\n    def group_text(self, data: LazyRow) -&gt; Dict[str, Any]:\n        concatenated_data = {k: sum(data[k], []) for k in data.keys()}\n        total_length = len(concatenated_data[list(data.keys())[0]])\n        total_length = (total_length // self.max_seq_length) * self.max_seq_length\n        result = {\n            k: [\n                t[i : i + self.max_seq_length]\n                for i in range(0, total_length, self.max_seq_length)\n            ]\n            for k, t in concatenated_data.items()\n        }\n        return result\n\n    def add_labels(self, data: LazyRow) -&gt; LazyRow:\n        if \"pixel_values\" in data:\n            raise NotImplementedError(\n                \"Label masking for vision datasets has not been implemented yet\"\n            )\n\n        # if the dataset uses prompts, mask them out so they don't contribute\n        # to the loss calculation\n        prompt_len = 0\n        if self.PROMPT_KEY in data:\n            prompt_len = len(data[self.PROMPT_KEY])\n        data[\"labels\"] = data[\"input_ids\"].copy()\n        data[\"labels\"][:prompt_len] = [LABELS_MASK_VALUE] * prompt_len\n\n        # mask out padding in the labels as well\n        padding = len(data[\"attention_mask\"]) - sum(data[\"attention_mask\"])\n        if padding &gt; 0:\n            data[\"labels\"][-padding:] = [LABELS_MASK_VALUE] * padding\n        return data\n\n    def map(\n        self,\n        dataset: Union[Dataset, IterableDataset],\n        function: Callable[[Any], Any],\n        **kwargs,\n    ) -&gt; Union[Dataset, IterableDataset]:\n        \"\"\"\n        Wrapper function around Dataset.map and IterableDataset.map.\n\n        If the dataset is streaming (in the case of IterableDataset), non-applicable\n        arguments are ignored and the dataset features are resolved\n        \"\"\"\n        if isinstance(dataset, IterableDataset):\n            # remove arguments that don't apply to streaming\n            kwargs.pop(\"num_proc\", None)\n            kwargs.pop(\"load_from_cache_file\", None)\n            kwargs.pop(\"desc\", None)\n            kwargs.pop(\"keep_in_memory\", None)\n\n        dataset = dataset.map(function, **kwargs)\n\n        if isinstance(dataset, IterableDataset):\n            dataset = dataset._resolve_features()\n\n        return dataset\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/base/#llmcompressor.transformers.finetune.data.base.TextGenerationDataset.preprocess","title":"<code>preprocess</code>  <code>cached</code> <code>property</code>","text":"<p>The function must return keys which correspond to processor/tokenizer kwargs, optionally including PROMPT_KEY</p>"},{"location":"reference/llmcompressor/transformers/finetune/data/base/#llmcompressor.transformers.finetune.data.base.TextGenerationDataset.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load the raw dataset from Hugging Face, using cached copy if available</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <p>disk location to search for cached dataset</p> required <p>Returns:</p> Type Description <p>the requested dataset</p> Source code in <code>src/llmcompressor/transformers/finetune/data/base.py</code> <pre><code>def load_dataset(self):\n    \"\"\"\n    Load the raw dataset from Hugging Face, using cached copy if available\n\n    :param cache_dir: disk location to search for cached dataset\n    :return: the requested dataset\n    \"\"\"\n    if self.dataset_args.dataset_path is not None:\n        if self.dataset_args.dvc_data_repository is not None:\n            self.dataset_args.raw_kwargs[\"storage_options\"] = {\n                \"url\": self.dataset_args.dvc_data_repository\n            }\n            self.dataset_args.raw_kwargs[\"data_files\"] = (\n                self.dataset_args.dataset_path\n            )\n        else:\n            self.dataset_args.raw_kwargs[\"data_files\"] = (\n                get_custom_datasets_from_path(\n                    self.dataset_args.dataset_path,\n                    self.dataset_args.dataset\n                    if hasattr(self.dataset_args, \"dataset\")\n                    else self.dataset_args.dataset_name,\n                )\n            )\n\n    logger.debug(f\"Loading dataset {self.dataset_args.dataset}\")\n    return get_raw_dataset(\n        self.dataset_args,\n        None,\n        split=self.split,\n        streaming=self.dataset_args.streaming,\n        **self.dataset_args.raw_kwargs,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/base/#llmcompressor.transformers.finetune.data.base.TextGenerationDataset.map","title":"<code>map(dataset, function, **kwargs)</code>","text":"<p>Wrapper function around Dataset.map and IterableDataset.map.</p> <p>If the dataset is streaming (in the case of IterableDataset), non-applicable arguments are ignored and the dataset features are resolved</p> Source code in <code>src/llmcompressor/transformers/finetune/data/base.py</code> <pre><code>def map(\n    self,\n    dataset: Union[Dataset, IterableDataset],\n    function: Callable[[Any], Any],\n    **kwargs,\n) -&gt; Union[Dataset, IterableDataset]:\n    \"\"\"\n    Wrapper function around Dataset.map and IterableDataset.map.\n\n    If the dataset is streaming (in the case of IterableDataset), non-applicable\n    arguments are ignored and the dataset features are resolved\n    \"\"\"\n    if isinstance(dataset, IterableDataset):\n        # remove arguments that don't apply to streaming\n        kwargs.pop(\"num_proc\", None)\n        kwargs.pop(\"load_from_cache_file\", None)\n        kwargs.pop(\"desc\", None)\n        kwargs.pop(\"keep_in_memory\", None)\n\n    dataset = dataset.map(function, **kwargs)\n\n    if isinstance(dataset, IterableDataset):\n        dataset = dataset._resolve_features()\n\n    return dataset\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/c4/","title":"llmcompressor.transformers.finetune.data.c4","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/c4/#llmcompressor.transformers.finetune.data.c4.C4Dataset","title":"<code>C4Dataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the C4 dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/c4.py</code> <pre><code>@TextGenerationDataset.register(name=\"c4\")\nclass C4Dataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the C4 dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"allenai/c4\"\n        dataset_args.text_column = \"text\"\n\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/cnn_dailymail/","title":"llmcompressor.transformers.finetune.data.cnn_dailymail","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/cnn_dailymail/#llmcompressor.transformers.finetune.data.cnn_dailymail.CNNDailyMailDataset","title":"<code>CNNDailyMailDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Text generation class for the CNN/DailyMail dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/cnn_dailymail.py</code> <pre><code>@TextGenerationDataset.register(name=\"cnn_dailymail\")\nclass CNNDailyMailDataset(TextGenerationDataset):\n    \"\"\"\n    Text generation class for the CNN/DailyMail dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    SAMPLE_TEMPLATE = \"Article:\\n{article}\\n\\n### Summarization:\\n{highlights}\\n\"\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"cnn_dailymail\"\n        dataset_args.dataset_config_name = \"3.0.0\"\n\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n\n    def dataset_template(self, sample):\n        return {\n            \"text\": self.SAMPLE_TEMPLATE.format(\n                article=sample[\"article\"], highlights=sample[\"highlights\"]\n            )\n        }\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/custom/","title":"llmcompressor.transformers.finetune.data.custom","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/custom/#llmcompressor.transformers.finetune.data.custom.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for custom local dataset supporting load for csv and json</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code> Can also be set to None to load all the splits</p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/custom.py</code> <pre><code>@TextGenerationDataset.register(name=\"custom\", alias=[\"json\", \"csv\"])\nclass CustomDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for custom local dataset supporting load\n    for csv and json\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n        Can also be set to None to load all the splits\n    :param processor: processor or tokenizer to use on dataset\n\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/data_helpers/","title":"llmcompressor.transformers.finetune.data.data_helpers","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/data_helpers/#llmcompressor.transformers.finetune.data.data_helpers.get_custom_datasets_from_path","title":"<code>get_custom_datasets_from_path(path, ext='json')</code>","text":"<p>Get a dictionary of custom datasets from a directory path. Support HF's load_dataset  for local folder datasets https://huggingface.co/docs/datasets/loading</p> <p>This function scans the specified directory path for files with a  specific extension (default is '.json'). It constructs a dictionary where the keys are either subdirectory names or  direct dataset names (depending on the directory structure) and the values are either file paths (if only one file exists with that name) or  lists of file paths (if multiple files exist).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <code>ext</code> <code>str</code> <p>The file extension to filter files by. Default is 'json'.</p> <code>'json'</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>A dictionary mapping dataset names to their file paths or lists of file paths.  Example: dataset = get_custom_datasets_from_path(\"/path/to/dataset/directory\", \"json\")  Note: If datasets are organized in subdirectories, the function constructs the dictionary with lists of file paths. If datasets are found directly in the main directory, they are included with their respective names.  Accepts: - path            train.json test.json val.json  - path            train                data1.json data2.json ... test                ... val                ...</p> Source code in <code>src/llmcompressor/transformers/finetune/data/data_helpers.py</code> <pre><code>def get_custom_datasets_from_path(path: str, ext: str = \"json\") -&gt; Dict[str, str]:\n    \"\"\"\n    Get a dictionary of custom datasets from a directory path. Support HF's load_dataset\n     for local folder datasets https://huggingface.co/docs/datasets/loading\n\n    This function scans the specified directory path for files with a\n     specific extension (default is '.json').\n    It constructs a dictionary where the keys are either subdirectory names or\n     direct dataset names (depending on the directory structure)\n    and the values are either file paths (if only one file exists with that name) or\n     lists of file paths (if multiple files exist).\n\n    :param path: The path to the directory containing the dataset files.\n    :param ext: The file extension to filter files by. Default is 'json'.\n\n    :return: A dictionary mapping dataset names to their file paths or lists of\n     file paths.\n\n    Example:\n        dataset = get_custom_datasets_from_path(\"/path/to/dataset/directory\", \"json\")\n\n    Note:\n        If datasets are organized in subdirectories, the function constructs the\n         dictionary with lists of file paths.\n        If datasets are found directly in the main directory, they are included with\n         their respective names.\n\n    Accepts:\n        - path\\\n            train.json\n            test.json\n            val.json\n\n        - path\\\n            train\\\n                data1.json\n                data2.json\n                ...\n            test\\\n                ...\n            val\\\n                ...\n\n    \"\"\"\n    data_files = {}\n\n    if any(filename.endswith(ext) for filename in os.listdir(path)):\n        # If there are files with the given extension in the path\n        for filename in os.listdir(path):\n            if filename.endswith(ext):\n                name, _ = os.path.splitext(filename)\n                data_files[name] = os.path.join(path, filename)\n    else:\n        # If datasets are organized in subdirectories\n        for root, dirs, files in os.walk(path):\n            for dir_name in dirs:\n                dir_path = os.path.join(root, dir_name)\n                dir_dataset = []\n                for filename in os.listdir(dir_path):\n                    if filename.endswith(ext):\n                        file_path = os.path.join(dir_path, filename)\n                        dir_dataset.append(file_path)\n                if dir_dataset:\n                    data_files[dir_name] = dir_dataset\n\n    return transform_dataset_keys(data_files)\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/data_helpers/#llmcompressor.transformers.finetune.data.data_helpers.get_raw_dataset","title":"<code>get_raw_dataset(dataset_args, cache_dir=None, streaming=False, **kwargs)</code>","text":"<p>Load the raw dataset from Hugging Face, using cached copy if available</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Optional[str]</code> <p>disk location to search for cached dataset</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>True to stream data from Hugging Face, otherwise download</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>the requested dataset</p> Source code in <code>src/llmcompressor/transformers/finetune/data/data_helpers.py</code> <pre><code>def get_raw_dataset(\n    dataset_args,\n    cache_dir: Optional[str] = None,\n    streaming: Optional[bool] = False,\n    **kwargs,\n) -&gt; Dataset:\n    \"\"\"\n    Load the raw dataset from Hugging Face, using cached copy if available\n\n    :param cache_dir: disk location to search for cached dataset\n    :param streaming: True to stream data from Hugging Face, otherwise download\n    :return: the requested dataset\n\n    \"\"\"\n    raw_datasets = load_dataset(\n        dataset_args.dataset,\n        dataset_args.dataset_config_name,\n        cache_dir=cache_dir,\n        streaming=streaming,\n        trust_remote_code=dataset_args.trust_remote_code_data,\n        **kwargs,\n    )\n    return raw_datasets\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/data_helpers/#llmcompressor.transformers.finetune.data.data_helpers.transform_dataset_keys","title":"<code>transform_dataset_keys(data_files)</code>","text":"<p>Transform dict keys to <code>train</code>, <code>val</code> or <code>test</code> for the given input dict if matches exist with the existing keys. Note that there can only be one matching file name. Ex. Folder(train_foo.json)           -&gt; Folder(train.json)     Folder(train1.json, train2.json) -&gt; Same</p> <p>Parameters:</p> Name Type Description Default <code>data_files</code> <code>Dict[str, Any]</code> <p>The dict where keys will be transformed</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/data_helpers.py</code> <pre><code>def transform_dataset_keys(data_files: Dict[str, Any]):\n    \"\"\"\n    Transform dict keys to `train`, `val` or `test` for the given input dict\n    if matches exist with the existing keys. Note that there can only be one\n    matching file name.\n    Ex. Folder(train_foo.json)           -&gt; Folder(train.json)\n        Folder(train1.json, train2.json) -&gt; Same\n\n    :param data_files: The dict where keys will be transformed\n    \"\"\"\n    keys = set(data_files.keys())\n\n    def transform_dataset_key(candidate: str) -&gt; None:\n        for key in keys:\n            if candidate in key:\n                if key == candidate:\n                    return\n                val = data_files.pop(key)\n                data_files[candidate] = val\n\n    def do_transform(candidate: str) -&gt; bool:\n        return sum(candidate in key for key in keys) == 1\n\n    dataset_keys = (\"train\", \"val\", \"test\")\n    for dataset_key in dataset_keys:\n        if do_transform(dataset_key):\n            transform_dataset_key(dataset_key)\n\n    return data_files\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/evolcodealpaca/","title":"llmcompressor.transformers.finetune.data.evolcodealpaca","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/evolcodealpaca/#llmcompressor.transformers.finetune.data.evolcodealpaca.EvolCodeAlpacaDataset","title":"<code>EvolCodeAlpacaDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the Evol Code Alpaca dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/evolcodealpaca.py</code> <pre><code>@TextGenerationDataset.register(name=\"evolcodealpaca\")\nclass EvolCodeAlpacaDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the Evol Code Alpaca dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    EVOL_ALPACA_TEMPLATE = (\n        \"Below is an instruction that describes a \"\n        \"programming task. Write a program that appropriately \"\n        \"completes the request.\\n\\n### Instruction:\\n{instruction}\"\n        \"\\n\\n### Response:\\n\"\n    )\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"theblackcat102/evol-codealpaca-v1\"\n        dataset_args.text_column = \"text\"\n\n        super().__init__(dataset_args, split=split, processor=processor)\n\n    def dataset_template(self, sample):\n        prompt = self.EVOL_ALPACA_TEMPLATE.format(instruction=sample[\"instruction\"])\n        text = prompt\n        if \"output\" in text:\n            text += sample[\"output\"]\n\n        return {\n            \"text\": text,\n            self.PROMPT_KEY: prompt,\n        }\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/flickr_30k/","title":"llmcompressor.transformers.finetune.data.flickr_30k","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/flickr_30k/#llmcompressor.transformers.finetune.data.flickr_30k.Flickr30K","title":"<code>Flickr30K</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/flickr_30k.py</code> <pre><code>@TextGenerationDataset.register(name=\"flickr\", alias=\"flickr30k\")\nclass Flickr30K(TextGenerationDataset):\n    \"\"\"\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    DEFAULT_CHAT_TEMPLATE = (\n        \"{% for message in messages %}\\n\"\n        \"{% if message['role'] == 'user' %}\\n\"\n        \"{{ '&lt;|user|&gt;\\n' + message['content'] + eos_token }}\\n\"\n        \"{% elif message['role'] == 'system' %}\\n\"\n        \"{{ '&lt;|system|&gt;\\n' + message['content'] + eos_token }}\\n\"\n        \"{% elif message['role'] == 'assistant' %}\\n\"\n        \"{{ '&lt;|assistant|&gt;\\n'  + message['content'] + eos_token }}\\n\"\n        \"{% endif %}\\n\"\n        \"{% if loop.last and add_generation_prompt %}\\n\"\n        \"{{ '&lt;|assistant|&gt;' }}\\n{% endif %}\\n{% endfor %}\"\n    )\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"lmms-lab/flickr30k\"\n\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n\n        if (\n            self.tokenizer is not None\n            and getattr(self.tokenizer, \"chat_template\", None) is None\n        ):\n            # note that since tokenizer is a member of processor,\n            # this change affects processor.apply_chat_template\n            self.tokenizer.chat_template = self.DEFAULT_CHAT_TEMPLATE\n            logger.warning(\n                \"tokenizer.chat_template is not set, using default chat template for \"\n                f\"{self.__class__.__name__}\"\n            )\n\n    def dataset_template(self, sample):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": \"What does this image show?\"},\n                ],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \" \".join(sample[\"caption\"]),\n            },\n        ]\n\n        return {\n            \"text\": self.processor.apply_chat_template(\n                messages,\n                add_generation_prompt=False,\n            ),\n            \"images\": sample[\"image\"],\n        }\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/gsm8k/","title":"llmcompressor.transformers.finetune.data.gsm8k","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/gsm8k/#llmcompressor.transformers.finetune.data.gsm8k.GSM8KDataset","title":"<code>GSM8KDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the Grade School Math 8k dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/gsm8k.py</code> <pre><code>@TextGenerationDataset.register(name=\"gsm8k\")\nclass GSM8KDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the Grade School Math 8k dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    GSM_TEMPLATE = \"Question: {question}\\nAnswer:\"\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"gsm8k\"\n        dataset_args.text_column = \"text\"\n\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n\n    def dataset_template(self, sample):\n        prompt = self.GSM_TEMPLATE.format(question=sample[\"question\"])\n        text = prompt\n        if \"answer\" in sample:\n            text += \" \" + sample[\"answer\"]\n\n        return {\n            \"text\": text,\n            self.PROMPT_KEY: prompt,\n        }\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/open_platypus/","title":"llmcompressor.transformers.finetune.data.open_platypus","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/open_platypus/#llmcompressor.transformers.finetune.data.open_platypus.OpenPlatypusDataset","title":"<code>OpenPlatypusDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the Open Platypus dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/open_platypus.py</code> <pre><code>@TextGenerationDataset.register(name=\"open_platypus\")\nclass OpenPlatypusDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the Open Platypus dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    ALPACA_TEMPLATE = {\n        \"prompt_input\": \"Below is an instruction that describes a task, paired with an \"\n        \"input that provides further context. Write a response that appropriately \"\n        \"completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n\"\n        \"{input}\\n\\n### Response:\\n\",\n        \"prompt_no_input\": \"Below is an instruction that describes a task. Write a \"\n        \"response that appropriately completes the request.\\n\\n### Instruction:\\n{\"\n        \"instruction}\\n\\n### Response:\\n\",\n    }\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"garage-bAInd/Open-Platypus\"\n        dataset_args.text_column = \"text\"\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n\n    def dataset_template(self, sample):\n        if \"input\" in sample and sample[\"input\"] != \"\":\n            prompt = self.ALPACA_TEMPLATE[\"prompt_input\"].format(\n                instruction=sample[\"instruction\"], input=sample[\"input\"]\n            )\n        else:\n            prompt = self.ALPACA_TEMPLATE[\"prompt_no_input\"].format(\n                instruction=sample[\"instruction\"]\n            )\n\n        text = prompt\n        if \"output\" in sample:\n            text += sample[\"output\"]\n\n        return {\n            \"text\": text,\n            self.PROMPT_KEY: prompt,\n        }\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/peoples_speech/","title":"llmcompressor.transformers.finetune.data.peoples_speech","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/peoples_speech/#llmcompressor.transformers.finetune.data.peoples_speech.PeoplesSpeech","title":"<code>PeoplesSpeech</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>ML Commons People's Speech audio dataset</p> <p>Unfortunately, due to the specialized nature of audio model preprocessing, some model specific code must be defined here. This dataset has been tested with the WhisperForConditionalGeneration and Qwen2AudioForConditionalGeneration model classes</p> <p>Parameters:</p> Name Type Description Default <code>data_args</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/peoples_speech.py</code> <pre><code>@TextGenerationDataset.register(name=\"peoples_speech\")\nclass PeoplesSpeech(TextGenerationDataset):\n    \"\"\"\n    ML Commons People's Speech audio dataset\n\n    Unfortunately, due to the specialized nature of audio model preprocessing, some\n    model specific code must be defined here. This dataset has been tested with the\n    WhisperForConditionalGeneration and Qwen2AudioForConditionalGeneration model classes\n\n    :param data_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    def __init__(self, dataset_args: \"DataArgs\", split: str, processor: Processor):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"MLCommons/peoples_speech\"\n        dataset_args.dataset_config_name = \"test\"\n        if not dataset_args.overwrite_cache:\n            logger.warning(\n                \"Because audio processors are more complex, dataset mapping functions \"\n                \"vary with model architecture and their results cannot be cached. \"\n                \"Setting overwrite_cache=True\"\n            )\n            dataset_args.overwrite_cache = True\n        self.processor_type = processor.__class__.__name__\n\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n\n    def dataset_template(self, example):\n        audio = example[\"audio\"][\"array\"]\n        sampling_rate = example[\"audio\"][\"sampling_rate\"]\n\n        if self.processor_type == \"Qwen2AudioProcessor\":\n            messages = [\n                {\"role\": \"user\", \"content\": [{\"audio\": None}]},\n                {\"role\": \"user\", \"content\": [{\"text\": \"What did the person say?\"}]},\n            ]\n            text = self.processor.apply_chat_template(messages)\n            return {\"audios\": [audio], \"sampling_rate\": sampling_rate, \"text\": text}\n\n        else:\n            # chat template decoder ids are appended later by self.processor.__call__\n            text = \" \" + example[\"text\"].capitalize()\n            return {\"audio\": audio, \"sampling_rate\": sampling_rate, \"text\": text}\n\n    def filter_tokenizer_args(self, dataset: DatasetType) -&gt; DatasetType:\n        if self.processor_type == \"WhisperProcessor\":\n            tokenizer_args = [\"audio\", \"sampling_rate\", \"text\"]\n            column_names = get_columns(dataset)\n\n            return dataset.remove_columns(list(set(column_names) - set(tokenizer_args)))\n\n        else:\n            return super().filter_tokenizer_args(dataset)\n\n    def tokenize(self, data: LazyRow) -&gt; Dict[str, Any]:\n        if self.processor_type == \"WhisperProcessor\":\n            inputs = self.processor(\n                audio=data[\"audio\"],\n                sampling_rate=data[\"sampling_rate\"],\n                text=data[\"text\"],\n                add_special_tokens=True,\n                return_tensors=\"pt\",\n            )\n\n            # TODO: inputs[\"input_features\"] is a float dtype, which may conflict with\n            # the dtype of the model. Add logic to in data pipeline to move inputs to\n            # the matching model device and dtype\n            inputs[\"decoder_input_ids\"] = inputs[\"labels\"]\n            del inputs[\"labels\"]\n\n            return inputs\n\n        else:\n            return super().tokenize(data)\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/ptb/","title":"llmcompressor.transformers.finetune.data.ptb","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/ptb/#llmcompressor.transformers.finetune.data.ptb.PtbDataset","title":"<code>PtbDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the PTB dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/ptb.py</code> <pre><code>@TextGenerationDataset.register(name=\"ptb\")\nclass PtbDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the PTB dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"ptb_text_only\"\n        dataset_args.text_column = \"sentence\"\n\n        super().__init__(\n            dataset_args=dataset_args,\n            split=split,\n            processor=processor,\n        )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/ultrachat_200k/","title":"llmcompressor.transformers.finetune.data.ultrachat_200k","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/ultrachat_200k/#llmcompressor.transformers.finetune.data.ultrachat_200k.UltraChatDataset","title":"<code>UltraChatDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the Ultra Chat 200k dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/ultrachat_200k.py</code> <pre><code>@TextGenerationDataset.register(name=\"ultrachat_200k\")\nclass UltraChatDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the Ultra Chat 200k dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    DEFAULT_CHAT_TEMPLATE = (\n        \"{% for message in messages %}\\n\"\n        \"{% if message['role'] == 'user' %}\\n\"\n        \"{{ '&lt;|user|&gt;\\n' + message['content'] + eos_token }}\\n\"\n        \"{% elif message['role'] == 'system' %}\\n\"\n        \"{{ '&lt;|system|&gt;\\n' + message['content'] + eos_token }}\\n\"\n        \"{% elif message['role'] == 'assistant' %}\\n\"\n        \"{{ '&lt;|assistant|&gt;\\n'  + message['content'] + eos_token }}\\n\"\n        \"{% endif %}\\n\"\n        \"{% if loop.last and add_generation_prompt %}\\n\"\n        \"{{ '&lt;|assistant|&gt;' }}\\n{% endif %}\\n{% endfor %}\"\n    )\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"HuggingFaceH4/ultrachat_200k\"\n        dataset_args.text_column = \"messages\"\n\n        if split in [\"train\", \"test\"]:\n            split += \"_sft\"\n\n        super().__init__(dataset_args=dataset_args, split=split, processor=processor)\n\n        if (\n            self.tokenizer is not None\n            and getattr(self.tokenizer, \"chat_template\", None) is None\n        ):\n            # note that since tokenizer is a member of processor,\n            # this change affects processor.apply_chat_template\n            self.tokenizer.chat_template = self.DEFAULT_CHAT_TEMPLATE\n            logger.warning(\n                \"tokenizer.chat_template is not set, using default chat template for \"\n                f\"{self.__class__.__name__}\"\n            )\n\n    def dataset_template(self, sample):\n        messages = sample[\"messages\"]\n        if messages[0][\"role\"] != \"system\":\n            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n\n        return {\n            \"text\": self.processor.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=False\n            )\n        }\n</code></pre>"},{"location":"reference/llmcompressor/transformers/finetune/data/wikitext/","title":"llmcompressor.transformers.finetune.data.wikitext","text":""},{"location":"reference/llmcompressor/transformers/finetune/data/wikitext/#llmcompressor.transformers.finetune.data.wikitext.WikiTextDataset","title":"<code>WikiTextDataset</code>","text":"<p>               Bases: <code>TextGenerationDataset</code></p> <p>Child text generation class for the Open Platypus dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset_args</code> <code>DatasetArguments</code> <p>configuration settings for dataset loading</p> required <code>split</code> <code>str</code> <p>split from dataset to load, for instance <code>test</code> or <code>train[:5%]</code></p> required <code>processor</code> <code>Processor</code> <p>processor or tokenizer to use on dataset</p> required Source code in <code>src/llmcompressor/transformers/finetune/data/wikitext.py</code> <pre><code>@TextGenerationDataset.register(name=\"wikitext\")\nclass WikiTextDataset(TextGenerationDataset):\n    \"\"\"\n    Child text generation class for the Open Platypus dataset\n\n    :param dataset_args: configuration settings for dataset loading\n    :param split: split from dataset to load, for instance `test` or `train[:5%]`\n    :param processor: processor or tokenizer to use on dataset\n    \"\"\"\n\n    def __init__(\n        self, dataset_args: \"DatasetArguments\", split: str, processor: Processor\n    ):\n        dataset_args = deepcopy(dataset_args)\n        dataset_args.dataset = \"Salesforce/wikitext\"\n        dataset_args.text_column = \"text\"\n\n        super().__init__(\n            dataset_args=dataset_args,\n            split=split,\n            processor=processor,\n        )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/sparsification/","title":"llmcompressor.transformers.sparsification","text":"<p>Objects, classes, and methods for applying sparsification algorithms to Hugging Face transformers flows</p>"},{"location":"reference/llmcompressor/transformers/sparsification/#llmcompressor.transformers.sparsification.get_processor_name_from_model","title":"<code>get_processor_name_from_model(student, teacher)</code>","text":"<p>Get a processor/tokenizer source used for both student and teacher, assuming that they could be shared</p> <p>Parameters:</p> Name Type Description Default <code>student</code> <code>Module</code> <p>the student model</p> required <code>teacher</code> <code>Optional[Module]</code> <p>the teacher model</p> required <p>Returns:</p> Type Description <code>str</code> <p>the source for the processor/tokenizer shared between teacher and model</p> Source code in <code>src/llmcompressor/transformers/sparsification/sparse_model.py</code> <pre><code>def get_processor_name_from_model(student: Module, teacher: Optional[Module]) -&gt; str:\n    \"\"\"\n    Get a processor/tokenizer source used for both student and teacher, assuming\n    that they could be shared\n\n    :param student: the student model\n    :param teacher: the teacher model\n    :return: the source for the processor/tokenizer shared between teacher and model\n    \"\"\"\n\n    if teacher is not None and teacher not in (\"disable\", \"self\"):\n        student_forward_params = list(\n            inspect.signature(student.forward).parameters.keys()\n        )\n        teacher_forward_params = list(\n            inspect.signature(teacher.forward).parameters.keys()\n        )\n        diff = [p for p in student_forward_params if p not in teacher_forward_params]\n        if diff:\n            raise RuntimeError(\n                \"Teacher tokenizer cannot be used for student \"\n                f\"due to missing args: {diff}\"\n            )\n        src_model = teacher\n    else:\n        src_model = student\n    return src_model.config._name_or_path\n</code></pre>"},{"location":"reference/llmcompressor/transformers/sparsification/compressed_tensors_utils/","title":"llmcompressor.transformers.sparsification.compressed_tensors_utils","text":""},{"location":"reference/llmcompressor/transformers/sparsification/compressed_tensors_utils/#llmcompressor.transformers.sparsification.compressed_tensors_utils.get_model_compressor","title":"<code>get_model_compressor(model, sparsity_config=None, quantization_format=None, save_compressed=True, skip_sparsity_compression_stats=True, state_dict=None, disable_sparse_compression=False)</code>","text":"<p>Obtain the compressor based on the config and the     quantization_format</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>torch model</p> required <code>sparsify_config</code> <p>Sparsity Compression config</p> required <code>quantization_format</code> <code>Optional[str]</code> <p>Format that the model was quantized to. if not provivided, will be extrapolated from <code>infer_quantization_format</code></p> <code>None</code> <code>save_compressed</code> <code>bool</code> <p>boolean representing to save in a compressed format</p> <code>True</code> <code>skip_sparsity_compression_stats</code> <code>bool</code> <p>bool allowing compression stats on std out</p> <code>True</code> <code>state_dict</code> <code>Optional[Dict]</code> <p>state_dict of the model</p> <code>None</code> <code>disable_sparse_compression</code> <code>bool</code> <p>bool to skip sparse compression</p> <code>False</code> Source code in <code>src/llmcompressor/transformers/sparsification/compressed_tensors_utils.py</code> <pre><code>def get_model_compressor(\n    model: torch.nn.Module,\n    sparsity_config: Optional[SparsityCompressionConfig] = None,\n    quantization_format: Optional[str] = None,\n    save_compressed: bool = True,\n    skip_sparsity_compression_stats: bool = True,\n    state_dict: Optional[Dict] = None,\n    disable_sparse_compression: bool = False,\n):\n    \"\"\"\n    Obtain the compressor based on the config and the\n        quantization_format\n\n    :param model: torch model\n    :param sparsify_config: Sparsity Compression config\n    :param quantization_format: Format that the model was quantized to.\n        if not provivided, will be extrapolated from `infer_quantization_format`\n    :param save_compressed: boolean representing to save in a compressed\n        format\n    :param skip_sparsity_compression_stats: bool allowing compression stats on std out\n    :param state_dict: state_dict of the model\n    :param disable_sparse_compression: bool to skip sparse compression\n    \"\"\"\n    # find offloaded state dict if none is provided\n    if state_dict is None:\n        state_dict = get_state_dict_offloaded_model(model)\n\n    if sparsity_config is None:\n        \"\"\"\n        Case 1: No sparsity config is provided\n            1. Will either skip sparsity compression\n            2. Or we will infer sparsity from the model directly\n\n        Check recipe for applied sparsity:\n            - Set skip_sparsity_compression_stats to False if don't find a\n                sparsity structure from the recipe\n            - If we identify sparsity based on the recipe or the user\n                set skip_sparsity_compression_stats to False, generate config\n        \"\"\"\n        sparsity_structure = SparsityConfigMetadata.infer_sparsity_structure(\n            model, check_only_modifiers=True\n        )\n        if sparsity_structure is not None:\n            skip_sparsity_compression_stats = False\n\n        if skip_sparsity_compression_stats:\n            logger.info(\n                \"skip_sparsity_compression_stats set to True. Skipping sparsity \"\n                \"compression statistic calculations. No sparsity compressor will \"\n                \"be applied.\"\n            )\n            sparsity_config = None\n        else:\n            sparsity_config = SparsityConfigMetadata.from_pretrained(\n                model,\n                state_dict=state_dict,\n                compress=save_compressed,\n                quantization_format=quantization_format,\n                disable_sparse_compression=disable_sparse_compression,\n                sparsity_structure=sparsity_structure,\n            )\n    else:\n        \"\"\"\n        # Case 2: User provides a Sparsity Config\n            - This is the case when there is existing sparsity in the\n                model that we'd like to account for while compressing\n            - Users should provide a SparsityConfig, conveying the model's\n                sparsity structure when saving the model\n        \"\"\"\n        if sparsity_config.sparsity_structure is None:\n            logger.info(\n                \"SparsityConfigMetadata provided without indicating \",\n                \"the sparsity structure. Sparisty will be inferred from the model. \"\n                \"Consider providing the structure to skip this step \",\n            )\n            sparsity_config.sparsity_structure = (\n                SparsityConfigMetadata.infer_sparsity_structure(model)\n            )\n\n    quantization_format: Optional[CompressionFormat] = infer_quantization_format(\n        model=model,\n        quantization_format=quantization_format,\n        save_compressed=save_compressed,\n        sparsity_structure=None\n        if sparsity_config is None\n        else sparsity_config.sparsity_structure,\n    )\n\n    return ModelCompressor.from_pretrained_model(\n        model,\n        sparsity_config=sparsity_config,\n        quantization_format=quantization_format,\n    )\n</code></pre>"},{"location":"reference/llmcompressor/transformers/sparsification/compressed_tensors_utils/#llmcompressor.transformers.sparsification.compressed_tensors_utils.modify_save_pretrained","title":"<code>modify_save_pretrained(model)</code>","text":"<p>Overrides a PreTrainedModel's save_pretrained() method with a wrapped version that supports compression. The new save_pretrained function performs the following saving operations:</p> <ol> <li>Saves the model state, potentially in a compressed format</li> <li>Saves the recipe, appending any current recipes to existing recipe files</li> <li>Copies any necessary python files from the model cache</li> </ol> Source code in <code>src/llmcompressor/transformers/sparsification/compressed_tensors_utils.py</code> <pre><code>def modify_save_pretrained(model: PreTrainedModel):\n    \"\"\"\n    Overrides a PreTrainedModel's save_pretrained() method with a wrapped version that\n    supports compression. The new save_pretrained function performs the following saving\n    operations:\n\n    1. Saves the model state, potentially in a compressed format\n    2. Saves the recipe, appending any current recipes to existing recipe files\n    3. Copies any necessary python files from the model cache\n    \"\"\"\n\n    def save_pretrained_compressed(save_pretrained_method):\n        if getattr(save_pretrained_method, \"_overridden\", False):\n            # `model.save_pretrained` has already been replaced, return.\n            return save_pretrained_method\n\n        # Keep a weak reference to the model class and unbound save_pretrained\n        # method so we can call the original\n        model_ref = weakref.ref(save_pretrained_method.__self__)\n        original_save_pretrained = save_pretrained_method.__func__\n        model_class = model_ref().__class__\n        del save_pretrained_method\n\n        @wraps(original_save_pretrained)\n        def save_pretrained_wrapper(\n            save_directory: str,\n            sparsity_config: Optional[SparsityCompressionConfig] = None,\n            quantization_format: Optional[str] = None,\n            save_compressed: bool = True,\n            safe_serialization: bool = True,\n            skip_sparsity_compression_stats: bool = True,\n            disable_sparse_compression: bool = False,\n            **kwargs,\n        ):\n            \"\"\"\n            Wrapper around PreTrainedModel.save_pretrained(), adds functionality for\n            saving models in a compressed format on disk. The compression format is\n            saved to the model's config file\n\n            :param save_directory: output directory to save model to\n            :param sparsity_config: optional sparsity config to compress model with,\n                if no config is provided it will be inferred from the model\n            :param quantization_format: optional compression format for quantized\n                models. If none is provided it will be inferred from the model\n            :param save_compressed: whether or not to compress the model on disk\n            :param skip_sparsity_compression_stats: whether to skip the calculation of\n                sparsity statistics (such as global sparsity and sparsity structure)\n                when saving a model in dense format\n            :param disable_sparse_compression: whether to skip sparse compression\n                during save, default is False\n            :param kwargs: additional kwargs to pass on to model.save_pretrained\n            \"\"\"\n\n            # HACK: Override the dtype_byte_size function in transformers to\n            # support float8 types. Fix is posted upstream\n            # https://github.com/huggingface/transformers/pull/30488\n            transformers.modeling_utils.dtype_byte_size = new_dtype_byte_size\n\n            # state_dict gets passed in as a kwarg for FSDP models\n            state_dict = kwargs.pop(\"state_dict\", None)\n            if state_dict is None:\n                logger.info(\"Fetching state_dict - this may take some time\")\n                state_dict = get_state_dict_offloaded_model(model)\n\n            logger.info(\"Fetching compressor\")\n            compressor = get_model_compressor(\n                model=model,\n                sparsity_config=sparsity_config,\n                quantization_format=quantization_format,\n                save_compressed=save_compressed,\n                skip_sparsity_compression_stats=skip_sparsity_compression_stats,\n                state_dict=state_dict,\n                disable_sparse_compression=disable_sparse_compression,\n            )\n\n            if compressor is None:\n                # model is not compressed or quantized, save as normal\n                original_save_pretrained_func = original_save_pretrained.__get__(\n                    model, model_class\n                )\n                original_save_pretrained_func(\n                    save_directory, state_dict=state_dict, **kwargs\n                )\n                return\n\n            # make sure we're on the main process when saving\n            if state_dict is not None and len(state_dict) &gt; 0:\n                compressed_state_dict = compressor.compress(model, state_dict)\n                logger.info(\"Saving compressed model to disk\")\n                original_save_pretrained.__get__(model, model_class)(\n                    save_directory,\n                    state_dict=compressed_state_dict,\n                    safe_serialization=safe_serialization,\n                    **kwargs,\n                )\n                compressor.update_config(save_directory)\n\n            # update existing recipe\n            update_and_save_recipe(model.name_or_path, save_directory)\n\n            # copy python files from cache dir to save_path if any\n            copy_python_files_from_model_cache(model, save_directory)\n\n        save_pretrained_wrapper._overridden = True\n        return save_pretrained_wrapper\n\n    # wrap save_pretrained if not already\n    if not getattr(model.save_pretrained, \"_overridden\", False):\n        model.save_pretrained = save_pretrained_compressed(model.save_pretrained)\n</code></pre>"},{"location":"reference/llmcompressor/transformers/sparsification/compressed_tensors_utils/#llmcompressor.transformers.sparsification.compressed_tensors_utils.patch_tied_tensors_bug","title":"<code>patch_tied_tensors_bug(model)</code>","text":"<p>Patches bug where HF transformers will fail to untie weights under specific circumstances (https://github.com/huggingface/transformers/issues/33689).</p> <p>This function detects those cases and unties the tensors if applicable</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to fix</p> required Source code in <code>src/llmcompressor/transformers/sparsification/compressed_tensors_utils.py</code> <pre><code>def patch_tied_tensors_bug(model: torch.nn.Module):\n    \"\"\"\n    Patches bug where HF transformers will fail to untie weights under specific\n    circumstances (https://github.com/huggingface/transformers/issues/33689).\n\n    This function detects those cases and unties the tensors if applicable\n\n    :param model: model to fix\n    \"\"\"\n    if (\n        hasattr(model.config, \"tie_word_embeddings\")\n        and not model.config.tie_word_embeddings\n    ):\n        input_embed = model.get_input_embeddings()\n        output_embed = model.get_output_embeddings()\n\n        if input_embed is None or output_embed is None:\n            # some models fail to properly override the abstract methods\n            return\n\n        if storage_ptr(input_embed.weight) == storage_ptr(output_embed.weight):\n            for module in (input_embed, output_embed):\n                if not is_module_offloaded(module):\n                    # create new storage ptr for onloaded weight\n                    untied_data = module.weight.data.clone()\n                    module.weight.data = untied_data\n                else:\n                    # create new storage ptr for offloaded weight\n                    # note `update_offload_parameter` does not create a new storage ptr\n                    untied_data = module._hf_hook.weights_map[\"weight\"].clone()\n                    update_offload_parameter(module, \"weight\", untied_data)\n</code></pre>"},{"location":"reference/llmcompressor/transformers/sparsification/compressed_tensors_utils/#llmcompressor.transformers.sparsification.compressed_tensors_utils.update_and_save_recipe","title":"<code>update_and_save_recipe(model_stub, save_directory)</code>","text":"<p>Save a recipe ontop of any existing recipe files located at model_stub</p> <p>Parameters:</p> Name Type Description Default <code>model_stub</code> <code>str</code> <p>path to existing model or model stub which may contain an existing recipe</p> required <code>save_directory</code> <code>str</code> <p>path to save combined existing recipe and current recipe</p> required Source code in <code>src/llmcompressor/transformers/sparsification/compressed_tensors_utils.py</code> <pre><code>def update_and_save_recipe(model_stub: str, save_directory: str):\n    \"\"\"\n    Save a recipe ontop of any existing recipe files located at model_stub\n\n    :param model_stub: path to existing model or model stub which may contain an\n        existing recipe\n    :param save_directory: path to save combined existing recipe and current recipe\n    \"\"\"\n    recipes_to_save = []\n    existing_recipe = infer_recipe_from_model_path(model_stub)\n    if existing_recipe is not None:\n        recipes_to_save.append(existing_recipe)\n\n    new_recipe = active_session().lifecycle.recipe_container.compiled_recipe\n    if new_recipe is not None:\n        recipes_to_save.append(new_recipe)\n\n    recipe = Recipe.simplify_combine_recipes(recipes_to_save)\n\n    # save recipe\n    recipe_path = os.path.join(save_directory, RECIPE_FILE_NAME)\n    recipe.yaml(recipe_path)\n</code></pre>"},{"location":"reference/llmcompressor/transformers/sparsification/sparse_model/","title":"llmcompressor.transformers.sparsification.sparse_model","text":""},{"location":"reference/llmcompressor/transformers/sparsification/sparse_model/#llmcompressor.transformers.sparsification.sparse_model.get_processor_name_from_model","title":"<code>get_processor_name_from_model(student, teacher)</code>","text":"<p>Get a processor/tokenizer source used for both student and teacher, assuming that they could be shared</p> <p>Parameters:</p> Name Type Description Default <code>student</code> <code>Module</code> <p>the student model</p> required <code>teacher</code> <code>Optional[Module]</code> <p>the teacher model</p> required <p>Returns:</p> Type Description <code>str</code> <p>the source for the processor/tokenizer shared between teacher and model</p> Source code in <code>src/llmcompressor/transformers/sparsification/sparse_model.py</code> <pre><code>def get_processor_name_from_model(student: Module, teacher: Optional[Module]) -&gt; str:\n    \"\"\"\n    Get a processor/tokenizer source used for both student and teacher, assuming\n    that they could be shared\n\n    :param student: the student model\n    :param teacher: the teacher model\n    :return: the source for the processor/tokenizer shared between teacher and model\n    \"\"\"\n\n    if teacher is not None and teacher not in (\"disable\", \"self\"):\n        student_forward_params = list(\n            inspect.signature(student.forward).parameters.keys()\n        )\n        teacher_forward_params = list(\n            inspect.signature(teacher.forward).parameters.keys()\n        )\n        diff = [p for p in student_forward_params if p not in teacher_forward_params]\n        if diff:\n            raise RuntimeError(\n                \"Teacher tokenizer cannot be used for student \"\n                f\"due to missing args: {diff}\"\n            )\n        src_model = teacher\n    else:\n        src_model = student\n    return src_model.config._name_or_path\n</code></pre>"},{"location":"reference/llmcompressor/transformers/tracing/","title":"llmcompressor.transformers.tracing","text":""},{"location":"reference/llmcompressor/transformers/tracing/debug/","title":"llmcompressor.transformers.tracing.debug","text":""},{"location":"reference/llmcompressor/transformers/tracing/debug/#llmcompressor.transformers.tracing.debug.trace","title":"<code>trace(model_id, model_class, sequential_targets=None, ignore=[], modality='text', trust_remote_code=True)</code>","text":"<p>Debug traceability by tracing a pre-trained model into subgraphs</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>stub of the model to load</p> required <code>model_class</code> <code>Type[PreTrainedModel]</code> <p>class constructor of the pre-trained model. Can use either HF transformers classes or <code>Traceable</code> classes defined by LLM Compressor</p> required <code>sequential_targets</code> <code>Optional[Union[List[str], str]]</code> <p>targets for sequential tracing, defaults to automatic inference</p> <code>None</code> <code>ignore</code> <code>Union[List[str], str]</code> <p>patterns to ignore during tracing</p> <code>[]</code> <code>modality</code> <code>str</code> <p>data modality for dummy tracing data, defaults to 'text'</p> <code>'text'</code> <code>trust_remote_code</code> <code>bool</code> <p>trust remote model code  Example usage from CLI llmcompressor.trace         --model_id Qwen/Qwen2-VL-2B-Instruct         --model_class Qwen2VLForConditionalGeneration         --sequential_targets Qwen2VLDecoderLayer         --ignore \"lm_head\" \"re:visual.*\"         --modality text</p> <code>True</code> Source code in <code>src/llmcompressor/transformers/tracing/debug.py</code> <pre><code>def trace(\n    model_id: str,\n    model_class: Type[PreTrainedModel],\n    sequential_targets: Optional[Union[List[str], str]] = None,\n    ignore: Union[List[str], str] = [],\n    modality: str = \"text\",\n    trust_remote_code: bool = True\n):\n    \"\"\"\n    Debug traceability by tracing a pre-trained model into subgraphs\n\n    :param model_id: stub of the model to load\n    :param model_class: class constructor of the pre-trained model. Can use either\n        HF transformers classes or `Traceable` classes defined by LLM Compressor\n    :param sequential_targets: targets for sequential tracing, defaults to automatic\n        inference\n    :param ignore: patterns to ignore during tracing\n    :param modality: data modality for dummy tracing data, defaults to 'text'\n    :param trust_remote_code: trust remote model code\n\n    Example usage from CLI\n    llmcompressor.trace \\\n        --model_id Qwen/Qwen2-VL-2B-Instruct \\\n        --model_class Qwen2VLForConditionalGeneration \\\n        --sequential_targets Qwen2VLDecoderLayer \\\n        --ignore \"lm_head\" \"re:visual.*\" \\\n        --modality text\n    \"\"\"\n    # Load model\n    with skip_weights_download(model_class):\n        model = model_class.from_pretrained(\n            model_id,\n            device_map=\"cpu\",\n            torch_dtype=\"auto\",\n            trust_remote_code=trust_remote_code,\n        )\n    processor = AutoProcessor.from_pretrained(\n        model_id, trust_remote_code=trust_remote_code\n    )\n    print(\"Loaded model\")\n\n    # Prepare sample data\n    dataset_args = DatasetArguments(**get_dataset_kwargs(modality))\n    dataset = TextGenerationDataset.load_from_registry(\n        dataset_args.dataset,\n        dataset_args=dataset_args,\n        split=dataset_args.splits[\"calibration\"],\n        processor=processor,\n    )(add_labels=False)\n    sample_input = next(iter(dataset))\n    sample_input = {k: torch.tensor(v) for k, v in sample_input.items()}\n    print(\"Loaded sample data\")\n\n    # infer sequential targets\n    if sequential_targets is None:\n        sequential_targets = get_no_split_params(model)\n    if isinstance(sequential_targets, str):\n        sequential_targets = [sequential_targets]\n\n    # infer ignore\n    if isinstance(ignore, str):\n        ignore = [ignore]\n\n    # Attempt trace\n    print(\n        \"\\nAttempting trace\\n\"\n        f\"    model_id={model_id}\\n\"\n        f\"    model_class={model_class.__name__}\\n\"\n        f\"    dataset={dataset_args.dataset}\\n\"\n        f\"    split={dataset.split}\\n\"\n        f\"    inputs={sample_input.keys()}\\n\"\n        f\"    sequential_targets={sequential_targets}\\n\"\n        f\"    ignore={ignore}\\n\"\n    )\n    subgraphs = trace_subgraphs(model, sample_input, sequential_targets, ignore)\n    print(f\"Successfully traced model into {len(subgraphs)} subgraphs!\\n\")\n</code></pre>"},{"location":"reference/llmcompressor/transformers/tracing/gemma3/","title":"llmcompressor.transformers.tracing.gemma3","text":"<p>PyTorch Gemma3 model.</p>"},{"location":"reference/llmcompressor/transformers/tracing/idefics3/","title":"llmcompressor.transformers.tracing.idefics3","text":"<p>PyTorch Idefics3 model.</p>"},{"location":"reference/llmcompressor/transformers/tracing/llava/","title":"llmcompressor.transformers.tracing.llava","text":"<p>PyTorch Llava model.</p>"},{"location":"reference/llmcompressor/transformers/tracing/mllama/","title":"llmcompressor.transformers.tracing.mllama","text":"<p>PyTorch Mllama model.</p>"},{"location":"reference/llmcompressor/transformers/tracing/qwen2_5_vl/","title":"llmcompressor.transformers.tracing.qwen2_5_vl","text":"<p>PyTorch Qwen2-5-VL model.</p>"},{"location":"reference/llmcompressor/transformers/tracing/qwen2_vl/","title":"llmcompressor.transformers.tracing.qwen2_vl","text":"<p>PyTorch Qwen2-VL model.</p>"},{"location":"reference/llmcompressor/transformers/tracing/qwen2_vl/#llmcompressor.transformers.tracing.qwen2_vl.get_rope_index","title":"<code>get_rope_index(config, input_ids, image_grid_thw=None, video_grid_thw=None, attention_mask=None)</code>","text":"<p>Calculate the 3D rope index based on image and video's temporal, height and width in LLM.</p> <p>Returns:     position_ids (<code>torch.LongTensor</code> of shape <code>(3, batch_size, sequence_length)</code>)     mrope_position_deltas (<code>torch.Tensor</code> of shape <code>(batch_size)</code>)</p> Source code in <code>src/llmcompressor/transformers/tracing/qwen2_vl.py</code> <pre><code>@torch.fx.wrap\ndef get_rope_index(\n    config: Qwen2VLConfig,\n    input_ids: torch.LongTensor,\n    image_grid_thw: Optional[torch.LongTensor] = None,\n    video_grid_thw: Optional[torch.LongTensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Calculate the 3D rope index based on image and video's temporal, height and width in LLM.\n\n    Returns:\n        position_ids (`torch.LongTensor` of shape `(3, batch_size, sequence_length)`)\n        mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)\n    \"\"\"\n    spatial_merge_size = config.vision_config.spatial_merge_size\n    image_token_id = config.image_token_id\n    video_token_id = config.video_token_id\n    vision_start_token_id = config.vision_start_token_id\n    mrope_position_deltas = []\n    if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n        total_input_ids = input_ids\n        if attention_mask is None:\n            attention_mask = torch.ones_like(total_input_ids)\n        position_ids = torch.ones(\n            3, input_ids.shape[0], input_ids.shape[1], dtype=input_ids.dtype, device=input_ids.device\n        )\n        image_index, video_index = 0, 0\n        # TRACING: cannot iterate input ids\n        for i, input_ids in enumerate(total_input_ids):\n            input_ids = input_ids[attention_mask[i] == 1]\n            image_nums, video_nums = 0, 0\n            vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n            vision_tokens = input_ids[vision_start_indices + 1]\n            image_nums = (vision_tokens == image_token_id).sum()\n            video_nums = (vision_tokens == video_token_id).sum()\n            input_tokens = input_ids.tolist()\n            llm_pos_ids_list: list = []\n            st = 0\n            remain_images, remain_videos = image_nums, video_nums\n            for _ in range(image_nums + video_nums):\n                if image_token_id in input_tokens and remain_images &gt; 0:\n                    ed_image = input_tokens.index(image_token_id, st)\n                else:\n                    ed_image = len(input_tokens) + 1\n                if video_token_id in input_tokens and remain_videos &gt; 0:\n                    ed_video = input_tokens.index(video_token_id, st)\n                else:\n                    ed_video = len(input_tokens) + 1\n                if ed_image &lt; ed_video:\n                    t, h, w = (\n                        image_grid_thw[image_index][0],\n                        image_grid_thw[image_index][1],\n                        image_grid_thw[image_index][2],\n                    )\n                    image_index += 1\n                    remain_images -= 1\n                    ed = ed_image\n                else:\n                    t, h, w = (\n                        video_grid_thw[video_index][0],\n                        video_grid_thw[video_index][1],\n                        video_grid_thw[video_index][2],\n                    )\n                    video_index += 1\n                    remain_videos -= 1\n                    ed = ed_video\n                llm_grid_t, llm_grid_h, llm_grid_w = (\n                    t.item(),\n                    h.item() // spatial_merge_size,\n                    w.item() // spatial_merge_size,\n                )\n                text_len = ed - st\n\n                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) &gt; 0 else 0\n                llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n\n                t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()\n                h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()\n                w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()\n                llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)\n                st = ed + llm_grid_t * llm_grid_h * llm_grid_w\n\n            if st &lt; len(input_tokens):\n                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) &gt; 0 else 0\n                text_len = len(input_tokens) - st\n                llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n\n            llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n            position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n            mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))\n        mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n        return position_ids, mrope_position_deltas\n    else:\n        if attention_mask is not None:\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)\n            max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]\n            mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]\n        else:\n            position_ids = (\n                torch.arange(input_ids.shape[1], device=input_ids.device)\n                .view(1, 1, -1)\n                .expand(3, input_ids.shape[0], -1)\n            )\n            mrope_position_deltas = torch.zeros(\n                [input_ids.shape[0], 1],\n                device=input_ids.device,\n                dtype=input_ids.dtype,\n            )\n\n        return position_ids, mrope_position_deltas\n</code></pre>"},{"location":"reference/llmcompressor/transformers/utils/","title":"llmcompressor.transformers.utils","text":"<p>Utilities for applying sparsification algorithms to Hugging Face transformers flows</p>"},{"location":"reference/llmcompressor/transformers/utils/#llmcompressor.transformers.utils.is_model_ct_quantized_from_path","title":"<code>is_model_ct_quantized_from_path(path)</code>","text":"<p>Determine if model from path is quantized based on the config</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the model or HF stub</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if config contains quantization_config from the given path</p> Source code in <code>src/llmcompressor/transformers/utils/helpers.py</code> <pre><code>def is_model_ct_quantized_from_path(path: str) -&gt; bool:\n    \"\"\"\n    Determine if model from path is quantized based\n    on the config\n\n    :param path: path to the model or HF stub\n    :return: True if config contains quantization_config from the given path\n\n    \"\"\"\n    config = AutoConfig.from_pretrained(path)\n    if config is not None:\n        if (\n            hasattr(config, \"quantization_config\")\n            and config.quantization_config[\"quant_method\"] == \"compressed-tensors\"\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/transformers/utils/helpers/","title":"llmcompressor.transformers.utils.helpers","text":"<p>Helper variables and functions for integrating LLM Compressor with huggingface/transformers flows</p>"},{"location":"reference/llmcompressor/transformers/utils/helpers/#llmcompressor.transformers.utils.helpers.infer_recipe_from_model_path","title":"<code>infer_recipe_from_model_path(model_path)</code>","text":"<p>Infer the recipe from the model_path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Union[str, Path]</code> <p>The path to the model to load. It can be one of the following: - a path to the model directory - a path to the model file - Hugging face model ID</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The path to the recipe file if found, None otherwise.</p> Source code in <code>src/llmcompressor/transformers/utils/helpers.py</code> <pre><code>def infer_recipe_from_model_path(model_path: Union[str, Path]) -&gt; Optional[str]:\n    \"\"\"\n    Infer the recipe from the model_path.\n\n    :param model_path: The path to the model to load. It can be one of the following:\n        - a path to the model directory\n        - a path to the model file\n        - Hugging face model ID\n    :return: The path to the recipe file if found, None otherwise.\n    \"\"\"\n    model_path = model_path.as_posix() if isinstance(model_path, Path) else model_path\n\n    if os.path.isdir(model_path) or os.path.isfile(model_path):\n        # Model path is a local path to the model directory or file\n        model_path = (\n            os.path.dirname(model_path) if os.path.isfile(model_path) else model_path\n        )\n        recipe = os.path.join(model_path, RECIPE_FILE_NAME)\n\n        if os.path.isfile(recipe):\n            logger.info(f\"Found recipe in the model_path: {recipe}\")\n            return recipe\n        logger.debug(f\"No recipe found in the model_path: {model_path}\")\n        return None\n\n    # If the model path is a Hugging Face model ID\n    recipe = recipe_from_huggingface_model_id(hf_stub=model_path)\n\n    if recipe is None:\n        logger.debug(\"Failed to infer the recipe from the model_path\")\n\n    return recipe\n</code></pre>"},{"location":"reference/llmcompressor/transformers/utils/helpers/#llmcompressor.transformers.utils.helpers.is_model_ct_quantized_from_path","title":"<code>is_model_ct_quantized_from_path(path)</code>","text":"<p>Determine if model from path is quantized based on the config</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the model or HF stub</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if config contains quantization_config from the given path</p> Source code in <code>src/llmcompressor/transformers/utils/helpers.py</code> <pre><code>def is_model_ct_quantized_from_path(path: str) -&gt; bool:\n    \"\"\"\n    Determine if model from path is quantized based\n    on the config\n\n    :param path: path to the model or HF stub\n    :return: True if config contains quantization_config from the given path\n\n    \"\"\"\n    config = AutoConfig.from_pretrained(path)\n    if config is not None:\n        if (\n            hasattr(config, \"quantization_config\")\n            and config.quantization_config[\"quant_method\"] == \"compressed-tensors\"\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/transformers/utils/helpers/#llmcompressor.transformers.utils.helpers.recipe_from_huggingface_model_id","title":"<code>recipe_from_huggingface_model_id(hf_stub, recipe_file_name=RECIPE_FILE_NAME)</code>","text":"<p>Attempts to download the recipe from the Hugging Face model ID.</p> <p>Parameters:</p> Name Type Description Default <code>hf_stub</code> <code>str</code> <p>Assumed to be the Hugging Face model ID.</p> required <code>recipe_file_name</code> <code>str</code> <p>The name of the recipe file to download. Defaults to RECIPE_FILE_NAME.</p> <code>RECIPE_FILE_NAME</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>A tuple: - The path to the recipe file if found, None otherwise. - True if hf_stub is a valid Hugging Face model ID, False otherwise.</p> Source code in <code>src/llmcompressor/transformers/utils/helpers.py</code> <pre><code>def recipe_from_huggingface_model_id(\n    hf_stub: str, recipe_file_name: str = RECIPE_FILE_NAME\n) -&gt; Optional[str]:\n    \"\"\"\n    Attempts to download the recipe from the Hugging Face model ID.\n\n    :param hf_stub: Assumed to be the Hugging Face model ID.\n    :param recipe_file_name: The name of the recipe file to download.\n     Defaults to RECIPE_FILE_NAME.\n    :return: A tuple:\n        - The path to the recipe file if found, None otherwise.\n        - True if hf_stub is a valid Hugging Face model ID, False otherwise.\n    \"\"\"\n    model_id_url = os.path.join(HUGGINGFACE_CO_URL_HOME, hf_stub)\n    request = requests.head(model_id_url)\n\n    if request.status_code != 200:\n        logger.debug(\n            (\n                \"hf_stub is not a valid Hugging Face model ID. \",\n                \"Skipping recipe resolution.\",\n            )\n        )\n        return None\n\n    try:\n        recipe = hf_hub_download(repo_id=hf_stub, filename=recipe_file_name)\n        logger.info(f\"Found recipe: {recipe_file_name} for model ID: {hf_stub}.\")\n    except Exception as e:  # TODO: narrow acceptable exceptions\n        logger.debug(\n            (\n                f\"Unable to find recipe {recipe_file_name} \"\n                f\"for model ID: {hf_stub}: {e}.\"\n                \"Skipping recipe resolution.\"\n            )\n        )\n        recipe = None\n\n    return recipe\n</code></pre>"},{"location":"reference/llmcompressor/transformers/utils/preprocessing_functions/","title":"llmcompressor.transformers.utils.preprocessing_functions","text":""},{"location":"reference/llmcompressor/utils/","title":"llmcompressor.utils","text":"<p>General utility functions used throughout llmcompressor</p>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.DisableKVCache","title":"<code>DisableKVCache</code>","text":"<p>Temporarily disable the key-value cache for transformer models. Used to prevent excess memory use in one-shot cases where the model only performs the prefill phase and not the generation phase.</p> <p>Example:</p> <p>model = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\") input = torch.randint(0, 32, size=(1, 32)) with DisableKVCache(model): ...     output = model(input)</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>class DisableKVCache:\n    \"\"\"\n    Temporarily disable the key-value cache for transformer models. Used to prevent\n    excess memory use in one-shot cases where the model only performs the prefill\n    phase and not the generation phase.\n\n    Example:\n    &gt;&gt;&gt; model = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n    &gt;&gt;&gt; input = torch.randint(0, 32, size=(1, 32))\n    &gt;&gt;&gt; with DisableKVCache(model):\n    ...     output = model(input)\n    \"\"\"\n\n    def __init__(self, model: PreTrainedModel):\n        if hasattr(model.config, \"use_cache\"):\n            self.config = model.config\n\n        # MllamaConfig\n        elif hasattr(model.config, \"text_config\") and hasattr(\n            model.config.text_config, \"use_cache\"\n        ):\n            self.config = model.config.text_config\n\n        # unknown config structure\n        else:\n            raise NotImplementedError(f\"Cannot find `use_cache` for {model.config}\")\n\n        self.restore_value = self.config.use_cache\n\n    def __enter__(self):\n        self.restore_value = self.config.use_cache\n        self.config.use_cache = False\n\n    def __exit__(self, _exc_type, _exc_val, _exc_tb):\n        self.config.use_cache = self.restore_value\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.NumpyArrayBatcher","title":"<code>NumpyArrayBatcher</code>","text":"<p>               Bases: <code>object</code></p> <p>Batcher instance to handle taking in dictionaries of numpy arrays, appending multiple items to them to increase their batch size, and then stack them into a single batched numpy array for all keys in the dicts.</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>class NumpyArrayBatcher(object):\n    \"\"\"\n    Batcher instance to handle taking in dictionaries of numpy arrays,\n    appending multiple items to them to increase their batch size,\n    and then stack them into a single batched numpy array for all keys in the dicts.\n    \"\"\"\n\n    def __init__(self):\n        self._items = OrderedDict()  # type: Dict[str, List[numpy.ndarray]]\n\n    def __len__(self):\n        if len(self._items) == 0:\n            return 0\n\n        return len(self._items[list(self._items.keys())[0]])\n\n    def append(self, item: Union[numpy.ndarray, Dict[str, numpy.ndarray]]):\n        \"\"\"\n        Append a new item into the current batch.\n        All keys and shapes must match the current state.\n\n        :param item: the item to add for batching\n        \"\"\"\n        if len(self) &lt; 1 and isinstance(item, numpy.ndarray):\n            self._items[NDARRAY_KEY] = [item]\n        elif len(self) &lt; 1:\n            for key, val in item.items():\n                self._items[key] = [val]\n        elif isinstance(item, numpy.ndarray):\n            if NDARRAY_KEY not in self._items:\n                raise ValueError(\n                    \"numpy ndarray passed for item, but prev_batch does not contain one\"\n                )\n\n            if item.shape != self._items[NDARRAY_KEY][0].shape:\n                raise ValueError(\n                    (\n                        \"item of numpy ndarray of shape {} does not \"\n                        \"match the current batch shape of {}\".format(\n                            item.shape, self._items[NDARRAY_KEY][0].shape\n                        )\n                    )\n                )\n\n            self._items[NDARRAY_KEY].append(item)\n        else:\n            diff_keys = list(set(item.keys()) - set(self._items.keys()))\n\n            if len(diff_keys) &gt; 0:\n                raise ValueError(\n                    (\n                        \"numpy dict passed for item, not all keys match \"\n                        \"with the prev_batch. difference: {}\"\n                    ).format(diff_keys)\n                )\n\n            for key, val in item.items():\n                if val.shape != self._items[key][0].shape:\n                    raise ValueError(\n                        (\n                            \"item with key {} of shape {} does not \"\n                            \"match the current batch shape of {}\".format(\n                                key, val.shape, self._items[key][0].shape\n                            )\n                        )\n                    )\n\n                self._items[key].append(val)\n\n    def stack(self) -&gt; Dict[str, numpy.ndarray]:\n        \"\"\"\n        Stack the current items into a batch along a new, zeroed dimension\n\n        :return: the stacked items\n        \"\"\"\n        batch_dict = OrderedDict()\n\n        for key, val in self._items.items():\n            batch_dict[key] = numpy.stack(self._items[key])\n\n        return batch_dict\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.NumpyArrayBatcher.append","title":"<code>append(item)</code>","text":"<p>Append a new item into the current batch. All keys and shapes must match the current state.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[ndarray, Dict[str, ndarray]]</code> <p>the item to add for batching</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def append(self, item: Union[numpy.ndarray, Dict[str, numpy.ndarray]]):\n    \"\"\"\n    Append a new item into the current batch.\n    All keys and shapes must match the current state.\n\n    :param item: the item to add for batching\n    \"\"\"\n    if len(self) &lt; 1 and isinstance(item, numpy.ndarray):\n        self._items[NDARRAY_KEY] = [item]\n    elif len(self) &lt; 1:\n        for key, val in item.items():\n            self._items[key] = [val]\n    elif isinstance(item, numpy.ndarray):\n        if NDARRAY_KEY not in self._items:\n            raise ValueError(\n                \"numpy ndarray passed for item, but prev_batch does not contain one\"\n            )\n\n        if item.shape != self._items[NDARRAY_KEY][0].shape:\n            raise ValueError(\n                (\n                    \"item of numpy ndarray of shape {} does not \"\n                    \"match the current batch shape of {}\".format(\n                        item.shape, self._items[NDARRAY_KEY][0].shape\n                    )\n                )\n            )\n\n        self._items[NDARRAY_KEY].append(item)\n    else:\n        diff_keys = list(set(item.keys()) - set(self._items.keys()))\n\n        if len(diff_keys) &gt; 0:\n            raise ValueError(\n                (\n                    \"numpy dict passed for item, not all keys match \"\n                    \"with the prev_batch. difference: {}\"\n                ).format(diff_keys)\n            )\n\n        for key, val in item.items():\n            if val.shape != self._items[key][0].shape:\n                raise ValueError(\n                    (\n                        \"item with key {} of shape {} does not \"\n                        \"match the current batch shape of {}\".format(\n                            key, val.shape, self._items[key][0].shape\n                        )\n                    )\n                )\n\n            self._items[key].append(val)\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.NumpyArrayBatcher.stack","title":"<code>stack()</code>","text":"<p>Stack the current items into a batch along a new, zeroed dimension</p> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>the stacked items</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def stack(self) -&gt; Dict[str, numpy.ndarray]:\n    \"\"\"\n    Stack the current items into a batch along a new, zeroed dimension\n\n    :return: the stacked items\n    \"\"\"\n    batch_dict = OrderedDict()\n\n    for key, val in self._items.items():\n        batch_dict[key] = numpy.stack(self._items[key])\n\n    return batch_dict\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.DisableQuantization","title":"<code>DisableQuantization(module)</code>","text":"<p>Disable quantization during forward passes after applying a quantization config</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>@contextlib.contextmanager\ndef DisableQuantization(module: torch.nn.Module):\n    \"\"\"\n    Disable quantization during forward passes after applying a quantization config\n    \"\"\"\n    try:\n        module.apply(disable_quantization)\n        yield\n    finally:\n        module.apply(enable_quantization)\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.bucket_iterable","title":"<code>bucket_iterable(val, num_buckets=3, edge_percent=0.05, sort_highest=True, sort_key=None)</code>","text":"<p>Bucket iterable into subarray consisting of the first top percentage followed by the rest of the iterable sliced into equal sliced groups.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Iterable[Any]</code> <p>The iterable to bucket</p> required <code>num_buckets</code> <code>int</code> <p>The number of buckets to group the iterable into, does not include the top bucket</p> <code>3</code> <code>edge_percent</code> <code>float</code> <p>Group the first percent into its own bucket. If sort_highest, then this is the top percent, else bottom percent. If &lt;= 0, then will not create an edge bucket</p> <code>0.05</code> <code>sort_highest</code> <code>bool</code> <p>True to sort such that the highest percent is first and will create buckets in descending order. False to sort so lowest is first and create buckets in ascending order.</p> <code>True</code> <code>sort_key</code> <code>Callable[[Any], Any]</code> <p>The sort_key, if any, to use for sorting the iterable after converting it to a list</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[int, Any]]</code> <p>a list of each value mapped to the bucket it was sorted into</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def bucket_iterable(\n    val: Iterable[Any],\n    num_buckets: int = 3,\n    edge_percent: float = 0.05,\n    sort_highest: bool = True,\n    sort_key: Callable[[Any], Any] = None,\n) -&gt; List[Tuple[int, Any]]:\n    \"\"\"\n    Bucket iterable into subarray consisting of the first top percentage\n    followed by the rest of the iterable sliced into equal sliced groups.\n\n    :param val: The iterable to bucket\n    :param num_buckets: The number of buckets to group the iterable into,\n        does not include the top bucket\n    :param edge_percent: Group the first percent into its own bucket.\n        If sort_highest, then this is the top percent, else bottom percent.\n        If &lt;= 0, then will not create an edge bucket\n    :param sort_highest: True to sort such that the highest percent is first\n        and will create buckets in descending order.\n        False to sort so lowest is first and create buckets in ascending order.\n    :param sort_key: The sort_key, if any, to use for sorting the iterable\n        after converting it to a list\n    :return: a list of each value mapped to the bucket it was sorted into\n    \"\"\"\n\n    val_list = [v for v in val]\n    val_list.sort(key=sort_key, reverse=sort_highest)\n    bucketed_values = []\n    edge_count = round(edge_percent * len(val_list))\n\n    if edge_count &gt; 0:\n        bucketed_values.extend([(-1, val) for val in val_list[:edge_count]])\n        val_list = val_list[edge_count:]\n\n    buckets_count = round(len(val_list) / float(num_buckets))\n\n    for bucket in range(num_buckets):\n        add_vals = val_list[:buckets_count] if bucket &lt; num_buckets - 1 else val_list\n        val_list = val_list[buckets_count:] if bucket &lt; num_buckets - 1 else []\n        bucketed_values.extend([(bucket, val) for val in add_vals])\n\n    return bucketed_values\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.calibration_forward_context","title":"<code>calibration_forward_context(model)</code>","text":"<p>Context in which all calibration forward passes should occur.</p> <ul> <li>Remove gradient calculations</li> <li>Disable the KV cache</li> <li>Disable train mode and enable eval mode</li> </ul> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>@contextlib.contextmanager\ndef calibration_forward_context(model: PreTrainedModel):\n    \"\"\"\n    Context in which all calibration forward passes should occur.\n\n    - Remove gradient calculations\n    - Disable the KV cache\n    - Disable train mode and enable eval mode\n    \"\"\"\n    with (\n        torch.no_grad(),\n        DisableKVCache(model),\n        eval_context(model),\n    ):\n        yield\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.clean_path","title":"<code>clean_path(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the directory or file path to clean</p> required <p>Returns:</p> Type Description <code>str</code> <p>a cleaned version that expands the user path and creates an absolute path</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def clean_path(path: str) -&gt; str:\n    \"\"\"\n    :param path: the directory or file path to clean\n    :return: a cleaned version that expands the user path and creates an absolute path\n    \"\"\"\n    return os.path.abspath(os.path.expanduser(path))\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.convert_to_bool","title":"<code>convert_to_bool(val)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>val</code> <code>Any</code> <p>the value to be converted to a bool, supports logical values as strings ie True, t, false, 0</p> required <p>Returns:</p> Type Description <p>the boolean representation of the value, if it can't be determined, falls back on returning True</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def convert_to_bool(val: Any):\n    \"\"\"\n    :param val: the value to be converted to a bool,\n        supports logical values as strings ie True, t, false, 0\n    :return: the boolean representation of the value, if it can't be determined,\n        falls back on returning True\n    \"\"\"\n    return (\n        bool(val)\n        if not isinstance(val, str)\n        else bool(val) and \"f\" not in val.lower() and \"0\" not in val.lower()\n    )\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.create_dirs","title":"<code>create_dirs(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the directory path to try and create</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def create_dirs(path: str):\n    \"\"\"\n    :param path: the directory path to try and create\n    \"\"\"\n    path = clean_path(path)\n\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            # Unexpected OSError, re-raise.\n            raise\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.create_parent_dirs","title":"<code>create_parent_dirs(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the file path to try to create the parent directories for</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def create_parent_dirs(path: str):\n    \"\"\"\n    :param path: the file path to try to create the parent directories for\n    \"\"\"\n    parent = os.path.dirname(path)\n    create_dirs(parent)\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.create_unique_dir","title":"<code>create_unique_dir(path, check_number=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the file path to create a unique version of (append numbers until one doesn't exist)</p> required <code>check_number</code> <code>int</code> <p>the number to begin checking for unique versions at</p> <code>0</code> <p>Returns:</p> Type Description <code>str</code> <p>the unique directory path</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def create_unique_dir(path: str, check_number: int = 0) -&gt; str:\n    \"\"\"\n    :param path: the file path to create a unique version of\n        (append numbers until one doesn't exist)\n    :param check_number: the number to begin checking for unique versions at\n    :return: the unique directory path\n    \"\"\"\n    check_path = clean_path(\"{}-{:04d}\".format(path, check_number))\n\n    if not os.path.exists(check_path):\n        return check_path\n\n    return create_unique_dir(path, check_number + 1)\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.flatten_iterable","title":"<code>flatten_iterable(li)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>li</code> <code>Iterable</code> <p>a possibly nested iterable of items to be flattened</p> required <p>Returns:</p> Type Description <p>a flattened version of the list where all elements are in a single list flattened in a depth first pattern</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def flatten_iterable(li: Iterable):\n    \"\"\"\n    :param li: a possibly nested iterable of items to be flattened\n    :return: a flattened version of the list where all elements are in a single list\n             flattened in a depth first pattern\n    \"\"\"\n\n    def _flatten_gen(_li):\n        for el in _li:\n            if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\n                yield from _flatten_gen(el)\n            else:\n                yield el\n\n    return list(_flatten_gen(li))\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.getattr_chain","title":"<code>getattr_chain(obj, chain_str, *args, **kwargs)</code>","text":"<p>Chain multiple getattr calls, separated by <code>.</code></p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>base object whose attributes are being retrieved</p> required <code>chain_str</code> <code>str</code> <p>attribute names separated by <code>.</code></p> required <code>default</code> <p>default value, throw error otherwise</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def getattr_chain(obj: Any, chain_str: str, *args, **kwargs) -&gt; Any:\n    \"\"\"\n    Chain multiple getattr calls, separated by `.`\n\n    :param obj: base object whose attributes are being retrieved\n    :param chain_str: attribute names separated by `.`\n    :param default: default value, throw error otherwise\n\n    \"\"\"\n    if len(args) &gt;= 1:\n        has_default = True\n        default = args[0]\n    elif \"default\" in kwargs:\n        has_default = True\n        default = kwargs[\"default\"]\n    else:\n        has_default = False\n\n    attr_names = chain_str.split(\".\")\n\n    res = obj\n    for attr_name in attr_names:\n        if not hasattr(res, attr_name):\n            if has_default:\n                return default\n            else:\n                raise AttributeError(f\"{res} object has no attribute {attr_name}\")\n        res = getattr(res, attr_name)\n\n    return res\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.import_from_path","title":"<code>import_from_path(path)</code>","text":"<p>Import the module and the name of the function/class separated by : Examples:   path = \"/path/to/file.py:func_or_class_name\"   path = \"/path/to/file:focn\"   path = \"path.to.file:focn\"</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path including the file path and object name</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def import_from_path(path: str) -&gt; str:\n    \"\"\"\n    Import the module and the name of the function/class separated by :\n    Examples:\n      path = \"/path/to/file.py:func_or_class_name\"\n      path = \"/path/to/file:focn\"\n      path = \"path.to.file:focn\"\n    :param path: path including the file path and object name\n    :return Function or class object\n    \"\"\"\n    original_path, class_name = path.split(\":\")\n    _path = original_path\n\n    path = original_path.split(\".py\")[0]\n    path = re.sub(r\"/+\", \".\", path)\n    try:\n        module = importlib.import_module(path)\n    except ImportError:\n        raise ImportError(f\"Cannot find module with path {_path}\")\n\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        raise AttributeError(f\"Cannot find {class_name} in {_path}\")\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.interpolate","title":"<code>interpolate(x_cur, x0, x1, y0, y1, inter_func='linear')</code>","text":"<p>note, caps values at their min of x0 and max x1, designed to not work outside of that range for implementation reasons</p> <p>Parameters:</p> Name Type Description Default <code>x_cur</code> <code>float</code> <p>the current value for x, should be between x0 and x1</p> required <code>x0</code> <code>float</code> <p>the minimum for x to interpolate between</p> required <code>x1</code> <code>float</code> <p>the maximum for x to interpolate between</p> required <code>y0</code> <code>Any</code> <p>the minimum for y to interpolate between</p> required <code>y1</code> <code>Any</code> <p>the maximum for y to interpolate between</p> required <code>inter_func</code> <code>str</code> <p>the type of function to interpolate with: linear, cubic, inverse_cubic</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Any</code> <p>the interpolated value projecting x into y for the given interpolation function</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def interpolate(\n    x_cur: float, x0: float, x1: float, y0: Any, y1: Any, inter_func: str = \"linear\"\n) -&gt; Any:\n    \"\"\"\n    note, caps values at their min of x0 and max x1,\n    designed to not work outside of that range for implementation reasons\n\n    :param x_cur: the current value for x, should be between x0 and x1\n    :param x0: the minimum for x to interpolate between\n    :param x1: the maximum for x to interpolate between\n    :param y0: the minimum for y to interpolate between\n    :param y1: the maximum for y to interpolate between\n    :param inter_func: the type of function to interpolate with:\n        linear, cubic, inverse_cubic\n    :return: the interpolated value projecting x into y for the given\n        interpolation function\n    \"\"\"\n    if inter_func not in INTERPOLATION_FUNCS:\n        raise ValueError(\n            \"unsupported inter_func given of {} must be one of {}\".format(\n                inter_func, INTERPOLATION_FUNCS\n            )\n        )\n\n    # convert our x to 0-1 range since equations are designed to fit in\n    # (0,0)-(1,1) space\n    x_per = (x_cur - x0) / (x1 - x0)\n\n    # map x to y using the desired function in (0,0)-(1,1) space\n    if inter_func == \"linear\":\n        y_per = x_per\n    elif inter_func == \"cubic\":\n        # https://www.wolframalpha.com/input/?i=1-(1-x)%5E3+from+0+to+1\n        y_per = 1 - (1 - x_per) ** 3\n    elif inter_func == \"inverse_cubic\":\n        # https://www.wolframalpha.com/input/?i=1-(1-x)%5E(1%2F3)+from+0+to+1\n        y_per = 1 - (1 - x_per) ** (1 / 3)\n    else:\n        raise ValueError(\n            \"unsupported inter_func given of {} in interpolate\".format(inter_func)\n        )\n\n    if y_per &lt;= 0.0 + sys.float_info.epsilon:\n        return y0\n\n    if y_per &gt;= 1.0 - sys.float_info.epsilon:\n        return y1\n\n    # scale the threshold based on what we want the current to be\n    return y_per * (y1 - y0) + y0\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.interpolate_list_linear","title":"<code>interpolate_list_linear(measurements, x_val)</code>","text":"<p>interpolate for input values within a list of measurements linearly</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>List[Tuple[float, float]]</code> <p>the measurements to interpolate the output value between</p> required <code>x_val</code> <code>Union[float, List[float]]</code> <p>the target values to interpolate to the second dimension</p> required <p>Returns:</p> Type Description <code>List[Tuple[float, float]]</code> <p>a list of tuples containing the target values, interpolated values</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def interpolate_list_linear(\n    measurements: List[Tuple[float, float]], x_val: Union[float, List[float]]\n) -&gt; List[Tuple[float, float]]:\n    \"\"\"\n    interpolate for input values within a list of measurements linearly\n\n    :param measurements: the measurements to interpolate the output value between\n    :param x_val: the target values to interpolate to the second dimension\n    :return: a list of tuples containing the target values, interpolated values\n    \"\"\"\n    assert len(measurements) &gt; 1\n    measurements.sort(key=lambda v: v[0])\n\n    x_vals = [x_val] if isinstance(x_val, float) else x_val\n    x_vals.sort()\n\n    interpolated = []\n    lower_index = 0\n    higher_index = 1\n\n    for x_val in x_vals:\n        while (\n            x_val &gt; measurements[higher_index][0]\n            and higher_index &lt; len(measurements) - 1\n        ):\n            lower_index += 1\n            higher_index += 1\n\n        x0, y0 = measurements[lower_index]\n        x1, y1 = measurements[higher_index]\n        y_val = y0 + (x_val - x0) * ((y1 - y0) / (x1 - x0))\n        interpolated.append((x_val, y_val))\n\n    return interpolated\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.interpolated_integral","title":"<code>interpolated_integral(measurements)</code>","text":"<p>Calculate the interpolated integal for a group of measurements of the form [(x0, y0), (x1, y1), ...]</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>List[Tuple[float, float]]</code> <p>the measurements to calculate the integral for</p> required <p>Returns:</p> Type Description <p>the integral or area under the curve for the measurements given</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def interpolated_integral(measurements: List[Tuple[float, float]]):\n    \"\"\"\n    Calculate the interpolated integal for a group of measurements of the form\n    [(x0, y0), (x1, y1), ...]\n\n    :param measurements: the measurements to calculate the integral for\n    :return: the integral or area under the curve for the measurements given\n    \"\"\"\n    if len(measurements) &lt; 1:\n        return 0.0\n\n    if len(measurements) == 1:\n        return measurements[0][1]\n\n    measurements.sort(key=lambda v: v[0])\n    integral = 0.0\n\n    for index, (x_val, y_val) in enumerate(measurements):\n        if index &gt;= len(measurements) - 1:\n            continue\n\n        x_next, y_next = measurements[index + 1]\n        x_dist = x_next - x_val\n        area = y_val * x_dist + (y_next - y_val) * x_dist / 2.0\n        integral += area\n\n    return integral\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.is_package_available","title":"<code>is_package_available(package_name, return_version=False)</code>","text":"<p>A helper function to check if a package is available and optionally return its version. This function enforces a check that the package is available and is not just a directory/file with the same name as the package.</p> <p>inspired from: https://github.com/huggingface/transformers/blob/965cf677695dd363285831afca8cf479cf0c600c/src/transformers/utils/import_utils.py#L41</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The package name to check for</p> required <code>return_version</code> <code>bool</code> <p>True to return the version of the package if available</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[bool, str], bool]</code> <p>True if the package is available, False otherwise or a tuple of (bool, version) if return_version is True</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def is_package_available(\n    package_name: str,\n    return_version: bool = False,\n) -&gt; Union[Tuple[bool, str], bool]:\n    \"\"\"\n    A helper function to check if a package is available\n    and optionally return its version. This function enforces\n    a check that the package is available and is not\n    just a directory/file with the same name as the package.\n\n    inspired from:\n    https://github.com/huggingface/transformers/blob/965cf677695dd363285831afca8cf479cf0c600c/src/transformers/utils/import_utils.py#L41\n\n    :param package_name: The package name to check for\n    :param return_version: True to return the version of\n        the package if available\n    :return: True if the package is available, False otherwise or a tuple of\n        (bool, version) if return_version is True\n    \"\"\"\n\n    package_exists = importlib.util.find_spec(package_name) is not None\n    package_version = \"N/A\"\n    if package_exists:\n        try:\n            package_version = importlib.metadata.version(package_name)\n            package_exists = True\n        except importlib.metadata.PackageNotFoundError:\n            package_exists = False\n        logger.debug(f\"Detected {package_name} version {package_version}\")\n    if return_version:\n        return package_exists, package_version\n    else:\n        return package_exists\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.is_url","title":"<code>is_url(val)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>val</code> <code>str</code> <p>value to check if it is a url or not</p> required <p>Returns:</p> Type Description <p>True if value is a URL, False otherwise</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def is_url(val: str):\n    \"\"\"\n    :param val: value to check if it is a url or not\n    :return: True if value is a URL, False otherwise\n    \"\"\"\n\n    try:\n        result = urlparse(val)\n\n        return all([result.scheme, result.netloc])\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.json_to_jsonl","title":"<code>json_to_jsonl(json_file_path, overwrite=True)</code>","text":"<p>Converts a json list file to jsonl file format (used for sharding efficienty)     e.x.         [{\"a\": 1}, {\"a\": 1}]     would convert to:         {\"a\": 1}</p> <p>Parameters:</p> Name Type Description Default <code>json_file_path</code> <code>str</code> <p>file path to a json file path containing a json list of objects</p> required <code>overwrite</code> <code>bool</code> <p>If True, the existing json file will be overwritten, if False, the file will have the same name but with a .jsonl extension</p> <code>True</code> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def json_to_jsonl(json_file_path: str, overwrite: bool = True):\n    \"\"\"\n    Converts a json list file to jsonl file format (used for sharding efficienty)\n        e.x.\n            [{\"a\": 1}, {\"a\": 1}]\n        would convert to:\n            {\"a\": 1}\n            {\"a\": 1}\n    :param json_file_path: file path to a json file path containing a json list\n        of objects\n    :param overwrite: If True, the existing json file will be overwritten, if False,\n        the file will have the same name but with a .jsonl extension\n    \"\"\"\n    if not json_file_path.endswith(\".json\"):\n        raise ValueError(\"json file must have .json extension\")\n    with open(json_file_path) as json_file:\n        json_data = json.load(json_file)\n\n    if not isinstance(json_data, List):\n        raise ValueError(\n            \"Json data must be a list to conver to jsonl format. \"\n            f\"found {type(json_data)}\"\n        )\n\n    jsonl_file_path = json_file_path + (\"\" if overwrite else \"l\")\n    with open(jsonl_file_path, \"w\") as jsonl_file:\n        for json_line in json_data:\n            json.dump(json_line, jsonl_file)  # append json line\n            jsonl_file.write(\"\\n\")  # newline\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.load_labeled_data","title":"<code>load_labeled_data(data, labels, raise_on_error=True)</code>","text":"<p>Load labels and data from disk or from memory and group them together. Assumes sorted ordering for on disk. Will match between when a file glob is passed for either data and/or labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Iterable[Union[str, ndarray, Dict[str, ndarray]]]]</code> <p>the file glob, file path to numpy data tar ball, or list of arrays to use for data</p> required <code>labels</code> <code>Union[None, str, Iterable[Union[str, ndarray, Dict[str, ndarray]]]]</code> <p>the file glob, file path to numpy data tar ball, or list of arrays to use for labels, if any</p> required <code>raise_on_error</code> <code>bool</code> <p>True to raise on any error that occurs; False to log a warning, ignore, and continue</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Tuple[Union[ndarray, Dict[str, ndarray]], Union[None, ndarray, Dict[str, ndarray]]]]</code> <p>a list containing tuples of the data, labels. If labels was passed in as None, will now contain a None for the second index in each tuple</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def load_labeled_data(\n    data: Union[str, Iterable[Union[str, numpy.ndarray, Dict[str, numpy.ndarray]]]],\n    labels: Union[\n        None, str, Iterable[Union[str, numpy.ndarray, Dict[str, numpy.ndarray]]]\n    ],\n    raise_on_error: bool = True,\n) -&gt; List[\n    Tuple[\n        Union[numpy.ndarray, Dict[str, numpy.ndarray]],\n        Union[None, numpy.ndarray, Dict[str, numpy.ndarray]],\n    ]\n]:\n    \"\"\"\n    Load labels and data from disk or from memory and group them together.\n    Assumes sorted ordering for on disk. Will match between when a file glob is passed\n    for either data and/or labels.\n\n    :param data: the file glob, file path to numpy data tar ball, or list of arrays to\n        use for data\n    :param labels: the file glob, file path to numpy data tar ball, or list of arrays\n        to use for labels, if any\n    :param raise_on_error: True to raise on any error that occurs;\n        False to log a warning, ignore, and continue\n    :return: a list containing tuples of the data, labels. If labels was passed in\n        as None, will now contain a None for the second index in each tuple\n    \"\"\"\n    if isinstance(data, str):\n        data = load_numpy_list(data)\n\n    if labels is None:\n        labels = [None for _ in range(len(data))]\n    elif isinstance(labels, str):\n        labels = load_numpy_list(labels)\n\n    if len(data) != len(labels) and labels:\n        # always raise this error, lengths must match\n        raise ValueError(\n            \"len(data) given of {} does not match len(labels) given of {}\".format(\n                len(data), len(labels)\n            )\n        )\n\n    labeled_data = []\n\n    for dat, lab in zip(data, labels):\n        try:\n            if isinstance(dat, str):\n                dat = load_numpy(dat)\n\n            if lab is not None and isinstance(lab, str):\n                lab = load_numpy(lab)\n\n            labeled_data.append((dat, lab))\n        except Exception as err:\n            if raise_on_error:\n                raise err\n            else:\n                logger.error(\"Error creating labeled data: {}\".format(err))\n\n    return labeled_data\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.load_numpy","title":"<code>load_numpy(file_path)</code>","text":"<p>Load a numpy file into either an ndarray or an OrderedDict representing what was in the npz file</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>the file_path to load</p> required <p>Returns:</p> Type Description <code>Union[ndarray, Dict[str, ndarray]]</code> <p>the loaded values from the file</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def load_numpy(file_path: str) -&gt; Union[numpy.ndarray, Dict[str, numpy.ndarray]]:\n    \"\"\"\n    Load a numpy file into either an ndarray or an OrderedDict representing what\n    was in the npz file\n\n    :param file_path: the file_path to load\n    :return: the loaded values from the file\n    \"\"\"\n    file_path = clean_path(file_path)\n    array = numpy.load(file_path)\n\n    if not isinstance(array, numpy.ndarray):\n        tmp_arrray = array\n        array = OrderedDict()\n        for key, val in tmp_arrray.items():\n            array[key] = val\n\n    return array\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.patch_attr","title":"<code>patch_attr(base, attr, value)</code>","text":"<p>Patch the value of an object attribute. Original value is restored upon exit</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>object</code> <p>object which has the attribute to patch</p> required <code>attr</code> <code>str</code> <p>name of the the attribute to patch</p> required <code>value</code> <code>Any</code> <p>used to replace original value  Usage: &gt;&gt;&gt; from types import SimpleNamespace &gt;&gt;&gt; obj = SimpleNamespace() &gt;&gt;&gt; with patch_attr(obj, \"attribute\", \"value\"): ...     assert obj.attribute == \"value\" &gt;&gt;&gt; assert not hasattr(obj, \"attribute\")</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>@contextlib.contextmanager\ndef patch_attr(base: object, attr: str, value: Any):\n    \"\"\"\n    Patch the value of an object attribute. Original value is restored upon exit\n\n    :param base: object which has the attribute to patch\n    :param attr: name of the the attribute to patch\n    :param value: used to replace original value\n\n    Usage:\n    &gt;&gt;&gt; from types import SimpleNamespace\n    &gt;&gt;&gt; obj = SimpleNamespace()\n    &gt;&gt;&gt; with patch_attr(obj, \"attribute\", \"value\"):\n    ...     assert obj.attribute == \"value\"\n    &gt;&gt;&gt; assert not hasattr(obj, \"attribute\")\n    \"\"\"\n    _sentinel = object()\n    original_value = getattr(base, attr, _sentinel)\n\n    setattr(base, attr, value)\n    try:\n        yield\n    finally:\n        if original_value is not _sentinel:\n            setattr(base, attr, original_value)\n        else:\n            delattr(base, attr)\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.path_file_count","title":"<code>path_file_count(path, pattern='*')</code>","text":"<p>Return the number of files that match the given pattern under the given path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path to the directory to look for files under</p> required <code>pattern</code> <code>str</code> <p>the pattern the files must match to be counted</p> <code>'*'</code> <p>Returns:</p> Type Description <code>int</code> <p>the number of files matching the pattern under the directory</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def path_file_count(path: str, pattern: str = \"*\") -&gt; int:\n    \"\"\"\n    Return the number of files that match the given pattern under the given path\n\n    :param path: the path to the directory to look for files under\n    :param pattern: the pattern the files must match to be counted\n    :return: the number of files matching the pattern under the directory\n    \"\"\"\n    path = clean_path(path)\n\n    return len(fnmatch.filter(os.listdir(path), pattern))\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.path_file_size","title":"<code>path_file_size(path)</code>","text":"<p>Return the total size, in bytes, for a path on the file system</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path (directory or file) to get the size for</p> required <p>Returns:</p> Type Description <code>int</code> <p>the size of the path, in bytes, as stored on disk</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def path_file_size(path: str) -&gt; int:\n    \"\"\"\n    Return the total size, in bytes, for a path on the file system\n\n    :param path: the path (directory or file) to get the size for\n    :return: the size of the path, in bytes, as stored on disk\n    \"\"\"\n\n    if not os.path.isdir(path):\n        stat = os.stat(path)\n\n        return stat.st_size\n\n    total_size = 0\n    seen = {}\n\n    for dir_path, dir_names, filenames in os.walk(path):\n        for file in filenames:\n            file_path = os.path.join(dir_path, file)\n\n            try:\n                stat = os.stat(file_path)\n            except OSError:\n                continue\n\n            try:\n                seen[stat.st_ino]\n            except KeyError:\n                seen[stat.st_ino] = True\n            else:\n                continue\n\n            total_size += stat.st_size\n\n    return total_size\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.save_numpy","title":"<code>save_numpy(array, export_dir, name, npz=True)</code>","text":"<p>Save a numpy array or collection of numpy arrays to disk</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Union[ndarray, Dict[str, ndarray], Iterable[ndarray]]</code> <p>the array or collection of arrays to save</p> required <code>export_dir</code> <code>str</code> <p>the directory to export the numpy file into</p> required <code>name</code> <code>str</code> <p>the name of the file to export to (without extension)</p> required <code>npz</code> <code>bool</code> <p>True to save as an npz compressed file, False for standard npy. Note, npy can only be used for single numpy arrays</p> <code>True</code> <p>Returns:</p> Type Description <p>the saved path</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def save_numpy(\n    array: Union[numpy.ndarray, Dict[str, numpy.ndarray], Iterable[numpy.ndarray]],\n    export_dir: str,\n    name: str,\n    npz: bool = True,\n):\n    \"\"\"\n    Save a numpy array or collection of numpy arrays to disk\n\n    :param array: the array or collection of arrays to save\n    :param export_dir: the directory to export the numpy file into\n    :param name: the name of the file to export to (without extension)\n    :param npz: True to save as an npz compressed file, False for standard npy.\n        Note, npy can only be used for single numpy arrays\n    :return: the saved path\n    \"\"\"\n    create_dirs(export_dir)\n    export_path = os.path.join(\n        export_dir, \"{}.{}\".format(name, \"npz\" if npz else \"npy\")\n    )\n\n    if isinstance(array, numpy.ndarray) and npz:\n        numpy.savez_compressed(export_path, array)\n    elif isinstance(array, numpy.ndarray):\n        numpy.save(export_path, array)\n    elif isinstance(array, Dict) and npz:\n        numpy.savez_compressed(export_path, **array)\n    elif isinstance(array, Dict):\n        raise ValueError(\"Dict can only be exported to an npz file\")\n    elif isinstance(array, Iterable) and npz:\n        numpy.savez_compressed(export_path, *[val for val in array])\n    elif isinstance(array, Iterable):\n        raise ValueError(\"Iterable can only be exported to an npz file\")\n    else:\n        raise ValueError(\"Unrecognized type given for array {}\".format(array))\n\n    return export_path\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.tensor_export","title":"<code>tensor_export(tensor, export_dir, name, npz=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Union[ndarray, Dict[str, ndarray], Iterable[ndarray]]</code> <p>tensor to export to a saved numpy array file</p> required <code>export_dir</code> <code>str</code> <p>the directory to export the file in</p> required <code>name</code> <code>str</code> <p>the name of the file, .npy will be appended to it</p> required <code>npz</code> <code>bool</code> <p>True to export as an npz file, False otherwise</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>the path of the numpy file the tensor was exported to</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def tensor_export(\n    tensor: Union[numpy.ndarray, Dict[str, numpy.ndarray], Iterable[numpy.ndarray]],\n    export_dir: str,\n    name: str,\n    npz: bool = True,\n) -&gt; str:\n    \"\"\"\n    :param tensor: tensor to export to a saved numpy array file\n    :param export_dir: the directory to export the file in\n    :param name: the name of the file, .npy will be appended to it\n    :param npz: True to export as an npz file, False otherwise\n    :return: the path of the numpy file the tensor was exported to\n    \"\"\"\n    create_dirs(export_dir)\n    export_path = os.path.join(\n        export_dir, \"{}.{}\".format(name, \"npz\" if npz else \"npy\")\n    )\n\n    if isinstance(tensor, numpy.ndarray) and npz:\n        numpy.savez_compressed(export_path, tensor)\n    elif isinstance(tensor, numpy.ndarray):\n        numpy.save(export_path, tensor)\n    elif isinstance(tensor, Dict) and npz:\n        numpy.savez_compressed(export_path, **tensor)\n    elif isinstance(tensor, Dict):\n        raise ValueError(\"tensor dictionaries can only be saved as npz\")\n    elif isinstance(tensor, Iterable) and npz:\n        numpy.savez_compressed(export_path, *tensor)\n    elif isinstance(tensor, Iterable):\n        raise ValueError(\"tensor iterables can only be saved as npz\")\n    else:\n        raise ValueError(\"unknown type give for tensor {}\".format(tensor))\n\n    return export_path\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.tensors_export","title":"<code>tensors_export(tensors, export_dir, name_prefix, counter=0, break_batch=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[ndarray, Dict[str, ndarray], Iterable[ndarray]]</code> <p>the tensors to export to a saved numpy array file</p> required <code>export_dir</code> <code>str</code> <p>the directory to export the files in</p> required <code>name_prefix</code> <code>str</code> <p>the prefix name for the tensors to save as, will append info about the position of the tensor in a list or dict in addition to the .npy file format</p> required <code>counter</code> <code>int</code> <p>the current counter to save the tensor at</p> <code>0</code> <code>break_batch</code> <code>bool</code> <p>treat the tensor as a batch and break apart into multiple tensors</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>the exported paths</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def tensors_export(\n    tensors: Union[numpy.ndarray, Dict[str, numpy.ndarray], Iterable[numpy.ndarray]],\n    export_dir: str,\n    name_prefix: str,\n    counter: int = 0,\n    break_batch: bool = False,\n) -&gt; List[str]:\n    \"\"\"\n    :param tensors: the tensors to export to a saved numpy array file\n    :param export_dir: the directory to export the files in\n    :param name_prefix: the prefix name for the tensors to save as, will append\n        info about the position of the tensor in a list or dict in addition\n        to the .npy file format\n    :param counter: the current counter to save the tensor at\n    :param break_batch: treat the tensor as a batch and break apart into\n        multiple tensors\n    :return: the exported paths\n    \"\"\"\n    create_dirs(export_dir)\n    exported_paths = []\n\n    if break_batch:\n        _tensors_export_batch(tensors, export_dir, name_prefix, counter, exported_paths)\n    else:\n        _tensors_export_recursive(\n            tensors, export_dir, name_prefix, counter, exported_paths\n        )\n\n    return exported_paths\n</code></pre>"},{"location":"reference/llmcompressor/utils/#llmcompressor.utils.validate_str_iterable","title":"<code>validate_str_iterable(val, error_desc='')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>val</code> <code>Union[str, Iterable[str]]</code> <p>the value to validate, check that it is a list (and flattens it), otherwise checks that it's an ALL or ALL_PRUNABLE string, otherwise raises a ValueError</p> required <code>error_desc</code> <code>str</code> <p>the description to raise an error with in the event that the val wasn't valid</p> <code>''</code> <p>Returns:</p> Type Description <code>Union[str, Iterable[str]]</code> <p>the validated version of the param</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def validate_str_iterable(\n    val: Union[str, Iterable[str]], error_desc: str = \"\"\n) -&gt; Union[str, Iterable[str]]:\n    \"\"\"\n    :param val: the value to validate, check that it is a list (and flattens it),\n        otherwise checks that it's an __ALL__ or __ALL_PRUNABLE__ string,\n        otherwise raises a ValueError\n    :param error_desc: the description to raise an error with in the event that\n        the val wasn't valid\n    :return: the validated version of the param\n    \"\"\"\n    if isinstance(val, str):\n        if val.upper() != ALL_TOKEN and val.upper() != ALL_PRUNABLE_TOKEN:\n            raise ValueError(\n                \"unsupported string ({}) given in {}\".format(val, error_desc)\n            )\n\n        return val.upper()\n\n    if isinstance(val, Iterable):\n        return flatten_iterable(val)\n\n    raise ValueError(\"unsupported type ({}) given in {}\".format(val, error_desc))\n</code></pre>"},{"location":"reference/llmcompressor/utils/dev/","title":"llmcompressor.utils.dev","text":""},{"location":"reference/llmcompressor/utils/dev/#llmcompressor.utils.dev.patch_transformers_logger_level","title":"<code>patch_transformers_logger_level(level=logging.ERROR)</code>","text":"<p>Context under which the transformers logger's level is modified</p> <p>This can be used with <code>skip_weights_download</code> to squelch warnings related to missing parameters in the checkpoint</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>new logging level for transformers logger. Logs whose level is below this level will not be logged</p> <code>ERROR</code> Source code in <code>src/llmcompressor/utils/dev.py</code> <pre><code>@contextlib.contextmanager\ndef patch_transformers_logger_level(level: int = logging.ERROR):\n    \"\"\"\n    Context under which the transformers logger's level is modified\n\n    This can be used with `skip_weights_download` to squelch warnings related to\n    missing parameters in the checkpoint\n\n    :param level: new logging level for transformers logger. Logs whose level is below\n        this level will not be logged\n    \"\"\"\n    transformers_logger = logging.getLogger(\"transformers.modeling_utils\")\n    restore_log_level = transformers_logger.getEffectiveLevel()\n\n    transformers_logger.setLevel(level=level)\n    yield\n    transformers_logger.setLevel(level=restore_log_level)\n</code></pre>"},{"location":"reference/llmcompressor/utils/dev/#llmcompressor.utils.dev.skip_weights_download","title":"<code>skip_weights_download(model_class=AutoModelForCausalLM)</code>","text":"<p>Context manager under which models are initialized without having to download the model weight files. This differs from <code>init_empty_weights</code> in that weights are allocated on to assigned devices with random values, as opposed to being on the meta device</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>Type[PreTrainedModel]</code> <p>class to patch, defaults to <code>AutoModelForCausalLM</code></p> <code>AutoModelForCausalLM</code> Source code in <code>src/llmcompressor/utils/dev.py</code> <pre><code>@contextlib.contextmanager\ndef skip_weights_download(model_class: Type[PreTrainedModel] = AutoModelForCausalLM):\n    \"\"\"\n    Context manager under which models are initialized without having to download\n    the model weight files. This differs from `init_empty_weights` in that weights are\n    allocated on to assigned devices with random values, as opposed to being on the meta\n    device\n\n    :param model_class: class to patch, defaults to `AutoModelForCausalLM`\n    \"\"\"\n    original_fn = model_class.from_pretrained\n    weights_files = [\n        \"*.bin\",\n        \"*.safetensors\",\n        \"*.pth\",\n        SAFE_WEIGHTS_INDEX_NAME,\n        WEIGHTS_INDEX_NAME,\n        \"*.msgpack\",\n    ]\n\n    @classmethod\n    def patched(cls, *args, **kwargs):\n        nonlocal tmp_dir\n\n        # intercept model stub\n        model_stub = args[0] if args else kwargs.pop(\"pretrained_model_name_or_path\")\n\n        # download files into tmp dir\n        os.makedirs(tmp_dir, exist_ok=True)\n        snapshot_download(\n            repo_id=model_stub, local_dir=tmp_dir, ignore_patterns=weights_files\n        )\n\n        # make an empty weights file to avoid errors\n        weights_file_path = os.path.join(tmp_dir, \"model.safetensors\")\n        save_file({}, weights_file_path, metadata={\"format\": \"pt\"})\n\n        # load from tmp dir\n        model = original_fn(tmp_dir, **kwargs)\n\n        # replace model_path\n        model.name_or_path = model_stub\n        model.config._name_or_path = model_stub\n\n        return model\n\n    with tempfile.TemporaryDirectory() as tmp_dir, patch_attr(\n        model_class, \"from_pretrained\", patched\n    ), skip_weights_initialize(), patch_transformers_logger_level():\n        yield\n</code></pre>"},{"location":"reference/llmcompressor/utils/dev/#llmcompressor.utils.dev.skip_weights_initialize","title":"<code>skip_weights_initialize(use_zeros=False)</code>","text":"<p>Very similar to <code>transformers.model_utils.no_init_weights</code>, except that torch.Tensor initialization functions are also patched to account for tensors which are initialized not on the meta device</p> Source code in <code>src/llmcompressor/utils/dev.py</code> <pre><code>@contextlib.contextmanager\ndef skip_weights_initialize(use_zeros: bool = False):\n    \"\"\"\n    Very similar to `transformers.model_utils.no_init_weights`, except that torch.Tensor\n    initialization functions are also patched to account for tensors which are\n    initialized not on the meta device\n    \"\"\"\n\n    def skip(tensor: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        if use_zeros:\n            return tensor.fill_(0)\n        return tensor\n\n    with contextlib.ExitStack() as stack:\n        for name in TORCH_INIT_FUNCTIONS.keys():\n            stack.enter_context(patch_attr(torch.nn.init, name, skip))\n            stack.enter_context(patch_attr(torch.Tensor, name, skip))\n        yield\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/","title":"llmcompressor.utils.helpers","text":"<p>General utility helper functions. Common functions for interfacing with python primitives and directories/files.</p>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.DisableKVCache","title":"<code>DisableKVCache</code>","text":"<p>Temporarily disable the key-value cache for transformer models. Used to prevent excess memory use in one-shot cases where the model only performs the prefill phase and not the generation phase.</p> <p>Example:</p> <p>model = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\") input = torch.randint(0, 32, size=(1, 32)) with DisableKVCache(model): ...     output = model(input)</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>class DisableKVCache:\n    \"\"\"\n    Temporarily disable the key-value cache for transformer models. Used to prevent\n    excess memory use in one-shot cases where the model only performs the prefill\n    phase and not the generation phase.\n\n    Example:\n    &gt;&gt;&gt; model = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n    &gt;&gt;&gt; input = torch.randint(0, 32, size=(1, 32))\n    &gt;&gt;&gt; with DisableKVCache(model):\n    ...     output = model(input)\n    \"\"\"\n\n    def __init__(self, model: PreTrainedModel):\n        if hasattr(model.config, \"use_cache\"):\n            self.config = model.config\n\n        # MllamaConfig\n        elif hasattr(model.config, \"text_config\") and hasattr(\n            model.config.text_config, \"use_cache\"\n        ):\n            self.config = model.config.text_config\n\n        # unknown config structure\n        else:\n            raise NotImplementedError(f\"Cannot find `use_cache` for {model.config}\")\n\n        self.restore_value = self.config.use_cache\n\n    def __enter__(self):\n        self.restore_value = self.config.use_cache\n        self.config.use_cache = False\n\n    def __exit__(self, _exc_type, _exc_val, _exc_tb):\n        self.config.use_cache = self.restore_value\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.NumpyArrayBatcher","title":"<code>NumpyArrayBatcher</code>","text":"<p>               Bases: <code>object</code></p> <p>Batcher instance to handle taking in dictionaries of numpy arrays, appending multiple items to them to increase their batch size, and then stack them into a single batched numpy array for all keys in the dicts.</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>class NumpyArrayBatcher(object):\n    \"\"\"\n    Batcher instance to handle taking in dictionaries of numpy arrays,\n    appending multiple items to them to increase their batch size,\n    and then stack them into a single batched numpy array for all keys in the dicts.\n    \"\"\"\n\n    def __init__(self):\n        self._items = OrderedDict()  # type: Dict[str, List[numpy.ndarray]]\n\n    def __len__(self):\n        if len(self._items) == 0:\n            return 0\n\n        return len(self._items[list(self._items.keys())[0]])\n\n    def append(self, item: Union[numpy.ndarray, Dict[str, numpy.ndarray]]):\n        \"\"\"\n        Append a new item into the current batch.\n        All keys and shapes must match the current state.\n\n        :param item: the item to add for batching\n        \"\"\"\n        if len(self) &lt; 1 and isinstance(item, numpy.ndarray):\n            self._items[NDARRAY_KEY] = [item]\n        elif len(self) &lt; 1:\n            for key, val in item.items():\n                self._items[key] = [val]\n        elif isinstance(item, numpy.ndarray):\n            if NDARRAY_KEY not in self._items:\n                raise ValueError(\n                    \"numpy ndarray passed for item, but prev_batch does not contain one\"\n                )\n\n            if item.shape != self._items[NDARRAY_KEY][0].shape:\n                raise ValueError(\n                    (\n                        \"item of numpy ndarray of shape {} does not \"\n                        \"match the current batch shape of {}\".format(\n                            item.shape, self._items[NDARRAY_KEY][0].shape\n                        )\n                    )\n                )\n\n            self._items[NDARRAY_KEY].append(item)\n        else:\n            diff_keys = list(set(item.keys()) - set(self._items.keys()))\n\n            if len(diff_keys) &gt; 0:\n                raise ValueError(\n                    (\n                        \"numpy dict passed for item, not all keys match \"\n                        \"with the prev_batch. difference: {}\"\n                    ).format(diff_keys)\n                )\n\n            for key, val in item.items():\n                if val.shape != self._items[key][0].shape:\n                    raise ValueError(\n                        (\n                            \"item with key {} of shape {} does not \"\n                            \"match the current batch shape of {}\".format(\n                                key, val.shape, self._items[key][0].shape\n                            )\n                        )\n                    )\n\n                self._items[key].append(val)\n\n    def stack(self) -&gt; Dict[str, numpy.ndarray]:\n        \"\"\"\n        Stack the current items into a batch along a new, zeroed dimension\n\n        :return: the stacked items\n        \"\"\"\n        batch_dict = OrderedDict()\n\n        for key, val in self._items.items():\n            batch_dict[key] = numpy.stack(self._items[key])\n\n        return batch_dict\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.NumpyArrayBatcher.append","title":"<code>append(item)</code>","text":"<p>Append a new item into the current batch. All keys and shapes must match the current state.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[ndarray, Dict[str, ndarray]]</code> <p>the item to add for batching</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def append(self, item: Union[numpy.ndarray, Dict[str, numpy.ndarray]]):\n    \"\"\"\n    Append a new item into the current batch.\n    All keys and shapes must match the current state.\n\n    :param item: the item to add for batching\n    \"\"\"\n    if len(self) &lt; 1 and isinstance(item, numpy.ndarray):\n        self._items[NDARRAY_KEY] = [item]\n    elif len(self) &lt; 1:\n        for key, val in item.items():\n            self._items[key] = [val]\n    elif isinstance(item, numpy.ndarray):\n        if NDARRAY_KEY not in self._items:\n            raise ValueError(\n                \"numpy ndarray passed for item, but prev_batch does not contain one\"\n            )\n\n        if item.shape != self._items[NDARRAY_KEY][0].shape:\n            raise ValueError(\n                (\n                    \"item of numpy ndarray of shape {} does not \"\n                    \"match the current batch shape of {}\".format(\n                        item.shape, self._items[NDARRAY_KEY][0].shape\n                    )\n                )\n            )\n\n        self._items[NDARRAY_KEY].append(item)\n    else:\n        diff_keys = list(set(item.keys()) - set(self._items.keys()))\n\n        if len(diff_keys) &gt; 0:\n            raise ValueError(\n                (\n                    \"numpy dict passed for item, not all keys match \"\n                    \"with the prev_batch. difference: {}\"\n                ).format(diff_keys)\n            )\n\n        for key, val in item.items():\n            if val.shape != self._items[key][0].shape:\n                raise ValueError(\n                    (\n                        \"item with key {} of shape {} does not \"\n                        \"match the current batch shape of {}\".format(\n                            key, val.shape, self._items[key][0].shape\n                        )\n                    )\n                )\n\n            self._items[key].append(val)\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.NumpyArrayBatcher.stack","title":"<code>stack()</code>","text":"<p>Stack the current items into a batch along a new, zeroed dimension</p> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>the stacked items</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def stack(self) -&gt; Dict[str, numpy.ndarray]:\n    \"\"\"\n    Stack the current items into a batch along a new, zeroed dimension\n\n    :return: the stacked items\n    \"\"\"\n    batch_dict = OrderedDict()\n\n    for key, val in self._items.items():\n        batch_dict[key] = numpy.stack(self._items[key])\n\n    return batch_dict\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.DisableQuantization","title":"<code>DisableQuantization(module)</code>","text":"<p>Disable quantization during forward passes after applying a quantization config</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>@contextlib.contextmanager\ndef DisableQuantization(module: torch.nn.Module):\n    \"\"\"\n    Disable quantization during forward passes after applying a quantization config\n    \"\"\"\n    try:\n        module.apply(disable_quantization)\n        yield\n    finally:\n        module.apply(enable_quantization)\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.bucket_iterable","title":"<code>bucket_iterable(val, num_buckets=3, edge_percent=0.05, sort_highest=True, sort_key=None)</code>","text":"<p>Bucket iterable into subarray consisting of the first top percentage followed by the rest of the iterable sliced into equal sliced groups.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Iterable[Any]</code> <p>The iterable to bucket</p> required <code>num_buckets</code> <code>int</code> <p>The number of buckets to group the iterable into, does not include the top bucket</p> <code>3</code> <code>edge_percent</code> <code>float</code> <p>Group the first percent into its own bucket. If sort_highest, then this is the top percent, else bottom percent. If &lt;= 0, then will not create an edge bucket</p> <code>0.05</code> <code>sort_highest</code> <code>bool</code> <p>True to sort such that the highest percent is first and will create buckets in descending order. False to sort so lowest is first and create buckets in ascending order.</p> <code>True</code> <code>sort_key</code> <code>Callable[[Any], Any]</code> <p>The sort_key, if any, to use for sorting the iterable after converting it to a list</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[int, Any]]</code> <p>a list of each value mapped to the bucket it was sorted into</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def bucket_iterable(\n    val: Iterable[Any],\n    num_buckets: int = 3,\n    edge_percent: float = 0.05,\n    sort_highest: bool = True,\n    sort_key: Callable[[Any], Any] = None,\n) -&gt; List[Tuple[int, Any]]:\n    \"\"\"\n    Bucket iterable into subarray consisting of the first top percentage\n    followed by the rest of the iterable sliced into equal sliced groups.\n\n    :param val: The iterable to bucket\n    :param num_buckets: The number of buckets to group the iterable into,\n        does not include the top bucket\n    :param edge_percent: Group the first percent into its own bucket.\n        If sort_highest, then this is the top percent, else bottom percent.\n        If &lt;= 0, then will not create an edge bucket\n    :param sort_highest: True to sort such that the highest percent is first\n        and will create buckets in descending order.\n        False to sort so lowest is first and create buckets in ascending order.\n    :param sort_key: The sort_key, if any, to use for sorting the iterable\n        after converting it to a list\n    :return: a list of each value mapped to the bucket it was sorted into\n    \"\"\"\n\n    val_list = [v for v in val]\n    val_list.sort(key=sort_key, reverse=sort_highest)\n    bucketed_values = []\n    edge_count = round(edge_percent * len(val_list))\n\n    if edge_count &gt; 0:\n        bucketed_values.extend([(-1, val) for val in val_list[:edge_count]])\n        val_list = val_list[edge_count:]\n\n    buckets_count = round(len(val_list) / float(num_buckets))\n\n    for bucket in range(num_buckets):\n        add_vals = val_list[:buckets_count] if bucket &lt; num_buckets - 1 else val_list\n        val_list = val_list[buckets_count:] if bucket &lt; num_buckets - 1 else []\n        bucketed_values.extend([(bucket, val) for val in add_vals])\n\n    return bucketed_values\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.calibration_forward_context","title":"<code>calibration_forward_context(model)</code>","text":"<p>Context in which all calibration forward passes should occur.</p> <ul> <li>Remove gradient calculations</li> <li>Disable the KV cache</li> <li>Disable train mode and enable eval mode</li> </ul> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>@contextlib.contextmanager\ndef calibration_forward_context(model: PreTrainedModel):\n    \"\"\"\n    Context in which all calibration forward passes should occur.\n\n    - Remove gradient calculations\n    - Disable the KV cache\n    - Disable train mode and enable eval mode\n    \"\"\"\n    with (\n        torch.no_grad(),\n        DisableKVCache(model),\n        eval_context(model),\n    ):\n        yield\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.clean_path","title":"<code>clean_path(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the directory or file path to clean</p> required <p>Returns:</p> Type Description <code>str</code> <p>a cleaned version that expands the user path and creates an absolute path</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def clean_path(path: str) -&gt; str:\n    \"\"\"\n    :param path: the directory or file path to clean\n    :return: a cleaned version that expands the user path and creates an absolute path\n    \"\"\"\n    return os.path.abspath(os.path.expanduser(path))\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.convert_to_bool","title":"<code>convert_to_bool(val)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>val</code> <code>Any</code> <p>the value to be converted to a bool, supports logical values as strings ie True, t, false, 0</p> required <p>Returns:</p> Type Description <p>the boolean representation of the value, if it can't be determined, falls back on returning True</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def convert_to_bool(val: Any):\n    \"\"\"\n    :param val: the value to be converted to a bool,\n        supports logical values as strings ie True, t, false, 0\n    :return: the boolean representation of the value, if it can't be determined,\n        falls back on returning True\n    \"\"\"\n    return (\n        bool(val)\n        if not isinstance(val, str)\n        else bool(val) and \"f\" not in val.lower() and \"0\" not in val.lower()\n    )\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.create_dirs","title":"<code>create_dirs(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the directory path to try and create</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def create_dirs(path: str):\n    \"\"\"\n    :param path: the directory path to try and create\n    \"\"\"\n    path = clean_path(path)\n\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            # Unexpected OSError, re-raise.\n            raise\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.create_parent_dirs","title":"<code>create_parent_dirs(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the file path to try to create the parent directories for</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def create_parent_dirs(path: str):\n    \"\"\"\n    :param path: the file path to try to create the parent directories for\n    \"\"\"\n    parent = os.path.dirname(path)\n    create_dirs(parent)\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.create_unique_dir","title":"<code>create_unique_dir(path, check_number=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the file path to create a unique version of (append numbers until one doesn't exist)</p> required <code>check_number</code> <code>int</code> <p>the number to begin checking for unique versions at</p> <code>0</code> <p>Returns:</p> Type Description <code>str</code> <p>the unique directory path</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def create_unique_dir(path: str, check_number: int = 0) -&gt; str:\n    \"\"\"\n    :param path: the file path to create a unique version of\n        (append numbers until one doesn't exist)\n    :param check_number: the number to begin checking for unique versions at\n    :return: the unique directory path\n    \"\"\"\n    check_path = clean_path(\"{}-{:04d}\".format(path, check_number))\n\n    if not os.path.exists(check_path):\n        return check_path\n\n    return create_unique_dir(path, check_number + 1)\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.flatten_iterable","title":"<code>flatten_iterable(li)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>li</code> <code>Iterable</code> <p>a possibly nested iterable of items to be flattened</p> required <p>Returns:</p> Type Description <p>a flattened version of the list where all elements are in a single list flattened in a depth first pattern</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def flatten_iterable(li: Iterable):\n    \"\"\"\n    :param li: a possibly nested iterable of items to be flattened\n    :return: a flattened version of the list where all elements are in a single list\n             flattened in a depth first pattern\n    \"\"\"\n\n    def _flatten_gen(_li):\n        for el in _li:\n            if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\n                yield from _flatten_gen(el)\n            else:\n                yield el\n\n    return list(_flatten_gen(li))\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.getattr_chain","title":"<code>getattr_chain(obj, chain_str, *args, **kwargs)</code>","text":"<p>Chain multiple getattr calls, separated by <code>.</code></p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>base object whose attributes are being retrieved</p> required <code>chain_str</code> <code>str</code> <p>attribute names separated by <code>.</code></p> required <code>default</code> <p>default value, throw error otherwise</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def getattr_chain(obj: Any, chain_str: str, *args, **kwargs) -&gt; Any:\n    \"\"\"\n    Chain multiple getattr calls, separated by `.`\n\n    :param obj: base object whose attributes are being retrieved\n    :param chain_str: attribute names separated by `.`\n    :param default: default value, throw error otherwise\n\n    \"\"\"\n    if len(args) &gt;= 1:\n        has_default = True\n        default = args[0]\n    elif \"default\" in kwargs:\n        has_default = True\n        default = kwargs[\"default\"]\n    else:\n        has_default = False\n\n    attr_names = chain_str.split(\".\")\n\n    res = obj\n    for attr_name in attr_names:\n        if not hasattr(res, attr_name):\n            if has_default:\n                return default\n            else:\n                raise AttributeError(f\"{res} object has no attribute {attr_name}\")\n        res = getattr(res, attr_name)\n\n    return res\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.import_from_path","title":"<code>import_from_path(path)</code>","text":"<p>Import the module and the name of the function/class separated by : Examples:   path = \"/path/to/file.py:func_or_class_name\"   path = \"/path/to/file:focn\"   path = \"path.to.file:focn\"</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path including the file path and object name</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def import_from_path(path: str) -&gt; str:\n    \"\"\"\n    Import the module and the name of the function/class separated by :\n    Examples:\n      path = \"/path/to/file.py:func_or_class_name\"\n      path = \"/path/to/file:focn\"\n      path = \"path.to.file:focn\"\n    :param path: path including the file path and object name\n    :return Function or class object\n    \"\"\"\n    original_path, class_name = path.split(\":\")\n    _path = original_path\n\n    path = original_path.split(\".py\")[0]\n    path = re.sub(r\"/+\", \".\", path)\n    try:\n        module = importlib.import_module(path)\n    except ImportError:\n        raise ImportError(f\"Cannot find module with path {_path}\")\n\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        raise AttributeError(f\"Cannot find {class_name} in {_path}\")\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.interpolate","title":"<code>interpolate(x_cur, x0, x1, y0, y1, inter_func='linear')</code>","text":"<p>note, caps values at their min of x0 and max x1, designed to not work outside of that range for implementation reasons</p> <p>Parameters:</p> Name Type Description Default <code>x_cur</code> <code>float</code> <p>the current value for x, should be between x0 and x1</p> required <code>x0</code> <code>float</code> <p>the minimum for x to interpolate between</p> required <code>x1</code> <code>float</code> <p>the maximum for x to interpolate between</p> required <code>y0</code> <code>Any</code> <p>the minimum for y to interpolate between</p> required <code>y1</code> <code>Any</code> <p>the maximum for y to interpolate between</p> required <code>inter_func</code> <code>str</code> <p>the type of function to interpolate with: linear, cubic, inverse_cubic</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Any</code> <p>the interpolated value projecting x into y for the given interpolation function</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def interpolate(\n    x_cur: float, x0: float, x1: float, y0: Any, y1: Any, inter_func: str = \"linear\"\n) -&gt; Any:\n    \"\"\"\n    note, caps values at their min of x0 and max x1,\n    designed to not work outside of that range for implementation reasons\n\n    :param x_cur: the current value for x, should be between x0 and x1\n    :param x0: the minimum for x to interpolate between\n    :param x1: the maximum for x to interpolate between\n    :param y0: the minimum for y to interpolate between\n    :param y1: the maximum for y to interpolate between\n    :param inter_func: the type of function to interpolate with:\n        linear, cubic, inverse_cubic\n    :return: the interpolated value projecting x into y for the given\n        interpolation function\n    \"\"\"\n    if inter_func not in INTERPOLATION_FUNCS:\n        raise ValueError(\n            \"unsupported inter_func given of {} must be one of {}\".format(\n                inter_func, INTERPOLATION_FUNCS\n            )\n        )\n\n    # convert our x to 0-1 range since equations are designed to fit in\n    # (0,0)-(1,1) space\n    x_per = (x_cur - x0) / (x1 - x0)\n\n    # map x to y using the desired function in (0,0)-(1,1) space\n    if inter_func == \"linear\":\n        y_per = x_per\n    elif inter_func == \"cubic\":\n        # https://www.wolframalpha.com/input/?i=1-(1-x)%5E3+from+0+to+1\n        y_per = 1 - (1 - x_per) ** 3\n    elif inter_func == \"inverse_cubic\":\n        # https://www.wolframalpha.com/input/?i=1-(1-x)%5E(1%2F3)+from+0+to+1\n        y_per = 1 - (1 - x_per) ** (1 / 3)\n    else:\n        raise ValueError(\n            \"unsupported inter_func given of {} in interpolate\".format(inter_func)\n        )\n\n    if y_per &lt;= 0.0 + sys.float_info.epsilon:\n        return y0\n\n    if y_per &gt;= 1.0 - sys.float_info.epsilon:\n        return y1\n\n    # scale the threshold based on what we want the current to be\n    return y_per * (y1 - y0) + y0\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.interpolate_list_linear","title":"<code>interpolate_list_linear(measurements, x_val)</code>","text":"<p>interpolate for input values within a list of measurements linearly</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>List[Tuple[float, float]]</code> <p>the measurements to interpolate the output value between</p> required <code>x_val</code> <code>Union[float, List[float]]</code> <p>the target values to interpolate to the second dimension</p> required <p>Returns:</p> Type Description <code>List[Tuple[float, float]]</code> <p>a list of tuples containing the target values, interpolated values</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def interpolate_list_linear(\n    measurements: List[Tuple[float, float]], x_val: Union[float, List[float]]\n) -&gt; List[Tuple[float, float]]:\n    \"\"\"\n    interpolate for input values within a list of measurements linearly\n\n    :param measurements: the measurements to interpolate the output value between\n    :param x_val: the target values to interpolate to the second dimension\n    :return: a list of tuples containing the target values, interpolated values\n    \"\"\"\n    assert len(measurements) &gt; 1\n    measurements.sort(key=lambda v: v[0])\n\n    x_vals = [x_val] if isinstance(x_val, float) else x_val\n    x_vals.sort()\n\n    interpolated = []\n    lower_index = 0\n    higher_index = 1\n\n    for x_val in x_vals:\n        while (\n            x_val &gt; measurements[higher_index][0]\n            and higher_index &lt; len(measurements) - 1\n        ):\n            lower_index += 1\n            higher_index += 1\n\n        x0, y0 = measurements[lower_index]\n        x1, y1 = measurements[higher_index]\n        y_val = y0 + (x_val - x0) * ((y1 - y0) / (x1 - x0))\n        interpolated.append((x_val, y_val))\n\n    return interpolated\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.interpolated_integral","title":"<code>interpolated_integral(measurements)</code>","text":"<p>Calculate the interpolated integal for a group of measurements of the form [(x0, y0), (x1, y1), ...]</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>List[Tuple[float, float]]</code> <p>the measurements to calculate the integral for</p> required <p>Returns:</p> Type Description <p>the integral or area under the curve for the measurements given</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def interpolated_integral(measurements: List[Tuple[float, float]]):\n    \"\"\"\n    Calculate the interpolated integal for a group of measurements of the form\n    [(x0, y0), (x1, y1), ...]\n\n    :param measurements: the measurements to calculate the integral for\n    :return: the integral or area under the curve for the measurements given\n    \"\"\"\n    if len(measurements) &lt; 1:\n        return 0.0\n\n    if len(measurements) == 1:\n        return measurements[0][1]\n\n    measurements.sort(key=lambda v: v[0])\n    integral = 0.0\n\n    for index, (x_val, y_val) in enumerate(measurements):\n        if index &gt;= len(measurements) - 1:\n            continue\n\n        x_next, y_next = measurements[index + 1]\n        x_dist = x_next - x_val\n        area = y_val * x_dist + (y_next - y_val) * x_dist / 2.0\n        integral += area\n\n    return integral\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.is_package_available","title":"<code>is_package_available(package_name, return_version=False)</code>","text":"<p>A helper function to check if a package is available and optionally return its version. This function enforces a check that the package is available and is not just a directory/file with the same name as the package.</p> <p>inspired from: https://github.com/huggingface/transformers/blob/965cf677695dd363285831afca8cf479cf0c600c/src/transformers/utils/import_utils.py#L41</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The package name to check for</p> required <code>return_version</code> <code>bool</code> <p>True to return the version of the package if available</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[bool, str], bool]</code> <p>True if the package is available, False otherwise or a tuple of (bool, version) if return_version is True</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def is_package_available(\n    package_name: str,\n    return_version: bool = False,\n) -&gt; Union[Tuple[bool, str], bool]:\n    \"\"\"\n    A helper function to check if a package is available\n    and optionally return its version. This function enforces\n    a check that the package is available and is not\n    just a directory/file with the same name as the package.\n\n    inspired from:\n    https://github.com/huggingface/transformers/blob/965cf677695dd363285831afca8cf479cf0c600c/src/transformers/utils/import_utils.py#L41\n\n    :param package_name: The package name to check for\n    :param return_version: True to return the version of\n        the package if available\n    :return: True if the package is available, False otherwise or a tuple of\n        (bool, version) if return_version is True\n    \"\"\"\n\n    package_exists = importlib.util.find_spec(package_name) is not None\n    package_version = \"N/A\"\n    if package_exists:\n        try:\n            package_version = importlib.metadata.version(package_name)\n            package_exists = True\n        except importlib.metadata.PackageNotFoundError:\n            package_exists = False\n        logger.debug(f\"Detected {package_name} version {package_version}\")\n    if return_version:\n        return package_exists, package_version\n    else:\n        return package_exists\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.is_url","title":"<code>is_url(val)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>val</code> <code>str</code> <p>value to check if it is a url or not</p> required <p>Returns:</p> Type Description <p>True if value is a URL, False otherwise</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def is_url(val: str):\n    \"\"\"\n    :param val: value to check if it is a url or not\n    :return: True if value is a URL, False otherwise\n    \"\"\"\n\n    try:\n        result = urlparse(val)\n\n        return all([result.scheme, result.netloc])\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.json_to_jsonl","title":"<code>json_to_jsonl(json_file_path, overwrite=True)</code>","text":"<p>Converts a json list file to jsonl file format (used for sharding efficienty)     e.x.         [{\"a\": 1}, {\"a\": 1}]     would convert to:         {\"a\": 1}</p> <p>Parameters:</p> Name Type Description Default <code>json_file_path</code> <code>str</code> <p>file path to a json file path containing a json list of objects</p> required <code>overwrite</code> <code>bool</code> <p>If True, the existing json file will be overwritten, if False, the file will have the same name but with a .jsonl extension</p> <code>True</code> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def json_to_jsonl(json_file_path: str, overwrite: bool = True):\n    \"\"\"\n    Converts a json list file to jsonl file format (used for sharding efficienty)\n        e.x.\n            [{\"a\": 1}, {\"a\": 1}]\n        would convert to:\n            {\"a\": 1}\n            {\"a\": 1}\n    :param json_file_path: file path to a json file path containing a json list\n        of objects\n    :param overwrite: If True, the existing json file will be overwritten, if False,\n        the file will have the same name but with a .jsonl extension\n    \"\"\"\n    if not json_file_path.endswith(\".json\"):\n        raise ValueError(\"json file must have .json extension\")\n    with open(json_file_path) as json_file:\n        json_data = json.load(json_file)\n\n    if not isinstance(json_data, List):\n        raise ValueError(\n            \"Json data must be a list to conver to jsonl format. \"\n            f\"found {type(json_data)}\"\n        )\n\n    jsonl_file_path = json_file_path + (\"\" if overwrite else \"l\")\n    with open(jsonl_file_path, \"w\") as jsonl_file:\n        for json_line in json_data:\n            json.dump(json_line, jsonl_file)  # append json line\n            jsonl_file.write(\"\\n\")  # newline\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.load_labeled_data","title":"<code>load_labeled_data(data, labels, raise_on_error=True)</code>","text":"<p>Load labels and data from disk or from memory and group them together. Assumes sorted ordering for on disk. Will match between when a file glob is passed for either data and/or labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Iterable[Union[str, ndarray, Dict[str, ndarray]]]]</code> <p>the file glob, file path to numpy data tar ball, or list of arrays to use for data</p> required <code>labels</code> <code>Union[None, str, Iterable[Union[str, ndarray, Dict[str, ndarray]]]]</code> <p>the file glob, file path to numpy data tar ball, or list of arrays to use for labels, if any</p> required <code>raise_on_error</code> <code>bool</code> <p>True to raise on any error that occurs; False to log a warning, ignore, and continue</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Tuple[Union[ndarray, Dict[str, ndarray]], Union[None, ndarray, Dict[str, ndarray]]]]</code> <p>a list containing tuples of the data, labels. If labels was passed in as None, will now contain a None for the second index in each tuple</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def load_labeled_data(\n    data: Union[str, Iterable[Union[str, numpy.ndarray, Dict[str, numpy.ndarray]]]],\n    labels: Union[\n        None, str, Iterable[Union[str, numpy.ndarray, Dict[str, numpy.ndarray]]]\n    ],\n    raise_on_error: bool = True,\n) -&gt; List[\n    Tuple[\n        Union[numpy.ndarray, Dict[str, numpy.ndarray]],\n        Union[None, numpy.ndarray, Dict[str, numpy.ndarray]],\n    ]\n]:\n    \"\"\"\n    Load labels and data from disk or from memory and group them together.\n    Assumes sorted ordering for on disk. Will match between when a file glob is passed\n    for either data and/or labels.\n\n    :param data: the file glob, file path to numpy data tar ball, or list of arrays to\n        use for data\n    :param labels: the file glob, file path to numpy data tar ball, or list of arrays\n        to use for labels, if any\n    :param raise_on_error: True to raise on any error that occurs;\n        False to log a warning, ignore, and continue\n    :return: a list containing tuples of the data, labels. If labels was passed in\n        as None, will now contain a None for the second index in each tuple\n    \"\"\"\n    if isinstance(data, str):\n        data = load_numpy_list(data)\n\n    if labels is None:\n        labels = [None for _ in range(len(data))]\n    elif isinstance(labels, str):\n        labels = load_numpy_list(labels)\n\n    if len(data) != len(labels) and labels:\n        # always raise this error, lengths must match\n        raise ValueError(\n            \"len(data) given of {} does not match len(labels) given of {}\".format(\n                len(data), len(labels)\n            )\n        )\n\n    labeled_data = []\n\n    for dat, lab in zip(data, labels):\n        try:\n            if isinstance(dat, str):\n                dat = load_numpy(dat)\n\n            if lab is not None and isinstance(lab, str):\n                lab = load_numpy(lab)\n\n            labeled_data.append((dat, lab))\n        except Exception as err:\n            if raise_on_error:\n                raise err\n            else:\n                logger.error(\"Error creating labeled data: {}\".format(err))\n\n    return labeled_data\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.load_numpy","title":"<code>load_numpy(file_path)</code>","text":"<p>Load a numpy file into either an ndarray or an OrderedDict representing what was in the npz file</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>the file_path to load</p> required <p>Returns:</p> Type Description <code>Union[ndarray, Dict[str, ndarray]]</code> <p>the loaded values from the file</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def load_numpy(file_path: str) -&gt; Union[numpy.ndarray, Dict[str, numpy.ndarray]]:\n    \"\"\"\n    Load a numpy file into either an ndarray or an OrderedDict representing what\n    was in the npz file\n\n    :param file_path: the file_path to load\n    :return: the loaded values from the file\n    \"\"\"\n    file_path = clean_path(file_path)\n    array = numpy.load(file_path)\n\n    if not isinstance(array, numpy.ndarray):\n        tmp_arrray = array\n        array = OrderedDict()\n        for key, val in tmp_arrray.items():\n            array[key] = val\n\n    return array\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.load_numpy_from_tar","title":"<code>load_numpy_from_tar(path)</code>","text":"<p>Load numpy data into a list from a tar file. All files contained in the tar are expected to be the numpy files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the tarfile to load the numpy data from</p> required <p>Returns:</p> Type Description <code>List[Union[ndarray, Dict[str, ndarray]]]</code> <p>the list of loaded numpy data, either arrays or ordereddicts of arrays</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def load_numpy_from_tar(\n    path: str,\n) -&gt; List[Union[numpy.ndarray, Dict[str, numpy.ndarray]]]:\n    \"\"\"\n    Load numpy data into a list from a tar file.\n    All files contained in the tar are expected to be the numpy files.\n    :param path: path to the tarfile to load the numpy data from\n    :return: the list of loaded numpy data, either arrays or ordereddicts of arrays\n    \"\"\"\n    tar = tarfile.open(path, \"r\")\n    files = tar.getmembers()\n    files = sorted([file.name for file in files])\n    data = []\n\n    for file in files:\n        extracted = BytesIO()\n        extracted.write(tar.extractfile(file).read())\n        extracted.seek(0)\n        array = numpy.load(extracted)\n        data.append(_fix_loaded_numpy(array))\n\n    return data\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.load_numpy_list","title":"<code>load_numpy_list(data)</code>","text":"<p>Load numpy data into a list</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Iterable[Union[str, ndarray, Dict[str, ndarray]]]]</code> <p>the data to load, one of: [folder path, iterable of file paths, iterable of numpy arrays]</p> required <p>Returns:</p> Type Description <code>List[Union[ndarray, Dict[str, ndarray]]]</code> <p>the list of loaded data items</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def load_numpy_list(\n    data: Union[str, Iterable[Union[str, numpy.ndarray, Dict[str, numpy.ndarray]]]],\n) -&gt; List[Union[numpy.ndarray, Dict[str, numpy.ndarray]]]:\n    \"\"\"\n    Load numpy data into a list\n    :param data: the data to load, one of:\n        [folder path, iterable of file paths, iterable of numpy arrays]\n    :return: the list of loaded data items\n    \"\"\"\n    loaded = []\n\n    if isinstance(data, str):\n        if os.path.isfile(data) and tarfile.is_tarfile(data):\n            data = load_numpy_from_tar(data)\n        elif os.path.isfile(data) and \".np\" in data:\n            # treat as a numpy file to load from\n            data = [load_numpy(data)]\n        else:\n            # load from directory or glob\n            glob_path = os.path.join(data, \"*\") if os.path.isdir(data) else data\n            data = sorted(glob.glob(glob_path))\n\n    for dat in data:\n        if isinstance(dat, str):\n            dat = load_numpy(dat)\n\n        loaded.append(dat)\n\n    return loaded\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.patch_attr","title":"<code>patch_attr(base, attr, value)</code>","text":"<p>Patch the value of an object attribute. Original value is restored upon exit</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>object</code> <p>object which has the attribute to patch</p> required <code>attr</code> <code>str</code> <p>name of the the attribute to patch</p> required <code>value</code> <code>Any</code> <p>used to replace original value  Usage: &gt;&gt;&gt; from types import SimpleNamespace &gt;&gt;&gt; obj = SimpleNamespace() &gt;&gt;&gt; with patch_attr(obj, \"attribute\", \"value\"): ...     assert obj.attribute == \"value\" &gt;&gt;&gt; assert not hasattr(obj, \"attribute\")</p> required Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>@contextlib.contextmanager\ndef patch_attr(base: object, attr: str, value: Any):\n    \"\"\"\n    Patch the value of an object attribute. Original value is restored upon exit\n\n    :param base: object which has the attribute to patch\n    :param attr: name of the the attribute to patch\n    :param value: used to replace original value\n\n    Usage:\n    &gt;&gt;&gt; from types import SimpleNamespace\n    &gt;&gt;&gt; obj = SimpleNamespace()\n    &gt;&gt;&gt; with patch_attr(obj, \"attribute\", \"value\"):\n    ...     assert obj.attribute == \"value\"\n    &gt;&gt;&gt; assert not hasattr(obj, \"attribute\")\n    \"\"\"\n    _sentinel = object()\n    original_value = getattr(base, attr, _sentinel)\n\n    setattr(base, attr, value)\n    try:\n        yield\n    finally:\n        if original_value is not _sentinel:\n            setattr(base, attr, original_value)\n        else:\n            delattr(base, attr)\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.path_file_count","title":"<code>path_file_count(path, pattern='*')</code>","text":"<p>Return the number of files that match the given pattern under the given path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path to the directory to look for files under</p> required <code>pattern</code> <code>str</code> <p>the pattern the files must match to be counted</p> <code>'*'</code> <p>Returns:</p> Type Description <code>int</code> <p>the number of files matching the pattern under the directory</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def path_file_count(path: str, pattern: str = \"*\") -&gt; int:\n    \"\"\"\n    Return the number of files that match the given pattern under the given path\n\n    :param path: the path to the directory to look for files under\n    :param pattern: the pattern the files must match to be counted\n    :return: the number of files matching the pattern under the directory\n    \"\"\"\n    path = clean_path(path)\n\n    return len(fnmatch.filter(os.listdir(path), pattern))\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.path_file_size","title":"<code>path_file_size(path)</code>","text":"<p>Return the total size, in bytes, for a path on the file system</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path (directory or file) to get the size for</p> required <p>Returns:</p> Type Description <code>int</code> <p>the size of the path, in bytes, as stored on disk</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def path_file_size(path: str) -&gt; int:\n    \"\"\"\n    Return the total size, in bytes, for a path on the file system\n\n    :param path: the path (directory or file) to get the size for\n    :return: the size of the path, in bytes, as stored on disk\n    \"\"\"\n\n    if not os.path.isdir(path):\n        stat = os.stat(path)\n\n        return stat.st_size\n\n    total_size = 0\n    seen = {}\n\n    for dir_path, dir_names, filenames in os.walk(path):\n        for file in filenames:\n            file_path = os.path.join(dir_path, file)\n\n            try:\n                stat = os.stat(file_path)\n            except OSError:\n                continue\n\n            try:\n                seen[stat.st_ino]\n            except KeyError:\n                seen[stat.st_ino] = True\n            else:\n                continue\n\n            total_size += stat.st_size\n\n    return total_size\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.save_numpy","title":"<code>save_numpy(array, export_dir, name, npz=True)</code>","text":"<p>Save a numpy array or collection of numpy arrays to disk</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Union[ndarray, Dict[str, ndarray], Iterable[ndarray]]</code> <p>the array or collection of arrays to save</p> required <code>export_dir</code> <code>str</code> <p>the directory to export the numpy file into</p> required <code>name</code> <code>str</code> <p>the name of the file to export to (without extension)</p> required <code>npz</code> <code>bool</code> <p>True to save as an npz compressed file, False for standard npy. Note, npy can only be used for single numpy arrays</p> <code>True</code> <p>Returns:</p> Type Description <p>the saved path</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def save_numpy(\n    array: Union[numpy.ndarray, Dict[str, numpy.ndarray], Iterable[numpy.ndarray]],\n    export_dir: str,\n    name: str,\n    npz: bool = True,\n):\n    \"\"\"\n    Save a numpy array or collection of numpy arrays to disk\n\n    :param array: the array or collection of arrays to save\n    :param export_dir: the directory to export the numpy file into\n    :param name: the name of the file to export to (without extension)\n    :param npz: True to save as an npz compressed file, False for standard npy.\n        Note, npy can only be used for single numpy arrays\n    :return: the saved path\n    \"\"\"\n    create_dirs(export_dir)\n    export_path = os.path.join(\n        export_dir, \"{}.{}\".format(name, \"npz\" if npz else \"npy\")\n    )\n\n    if isinstance(array, numpy.ndarray) and npz:\n        numpy.savez_compressed(export_path, array)\n    elif isinstance(array, numpy.ndarray):\n        numpy.save(export_path, array)\n    elif isinstance(array, Dict) and npz:\n        numpy.savez_compressed(export_path, **array)\n    elif isinstance(array, Dict):\n        raise ValueError(\"Dict can only be exported to an npz file\")\n    elif isinstance(array, Iterable) and npz:\n        numpy.savez_compressed(export_path, *[val for val in array])\n    elif isinstance(array, Iterable):\n        raise ValueError(\"Iterable can only be exported to an npz file\")\n    else:\n        raise ValueError(\"Unrecognized type given for array {}\".format(array))\n\n    return export_path\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.tensor_export","title":"<code>tensor_export(tensor, export_dir, name, npz=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Union[ndarray, Dict[str, ndarray], Iterable[ndarray]]</code> <p>tensor to export to a saved numpy array file</p> required <code>export_dir</code> <code>str</code> <p>the directory to export the file in</p> required <code>name</code> <code>str</code> <p>the name of the file, .npy will be appended to it</p> required <code>npz</code> <code>bool</code> <p>True to export as an npz file, False otherwise</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>the path of the numpy file the tensor was exported to</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def tensor_export(\n    tensor: Union[numpy.ndarray, Dict[str, numpy.ndarray], Iterable[numpy.ndarray]],\n    export_dir: str,\n    name: str,\n    npz: bool = True,\n) -&gt; str:\n    \"\"\"\n    :param tensor: tensor to export to a saved numpy array file\n    :param export_dir: the directory to export the file in\n    :param name: the name of the file, .npy will be appended to it\n    :param npz: True to export as an npz file, False otherwise\n    :return: the path of the numpy file the tensor was exported to\n    \"\"\"\n    create_dirs(export_dir)\n    export_path = os.path.join(\n        export_dir, \"{}.{}\".format(name, \"npz\" if npz else \"npy\")\n    )\n\n    if isinstance(tensor, numpy.ndarray) and npz:\n        numpy.savez_compressed(export_path, tensor)\n    elif isinstance(tensor, numpy.ndarray):\n        numpy.save(export_path, tensor)\n    elif isinstance(tensor, Dict) and npz:\n        numpy.savez_compressed(export_path, **tensor)\n    elif isinstance(tensor, Dict):\n        raise ValueError(\"tensor dictionaries can only be saved as npz\")\n    elif isinstance(tensor, Iterable) and npz:\n        numpy.savez_compressed(export_path, *tensor)\n    elif isinstance(tensor, Iterable):\n        raise ValueError(\"tensor iterables can only be saved as npz\")\n    else:\n        raise ValueError(\"unknown type give for tensor {}\".format(tensor))\n\n    return export_path\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.tensors_export","title":"<code>tensors_export(tensors, export_dir, name_prefix, counter=0, break_batch=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Union[ndarray, Dict[str, ndarray], Iterable[ndarray]]</code> <p>the tensors to export to a saved numpy array file</p> required <code>export_dir</code> <code>str</code> <p>the directory to export the files in</p> required <code>name_prefix</code> <code>str</code> <p>the prefix name for the tensors to save as, will append info about the position of the tensor in a list or dict in addition to the .npy file format</p> required <code>counter</code> <code>int</code> <p>the current counter to save the tensor at</p> <code>0</code> <code>break_batch</code> <code>bool</code> <p>treat the tensor as a batch and break apart into multiple tensors</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>the exported paths</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def tensors_export(\n    tensors: Union[numpy.ndarray, Dict[str, numpy.ndarray], Iterable[numpy.ndarray]],\n    export_dir: str,\n    name_prefix: str,\n    counter: int = 0,\n    break_batch: bool = False,\n) -&gt; List[str]:\n    \"\"\"\n    :param tensors: the tensors to export to a saved numpy array file\n    :param export_dir: the directory to export the files in\n    :param name_prefix: the prefix name for the tensors to save as, will append\n        info about the position of the tensor in a list or dict in addition\n        to the .npy file format\n    :param counter: the current counter to save the tensor at\n    :param break_batch: treat the tensor as a batch and break apart into\n        multiple tensors\n    :return: the exported paths\n    \"\"\"\n    create_dirs(export_dir)\n    exported_paths = []\n\n    if break_batch:\n        _tensors_export_batch(tensors, export_dir, name_prefix, counter, exported_paths)\n    else:\n        _tensors_export_recursive(\n            tensors, export_dir, name_prefix, counter, exported_paths\n        )\n\n    return exported_paths\n</code></pre>"},{"location":"reference/llmcompressor/utils/helpers/#llmcompressor.utils.helpers.validate_str_iterable","title":"<code>validate_str_iterable(val, error_desc='')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>val</code> <code>Union[str, Iterable[str]]</code> <p>the value to validate, check that it is a list (and flattens it), otherwise checks that it's an ALL or ALL_PRUNABLE string, otherwise raises a ValueError</p> required <code>error_desc</code> <code>str</code> <p>the description to raise an error with in the event that the val wasn't valid</p> <code>''</code> <p>Returns:</p> Type Description <code>Union[str, Iterable[str]]</code> <p>the validated version of the param</p> Source code in <code>src/llmcompressor/utils/helpers.py</code> <pre><code>def validate_str_iterable(\n    val: Union[str, Iterable[str]], error_desc: str = \"\"\n) -&gt; Union[str, Iterable[str]]:\n    \"\"\"\n    :param val: the value to validate, check that it is a list (and flattens it),\n        otherwise checks that it's an __ALL__ or __ALL_PRUNABLE__ string,\n        otherwise raises a ValueError\n    :param error_desc: the description to raise an error with in the event that\n        the val wasn't valid\n    :return: the validated version of the param\n    \"\"\"\n    if isinstance(val, str):\n        if val.upper() != ALL_TOKEN and val.upper() != ALL_PRUNABLE_TOKEN:\n            raise ValueError(\n                \"unsupported string ({}) given in {}\".format(val, error_desc)\n            )\n\n        return val.upper()\n\n    if isinstance(val, Iterable):\n        return flatten_iterable(val)\n\n    raise ValueError(\"unsupported type ({}) given in {}\".format(val, error_desc))\n</code></pre>"},{"location":"reference/llmcompressor/utils/metric_logging/","title":"llmcompressor.utils.metric_logging","text":""},{"location":"reference/llmcompressor/utils/metric_logging/#llmcompressor.utils.metric_logging.CompressionLogger","title":"<code>CompressionLogger</code>","text":"<p>Log metrics related to compression algorithm</p> <p>Parameters:</p> Name Type Description Default <code>start_tick</code> <p>time when algorithm started\"</p> required <code>losses</code> <p>loss as result of algorithm</p> required Source code in <code>src/llmcompressor/utils/metric_logging.py</code> <pre><code>class CompressionLogger:\n    \"\"\"\n    Log metrics related to compression algorithm\n\n    :param start_tick: time when algorithm started\"\n    :param losses: loss as result of algorithm\n    \"\"\"\n\n    def __init__(self, module: torch.nn.Module):\n        self.module = module\n        self.start_tick = None\n        self.loss = None\n\n    def set_loss(self, loss: float):\n        self.loss = loss\n\n    def __enter__(self) -&gt; \"CompressionLogger\":\n        self.start_tick = time.time()\n        return self\n\n    def __exit__(self, _exc_type, _exc_val, _exc_tb):\n        stop_tick = time.time()\n        patch = logger.patch(lambda r: r.update(function=\"compress\"))\n\n        if self.start_tick is not None:\n            duration = stop_tick - self.start_tick\n            patch.log(\"METRIC\", f\"time {duration:.2f}s\")\n        if self.loss is not None:\n            patch.log(\"METRIC\", f\"error {self.loss:.2f}\")\n\n        gpu_usage = get_GPU_memory_usage()\n        if len(gpu_usage) &gt; 0:\n            for i in range(len(gpu_usage)):\n                perc = gpu_usage[i][0] * 100\n                total_memory = int(gpu_usage[i][1])  # GB\n                patch.log(\n                    \"METRIC\",\n                    (\n                        f\"GPU {i} | usage: {perc:.2f}%\"\n                        f\" | total memory: {total_memory} GB\"\n                    ),\n                )\n\n        compressed_size = get_layer_size_mb(self.module)\n        patch.log(\"METRIC\", f\"Compressed module size: {compressed_size} MB\")\n</code></pre>"},{"location":"reference/llmcompressor/utils/metric_logging/#llmcompressor.utils.metric_logging.get_GPU_usage_amd","title":"<code>get_GPU_usage_amd()</code>","text":"<p>get gpu usage for AMD GPUs using amdsmi lib</p> Source code in <code>src/llmcompressor/utils/metric_logging.py</code> <pre><code>def get_GPU_usage_amd() -&gt; List[Tuple[float, float]]:\n    \"\"\"\n    get gpu usage for AMD GPUs using amdsmi lib\n    \"\"\"\n    usage = []\n    try:\n        import amdsmi\n\n        try:\n            amdsmi.amdsmi_init()\n            devices = amdsmi.amdsmi_get_processor_handles()\n\n            for device in devices:\n                vram_memory_usage = amdsmi.amdsmi_get_gpu_memory_usage(\n                    device, amdsmi.amdsmi_interface.AmdSmiMemoryType.VRAM\n                )\n                vram_memory_total = amdsmi.amdsmi_get_gpu_memory_total(\n                    device, amdsmi.amdsmi_interface.AmdSmiMemoryType.VRAM\n                )\n\n                memory_percentage = vram_memory_usage / vram_memory_total\n                usage.append(\n                    (memory_percentage, vram_memory_total / (1e9)),\n                )\n            amdsmi.amdsmi_shut_down()\n        except amdsmi.AmdSmiException as error:\n            logger.warning(f\"amdsmi library error:\\n {error}\")\n    except ImportError:\n        logger.warning(\"Failed to obtain GPU usage from amdsmi\")\n\n    return usage\n</code></pre>"},{"location":"reference/llmcompressor/utils/metric_logging/#llmcompressor.utils.metric_logging.get_GPU_usage_nv","title":"<code>get_GPU_usage_nv()</code>","text":"<p>get gpu usage for Nvidia GPUs using nvml lib</p> Source code in <code>src/llmcompressor/utils/metric_logging.py</code> <pre><code>def get_GPU_usage_nv() -&gt; List[Tuple[float, float]]:\n    \"\"\"\n    get gpu usage for Nvidia GPUs using nvml lib\n    \"\"\"\n    try:\n        import pynvml\n        from pynvml import NVMLError\n\n        try:\n            pynvml.nvmlInit()\n        except NVMLError as _err:\n            logger.warning(f\"Pynml library error:\\n {_err}\")\n            return []\n\n        device_count = pynvml.nvmlDeviceGetCount()\n        usage = []  # [(percentage, total_memory_MB)]\n\n        # Iterate through all GPUs\n        for i in range(device_count):\n            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n            memory_usage_percentage = mem_info.used / mem_info.total\n            total_memory_gb = mem_info.total / (1e9)\n            usage.append(\n                (memory_usage_percentage, total_memory_gb),\n            )\n        pynvml.nvmlShutdown()\n        return usage\n\n    except ImportError:\n        logger.warning(\"Failed to obtain GPU usage from pynvml\")\n        return []\n</code></pre>"},{"location":"reference/llmcompressor/utils/fsdp/","title":"llmcompressor.utils.fsdp","text":""},{"location":"reference/llmcompressor/utils/fsdp/context/","title":"llmcompressor.utils.fsdp.context","text":""},{"location":"reference/llmcompressor/utils/fsdp/context/#llmcompressor.utils.fsdp.context.fix_fsdp_module_name","title":"<code>fix_fsdp_module_name(name)</code>","text":"<p>Remove FSDP wrapper prefixes from a module name. Accounts for scenario where FSDP_WRAPPED_MODULE is at the end of the name, as well as in the middle.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name to strip</p> required <p>Returns:</p> Type Description <code>str</code> <p>stripped name</p> Source code in <code>src/llmcompressor/utils/fsdp/context.py</code> <pre><code>def fix_fsdp_module_name(name: str) -&gt; str:\n    \"\"\"\n    Remove FSDP wrapper prefixes from a module name.\n    Accounts for scenario where FSDP_WRAPPED_MODULE is\n    at the end of the name, as well as in the middle.\n\n    :param name: name to strip\n    :return: stripped name\n    \"\"\"\n    if FullyShardedDataParallel is None:\n        return name\n\n    return name.replace(FSDP_WRAPPED_MODULE + \".\", \"\").replace(\n        \".\" + FSDP_WRAPPED_MODULE, \"\"\n    )\n</code></pre>"},{"location":"reference/llmcompressor/utils/fsdp/context/#llmcompressor.utils.fsdp.context.main_process_first_context","title":"<code>main_process_first_context()</code>","text":"<p>Creates a context manager where the main process runs the block before all other processes. Returns a nullcontext when called from a single process application.</p> Source code in <code>src/llmcompressor/utils/fsdp/context.py</code> <pre><code>def main_process_first_context():\n    \"\"\"\n    Creates a context manager where the main process runs the block before all other\n    processes. Returns a nullcontext when called from a single process application.\n    \"\"\"\n    if Accelerator is None:\n        return nullcontext()\n\n    return Accelerator().main_process_first()\n</code></pre>"},{"location":"reference/llmcompressor/utils/fsdp/helpers/","title":"llmcompressor.utils.fsdp.helpers","text":""},{"location":"reference/llmcompressor/utils/fsdp/helpers/#llmcompressor.utils.fsdp.helpers.get_fsdp_parent","title":"<code>get_fsdp_parent(layer_name, model)</code>","text":"<p>Gets the closest parent of layer_name that is wrapped by FSDP. If no FSDP wrapper is found just return None</p> <p>:model: pytorch module to search through</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>layer name in model to get parent of</p> required <p>Returns:</p> Type Description <code>Optional[Module]</code> <p>FSDP wrapped parent of layer_name if available, otherwise None</p> Source code in <code>src/llmcompressor/utils/fsdp/helpers.py</code> <pre><code>def get_fsdp_parent(layer_name: str, model: Module) -&gt; Optional[Module]:\n    \"\"\"\n    Gets the closest parent of layer_name that is wrapped by FSDP. If no FSDP wrapper\n    is found just return None\n\n    :param layer_name: layer name in model to get parent of\n    :model: pytorch module to search through\n    :return: FSDP wrapped parent of layer_name if available, otherwise None\n    \"\"\"\n    if not is_fsdp_model(model):\n        return None\n\n    parent_name = layer_name\n    parent = operator.attrgetter(parent_name)(model)\n    while not isinstance(parent, FullyShardedDataParallel):\n        if len(parent_name) == 0:  # we've reached the root module and its not FSDP\n            # this should never get hit because we check for an FSDP root above\n            # but while statements without a backup are too scary\n            return None\n        parent_name = \".\".join(parent_name.split(\".\")[:-1])\n        parent = operator.attrgetter(parent_name)(model)\n\n    return parent\n</code></pre>"},{"location":"reference/llmcompressor/utils/fsdp/helpers/#llmcompressor.utils.fsdp.helpers.is_fsdp_model","title":"<code>is_fsdp_model(model)</code>","text":"<p>Check if a model instance is wrapped by FSDP</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>pytorch model to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if module is wrapped, False otherwise</p> Source code in <code>src/llmcompressor/utils/fsdp/helpers.py</code> <pre><code>def is_fsdp_model(model: Module) -&gt; bool:\n    \"\"\"\n    Check if a model instance is wrapped by FSDP\n\n    :param model: pytorch model to check\n    :return: True if module is wrapped, False otherwise\n    \"\"\"\n    if not FullyShardedDataParallel:\n        return False\n\n    return isinstance(model, FullyShardedDataParallel)\n</code></pre>"},{"location":"reference/llmcompressor/utils/fsdp/helpers/#llmcompressor.utils.fsdp.helpers.maybe_get_wrapped","title":"<code>maybe_get_wrapped(model)</code>","text":"<p>Given a model that may or may not have a distributed wrapper, return the underlying wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>input model to get wrapped model from</p> required <p>Returns:</p> Type Description <code>Module</code> <p>wrapped model</p> Source code in <code>src/llmcompressor/utils/fsdp/helpers.py</code> <pre><code>def maybe_get_wrapped(model: Module) -&gt; Module:\n    \"\"\"\n    Given a model that may or may not have a distributed wrapper, return the underlying\n    wrapped model.\n\n    :param model: input model to get wrapped model from\n    :returns: wrapped model\n    \"\"\"\n    if is_fsdp_model(model=model):\n        return model._fsdp_wrapped_module\n    return model\n</code></pre>"},{"location":"reference/llmcompressor/utils/fsdp/helpers/#llmcompressor.utils.fsdp.helpers.set_wrapped_model","title":"<code>set_wrapped_model(state, wrapped_model)</code>","text":"<p>Given a state with a model that may or may not have a distributed wrapper, set the underlying wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>state to update model of</p> required <code>updated_wrapped</code> <p>model to inject into input_model</p> required Source code in <code>src/llmcompressor/utils/fsdp/helpers.py</code> <pre><code>def set_wrapped_model(state: State, wrapped_model: Module):\n    \"\"\"\n    Given a state with a model that may or may not have a distributed wrapper, set\n    the underlying wrapped model.\n\n    :param state: state to update model of\n    :param updated_wrapped: model to inject into input_model\n    \"\"\"\n    if is_fsdp_model(state.model):\n        state.model._fsdp_wrapped_module = wrapped_model\n    else:\n        state.model = wrapped_model\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/","title":"llmcompressor.utils.pytorch","text":""},{"location":"reference/llmcompressor/utils/pytorch/#llmcompressor.utils.pytorch.get_matching_layer","title":"<code>get_matching_layer(target, name_to_match, module)</code>","text":"<p>Given a target regex, find the layer name in the module that most closely matches the name_to_match string. This is used to matches submodules in the same layer, for instance matching \"re.*k_proj\" to \"model.decoder.layer.0.q_proj\" to find the k_proj that exists in layer 0.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>regex to search for</p> required <code>name_to_match</code> <code>str</code> <p>full layer name to match to, should exist in module</p> required <code>module</code> <code>Module</code> <p>module to search for target in</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[str, Module]]</code> <p>Tuple containing the layer name and module that fits the target regex and best matches name_to_match, or None if no match can be found</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def get_matching_layer(\n    target: str, name_to_match: str, module: Module\n) -&gt; Optional[Tuple[str, Module]]:\n    \"\"\"\n    Given a target regex, find the layer name in the module that most closely matches\n    the name_to_match string. This is used to matches submodules in the same layer, for\n    instance matching \"re.*k_proj\" to \"model.decoder.layer.0.q_proj\" to find the k_proj\n    that exists in layer 0.\n\n    :param target: regex to search for\n    :param name_to_match: full layer name to match to, should exist in module\n    :param module: module to search for target in\n    :return: Tuple containing the layer name and module that fits the target regex and\n    best matches name_to_match, or None if no match can be found\n    \"\"\"\n    potential_matches = get_layers(target, module)\n    largest_substring = 0\n    match = None\n    for name, module in potential_matches.items():\n        seq_matcher = difflib.SequenceMatcher(None, name, name_to_match)\n        _, _, match_length = seq_matcher.find_longest_match(\n            0, len(name), 0, len(name_to_match)\n        )\n        if match_length &gt; largest_substring:\n            match = (name, module)\n            largest_substring = match_length\n\n    return match\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/#llmcompressor.utils.pytorch.get_no_split_params","title":"<code>get_no_split_params(model)</code>","text":"<p>Get list of module classes that shouldn't be split when sharding. For Hugging Face Transformer models, this is the decoder layer type. For other types of models, this just returns all module names.</p> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>list of class names that shouldn't be split</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def get_no_split_params(model: PreTrainedModel) -&gt; Union[str, List[str]]:\n    \"\"\"\n    Get list of module classes that shouldn't be split when sharding. For\n    Hugging Face Transformer models, this is the decoder layer type. For other\n    types of models, this just returns all module names.\n\n    :return: list of class names that shouldn't be split\n    \"\"\"\n    # importing here to avoid circular import\n    from llmcompressor.utils.fsdp.helpers import maybe_get_wrapped\n\n    model = maybe_get_wrapped(model)\n    no_split_modules = model._get_no_split_modules(\"auto\")\n    if len(no_split_modules) &lt;= 0:\n        return ALL_TARGET\n\n    return no_split_modules\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/#llmcompressor.utils.pytorch.get_parent_by_name","title":"<code>get_parent_by_name(layer_name, model)</code>","text":"<p>Get the parent layer of a layer by name.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to find the parent of.</p> required <code>model</code> <code>Module</code> <p>Model to search for the parent layer.</p> required <p>Returns:</p> Type Description <code>Tuple[str, Module]</code> <p>Tuple containing the name of the parent layer and the parent layer itself.</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def get_parent_by_name(layer_name: str, model: Module) -&gt; Tuple[str, Module]:\n    \"\"\"\n    Get the parent layer of a layer by name.\n    :param layer_name: Name of the layer to find the parent of.\n    :param model: Model to search for the parent layer.\n    :return: Tuple containing the name of the parent layer\n        and the parent layer itself.\n    \"\"\"\n    if not any(layer_name == name for name, _ in model.named_modules()):\n        raise ValueError(f\"Layer '{layer_name}' not found in model\")\n\n    parent_name_parts = layer_name.split(\".\")[:-1]\n    if not parent_name_parts:\n        return \"\", model\n\n    parent_name = \".\".join(parent_name_parts)\n    return get_layer(parent_name, model)\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/#llmcompressor.utils.pytorch.qat_active","title":"<code>qat_active(module)</code>","text":"<p>Determines if any layers in the model have quantization enabled by checking for weight_fake_quant attributes</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>PyTorch model to check for quantization</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if quantization is active anywhere in the model, False otherwise</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def qat_active(module: Module) -&gt; bool:\n    \"\"\"\n    Determines if any layers in the model have quantization enabled by checking for\n    weight_fake_quant attributes\n\n    :param module: PyTorch model to check for quantization\n    :return: True if quantization is active anywhere in the model, False otherwise\n    \"\"\"\n    for _, layer in module.named_modules():\n        if isinstance(layer, torch.quantization.FakeQuantize):\n            return True\n        if is_module_quantized(layer):\n            return True\n\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/module/","title":"llmcompressor.utils.pytorch.module","text":"<p>Utility / helper functions</p>"},{"location":"reference/llmcompressor/utils/pytorch/module/#llmcompressor.utils.pytorch.module.get_matching_layer","title":"<code>get_matching_layer(target, name_to_match, module)</code>","text":"<p>Given a target regex, find the layer name in the module that most closely matches the name_to_match string. This is used to matches submodules in the same layer, for instance matching \"re.*k_proj\" to \"model.decoder.layer.0.q_proj\" to find the k_proj that exists in layer 0.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>regex to search for</p> required <code>name_to_match</code> <code>str</code> <p>full layer name to match to, should exist in module</p> required <code>module</code> <code>Module</code> <p>module to search for target in</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[str, Module]]</code> <p>Tuple containing the layer name and module that fits the target regex and best matches name_to_match, or None if no match can be found</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def get_matching_layer(\n    target: str, name_to_match: str, module: Module\n) -&gt; Optional[Tuple[str, Module]]:\n    \"\"\"\n    Given a target regex, find the layer name in the module that most closely matches\n    the name_to_match string. This is used to matches submodules in the same layer, for\n    instance matching \"re.*k_proj\" to \"model.decoder.layer.0.q_proj\" to find the k_proj\n    that exists in layer 0.\n\n    :param target: regex to search for\n    :param name_to_match: full layer name to match to, should exist in module\n    :param module: module to search for target in\n    :return: Tuple containing the layer name and module that fits the target regex and\n    best matches name_to_match, or None if no match can be found\n    \"\"\"\n    potential_matches = get_layers(target, module)\n    largest_substring = 0\n    match = None\n    for name, module in potential_matches.items():\n        seq_matcher = difflib.SequenceMatcher(None, name, name_to_match)\n        _, _, match_length = seq_matcher.find_longest_match(\n            0, len(name), 0, len(name_to_match)\n        )\n        if match_length &gt; largest_substring:\n            match = (name, module)\n            largest_substring = match_length\n\n    return match\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/module/#llmcompressor.utils.pytorch.module.get_no_split_params","title":"<code>get_no_split_params(model)</code>","text":"<p>Get list of module classes that shouldn't be split when sharding. For Hugging Face Transformer models, this is the decoder layer type. For other types of models, this just returns all module names.</p> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>list of class names that shouldn't be split</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def get_no_split_params(model: PreTrainedModel) -&gt; Union[str, List[str]]:\n    \"\"\"\n    Get list of module classes that shouldn't be split when sharding. For\n    Hugging Face Transformer models, this is the decoder layer type. For other\n    types of models, this just returns all module names.\n\n    :return: list of class names that shouldn't be split\n    \"\"\"\n    # importing here to avoid circular import\n    from llmcompressor.utils.fsdp.helpers import maybe_get_wrapped\n\n    model = maybe_get_wrapped(model)\n    no_split_modules = model._get_no_split_modules(\"auto\")\n    if len(no_split_modules) &lt;= 0:\n        return ALL_TARGET\n\n    return no_split_modules\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/module/#llmcompressor.utils.pytorch.module.get_parent_by_name","title":"<code>get_parent_by_name(layer_name, model)</code>","text":"<p>Get the parent layer of a layer by name.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to find the parent of.</p> required <code>model</code> <code>Module</code> <p>Model to search for the parent layer.</p> required <p>Returns:</p> Type Description <code>Tuple[str, Module]</code> <p>Tuple containing the name of the parent layer and the parent layer itself.</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def get_parent_by_name(layer_name: str, model: Module) -&gt; Tuple[str, Module]:\n    \"\"\"\n    Get the parent layer of a layer by name.\n    :param layer_name: Name of the layer to find the parent of.\n    :param model: Model to search for the parent layer.\n    :return: Tuple containing the name of the parent layer\n        and the parent layer itself.\n    \"\"\"\n    if not any(layer_name == name for name, _ in model.named_modules()):\n        raise ValueError(f\"Layer '{layer_name}' not found in model\")\n\n    parent_name_parts = layer_name.split(\".\")[:-1]\n    if not parent_name_parts:\n        return \"\", model\n\n    parent_name = \".\".join(parent_name_parts)\n    return get_layer(parent_name, model)\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/module/#llmcompressor.utils.pytorch.module.qat_active","title":"<code>qat_active(module)</code>","text":"<p>Determines if any layers in the model have quantization enabled by checking for weight_fake_quant attributes</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>PyTorch model to check for quantization</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if quantization is active anywhere in the model, False otherwise</p> Source code in <code>src/llmcompressor/utils/pytorch/module.py</code> <pre><code>def qat_active(module: Module) -&gt; bool:\n    \"\"\"\n    Determines if any layers in the model have quantization enabled by checking for\n    weight_fake_quant attributes\n\n    :param module: PyTorch model to check for quantization\n    :return: True if quantization is active anywhere in the model, False otherwise\n    \"\"\"\n    for _, layer in module.named_modules():\n        if isinstance(layer, torch.quantization.FakeQuantize):\n            return True\n        if is_module_quantized(layer):\n            return True\n\n    return False\n</code></pre>"},{"location":"reference/llmcompressor/utils/pytorch/utils/","title":"llmcompressor.utils.pytorch.utils","text":""}]}