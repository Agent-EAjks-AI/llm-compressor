{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "```\n",
    "git clone git@github.com:vllm-project/llm-compressor.git\\\n",
    "cd llm-compressor\\\n",
    "micromamba create -n weight-analyzer python=3.11\\\n",
    "pip install -e .\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988471c2e2aa4c89b73b7069b2d43f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-22T13:50:36.491824-0400 | save_pretrained_wrapper | INFO - Inferring a sparsity configuration requires a global sparsity calculation. This can be costly for large models. To skip the calculation of compression statistics set skip_compression_stats=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating model sparsity: 100%|██████████| 291/291 [00:00<00:00, 1173.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llmcompressor.transformers import SparseAutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = SparseAutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto',\n",
    "    cache_dir=\"/nm/drive0/shashata/weight-analysis\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"/nm/drive0/shashata/weight-analysis/dense_llama_3_8b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading using safetensors for individual weight files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "safetensors_path = \"/nm/drive0/shashata//weight-analysis/dense_llama_3_8b/model-00001-of-00004.safetensors\"\n",
    "with safe_open(safetensors_path, framework='pt', device='cpu') as f:\n",
    "    # tensors = safetensors.load(f)\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "\n",
    "for k, v in tensors.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0087, -0.0151, -0.0090,  ...,  0.0079, -0.0039,  0.0134],\n",
      "        [ 0.0204, -0.0107, -0.0057,  ...,  0.0010,  0.0172,  0.0011],\n",
      "        [ 0.0082, -0.0075, -0.0023,  ..., -0.0018,  0.0025, -0.0165],\n",
      "        ...,\n",
      "        [ 0.0085, -0.0208,  0.0217,  ..., -0.0199,  0.0081, -0.0129],\n",
      "        [-0.0135, -0.0059, -0.0110,  ...,  0.0093,  0.0015, -0.0131],\n",
      "        [-0.0029,  0.0069,  0.0085,  ..., -0.0082, -0.0051, -0.0120]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# print(tensors.keys())\n",
    "print(tensors['model.layers.0.mlp.down_proj.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
